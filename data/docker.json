{
  "topic": "Docker",
  "questions": [
    {
      "id": 1,
      "question": "What is Docker and why is it used?",
      "answer": "Docker is a containerization platform that packages applications with their dependencies.\n\nBenefits:\n• Consistency: Same environment everywhere\n• Isolation: Apps don't interfere with each other\n• Portability: Run anywhere Docker runs\n• Efficiency: Lightweight compared to VMs\n• Fast deployment: Start containers in seconds\n• Scalability: Easy to scale horizontally\n• Version control: Image versioning",
      "explanation": "Docker solves 'works on my machine' problem. Containers include everything an app needs to run, ensuring consistency across development, testing, and production.",
      "difficulty": "Easy"
    },
    {
      "id": 2,
      "question": "What is the difference between Docker Image and Container?",
      "answer": "Docker Image:\n• Blueprint/template for containers\n• Read-only filesystem layers\n• Created from Dockerfile\n• Can be stored in registry\n• Immutable - doesn't change\n\nDocker Container:\n• Running instance of an image\n• Has writable layer on top\n• Has state (running, stopped, etc.)\n• Can be started, stopped, deleted\n• Isolated process with own network, storage",
      "explanation": "Image is like a class, container is like an object. You create multiple containers from one image. Images are immutable; containers are mutable.",
      "difficulty": "Easy",
      "code": "# List images\ndocker images\n\n# Pull image from registry\ndocker pull nginx:latest\n\n# Create container from image\ndocker create nginx:latest\n\n# Run container (create + start)\ndocker run nginx:latest\n\n# List running containers\ndocker ps\n\n# List all containers (including stopped)\ndocker ps -a\n\n# Multiple containers from same image\ndocker run -d --name web1 nginx:latest\ndocker run -d --name web2 nginx:latest\ndocker run -d --name web3 nginx:latest\n\n# Container lifecycle\ndocker start web1\ndocker stop web1\ndocker restart web1\ndocker rm web1\n\n# Image cleanup\ndocker rmi nginx:latest"
    },
    {
      "id": 3,
      "question": "What is a Dockerfile and what are common instructions?",
      "answer": "Dockerfile is a text file with instructions to build a Docker image.\n\nCommon instructions:\n• FROM: Base image\n• WORKDIR: Set working directory\n• COPY/ADD: Copy files into image\n• RUN: Execute commands during build\n• ENV: Set environment variables\n• EXPOSE: Document port\n• CMD: Default command to run\n• ENTRYPOINT: Configure container as executable\n• ARG: Build-time variables",
      "explanation": "Each instruction creates a layer. Order matters for caching - put stable instructions first. Use multi-stage builds for smaller images.",
      "difficulty": "Medium",
      "code": "# Simple Dockerfile\nFROM node:18-alpine\n\n# Set working directory\nWORKDIR /app\n\n# Copy package files first (better caching)\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm install\n\n# Copy source code\nCOPY . .\n\n# Build application\nRUN npm run build\n\n# Expose port\nEXPOSE 3000\n\n# Default command\nCMD [\"npm\", \"start\"]\n\n\n# Java Spring Boot Dockerfile\nFROM eclipse-temurin:17-jdk-alpine AS build\nWORKDIR /app\nCOPY mvnw .\nCOPY .mvn .mvn\nCOPY pom.xml .\nRUN ./mvnw dependency:go-offline\nCOPY src src\nRUN ./mvnw package -DskipTests\n\nFROM eclipse-temurin:17-jre-alpine\nWORKDIR /app\nCOPY --from=build /app/target/*.jar app.jar\nEXPOSE 8080\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n\n\n# Build image\ndocker build -t myapp:1.0 .\n\n# Build with build args\ndocker build --build-arg VERSION=1.0 -t myapp:1.0 ."
    },
    {
      "id": 4,
      "question": "What is the difference between CMD and ENTRYPOINT?",
      "answer": "CMD:\n• Default command/arguments\n• Can be overridden at runtime\n• Only last CMD takes effect\n• Use for default arguments\n\nENTRYPOINT:\n• Configures container as executable\n• Harder to override (needs --entrypoint)\n• Runs before CMD\n• Use for main executable\n\nCombined: ENTRYPOINT is executable, CMD provides default arguments.",
      "explanation": "Use ENTRYPOINT for the main command that should always run. Use CMD for defaults that users might want to change.",
      "difficulty": "Medium",
      "code": "# CMD only - can be overridden\nFROM ubuntu\nCMD [\"echo\", \"Hello\"]\n\n# Run default\ndocker run myimage\n# Output: Hello\n\n# Override CMD\ndocker run myimage echo \"Goodbye\"\n# Output: Goodbye\n\n\n# ENTRYPOINT only - main executable\nFROM ubuntu\nENTRYPOINT [\"echo\"]\n\n# Run with argument\ndocker run myimage \"Hello\"\n# Output: Hello\n\n\n# Combined - best practice\nFROM ubuntu\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\nCMD [\"--server.port=8080\"]\n\n# Default run\ndocker run myimage\n# Runs: java -jar app.jar --server.port=8080\n\n# Override only args\ndocker run myimage --server.port=9090\n# Runs: java -jar app.jar --server.port=9090\n\n# Override entrypoint (rarely needed)\ndocker run --entrypoint /bin/bash myimage\n\n\n# Shell form vs Exec form\nCMD echo \"Hello\"              # Shell form - runs in /bin/sh -c\nCMD [\"echo\", \"Hello\"]         # Exec form - runs directly (preferred)"
    },
    {
      "id": 5,
      "question": "What is Docker Compose and when do you use it?",
      "answer": "Docker Compose defines and runs multi-container applications.\n\nFeatures:\n• Define services in YAML file\n• Single command to start all services\n• Service dependencies (depends_on)\n• Shared networks and volumes\n• Environment variables\n• Service scaling\n\nUse cases:\n• Development environments\n• Testing with dependencies\n• Multi-service applications\n• CI/CD pipelines",
      "explanation": "Compose simplifies running multiple containers that work together. Define once, run with one command. Great for local development with databases, caches, etc.",
      "difficulty": "Medium",
      "code": "# docker-compose.yml\nversion: '3.8'\n\nservices:\n  # Backend API\n  api:\n    build: ./backend\n    ports:\n      - \"8080:8080\"\n    environment:\n      - DATABASE_URL=jdbc:mysql://db:3306/mydb\n      - REDIS_HOST=redis\n    depends_on:\n      - db\n      - redis\n    networks:\n      - app-network\n\n  # Frontend\n  frontend:\n    build: ./frontend\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - api\n    networks:\n      - app-network\n\n  # Database\n  db:\n    image: mysql:8.0\n    environment:\n      MYSQL_ROOT_PASSWORD: rootpass\n      MYSQL_DATABASE: mydb\n    volumes:\n      - db-data:/var/lib/mysql\n    networks:\n      - app-network\n\n  # Redis cache\n  redis:\n    image: redis:alpine\n    networks:\n      - app-network\n\nvolumes:\n  db-data:\n\nnetworks:\n  app-network:\n\n\n# Commands\ndocker-compose up -d              # Start all services\ndocker-compose down               # Stop and remove\ndocker-compose logs -f api        # Follow logs\ndocker-compose ps                 # List services\ndocker-compose exec api bash      # Shell into container\ndocker-compose build              # Rebuild images\ndocker-compose up -d --scale api=3  # Scale service"
    },
    {
      "id": 6,
      "question": "What are Docker volumes and why are they important?",
      "answer": "Volumes persist data beyond container lifecycle.\n\nTypes:\n• Named volumes: Managed by Docker\n• Bind mounts: Host path mounted\n• tmpfs: In-memory (Linux)\n\nBenefits:\n• Data persistence after container removal\n• Share data between containers\n• Better performance than bind mounts\n• Backup and migration easier\n• Decouple data from container",
      "explanation": "Containers are ephemeral - data is lost when removed. Volumes store data outside the container's writable layer. Essential for databases.",
      "difficulty": "Medium",
      "code": "# Named volume\ndocker volume create my-data\ndocker run -v my-data:/app/data myimage\n\n# Bind mount (host directory)\ndocker run -v /host/path:/container/path myimage\ndocker run -v $(pwd)/data:/app/data myimage\n\n# Read-only mount\ndocker run -v /config:/app/config:ro myimage\n\n# List volumes\ndocker volume ls\n\n# Inspect volume\ndocker volume inspect my-data\n\n# Remove volume\ndocker volume rm my-data\n\n# Remove unused volumes\ndocker volume prune\n\n\n# Docker Compose volumes\nversion: '3.8'\nservices:\n  db:\n    image: postgres:14\n    volumes:\n      # Named volume for data\n      - postgres-data:/var/lib/postgresql/data\n      # Bind mount for init scripts\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro\n\n  api:\n    build: .\n    volumes:\n      # Bind mount for development (hot reload)\n      - ./src:/app/src\n      # Anonymous volume to preserve node_modules\n      - /app/node_modules\n\nvolumes:\n  postgres-data:  # Named volume definition"
    },
    {
      "id": 7,
      "question": "What is Docker networking and what are the network types?",
      "answer": "Docker networking connects containers and external networks.\n\nNetwork types:\n• bridge: Default, isolated network (container-to-container)\n• host: Use host's network directly\n• none: No networking\n• overlay: Multi-host networking (Swarm)\n• macvlan: Assign MAC address\n\nContainer DNS: Containers resolve each other by name within same network.",
      "explanation": "Containers on same network can communicate by container name. Create custom networks to isolate groups of containers.",
      "difficulty": "Medium",
      "code": "# List networks\ndocker network ls\n\n# Create custom network\ndocker network create my-network\n\n# Run container on network\ndocker run -d --name api --network my-network myapi\ndocker run -d --name db --network my-network mysql\n\n# api can connect to db using hostname 'db'\n# e.g., jdbc:mysql://db:3306/mydb\n\n# Inspect network\ndocker network inspect my-network\n\n# Connect running container to network\ndocker network connect my-network existing-container\n\n# Disconnect from network\ndocker network disconnect my-network container-name\n\n# Remove network\ndocker network rm my-network\n\n\n# Port mapping\ndocker run -p 8080:80 nginx       # Host 8080 -> Container 80\ndocker run -p 127.0.0.1:8080:80 nginx  # Only localhost\ndocker run -P nginx               # Random host ports\n\n\n# Docker Compose networking\nversion: '3.8'\nservices:\n  api:\n    networks:\n      - frontend\n      - backend\n  \n  db:\n    networks:\n      - backend  # Only accessible from backend network\n  \n  nginx:\n    networks:\n      - frontend\n\nnetworks:\n  frontend:\n  backend:"
    },
    {
      "id": 8,
      "question": "What is multi-stage build in Docker?",
      "answer": "Multi-stage builds use multiple FROM statements to create smaller, more secure images.\n\nBenefits:\n• Smaller final image\n• No build tools in production image\n• Better security (less attack surface)\n• Separate build and runtime\n• Better caching\n\nUse cases:\n• Compiled languages (Java, Go, Rust)\n• Frontend builds (npm build)\n• Any app with build dependencies",
      "explanation": "Build stage has all tools. Production stage only has runtime. Copy artifacts from build stage to final stage.",
      "difficulty": "Medium",
      "code": "# Multi-stage Spring Boot build\n\n# Stage 1: Build\nFROM maven:3.9-eclipse-temurin-17 AS build\nWORKDIR /app\n\n# Cache dependencies\nCOPY pom.xml .\nRUN mvn dependency:go-offline\n\n# Build application\nCOPY src ./src\nRUN mvn package -DskipTests\n\n\n# Stage 2: Production\nFROM eclipse-temurin:17-jre-alpine\nWORKDIR /app\n\n# Copy only the JAR from build stage\nCOPY --from=build /app/target/*.jar app.jar\n\n# Create non-root user\nRUN addgroup -S app && adduser -S app -G app\nUSER app\n\nEXPOSE 8080\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n\n\n# Multi-stage Node.js build\n\n# Stage 1: Build\nFROM node:18-alpine AS build\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n\n# Stage 2: Production\nFROM nginx:alpine\nCOPY --from=build /app/dist /usr/share/nginx/html\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n\n\n# Image size comparison:\n# Without multi-stage: ~800MB (includes Maven, JDK)\n# With multi-stage: ~200MB (only JRE)"
    },
    {
      "id": 9,
      "question": "How do you optimize Docker images?",
      "answer": "Optimization techniques:\n\n1. Use small base images (alpine, slim)\n2. Multi-stage builds\n3. Minimize layers (combine RUN commands)\n4. Order commands for better caching\n5. Use .dockerignore\n6. Don't install unnecessary packages\n7. Clean up in same layer\n8. Use specific image tags\n9. Avoid running as root\n10. Use COPY over ADD when possible",
      "explanation": "Smaller images = faster pulls, less storage, smaller attack surface. Each instruction creates a layer - optimize for caching and size.",
      "difficulty": "Medium",
      "code": "# BAD: Large image, poor caching\nFROM node:18\nCOPY . .\nRUN npm install\nRUN npm run build\n\n# GOOD: Optimized\nFROM node:18-alpine\n\nWORKDIR /app\n\n# Copy dependency files first (cached if unchanged)\nCOPY package*.json ./\nRUN npm ci --only=production\n\n# Copy source and build\nCOPY . .\nRUN npm run build\n\n\n# .dockerignore file\nnode_modules\n.git\n.gitignore\n*.md\nDockerfile\n.dockerignore\n.env\n*.log\n.DS_Store\ntarget/\ndist/\n\n\n# Combine RUN commands\n# BAD: Multiple layers\nRUN apt-get update\nRUN apt-get install -y curl\nRUN apt-get clean\n\n# GOOD: Single layer with cleanup\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends curl && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n\n# Non-root user\nRUN addgroup -S app && adduser -S app -G app\nUSER app\n\n\n# Specific tags (not latest)\n# BAD\nFROM node:latest\n\n# GOOD\nFROM node:18.19.0-alpine3.19\n\n\n# Check image size\ndocker images myapp\n\n# Analyze layers\ndocker history myapp"
    },
    {
      "id": 10,
      "question": "What is Docker Registry and how do you push/pull images?",
      "answer": "Docker Registry stores and distributes Docker images.\n\nPublic registries:\n• Docker Hub (default)\n• GitHub Container Registry\n• AWS ECR, Azure ACR, Google GCR\n\nOperations:\n• docker login: Authenticate\n• docker push: Upload image\n• docker pull: Download image\n• docker tag: Tag image for registry\n\nImage naming: registry/username/image:tag",
      "explanation": "Registries are like GitHub for Docker images. Push to share, pull to use. Use private registries for proprietary code.",
      "difficulty": "Easy",
      "code": "# Login to Docker Hub\ndocker login\n\n# Login to other registry\ndocker login ghcr.io\ndocker login myregistry.azurecr.io\n\n# Tag image for registry\ndocker tag myapp:latest myusername/myapp:1.0\ndocker tag myapp:latest ghcr.io/myusername/myapp:1.0\n\n# Push to registry\ndocker push myusername/myapp:1.0\n\n# Pull from registry\ndocker pull myusername/myapp:1.0\ndocker pull nginx:1.24-alpine\n\n# List tags (Docker Hub)\ncurl -s https://registry.hub.docker.com/v2/repositories/library/nginx/tags | jq\n\n\n# Private registry\n# Run local registry\ndocker run -d -p 5000:5000 --name registry registry:2\n\n# Push to local registry\ndocker tag myapp localhost:5000/myapp:1.0\ndocker push localhost:5000/myapp:1.0\n\n\n# CI/CD pipeline example (GitHub Actions)\n# - name: Login to Docker Hub\n#   uses: docker/login-action@v3\n#   with:\n#     username: ${{ secrets.DOCKER_USERNAME }}\n#     password: ${{ secrets.DOCKER_PASSWORD }}\n# \n# - name: Build and push\n#   uses: docker/build-push-action@v5\n#   with:\n#     push: true\n#     tags: myusername/myapp:${{ github.sha }}"
    },
    {
      "id": 11,
      "question": "What are Docker health checks?",
      "answer": "Health checks verify container is working correctly.\n\nConfiguration:\n• HEALTHCHECK instruction in Dockerfile\n• healthcheck in docker-compose\n• --health-cmd at runtime\n\nOptions:\n• --interval: Time between checks\n• --timeout: Max time for check\n• --start-period: Init time before checks\n• --retries: Failures before unhealthy\n\nStates: starting, healthy, unhealthy",
      "explanation": "Health checks help orchestrators know if container is truly ready. Essential for production - enables automatic restarts and load balancing.",
      "difficulty": "Medium",
      "code": "# Dockerfile health check\nFROM nginx:alpine\n\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost/ || exit 1\n\n\n# Spring Boot health check\nFROM eclipse-temurin:17-jre-alpine\nCOPY app.jar /app.jar\n\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n  CMD wget --no-verbose --tries=1 --spider http://localhost:8080/actuator/health || exit 1\n\nENTRYPOINT [\"java\", \"-jar\", \"/app.jar\"]\n\n\n# Docker Compose health check\nversion: '3.8'\nservices:\n  api:\n    build: .\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n\n  db:\n    image: mysql:8\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  # Wait for healthy dependency\n  app:\n    depends_on:\n      db:\n        condition: service_healthy\n\n\n# Check health status\ndocker ps  # Shows health status\ndocker inspect --format='{{.State.Health.Status}}' container_name"
    },
    {
      "id": 12,
      "question": "What is the difference between COPY and ADD in Dockerfile?",
      "answer": "COPY:\n• Simple file/directory copy\n• Local files only\n• Preferred for most cases\n• Transparent behavior\n\nADD:\n• All COPY features plus:\n• Auto-extracts tar archives\n• Can fetch remote URLs\n• More 'magic' behavior\n\nBest practice: Use COPY unless you need ADD's features.",
      "explanation": "COPY is simpler and more predictable. ADD has extra features but can be confusing. Use ADD only for tar extraction.",
      "difficulty": "Easy",
      "code": "# COPY - preferred for most cases\nCOPY package.json /app/\nCOPY src/ /app/src/\nCOPY . /app/\n\n# Copy with different name\nCOPY config.json /app/settings.json\n\n# Copy multiple files\nCOPY package.json package-lock.json /app/\n\n# Set ownership\nCOPY --chown=user:group files/ /app/\n\n\n# ADD - special features\n\n# Auto-extract tar files\nADD app.tar.gz /app/\n# Extracts to /app/\n\n# Fetch remote URL (not recommended)\nADD https://example.com/file.txt /app/\n# Better: Use curl in RUN\n\n\n# Why COPY is preferred:\n\n# COPY is explicit\nCOPY myarchive.tar.gz /app/\n# Results in: /app/myarchive.tar.gz\n\n# ADD has 'magic'\nADD myarchive.tar.gz /app/\n# Results in: extracted contents in /app/\n\n\n# Best practice for remote files:\n# BAD\nADD https://example.com/big-file.tar.gz /tmp/\nRUN tar -xzf /tmp/big-file.tar.gz\n\n# GOOD - can be cached and cleaned in same layer\nRUN curl -L https://example.com/big-file.tar.gz | tar -xz -C /app/"
    },
    {
      "id": 13,
      "question": "How do you manage secrets in Docker?",
      "answer": "Secrets management options:\n\n1. Environment variables (less secure)\n2. Docker secrets (Swarm)\n3. Docker Compose secrets\n4. Build-time secrets (BuildKit)\n5. External secret managers (Vault, AWS SM)\n\nNever:\n• Hardcode secrets in Dockerfile\n• Commit secrets to image\n• Use ARG for runtime secrets",
      "explanation": "Secrets require special handling. Don't bake them into images. Use proper secret management for production.",
      "difficulty": "Medium",
      "code": "# BAD - Never do this!\nENV DATABASE_PASSWORD=supersecret\nRUN echo \"password=supersecret\" > config.txt\n\n\n# Environment variables (simple but visible in inspection)\ndocker run -e DATABASE_PASSWORD=secret myapp\n\n# From file\ndocker run --env-file .env myapp\n\n\n# Docker Compose secrets (file-based)\nversion: '3.8'\nservices:\n  app:\n    image: myapp\n    secrets:\n      - db_password\n    environment:\n      - DB_PASSWORD_FILE=/run/secrets/db_password\n\nsecrets:\n  db_password:\n    file: ./secrets/db_password.txt\n\n\n# BuildKit secrets (build-time, not stored in image)\n# syntax=docker/dockerfile:1\nFROM node:18-alpine\n\n# Secret available only during this RUN\nRUN --mount=type=secret,id=npmrc,target=/root/.npmrc \\\n    npm install\n\n# Build with:\nDOCKER_BUILDKIT=1 docker build --secret id=npmrc,src=.npmrc -t myapp .\n\n\n# Production: Use external secret manager\n# AWS Secrets Manager, HashiCorp Vault, Azure Key Vault\n\n# Spring Boot with AWS Secrets Manager\nspring:\n  cloud:\n    aws:\n      secretsmanager:\n        enabled: true\n        name: myapp/prod\n\n\n# .dockerignore - exclude secret files\n.env\n*.pem\n*.key\nsecrets/\n.npmrc"
    },
    {
      "id": 14,
      "question": "What is Docker Swarm vs Kubernetes?",
      "answer": "Docker Swarm:\n• Built into Docker\n• Simpler to set up\n• Uses Docker Compose files\n• Good for smaller deployments\n• Fewer features\n\nKubernetes (K8s):\n• Industry standard\n• More complex\n• More features (autoscaling, RBAC)\n• Larger ecosystem\n• Better for large-scale\n\nBoth: Container orchestration, scaling, service discovery, load balancing",
      "explanation": "Swarm is simpler, K8s is more powerful. Start with Swarm for learning/small projects. Use K8s for production-scale systems.",
      "difficulty": "Medium",
      "code": "# Docker Swarm\n\n# Initialize swarm\ndocker swarm init\n\n# Join workers\ndocker swarm join --token <token> <manager-ip>:2377\n\n# Deploy stack\ndocker stack deploy -c docker-compose.yml myapp\n\n# Scale service\ndocker service scale myapp_api=5\n\n# List services\ndocker service ls\n\n# View logs\ndocker service logs myapp_api\n\n\n# Docker Compose for Swarm\nversion: '3.8'\nservices:\n  api:\n    image: myapp:latest\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1\n        delay: 10s\n      restart_policy:\n        condition: on-failure\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n    ports:\n      - \"8080:8080\"\n\n\n# Kubernetes equivalent (deployment.yaml)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: myapp:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n\n# kubectl commands\nkubectl apply -f deployment.yaml\nkubectl scale deployment api --replicas=5\nkubectl get pods\nkubectl logs -f pod-name"
    },
    {
      "id": 15,
      "question": "How do you debug Docker containers?",
      "answer": "Debugging techniques:\n\n1. View logs: docker logs\n2. Exec into container: docker exec -it bash\n3. Inspect container: docker inspect\n4. View processes: docker top\n5. View stats: docker stats\n6. Check events: docker events\n7. View filesystem changes: docker diff\n8. Debug build: docker build --progress=plain",
      "explanation": "Start with logs, then exec into container if needed. Use inspect for configuration issues. Stats for performance problems.",
      "difficulty": "Medium",
      "code": "# View logs\ndocker logs container_name\ndocker logs -f container_name          # Follow\ndocker logs --tail 100 container_name   # Last 100 lines\ndocker logs --since 10m container_name  # Last 10 minutes\n\n# Execute command in container\ndocker exec -it container_name bash\ndocker exec -it container_name sh       # Alpine\ndocker exec container_name cat /app/config.json\n\n# Inspect container\ndocker inspect container_name\ndocker inspect --format='{{.State.Status}}' container_name\ndocker inspect --format='{{.NetworkSettings.IPAddress}}' container_name\n\n# View running processes\ndocker top container_name\n\n# Resource usage\ndocker stats\ndocker stats container_name\n\n# Filesystem changes\ndocker diff container_name\n\n# Events stream\ndocker events\n\n# Debug build process\ndocker build --progress=plain -t myapp .\ndocker build --no-cache -t myapp .     # Force rebuild\n\n# Run with shell for debugging\ndocker run -it myapp /bin/sh\n\n# Override entrypoint for debugging\ndocker run -it --entrypoint /bin/sh myapp\n\n# Check if container crashed\ndocker ps -a --filter \"status=exited\"\ndocker logs crashed_container\n\n# Copy files from container\ndocker cp container_name:/app/logs ./local_logs"
    },
    {
      "id": 16,
      "question": "What are Docker best practices for production?",
      "answer": "Production best practices:\n\n1. Use specific image tags, not 'latest'\n2. Don't run as root\n3. Use health checks\n4. Limit resources (CPU, memory)\n5. Use read-only filesystem when possible\n6. Scan images for vulnerabilities\n7. Use multi-stage builds\n8. Keep images small\n9. Log to stdout/stderr\n10. Handle signals properly (PID 1)",
      "explanation": "Production containers need security, reliability, and observability. Follow these practices for stable deployments.",
      "difficulty": "Hard",
      "code": "# Production-ready Dockerfile\nFROM node:18.19-alpine3.19 AS build\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nRUN npm run build\n\n# Production stage\nFROM node:18.19-alpine3.19\n\n# Non-root user\nRUN addgroup -S app && adduser -S app -G app\n\nWORKDIR /app\n\n# Copy built files\nCOPY --from=build --chown=app:app /app/dist ./dist\nCOPY --from=build --chown=app:app /app/node_modules ./node_modules\nCOPY --from=build --chown=app:app /app/package.json ./\n\n# Switch to non-root user\nUSER app\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s \\\n  CMD wget --no-verbose --tries=1 --spider http://localhost:3000/health || exit 1\n\nEXPOSE 3000\n\n# Use exec form for proper signal handling\nCMD [\"node\", \"dist/server.js\"]\n\n\n# docker-compose.prod.yml\nversion: '3.8'\nservices:\n  app:\n    image: myapp:1.2.3  # Specific version\n    user: \"1000:1000\"   # Non-root\n    read_only: true     # Read-only filesystem\n    tmpfs:\n      - /tmp            # Writable temp\n    security_opt:\n      - no-new-privileges:true\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n        reservations:\n          cpus: '0.25'\n          memory: 256M\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"-q\", \"--spider\", \"http://localhost:3000/health\"]\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n    restart: unless-stopped"
    },
    {
      "id": 17,
      "question": "What is the Docker build cache and how does it work?",
      "answer": "Build cache stores intermediate layers for faster rebuilds.\n\nHow it works:\n• Each instruction creates a layer\n• Docker checks if layer can be reused\n• If instruction or files changed → cache miss\n• All subsequent layers rebuild after cache miss\n\nOptimize for caching:\n• Order stable instructions first\n• Copy dependencies before source code\n• Use .dockerignore\n• Multi-stage builds cache better",
      "explanation": "Understanding cache is key to fast builds. Put frequently changing files last. Copy package.json before source code.",
      "difficulty": "Medium",
      "code": "# BAD - Cache busted every change\nFROM node:18-alpine\nWORKDIR /app\nCOPY . .                  # Any file change invalidates cache\nRUN npm install           # Always runs\nRUN npm run build\n\n\n# GOOD - Optimized for cache\nFROM node:18-alpine\nWORKDIR /app\n\n# Dependencies change less often\nCOPY package*.json ./\nRUN npm ci                # Cached if package.json unchanged\n\n# Source code changes often\nCOPY . .\nRUN npm run build         # Only this rebuilds on source change\n\n\n# Cache busting strategies\n\n# Force no cache\ndocker build --no-cache -t myapp .\n\n# Bust cache at specific point\nARG CACHEBUST=1\nRUN echo \"Cache bust: $CACHEBUST\" && \\\n    npm install\n\ndocker build --build-arg CACHEBUST=$(date +%s) -t myapp .\n\n\n# BuildKit cache mounts\n# syntax=docker/dockerfile:1\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\n\n# Cache npm between builds\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci\n\nCOPY . .\nRUN npm run build\n\n\n# View cache usage\ndocker builder prune       # Clear build cache\ndocker system df           # Disk usage\ndocker builder du          # BuildKit cache usage\n\n\n# Enable BuildKit for better caching\nDOCKER_BUILDKIT=1 docker build -t myapp ."
    },
    {
      "id": 18,
      "question": "How do you handle logging in Docker?",
      "answer": "Docker logging best practices:\n\n1. Log to stdout/stderr (not files)\n2. Use logging drivers\n3. Configure log rotation\n4. Centralize logs for production\n\nLogging drivers:\n• json-file (default)\n• syslog\n• journald\n• fluentd\n• awslogs, gcplogs\n• splunk",
      "explanation": "Containers should log to stdout. Docker captures these logs. Use log aggregation in production for analysis.",
      "difficulty": "Medium",
      "code": "# Application should log to stdout\n// Node.js\nconsole.log('Info message');\nconsole.error('Error message');\n\n// Java\nSystem.out.println(\"Info message\");\n// Or configure logback to use ConsoleAppender\n\n\n# View logs\ndocker logs container_name\ndocker logs -f container_name          # Follow\ndocker logs --tail 100 container_name   # Last 100\ndocker logs --since 2h container_name   # Last 2 hours\ndocker logs --timestamps container_name # With timestamps\n\n# Docker Compose logs\ndocker-compose logs\ndocker-compose logs -f api\n\n\n# Configure logging driver\ndocker run --log-driver json-file \\\n  --log-opt max-size=10m \\\n  --log-opt max-file=3 \\\n  myapp\n\n# docker-compose.yml\nversion: '3.8'\nservices:\n  app:\n    image: myapp\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"5\"\n        labels: \"app,env\"\n        env: \"NODE_ENV\"\n\n\n# Send to AWS CloudWatch\nservices:\n  app:\n    logging:\n      driver: awslogs\n      options:\n        awslogs-region: us-east-1\n        awslogs-group: myapp-logs\n        awslogs-stream-prefix: prod\n\n\n# Daemon-level logging config (/etc/docker/daemon.json)\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  }\n}\n\n\n# ELK Stack for log aggregation\n# Filebeat -> Logstash -> Elasticsearch -> Kibana"
    },
    {
      "id": 19,
      "question": "What is Docker context and how do you manage remote Docker hosts?",
      "answer": "Docker context manages connections to Docker daemons.\n\nUse cases:\n• Local Docker\n• Remote Docker hosts (SSH)\n• Docker in Docker\n• Different Docker environments\n\nCommands:\n• docker context create\n• docker context use\n• docker context ls",
      "explanation": "Contexts let you switch between Docker environments. Deploy to remote servers or manage multiple environments from one CLI.",
      "difficulty": "Medium",
      "code": "# List contexts\ndocker context ls\n\n# Show current context\ndocker context show\n\n# Create SSH context for remote host\ndocker context create remote-server \\\n  --docker \"host=ssh://user@remote-host.com\"\n\n# Create with specific socket\ndocker context create local-custom \\\n  --docker \"host=unix:///var/run/docker-custom.sock\"\n\n# Switch context\ndocker context use remote-server\n\n# Now all commands run on remote\ndocker ps  # Shows remote containers\n\n# Switch back\ndocker context use default\n\n# Run single command with context\ndocker --context remote-server ps\n\n# Remove context\ndocker context rm remote-server\n\n# Export/import context\ndocker context export remote-server > remote.dockercontext\ndocker context import remote-restored < remote.dockercontext\n\n\n# Using DOCKER_HOST environment variable\nDOCKER_HOST=ssh://user@remote-host.com docker ps\n\n# Deploy to remote from CI/CD\nDOCKER_HOST=ssh://deploy@prod-server.com \\\n  docker-compose -f docker-compose.prod.yml up -d\n\n\n# SSH config for easier access\n# ~/.ssh/config\n# Host prod-docker\n#   HostName prod-server.com\n#   User deploy\n#   IdentityFile ~/.ssh/deploy_key\n\ndocker context create prod --docker \"host=ssh://prod-docker\""
    },
    {
      "id": 20,
      "question": "What is the difference between docker run, docker start, and docker create?",
      "answer": "docker create:\n• Creates container from image\n• Container is in 'created' state\n• Doesn't start the container\n• Returns container ID\n\ndocker start:\n• Starts existing container\n• Container moves to 'running' state\n• Works on stopped containers\n• Uses container name/ID\n\ndocker run:\n• Creates AND starts container\n• Combines create + start\n• Most commonly used\n• Many options available",
      "explanation": "docker run = create + start in one command. Use create when you need to configure before starting, or start for restarting stopped containers.",
      "difficulty": "Easy",
      "code": "# docker run: Create and start (most common)\ndocker run nginx                    # Foreground\ndocker run -d nginx                 # Detached (background)\ndocker run -d --name web nginx      # With name\ndocker run -d -p 8080:80 nginx      # With port mapping\ndocker run -it ubuntu bash          # Interactive terminal\ndocker run --rm nginx               # Remove after exit\n\n\n# docker create: Just create, don't start\ndocker create --name mycontainer nginx\n# Returns: container ID\n# Container is in 'created' state\n\ndocker ps -a  # Shows container in 'Created' status\n\n# Then start when ready\ndocker start mycontainer\n\n\n# docker start: Start existing container\ndocker start mycontainer           # Start by name\ndocker start abc123                # Start by ID\ndocker start -a mycontainer        # Start and attach stdout\ndocker start -i mycontainer        # Start with interactive input\n\n\n# Full lifecycle example\ndocker create --name myapp myimage:1.0  # Create\ndocker start myapp                       # Start\ndocker stop myapp                        # Stop (graceful)\ndocker start myapp                       # Restart\ndocker kill myapp                        # Force stop\ndocker rm myapp                          # Remove\n\n# Or with run\ndocker run -d --name myapp myimage:1.0  # Create + Start\ndocker stop myapp\ndocker start myapp                      # Restart same container\ndocker rm -f myapp                      # Force remove (even if running)"
    }
  ]
}
