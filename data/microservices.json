{
  "topic": "Microservices",
  "questions": [
    {
      "id": 1,
      "question": "What are Microservices?",
      "answer": "Microservices is an architectural style that structures an application as a collection of small, independent services that communicate through well-defined APIs.\n\nKey Characteristics:\n• Each service is independently deployable and scalable\n• Services are organized around business capabilities\n• Services own their own data and database\n• Services communicate via lightweight protocols (HTTP/REST, messaging)\n• Services can be developed using different technologies\n• Decentralized governance and data management\n\nBenefits:\n• Better scalability and flexibility\n• Faster development and deployment cycles\n• Technology diversity\n• Fault isolation\n• Easier to understand and maintain small codebases",
      "explanation": "Microservices is an architectural approach where applications are built as a collection of small, independent, loosely-coupled services that can be developed, deployed, and scaled independently.",
      "difficulty": "Easy"
    },
    {
      "id": 2,
      "question": "What is the difference between Microservices and Monolithic architecture?",
      "answer": "Monolithic and Microservices architectures differ fundamentally in their structure and approach to building applications.\n\nMonolithic Architecture:\n• Single unified codebase\n• All components are tightly coupled\n• Single deployment unit\n• Shared database\n• Scaling requires scaling entire application\n• Technology stack is uniform\n\nMicroservices Architecture:\n• Multiple independent services\n• Loosely coupled components\n• Independent deployment per service\n• Database per service\n• Selective scaling of individual services\n• Polyglot technology stack possible\n\nKey Differences:\n• Deployment: Monolith requires full redeployment; Microservices allow independent deployment\n• Scalability: Monolith scales as whole; Microservices scale individually\n• Development: Monolith has single team; Microservices enable multiple autonomous teams\n• Failure Impact: Monolith failure affects entire app; Microservices provide fault isolation",
      "explanation": "Monolithic architecture is a single unified application with tightly coupled components, while Microservices architecture splits functionality into independent, loosely-coupled services that can be developed and deployed separately.",
      "difficulty": "Easy"
    },
    {
      "id": 3,
      "question": "What are the advantages of Microservices architecture?",
      "answer": "Microservices architecture offers numerous benefits for modern application development.\n\nScalability Benefits:\n• Independent scaling of individual services\n• Resource optimization by scaling only what's needed\n• Better handling of varying load patterns\n• Cost-effective scaling\n\nDevelopment Benefits:\n• Faster development cycles\n• Smaller, focused codebases easier to understand\n• Multiple teams can work independently\n• Faster onboarding of new developers\n• Technology flexibility per service\n\nDeployment Benefits:\n• Independent deployment without affecting other services\n• Reduced deployment risk\n• Faster time to market\n• Continuous deployment friendly\n\nResilience Benefits:\n• Fault isolation prevents cascade failures\n• Better fault tolerance\n• Easier to implement failover strategies\n\nBusiness Benefits:\n• Aligned with business capabilities\n• Easier to modernize legacy systems\n• Better resource utilization\n• Supports organizational scaling",
      "explanation": "Microservices provide independent scalability, faster development and deployment, technology flexibility, fault isolation, and better alignment with business capabilities compared to monolithic architectures.",
      "difficulty": "Easy"
    },
    {
      "id": 4,
      "question": "What are the challenges of Microservices architecture?",
      "answer": "While Microservices offer many benefits, they also introduce significant complexity and challenges.\n\nOperational Complexity:\n• Managing multiple services and deployments\n• Complex distributed system monitoring\n• Increased infrastructure requirements\n• Network latency and reliability issues\n• More complex debugging and troubleshooting\n\nData Management:\n• Distributed data consistency challenges\n• No ACID transactions across services\n• Data duplication across services\n• Complex querying across multiple databases\n\nDevelopment Challenges:\n• Increased initial development effort\n• Need for sophisticated DevOps practices\n• Testing complexity (integration, end-to-end)\n• Service versioning and compatibility\n\nOrganizational Challenges:\n• Requires mature DevOps culture\n• Need for skilled developers\n• Communication overhead between teams\n• Standardization vs autonomy balance\n\nSecurity Challenges:\n• More endpoints to secure\n• Complex authentication and authorization\n• Network security between services",
      "explanation": "Microservices introduce challenges including operational complexity, distributed data management, testing difficulties, increased infrastructure needs, and requirements for mature DevOps practices and skilled teams.",
      "difficulty": "Easy"
    },
    {
      "id": 5,
      "question": "What is Service Discovery in Microservices?",
      "answer": "Service Discovery is a mechanism that enables services to find and communicate with each other dynamically without hard-coded addresses.\n\nWhy Service Discovery:\n• Services have dynamic locations (IP addresses, ports)\n• Services can scale up/down dynamically\n• Services can fail and restart on different hosts\n• Manual configuration is impractical\n\nTypes of Service Discovery:\n• Client-Side Discovery: Client queries service registry and chooses instance\n• Server-Side Discovery: Client makes request to load balancer which queries registry\n\nKey Components:\n• Service Registry: Database of available service instances\n• Service Registration: Process of registering service location\n• Health Checks: Monitoring service availability\n• Load Balancing: Distributing requests across instances\n\nPopular Tools:\n• Consul\n• Eureka (Netflix)\n• Zookeeper\n• etcd\n• Kubernetes DNS",
      "explanation": "Service Discovery automatically detects and maintains a registry of available service instances, allowing services to dynamically find and communicate with each other without hard-coded network locations.",
      "difficulty": "Easy",
      "code": "// Eureka Client Registration Example (Spring Boot)\n@SpringBootApplication\n@EnableEurekaClient\npublic class UserServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(UserServiceApplication.class, args);\n    }\n}\n\n// application.yml\neureka:\n  client:\n    serviceUrl:\n      defaultZone: http://localhost:8761/eureka/\n  instance:\n    preferIpAddress: true\n\nspring:\n  application:\n    name: user-service"
    },
    {
      "id": 6,
      "question": "What is an API Gateway in Microservices?",
      "answer": "An API Gateway is a server that acts as a single entry point for all client requests to the backend microservices.\n\nCore Functions:\n• Request routing to appropriate services\n• Request aggregation from multiple services\n• Authentication and authorization\n• Rate limiting and throttling\n• Load balancing\n• Caching responses\n• Request/response transformation\n• Protocol translation (REST to gRPC)\n\nBenefits:\n• Single entry point simplifies client code\n• Reduces client-service coupling\n• Centralized cross-cutting concerns\n• Reduces round trips (aggregation)\n• Simplified client authentication\n• Better security control\n\nPopular API Gateways:\n• Kong\n• Netflix Zuul\n• Spring Cloud Gateway\n• Amazon API Gateway\n• Nginx\n• Apigee",
      "explanation": "API Gateway is a single entry point that handles routing, authentication, aggregation, and other cross-cutting concerns for client requests to multiple backend microservices.",
      "difficulty": "Easy",
      "code": "// Spring Cloud Gateway Example\n@Configuration\npublic class GatewayConfig {\n    \n    @Bean\n    public RouteLocator customRouteLocator(RouteLocatorBuilder builder) {\n        return builder.routes()\n            .route(\"user-service\", r -> r.path(\"/users/**\")\n                .filters(f -> f.addRequestHeader(\"X-Request-From\", \"Gateway\"))\n                .uri(\"lb://USER-SERVICE\"))\n            .route(\"order-service\", r -> r.path(\"/orders/**\")\n                .filters(f -> f.circuitBreaker(c -> c.setName(\"orderCircuitBreaker\")))\n                .uri(\"lb://ORDER-SERVICE\"))\n            .build();\n    }\n}"
    },
    {
      "id": 7,
      "question": "What is the Circuit Breaker pattern?",
      "answer": "Circuit Breaker is a design pattern that prevents cascading failures by stopping requests to a failing service and providing fallback responses.\n\nCircuit States:\n• Closed: Normal operation, requests pass through\n• Open: Service is failing, requests fail immediately\n• Half-Open: Testing if service recovered, limited requests allowed\n\nHow It Works:\n• Monitor failures and timeouts\n• Open circuit after threshold breached\n• Return fallback response when open\n• After timeout, enter half-open state\n• Close circuit if test requests succeed\n\nBenefits:\n• Prevents resource exhaustion\n• Faster failure response\n• Allows failing service to recover\n• Improves overall system stability\n• Provides graceful degradation\n\nImplementations:\n• Hystrix (Netflix, deprecated)\n• Resilience4j\n• Polly (.NET)\n• Istio service mesh",
      "explanation": "Circuit Breaker pattern prevents cascading failures by automatically stopping requests to failing services and providing fallback responses until the service recovers.",
      "difficulty": "Medium",
      "code": "// Resilience4j Circuit Breaker Example\n@Service\npublic class OrderService {\n    \n    @CircuitBreaker(name = \"orderService\", fallbackMethod = \"fallbackGetOrder\")\n    public Order getOrder(String orderId) {\n        // Call to external service\n        return restTemplate.getForObject(\n            \"http://order-service/orders/\" + orderId, \n            Order.class\n        );\n    }\n    \n    // Fallback method\n    private Order fallbackGetOrder(String orderId, Exception ex) {\n        return new Order(orderId, \"Unavailable\", 0.0);\n    }\n}\n\n// application.yml\nresilience4j.circuitbreaker:\n  instances:\n    orderService:\n      failureRateThreshold: 50\n      waitDurationInOpenState: 10000\n      slidingWindowSize: 10"
    },
    {
      "id": 8,
      "question": "What is the Database per Service pattern?",
      "answer": "Database per Service pattern ensures each microservice has its own private database that cannot be accessed directly by other services.\n\nKey Principles:\n• Each service owns its data exclusively\n• Services access data only through service APIs\n• Data schema is private to the service\n• Different services can use different database types\n• Data consistency managed through service coordination\n\nBenefits:\n• Loose coupling between services\n• Independent schema evolution\n• Technology diversity (polyglot persistence)\n• Better scalability per service\n• Fault isolation\n• Clear ownership boundaries\n\nChallenges:\n• No ACID transactions across services\n• Distributed data consistency\n• Complex queries across services\n• Data duplication\n• Increased storage requirements\n\nSolutions for Distributed Transactions:\n• Saga pattern\n• Event sourcing\n• CQRS pattern\n• Eventual consistency",
      "explanation": "Database per Service pattern gives each microservice its own private database to ensure loose coupling, independent scalability, and clear data ownership boundaries.",
      "difficulty": "Easy"
    },
    {
      "id": 9,
      "question": "What is the Saga pattern in Microservices?",
      "answer": "Saga pattern manages distributed transactions across multiple microservices by breaking them into a sequence of local transactions with compensating actions.\n\nTypes of Sagas:\n• Choreography: Services publish events and react to events from other services\n• Orchestration: Central coordinator directs the saga flow\n\nHow It Works:\n• Each service performs local transaction and publishes event\n• Next service listens to event and performs its transaction\n• If any step fails, compensating transactions roll back changes\n• Eventually reaches consistent state\n\nChoreography Characteristics:\n• Decentralized control\n• Services are loosely coupled\n• Simple for basic workflows\n• Complex for complicated flows\n\nOrchestration Characteristics:\n• Centralized control and coordination\n• Easier to understand and maintain\n• Single point of failure risk\n• Better for complex workflows\n\nBenefits:\n• Maintains data consistency without distributed transactions\n• Better performance than 2PC\n• More resilient to failures",
      "explanation": "Saga pattern ensures data consistency across microservices by coordinating a sequence of local transactions with compensating actions to rollback changes if any step fails.",
      "difficulty": "Medium",
      "code": "// Saga Orchestration Example\npublic class OrderSaga {\n    \n    public void executeOrderSaga(Order order) {\n        try {\n            // Step 1: Create order\n            orderService.createOrder(order);\n            \n            // Step 2: Reserve inventory\n            inventoryService.reserveInventory(order.getItems());\n            \n            // Step 3: Process payment\n            paymentService.processPayment(order.getPayment());\n            \n            // Step 4: Confirm order\n            orderService.confirmOrder(order.getId());\n            \n        } catch (InventoryException e) {\n            // Compensate: Cancel order\n            orderService.cancelOrder(order.getId());\n        } catch (PaymentException e) {\n            // Compensate: Release inventory and cancel order\n            inventoryService.releaseInventory(order.getItems());\n            orderService.cancelOrder(order.getId());\n        }\n    }\n}"
    },
    {
      "id": 10,
      "question": "What is Event-Driven Architecture in Microservices?",
      "answer": "Event-Driven Architecture is a design pattern where services communicate by producing and consuming events asynchronously.\n\nCore Concepts:\n• Events: Immutable facts about something that happened\n• Event Producers: Services that publish events\n• Event Consumers: Services that subscribe to events\n• Event Bus/Broker: Infrastructure that routes events\n\nEvent Types:\n• Domain Events: Business-significant occurrences\n• Event Notifications: Lightweight notifications\n• Event-Carried State Transfer: Events contain full state\n• Event Sourcing: Events as source of truth\n\nBenefits:\n• Loose coupling between services\n• Asynchronous communication\n• Better scalability\n• Event history and audit trail\n• Enables reactive systems\n• Flexible integration patterns\n\nChallenges:\n• Eventually consistent data\n• Complex debugging and tracing\n• Event versioning\n• Duplicate event handling\n• Ordering guarantees\n\nPopular Event Brokers:\n• Apache Kafka\n• RabbitMQ\n• Amazon SNS/SQS\n• Azure Event Hub",
      "explanation": "Event-Driven Architecture enables loosely-coupled microservices communication through asynchronous production and consumption of events via a message broker.",
      "difficulty": "Medium",
      "code": "// Spring Boot Event Publishing\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private ApplicationEventPublisher eventPublisher;\n    \n    public void createOrder(Order order) {\n        // Save order\n        orderRepository.save(order);\n        \n        // Publish event\n        OrderCreatedEvent event = new OrderCreatedEvent(order.getId(), order.getCustomerId());\n        eventPublisher.publishEvent(event);\n    }\n}\n\n// Event Consumer\n@Service\npublic class InventoryService {\n    \n    @EventListener\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        // Reserve inventory for the order\n        inventoryRepository.reserveItems(event.getOrderId());\n    }\n}"
    },
    {
      "id": 11,
      "question": "What is Service Mesh and how does it work?",
      "answer": "Service Mesh is an infrastructure layer that handles service-to-service communication, providing observability, security, and reliability features.\n\nCore Components:\n• Data Plane: Lightweight proxies (sidecars) deployed with each service\n• Control Plane: Manages and configures proxies\n• Service proxies intercept all network traffic\n\nKey Features:\n• Traffic management and routing\n• Load balancing\n• Service discovery\n• Circuit breaking and retries\n• Mutual TLS (mTLS) encryption\n• Observability (metrics, logs, traces)\n• Access control and authentication\n\nBenefits:\n• No application code changes needed\n• Consistent policies across services\n• Advanced traffic control\n• Enhanced security\n• Better observability\n\nPopular Service Meshes:\n• Istio\n• Linkerd\n• Consul Connect\n• AWS App Mesh",
      "explanation": "Service Mesh is an infrastructure layer using sidecar proxies to handle service communication, providing traffic management, security, and observability without changing application code.",
      "difficulty": "Medium",
      "code": "// Istio Virtual Service Configuration\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: user-service\nspec:\n  hosts:\n  - user-service\n  http:\n  - match:\n    - headers:\n        version:\n          exact: v2\n    route:\n    - destination:\n        host: user-service\n        subset: v2\n  - route:\n    - destination:\n        host: user-service\n        subset: v1\n      weight: 90\n    - destination:\n        host: user-service\n        subset: v2\n      weight: 10"
    },
    {
      "id": 12,
      "question": "How do you handle authentication and authorization in Microservices?",
      "answer": "Authentication and authorization in Microservices require careful design to balance security with service independence.\n\nAuthentication Strategies:\n• Centralized authentication at API Gateway\n• Token-based authentication (JWT)\n• OAuth 2.0 and OpenID Connect\n• Mutual TLS for service-to-service\n\nAuthorization Approaches:\n• API Gateway level authorization\n• Service level authorization\n• Attribute-Based Access Control (ABAC)\n• Role-Based Access Control (RBAC)\n\nToken-Based Flow:\n• Client authenticates with auth service\n• Receives JWT token with claims\n• Token passed in requests to services\n• Services validate token signature\n• Services extract claims for authorization\n\nBest Practices:\n• Use short-lived access tokens\n• Implement refresh token mechanism\n• Store tokens securely\n• Validate tokens at each service\n• Use HTTPS for all communications\n• Implement proper token revocation",
      "explanation": "Microservices typically use token-based authentication with JWT, centralized at API Gateway, combined with service-level authorization using claims to secure communications.",
      "difficulty": "Medium",
      "code": "// Spring Security JWT Configuration\n@Configuration\n@EnableWebSecurity\npublic class SecurityConfig extends WebSecurityConfigurerAdapter {\n    \n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http\n            .csrf().disable()\n            .authorizeRequests()\n                .antMatchers(\"/api/public/**\").permitAll()\n                .antMatchers(\"/api/admin/**\").hasRole(\"ADMIN\")\n                .anyRequest().authenticated()\n            .and()\n            .oauth2ResourceServer()\n                .jwt();\n    }\n}\n\n// JWT Token Validation\n@Component\npublic class JwtTokenValidator {\n    \n    public boolean validateToken(String token) {\n        try {\n            Jwts.parser()\n                .setSigningKey(secretKey)\n                .parseClaimsJws(token);\n            return true;\n        } catch (Exception e) {\n            return false;\n        }\n    }\n}"
    },
    {
      "id": 13,
      "question": "What is CQRS pattern in Microservices?",
      "answer": "CQRS (Command Query Responsibility Segregation) separates read and write operations into different models for better performance and scalability.\n\nCore Concept:\n• Commands: Operations that change state\n• Queries: Operations that read state\n• Separate models optimized for each operation type\n• Different databases for read and write\n\nCommand Side:\n• Handles business logic and validation\n• Updates write database\n• Publishes events\n• Optimized for consistency\n\nQuery Side:\n• Handles read operations\n• Uses denormalized read models\n• Optimized for performance\n• Eventually consistent\n\nBenefits:\n• Independent scaling of reads and writes\n• Optimized data models for each use case\n• Better performance\n• Simplified queries\n• Clear separation of concerns\n\nChallenges:\n• Increased complexity\n• Eventual consistency\n• Data synchronization\n• More infrastructure to manage",
      "explanation": "CQRS pattern separates read and write operations into different models with separate databases, allowing independent optimization and scaling of each side.",
      "difficulty": "Hard",
      "code": "// CQRS Command Handler\n@Service\npublic class OrderCommandHandler {\n    \n    @Autowired\n    private OrderWriteRepository writeRepo;\n    \n    @Autowired\n    private EventPublisher eventPublisher;\n    \n    public void handleCreateOrder(CreateOrderCommand command) {\n        // Validate and create order\n        Order order = new Order(command);\n        writeRepo.save(order);\n        \n        // Publish event for read model update\n        eventPublisher.publish(new OrderCreatedEvent(order));\n    }\n}\n\n// CQRS Query Handler\n@Service\npublic class OrderQueryHandler {\n    \n    @Autowired\n    private OrderReadRepository readRepo;\n    \n    public OrderDTO getOrder(String orderId) {\n        // Query optimized read model\n        return readRepo.findById(orderId);\n    }\n    \n    public List<OrderDTO> getCustomerOrders(String customerId) {\n        // Denormalized query\n        return readRepo.findByCustomerId(customerId);\n    }\n}"
    },
    {
      "id": 14,
      "question": "What are the different types of testing in Microservices?",
      "answer": "Testing Microservices requires a comprehensive strategy covering multiple levels to ensure reliability.\n\nUnit Testing:\n• Test individual components in isolation\n• Mock external dependencies\n• Fast execution\n• High coverage\n\nIntegration Testing:\n• Test service integration with dependencies\n• Test database interactions\n• Test message queue interactions\n• Use test containers for real dependencies\n\nContract Testing:\n• Test API contracts between services\n• Consumer-driven contracts\n• Provider verification\n• Tools: Pact, Spring Cloud Contract\n\nComponent Testing:\n• Test service in isolation with mocked dependencies\n• Test business logic end-to-end within service\n• Use in-memory databases\n\nEnd-to-End Testing:\n• Test complete user workflows\n• Multiple services involved\n• Expensive and slow\n• Fewer tests, critical paths only\n\nPerformance Testing:\n• Load testing\n• Stress testing\n• Scalability testing\n\nChaos Testing:\n• Test system resilience\n• Simulate failures\n• Tools: Chaos Monkey",
      "explanation": "Microservices testing includes unit, integration, contract, component, end-to-end, performance, and chaos testing, with emphasis on contract testing to ensure service compatibility.",
      "difficulty": "Medium",
      "code": "// Contract Testing with Pact (Consumer)\n@ExtendWith(PactConsumerTestExt.class)\npublic class UserServiceContractTest {\n    \n    @Pact(consumer = \"order-service\")\n    public RequestResponsePact createPact(PactDslWithProvider builder) {\n        return builder\n            .given(\"user exists\")\n            .uponReceiving(\"get user by id\")\n            .path(\"/users/123\")\n            .method(\"GET\")\n            .willRespondWith()\n            .status(200)\n            .body(new PactDslJsonBody()\n                .stringValue(\"id\", \"123\")\n                .stringValue(\"name\", \"John\"))\n            .toPact();\n    }\n    \n    @Test\n    @PactTestFor(pactMethod = \"createPact\")\n    void testGetUser(MockServer mockServer) {\n        // Test consumer code against contract\n    }\n}"
    },
    {
      "id": 15,
      "question": "What is the Strangler Fig pattern?",
      "answer": "Strangler Fig pattern is a migration strategy for gradually replacing a monolithic application with microservices.\n\nNamed After:\n• Strangler fig vine that grows around a tree\n• Eventually replaces the host tree\n• Incremental replacement approach\n\nImplementation Steps:\n• Identify functionality to extract\n• Create new microservice for that functionality\n• Route new requests to microservice\n• Keep old functionality running\n• Gradually migrate all features\n• Retire old system when complete\n\nKey Techniques:\n• API Gateway for routing\n• Feature flags for gradual rollout\n• Database migration strategies\n• Parallel running of old and new\n\nBenefits:\n• Low risk incremental migration\n• Business continuity maintained\n• Can roll back if issues occur\n• Learn and adapt during migration\n• Immediate business value\n\nChallenges:\n• Maintaining two systems\n• Data synchronization\n• Complex routing logic\n• Extended migration period",
      "explanation": "Strangler Fig pattern enables safe migration from monolith to microservices by incrementally replacing functionality while maintaining the old system until complete replacement.",
      "difficulty": "Medium",
      "code": "// API Gateway routing for Strangler Pattern\n@Configuration\npublic class StranglerGatewayConfig {\n    \n    @Bean\n    public RouteLocator stranglerRoutes(RouteLocatorBuilder builder) {\n        return builder.routes()\n            // New microservice routes\n            .route(\"new-user-service\", r -> r\n                .path(\"/api/users/**\")\n                .and()\n                .header(\"X-Use-New-Service\", \"true\")\n                .uri(\"lb://NEW-USER-SERVICE\"))\n            \n            // Legacy monolith fallback\n            .route(\"legacy-monolith\", r -> r\n                .path(\"/api/**\")\n                .uri(\"http://legacy-monolith:8080\"))\n            \n            .build();\n    }\n}"
    },
    {
      "id": 16,
      "question": "How do you implement distributed logging in Microservices?",
      "answer": "Distributed logging aggregates logs from multiple services into a centralized system for monitoring and debugging.\n\nKey Requirements:\n• Centralized log aggregation\n• Correlation IDs to track requests\n• Structured logging format\n• Log levels and filtering\n• Fast search and analysis\n• Log retention policies\n\nCommon Architecture:\n• Services write logs to stdout/stderr\n• Log shippers collect logs (Fluentd, Logstash)\n• Logs stored in centralized system (Elasticsearch)\n• Visualization tool for analysis (Kibana)\n\nBest Practices:\n• Use correlation IDs across all services\n• Include service name, timestamp, log level\n• Log in structured format (JSON)\n• Avoid logging sensitive data\n• Set appropriate log levels\n• Include context information\n\nPopular Stacks:\n• ELK Stack (Elasticsearch, Logstash, Kibana)\n• EFK Stack (Elasticsearch, Fluentd, Kibana)\n• Splunk\n• Graylog\n• CloudWatch Logs (AWS)",
      "explanation": "Distributed logging aggregates logs from all microservices into a centralized system using correlation IDs and structured logging for easy tracking and debugging across services.",
      "difficulty": "Medium",
      "code": "// Spring Boot Distributed Logging with Correlation ID\n@Component\npublic class CorrelationIdFilter extends OncePerRequestFilter {\n    \n    private static final String CORRELATION_ID = \"X-Correlation-ID\";\n    \n    @Override\n    protected void doFilterInternal(HttpServletRequest request,\n                                   HttpServletResponse response,\n                                   FilterChain filterChain) throws ServletException, IOException {\n        String correlationId = request.getHeader(CORRELATION_ID);\n        if (correlationId == null) {\n            correlationId = UUID.randomUUID().toString();\n        }\n        \n        MDC.put(\"correlationId\", correlationId);\n        response.addHeader(CORRELATION_ID, correlationId);\n        \n        try {\n            filterChain.doFilter(request, response);\n        } finally {\n            MDC.clear();\n        }\n    }\n}\n\n// logback-spring.xml\n<pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} [%X{correlationId}] - %msg%n</pattern>"
    },
    {
      "id": 17,
      "question": "What is distributed tracing and why is it important?",
      "answer": "Distributed tracing tracks requests as they flow through multiple microservices, providing visibility into service interactions and performance.\n\nKey Concepts:\n• Trace: Complete journey of a request\n• Span: Individual operation within a trace\n• Trace ID: Unique identifier for entire request\n• Span ID: Unique identifier for each operation\n• Parent-child relationships between spans\n\nInformation Captured:\n• Service dependencies\n• Request latency per service\n• Error locations\n• Bottlenecks and slow operations\n• Service call hierarchy\n\nBenefits:\n• Identify performance bottlenecks\n• Debug issues across services\n• Understand service dependencies\n• Monitor service health\n• Optimize critical paths\n• Root cause analysis\n\nPopular Tools:\n• Jaeger\n• Zipkin\n• AWS X-Ray\n• Google Cloud Trace\n• Datadog APM\n\nImplementation:\n• Instrumentation in application code\n• Automatic instrumentation with agents\n• OpenTelemetry standard\n• Service mesh integration",
      "explanation": "Distributed tracing tracks requests across multiple services using trace and span IDs, providing visibility into service interactions, performance bottlenecks, and debugging distributed systems.",
      "difficulty": "Medium",
      "code": "// Spring Cloud Sleuth with Zipkin\n// pom.xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-sleuth</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-sleuth-zipkin</artifactId>\n</dependency>\n\n// application.yml\nspring:\n  sleuth:\n    sampler:\n      probability: 1.0  # Sample all requests\n  zipkin:\n    base-url: http://localhost:9411\n\n// Service code - automatic instrumentation\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private RestTemplate restTemplate;\n    \n    public Order createOrder(Order order) {\n        // Trace ID automatically propagated\n        User user = restTemplate.getForObject(\n            \"http://user-service/users/\" + order.getUserId(),\n            User.class\n        );\n        return orderRepository.save(order);\n    }\n}"
    },
    {
      "id": 18,
      "question": "What is the Bulkhead pattern in Microservices?",
      "answer": "Bulkhead pattern isolates resources for different parts of an application to prevent cascade failures and resource exhaustion.\n\nNamed After:\n• Ship bulkheads that compartmentalize hull\n• Prevents entire ship from sinking if one compartment breaches\n\nImplementation Strategies:\n• Thread pool isolation\n• Semaphore isolation\n• Connection pool partitioning\n• CPU/Memory limits per service\n\nTypes of Bulkheads:\n• Thread Pool Bulkhead: Separate thread pools for different dependencies\n• Semaphore Bulkhead: Limit concurrent calls with semaphores\n• Container Bulkhead: Resource limits at container level\n\nBenefits:\n• Fault isolation\n• Resource protection\n• Prevents resource starvation\n• Better system stability\n• Failure containment\n\nWhen to Use:\n• Critical vs non-critical operations\n• Different SLA requirements\n• Multiple external dependencies\n• Variable load patterns",
      "explanation": "Bulkhead pattern isolates resources using separate thread pools or semaphores to prevent one failing component from exhausting resources and affecting the entire system.",
      "difficulty": "Medium",
      "code": "// Resilience4j Bulkhead Configuration\n@Service\npublic class PaymentService {\n    \n    @Bulkhead(name = \"paymentService\", type = Bulkhead.Type.THREADPOOL, \n              fallbackMethod = \"paymentFallback\")\n    public Payment processPayment(PaymentRequest request) {\n        // External payment processing\n        return paymentGateway.process(request);\n    }\n    \n    private Payment paymentFallback(PaymentRequest request, Exception ex) {\n        return new Payment(\"PENDING\", \"Payment queued for retry\");\n    }\n}\n\n// application.yml\nresilience4j.bulkhead:\n  instances:\n    paymentService:\n      maxConcurrentCalls: 10\n      maxWaitDuration: 1000\n\nresilience4j.thread-pool-bulkhead:\n  instances:\n    paymentService:\n      maxThreadPoolSize: 5\n      coreThreadPoolSize: 3\n      queueCapacity: 20"
    },
    {
      "id": 19,
      "question": "How do you handle distributed transactions in Microservices?",
      "answer": "Distributed transactions in Microservices are challenging due to the absence of ACID guarantees across services.\n\nTraditional Approach (Avoided):\n• Two-Phase Commit (2PC)\n• Requires distributed transaction coordinator\n• Poor performance and scalability\n• Not recommended for microservices\n\nPreferred Patterns:\n• Saga Pattern: Sequence of local transactions with compensation\n• Event Sourcing: Events as source of truth\n• CQRS: Separate read and write models\n• Eventual Consistency: Accept temporary inconsistency\n\nSaga Implementation:\n• Choreography: Event-based coordination\n• Orchestration: Central coordinator\n• Compensating transactions for rollback\n\nBest Practices:\n• Design for eventual consistency\n• Implement idempotent operations\n• Use optimistic locking\n• Implement retry mechanisms\n• Proper error handling and compensation\n• Monitor and alert on inconsistencies\n\nTrade-offs:\n• Consistency vs availability\n• Complexity vs reliability\n• Performance vs data integrity",
      "explanation": "Microservices handle distributed transactions using Saga pattern with eventual consistency rather than traditional ACID transactions, implementing compensating transactions for rollback scenarios.",
      "difficulty": "Hard",
      "code": "// Saga with Compensating Transactions\n@Service\npublic class OrderSagaOrchestrator {\n    \n    @Autowired\n    private SagaExecutor sagaExecutor;\n    \n    public void createOrder(OrderRequest request) {\n        Saga saga = Saga.create()\n            .step(\"reserve-inventory\")\n                .invoke(() -> inventoryService.reserve(request.getItems()))\n                .compensate(() -> inventoryService.release(request.getItems()))\n            .step(\"process-payment\")\n                .invoke(() -> paymentService.charge(request.getPayment()))\n                .compensate(() -> paymentService.refund(request.getPayment()))\n            .step(\"create-order\")\n                .invoke(() -> orderService.create(request))\n                .compensate(() -> orderService.cancel(request.getOrderId()))\n            .step(\"notify-customer\")\n                .invoke(() -> notificationService.send(request.getCustomerId()))\n            .build();\n            \n        sagaExecutor.execute(saga);\n    }\n}"
    },
    {
      "id": 20,
      "question": "What is API versioning and what strategies can be used?",
      "answer": "API versioning manages changes to service APIs while maintaining backward compatibility with existing clients.\n\nVersioning Strategies:\n• URI Versioning: Version in URL path (/v1/users)\n• Query Parameter: Version as parameter (/users?version=1)\n• Header Versioning: Custom header (X-API-Version: 1)\n• Content Negotiation: Accept header (application/vnd.api.v1+json)\n• No Versioning: Backward compatible changes only\n\nURI Versioning:\n• Most visible and explicit\n• Easy to understand and test\n• Can clutter API design\n• Common in public APIs\n\nHeader Versioning:\n• Cleaner URLs\n• More flexible\n• Less visible to clients\n• Better for internal APIs\n\nBest Practices:\n• Version only when breaking changes needed\n• Maintain multiple versions temporarily\n• Deprecation policy and timeline\n• Document version differences\n• Use semantic versioning\n• Sunset old versions gradually\n• Monitor version usage\n\nBreaking vs Non-Breaking:\n• Breaking: Remove fields, change types, change behavior\n• Non-Breaking: Add optional fields, new endpoints",
      "explanation": "API versioning manages backward compatibility using strategies like URI versioning, header versioning, or content negotiation to support multiple API versions simultaneously during transitions.",
      "difficulty": "Easy",
      "code": "// URI Versioning with Spring Boot\n@RestController\n@RequestMapping(\"/api/v1/users\")\npublic class UserControllerV1 {\n    \n    @GetMapping(\"/{id}\")\n    public UserV1 getUser(@PathVariable String id) {\n        return userService.getUserV1(id);\n    }\n}\n\n@RestController\n@RequestMapping(\"/api/v2/users\")\npublic class UserControllerV2 {\n    \n    @GetMapping(\"/{id}\")\n    public UserV2 getUser(@PathVariable String id) {\n        return userService.getUserV2(id);\n    }\n}\n\n// Header Versioning\n@GetMapping(value = \"/users/{id}\", headers = \"X-API-Version=1\")\npublic UserV1 getUserV1(@PathVariable String id) {\n    return userService.getUserV1(id);\n}\n\n@GetMapping(value = \"/users/{id}\", headers = \"X-API-Version=2\")\npublic UserV2 getUserV2(@PathVariable String id) {\n    return userService.getUserV2(id);\n}"
    },
    {
      "id": 21,
      "question": "What is the Backend for Frontend (BFF) pattern?",
      "answer": "Backend for Frontend (BFF) pattern creates separate backend services tailored to specific frontend applications or user experiences.\n\nCore Concept:\n• Different frontends have different needs\n• Mobile apps need less data than web\n• Each frontend gets optimized backend\n• BFF sits between frontend and microservices\n\nBenefits:\n• Optimized responses for each client type\n• Reduced over-fetching and under-fetching\n• Frontend teams control their BFF\n• Simplified frontend code\n• Better performance\n• Independent evolution of frontends\n\nTypical BFF Types:\n• Mobile BFF: Optimized for mobile constraints\n• Web BFF: Rich data for web applications\n• Desktop BFF: Different requirements than mobile\n• Third-party BFF: API for external partners\n\nBFF Responsibilities:\n• Aggregate data from multiple services\n• Transform and format responses\n• Handle authentication\n• Caching strategies\n• Request routing\n\nWhen to Use:\n• Multiple frontend types\n• Different data requirements per client\n• Performance optimization needed\n• Frontend team autonomy desired",
      "explanation": "BFF pattern creates separate backend services optimized for each frontend type, allowing tailored data aggregation and transformation for mobile, web, or other client applications.",
      "difficulty": "Medium",
      "code": "// Mobile BFF Example\n@RestController\n@RequestMapping(\"/mobile/api\")\npublic class MobileBFFController {\n    \n    @Autowired\n    private UserService userService;\n    @Autowired\n    private OrderService orderService;\n    @Autowired\n    private ProductService productService;\n    \n    // Aggregated lightweight response for mobile\n    @GetMapping(\"/dashboard\")\n    public MobileDashboard getDashboard(@RequestParam String userId) {\n        // Parallel calls to multiple services\n        CompletableFuture<UserSummary> userFuture = \n            CompletableFuture.supplyAsync(() -> userService.getUserSummary(userId));\n        CompletableFuture<List<Order>> ordersFuture = \n            CompletableFuture.supplyAsync(() -> orderService.getRecentOrders(userId, 5));\n        CompletableFuture<List<Product>> productsFuture = \n            CompletableFuture.supplyAsync(() -> productService.getRecommended(userId, 3));\n        \n        // Combine results\n        return new MobileDashboard(\n            userFuture.join(),\n            ordersFuture.join(),\n            productsFuture.join()\n        );\n    }\n}"
    },
    {
      "id": 22,
      "question": "How do you implement rate limiting in Microservices?",
      "answer": "Rate limiting controls the number of requests a client can make to prevent abuse and ensure fair resource usage.\n\nRate Limiting Strategies:\n• Fixed Window: Count requests in fixed time windows\n• Sliding Window: More accurate, uses rolling time window\n• Token Bucket: Tokens added at fixed rate, consumed per request\n• Leaky Bucket: Requests processed at constant rate\n\nImplementation Levels:\n• API Gateway: Centralized rate limiting\n• Service Level: Individual service protection\n• Database Level: Query rate limiting\n• Client Level: Self-imposed limits\n\nRate Limiting Criteria:\n• By IP address\n• By user/API key\n• By endpoint\n• By tenant in multi-tenant systems\n• By request cost (expensive operations)\n\nBest Practices:\n• Return 429 (Too Many Requests) status\n• Include Retry-After header\n• Provide clear error messages\n• Different limits for different tiers\n• Monitor and adjust limits\n• Graceful degradation\n\nTools and Libraries:\n• Redis for distributed rate limiting\n• Bucket4j (Java)\n• Express Rate Limit (Node.js)\n• API Gateway built-in features",
      "explanation": "Rate limiting controls request frequency using algorithms like token bucket or sliding window, implemented at API Gateway or service level to prevent abuse and ensure fair resource usage.",
      "difficulty": "Medium",
      "code": "// Spring Boot Rate Limiting with Bucket4j\n@Service\npublic class RateLimitService {\n    \n    private final Map<String, Bucket> cache = new ConcurrentHashMap<>();\n    \n    public Bucket resolveBucket(String apiKey) {\n        return cache.computeIfAbsent(apiKey, k -> createNewBucket());\n    }\n    \n    private Bucket createNewBucket() {\n        // Allow 100 requests per minute\n        Bandwidth limit = Bandwidth.classic(100, Refill.intervally(100, Duration.ofMinutes(1)));\n        return Bucket.builder()\n            .addLimit(limit)\n            .build();\n    }\n}\n\n@RestController\npublic class ApiController {\n    \n    @Autowired\n    private RateLimitService rateLimitService;\n    \n    @GetMapping(\"/api/data\")\n    public ResponseEntity<?> getData(@RequestHeader(\"X-API-Key\") String apiKey) {\n        Bucket bucket = rateLimitService.resolveBucket(apiKey);\n        \n        if (bucket.tryConsume(1)) {\n            return ResponseEntity.ok(getData());\n        }\n        \n        return ResponseEntity.status(429)\n            .header(\"X-Rate-Limit-Retry-After-Seconds\", \"60\")\n            .body(\"Too many requests\");\n    }\n}"
    },
    {
      "id": 23,
      "question": "What is the Sidecar pattern in Microservices?",
      "answer": "Sidecar pattern deploys a helper component alongside a service to provide supporting features without changing the service code.\n\nCore Concept:\n• Sidecar runs in same host/pod as main service\n• Provides cross-cutting functionality\n• Shares resources with main service\n• Deployed and scaled together\n\nCommon Sidecar Functions:\n• Logging and monitoring\n• Service mesh proxy (Envoy, Linkerd)\n• Configuration management\n• Security (authentication, encryption)\n• Service discovery\n• Circuit breaking\n• Health checks\n\nBenefits:\n• No changes to application code\n• Language agnostic\n• Reusable across services\n• Separation of concerns\n• Independent updates\n• Standardized functionality\n\nChallenges:\n• Increased resource consumption\n• Added complexity\n• Deployment coordination\n• Debugging across containers\n\nKubernetes Sidecar:\n• Multiple containers in same pod\n• Share network namespace\n• Share storage volumes\n• Lifecycle tied together",
      "explanation": "Sidecar pattern deploys a helper container alongside the main service to provide cross-cutting functionality like logging, monitoring, or proxying without modifying application code.",
      "difficulty": "Medium",
      "code": "// Kubernetes Pod with Sidecar\napiVersion: v1\nkind: Pod\nmetadata:\n  name: user-service\nspec:\n  containers:\n  # Main application container\n  - name: user-service\n    image: user-service:1.0\n    ports:\n    - containerPort: 8080\n    \n  # Logging sidecar\n  - name: log-aggregator\n    image: fluentd:latest\n    volumeMounts:\n    - name: logs\n      mountPath: /var/log\n      \n  # Monitoring sidecar\n  - name: metrics-exporter\n    image: prometheus-exporter:latest\n    ports:\n    - containerPort: 9090\n    \n  volumes:\n  - name: logs\n    emptyDir: {}\n\n# Service Mesh Sidecar Injection (Istio)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: user-service\n  labels:\n    app: user-service\n  annotations:\n    sidecar.istio.io/inject: \"true\"\nspec:\n  containers:\n  - name: user-service\n    image: user-service:1.0"
    },
    {
      "id": 24,
      "question": "How do you implement service-to-service communication in Microservices?",
      "answer": "Service-to-service communication can be implemented using synchronous or asynchronous patterns.\n\nSynchronous Communication:\n• REST/HTTP: Most common, request-response pattern\n• gRPC: High-performance RPC framework\n• GraphQL: Flexible query language\n• Direct blocking calls\n\nAsynchronous Communication:\n• Message Queues: Point-to-point messaging\n• Publish-Subscribe: Event broadcasting\n• Event Streaming: Kafka, AWS Kinesis\n• Non-blocking, eventual consistency\n\nREST Communication:\n• Simple and widely supported\n• JSON over HTTP\n• Stateless\n• Tools: RestTemplate, WebClient, Feign\n\ngRPC Communication:\n• Protocol Buffers for serialization\n• HTTP/2 for transport\n• Better performance than REST\n• Strongly typed contracts\n\nMessage-Based Communication:\n• Decouples sender and receiver\n• Better scalability\n• Handles backpressure\n• Tools: RabbitMQ, Kafka, AWS SQS\n\nChoosing Pattern:\n• Synchronous for immediate response needed\n• Asynchronous for better decoupling\n• Consider latency requirements\n• Consider data consistency needs",
      "explanation": "Microservices communicate synchronously via REST or gRPC for immediate responses, or asynchronously via message queues and events for better decoupling and scalability.",
      "difficulty": "Easy",
      "code": "// REST Communication with Spring WebClient\n@Service\npublic class OrderService {\n    \n    private final WebClient webClient;\n    \n    public OrderService(WebClient.Builder builder) {\n        this.webClient = builder.baseUrl(\"http://user-service\").build();\n    }\n    \n    public Order createOrder(OrderRequest request) {\n        // Synchronous call to user service\n        User user = webClient.get()\n            .uri(\"/users/{id}\", request.getUserId())\n            .retrieve()\n            .bodyToMono(User.class)\n            .block();\n            \n        return new Order(request, user);\n    }\n}\n\n// Async Communication with RabbitMQ\n@Service\npublic class OrderEventPublisher {\n    \n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n    \n    public void publishOrderCreated(Order order) {\n        OrderCreatedEvent event = new OrderCreatedEvent(order);\n        rabbitTemplate.convertAndSend(\"order.exchange\", \"order.created\", event);\n    }\n}\n\n@Service\npublic class InventoryEventListener {\n    \n    @RabbitListener(queues = \"inventory.queue\")\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        inventoryService.reserveItems(event.getItems());\n    }\n}"
    },
    {
      "id": 25,
      "question": "What is the Retry pattern and how to implement it?",
      "answer": "Retry pattern automatically retries failed operations to handle transient failures in distributed systems.\n\nTransient Failures:\n• Network timeouts\n• Service temporarily unavailable\n• Resource constraints\n• Rate limiting\n• Database connection issues\n\nRetry Strategies:\n• Immediate Retry: Retry immediately without delay\n• Fixed Delay: Wait fixed time between retries\n• Exponential Backoff: Increase delay exponentially\n• Jitter: Add randomness to prevent thundering herd\n\nBest Practices:\n• Set maximum retry attempts\n• Use exponential backoff with jitter\n• Make operations idempotent\n• Distinguish transient vs permanent failures\n• Log retry attempts\n• Monitor retry metrics\n• Combine with circuit breaker\n• Set appropriate timeouts\n\nWhen to Retry:\n• Network timeouts\n• HTTP 5xx errors\n• HTTP 429 (rate limit)\n• Connection failures\n\nWhen NOT to Retry:\n• HTTP 4xx errors (client errors)\n• Authentication failures\n• Business logic errors\n• Data validation errors",
      "explanation": "Retry pattern automatically retries failed operations using strategies like exponential backoff with jitter to handle transient failures in distributed systems while avoiding permanent error retries.",
      "difficulty": "Medium",
      "code": "// Resilience4j Retry Configuration\n@Service\npublic class UserService {\n    \n    @Retry(name = \"userService\", fallbackMethod = \"getUserFallback\")\n    public User getUser(String userId) {\n        return restTemplate.getForObject(\n            \"http://user-service/users/\" + userId,\n            User.class\n        );\n    }\n    \n    private User getUserFallback(String userId, Exception ex) {\n        return new User(userId, \"Unknown\", \"unknown@email.com\");\n    }\n}\n\n// application.yml\nresilience4j.retry:\n  instances:\n    userService:\n      maxAttempts: 3\n      waitDuration: 1000\n      enableExponentialBackoff: true\n      exponentialBackoffMultiplier: 2\n      retryExceptions:\n        - java.net.ConnectException\n        - java.net.SocketTimeoutException\n      ignoreExceptions:\n        - java.lang.IllegalArgumentException\n\n// Manual Retry with Exponential Backoff\npublic <T> T retryWithBackoff(Supplier<T> operation, int maxAttempts) {\n    int attempt = 0;\n    while (attempt < maxAttempts) {\n        try {\n            return operation.get();\n        } catch (Exception e) {\n            attempt++;\n            if (attempt >= maxAttempts) throw e;\n            long delay = (long) Math.pow(2, attempt) * 1000;\n            Thread.sleep(delay + random.nextInt(1000)); // Add jitter\n        }\n    }\n    throw new RuntimeException(\"Max retries exceeded\");\n}"
    },
    {
      "id": 26,
      "question": "What is containerization and how does Docker help Microservices?",
      "answer": "Containerization packages applications with their dependencies into isolated, portable units that can run consistently across environments.\n\nDocker Benefits for Microservices:\n• Consistent environments across dev, test, production\n• Lightweight compared to virtual machines\n• Fast startup times\n• Isolation between services\n• Easy scaling and deployment\n• Version control for images\n• Efficient resource utilization\n\nKey Docker Concepts:\n• Image: Blueprint for containers\n• Container: Running instance of image\n• Dockerfile: Instructions to build image\n• Registry: Repository for images (Docker Hub)\n• Volume: Persistent data storage\n• Network: Communication between containers\n\nDocker vs Virtual Machines:\n• Containers share host OS kernel\n• VMs include full OS\n• Containers are more lightweight\n• Faster startup with containers\n• Better resource utilization\n\nBest Practices:\n• One process per container\n• Keep images small\n• Use multi-stage builds\n• Don't store data in containers\n• Use specific image tags\n• Scan images for vulnerabilities",
      "explanation": "Docker containerization packages microservices with dependencies into lightweight, portable containers that ensure consistent execution across environments with efficient resource usage and fast deployment.",
      "difficulty": "Easy",
      "code": "# Multi-stage Dockerfile for Java Microservice\n# Build stage\nFROM maven:3.8-openjdk-17 AS build\nWORKDIR /app\nCOPY pom.xml .\nCOPY src ./src\nRUN mvn clean package -DskipTests\n\n# Runtime stage\nFROM openjdk:17-jdk-slim\nWORKDIR /app\nCOPY --from=build /app/target/user-service-1.0.jar app.jar\nEXPOSE 8080\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n\n# docker-compose.yml for multiple services\nversion: '3.8'\nservices:\n  user-service:\n    build: ./user-service\n    ports:\n      - \"8081:8080\"\n    environment:\n      - SPRING_PROFILES_ACTIVE=docker\n    depends_on:\n      - postgres\n      \n  order-service:\n    build: ./order-service\n    ports:\n      - \"8082:8080\"\n    depends_on:\n      - postgres\n      - rabbitmq\n      \n  postgres:\n    image: postgres:14\n    environment:\n      POSTGRES_PASSWORD: secret\n      \n  rabbitmq:\n    image: rabbitmq:3-management\n    ports:\n      - \"5672:5672\"\n      - \"15672:15672\""
    },
    {
      "id": 27,
      "question": "What is Kubernetes and why is it important for Microservices?",
      "answer": "Kubernetes is an open-source container orchestration platform that automates deployment, scaling, and management of containerized applications.\n\nCore Features:\n• Automated deployment and rollback\n• Self-healing (restart failed containers)\n• Horizontal scaling\n• Load balancing and service discovery\n• Secret and configuration management\n• Storage orchestration\n• Batch execution\n\nKey Concepts:\n• Pod: Smallest deployable unit (one or more containers)\n• Service: Stable endpoint for pods\n• Deployment: Manages pod replicas\n• ReplicaSet: Ensures desired pod count\n• Namespace: Virtual cluster separation\n• ConfigMap: Configuration data\n• Secret: Sensitive information\n• Ingress: External access to services\n\nBenefits for Microservices:\n• Simplified deployment process\n• Automatic scaling based on load\n• Health monitoring and recovery\n• Rolling updates with zero downtime\n• Resource optimization\n• Multi-cloud portability\n\nKubernetes vs Docker:\n• Docker: Containerization platform\n• Kubernetes: Container orchestration\n• Often used together\n• Kubernetes manages Docker containers",
      "explanation": "Kubernetes orchestrates containerized microservices by automating deployment, scaling, load balancing, and self-healing, making it easier to manage complex distributed systems at scale.",
      "difficulty": "Easy",
      "code": "# Kubernetes Deployment for Microservice\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-service\n  template:\n    metadata:\n      labels:\n        app: user-service\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: \"production\"\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /actuator/health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\nspec:\n  selector:\n    app: user-service\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: LoadBalancer"
    },
    {
      "id": 28,
      "question": "What is the Timeout pattern in Microservices?",
      "answer": "Timeout pattern sets maximum wait time for operations to prevent indefinite blocking and resource exhaustion.\n\nWhy Timeouts Matter:\n• Prevent thread starvation\n• Avoid cascading failures\n• Improve system responsiveness\n• Resource management\n• Better user experience\n\nTimeout Types:\n• Connection Timeout: Time to establish connection\n• Read Timeout: Time to receive response\n• Overall Request Timeout: Total time for operation\n• Idle Timeout: Time connection stays open without activity\n\nBest Practices:\n• Set timeouts at every integration point\n• Use realistic timeout values\n• Different timeouts for different operations\n• Consider network latency\n• Combine with retry and circuit breaker\n• Monitor timeout occurrences\n• Log timeout events\n• Provide fallback behavior\n\nTimeout Configuration Levels:\n• HTTP client level\n• Application level\n• Infrastructure level (load balancer)\n• Database connection level\n\nCommon Pitfalls:\n• Timeouts too short: Unnecessary failures\n• Timeouts too long: Resource waste\n• No timeouts: System hangs\n• Inconsistent timeout chains",
      "explanation": "Timeout pattern sets maximum wait times for operations to prevent indefinite blocking, ensuring system responsiveness and protecting against resource exhaustion in distributed systems.",
      "difficulty": "Medium",
      "code": "// Spring WebClient with Timeouts\n@Configuration\npublic class WebClientConfig {\n    \n    @Bean\n    public WebClient webClient() {\n        HttpClient httpClient = HttpClient.create()\n            .responseTimeout(Duration.ofSeconds(5))\n            .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 3000);\n            \n        return WebClient.builder()\n            .clientConnector(new ReactorClientHttpConnector(httpClient))\n            .build();\n    }\n}\n\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private WebClient webClient;\n    \n    public User getUser(String userId) {\n        return webClient.get()\n            .uri(\"http://user-service/users/{id}\", userId)\n            .retrieve()\n            .bodyToMono(User.class)\n            .timeout(Duration.ofSeconds(3))\n            .onErrorResume(TimeoutException.class, e -> {\n                // Handle timeout\n                return Mono.just(getDefaultUser());\n            })\n            .block();\n    }\n}\n\n// RestTemplate with Timeouts\n@Bean\npublic RestTemplate restTemplate() {\n    HttpComponentsClientHttpRequestFactory factory = \n        new HttpComponentsClientHttpRequestFactory();\n    factory.setConnectTimeout(3000);\n    factory.setReadTimeout(5000);\n    return new RestTemplate(factory);\n}"
    },
    {
      "id": 29,
      "question": "What is the purpose of health checks in Microservices?",
      "answer": "Health checks monitor service availability and readiness to handle requests, enabling automated recovery and load balancing.\n\nTypes of Health Checks:\n• Liveness: Is service running and responsive?\n• Readiness: Is service ready to accept traffic?\n• Startup: Has service completed initialization?\n\nLiveness Checks:\n• Determines if container should be restarted\n• Checks basic application responsiveness\n• Failure triggers restart\n• Simple checks (HTTP endpoint)\n\nReadiness Checks:\n• Determines if service should receive traffic\n• Checks dependencies availability\n• Failure removes from load balancer\n• More comprehensive than liveness\n\nHealth Check Components:\n• Application health\n• Database connectivity\n• External service dependencies\n• Disk space\n• Memory usage\n• Message queue connectivity\n\nBest Practices:\n• Separate liveness and readiness endpoints\n• Keep health checks lightweight\n• Fast response times\n• Avoid cascading health checks\n• Include dependency checks in readiness\n• Monitor health check metrics\n• Implement graceful degradation",
      "explanation": "Health checks monitor service health through liveness and readiness probes, enabling orchestrators to restart failed services and load balancers to route traffic only to healthy instances.",
      "difficulty": "Easy",
      "code": "// Spring Boot Actuator Health Checks\n@Component\npublic class DatabaseHealthIndicator implements HealthIndicator {\n    \n    @Autowired\n    private DataSource dataSource;\n    \n    @Override\n    public Health health() {\n        try (Connection conn = dataSource.getConnection()) {\n            if (conn.isValid(1)) {\n                return Health.up()\n                    .withDetail(\"database\", \"available\")\n                    .build();\n            }\n        } catch (Exception e) {\n            return Health.down()\n                .withDetail(\"error\", e.getMessage())\n                .build();\n        }\n        return Health.down().build();\n    }\n}\n\n// application.yml\nmanagement:\n  endpoint:\n    health:\n      show-details: always\n  health:\n    readiness:\n      enabled: true\n    liveness:\n      enabled: true\n\n# Kubernetes Health Checks\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: user-service\n    image: user-service:1.0\n    livenessProbe:\n      httpGet:\n        path: /actuator/health/liveness\n        port: 8080\n      initialDelaySeconds: 30\n      periodSeconds: 10\n      failureThreshold: 3\n    readinessProbe:\n      httpGet:\n        path: /actuator/health/readiness\n        port: 8080\n      initialDelaySeconds: 10\n      periodSeconds: 5"
    },
    {
      "id": 30,
      "question": "What is service decomposition and what strategies can be used?",
      "answer": "Service decomposition is the process of breaking down a monolithic application or business domain into smaller, independent microservices.\n\nDecomposition Strategies:\n• Decompose by Business Capability\n• Decompose by Subdomain (DDD)\n• Decompose by Transactions\n• Decompose by Use Cases\n\nBusiness Capability Approach:\n• Identify business functions\n• Group related capabilities\n• One service per capability\n• Aligns with organization structure\n• Examples: Order Management, User Management, Inventory\n\nDomain-Driven Design (DDD):\n• Identify bounded contexts\n• Define domain models\n• Aggregate roots become services\n• Ubiquitous language\n• Domain experts collaboration\n\nDecomposition Principles:\n• Single Responsibility Principle\n• High cohesion within services\n• Loose coupling between services\n• Independent deployability\n• Data ownership per service\n\nCommon Mistakes:\n• Too fine-grained (nano-services)\n• Too coarse-grained\n• Shared databases\n• Ignoring transaction boundaries\n• Wrong service boundaries",
      "explanation": "Service decomposition breaks monoliths into microservices using strategies like business capability or domain-driven design, ensuring services are cohesive, loosely coupled, and independently deployable.",
      "difficulty": "Medium",
      "code": "// Business Capability Decomposition Example\n\n// Before: Monolithic E-commerce Application\npublic class EcommerceApplication {\n    UserManagement userManagement;\n    OrderManagement orderManagement;\n    ProductCatalog productCatalog;\n    PaymentProcessing paymentProcessing;\n    ShippingManagement shippingManagement;\n    InventoryManagement inventoryManagement;\n}\n\n// After: Decomposed Microservices\n\n// User Service (Identity & Access)\n@SpringBootApplication\npublic class UserServiceApplication {\n    // User registration, authentication, profile\n}\n\n// Order Service (Order Management)\n@SpringBootApplication\npublic class OrderServiceApplication {\n    // Order creation, tracking, history\n}\n\n// Product Service (Catalog Management)\n@SpringBootApplication\npublic class ProductServiceApplication {\n    // Product listings, search, categories\n}\n\n// Payment Service (Payment Processing)\n@SpringBootApplication\npublic class PaymentServiceApplication {\n    // Payment processing, refunds\n}\n\n// Inventory Service (Stock Management)\n@SpringBootApplication\npublic class InventoryServiceApplication {\n    // Stock levels, reservations, updates\n}"
    },
    {
      "id": 31,
      "question": "What is Domain-Driven Design (DDD) and its role in Microservices?",
      "answer": "Domain-Driven Design is a software design approach that focuses on modeling software based on the business domain and its complexity.\n\nCore DDD Concepts:\n• Bounded Context: Explicit boundary for a model\n• Ubiquitous Language: Shared language between developers and domain experts\n• Aggregates: Cluster of objects treated as unit\n• Entities: Objects with unique identity\n• Value Objects: Immutable objects without identity\n• Domain Events: Something significant that happened\n\nDDD in Microservices:\n• Bounded contexts map to microservices\n• Each service has its own domain model\n• Clear service boundaries\n• Prevents model leakage\n• Reduces coupling\n\nStrategic Design:\n• Context mapping\n• Upstream/downstream relationships\n• Anti-corruption layer\n• Shared kernel\n• Customer-supplier pattern\n\nTactical Design:\n• Entities and value objects\n• Aggregates and repositories\n• Domain services\n• Application services\n• Domain events\n\nBenefits:\n• Better alignment with business\n• Clear service boundaries\n• Shared language reduces miscommunication\n• Easier to understand complex domains",
      "explanation": "DDD provides strategic design patterns like bounded contexts that naturally map to microservices boundaries, ensuring each service has a clear domain model aligned with business capabilities.",
      "difficulty": "Medium",
      "code": "// DDD Aggregate Example\n@Entity\npublic class Order {\n    @Id\n    private String orderId;\n    \n    @Embedded\n    private CustomerId customerId;\n    \n    @ElementCollection\n    private List<OrderLine> orderLines = new ArrayList<>();\n    \n    private OrderStatus status;\n    \n    // Aggregate root controls all modifications\n    public void addOrderLine(Product product, int quantity) {\n        if (status != OrderStatus.DRAFT) {\n            throw new IllegalStateException(\"Cannot modify confirmed order\");\n        }\n        orderLines.add(new OrderLine(product, quantity));\n    }\n    \n    public void confirm() {\n        if (orderLines.isEmpty()) {\n            throw new IllegalStateException(\"Cannot confirm empty order\");\n        }\n        this.status = OrderStatus.CONFIRMED;\n        // Publish domain event\n        DomainEventPublisher.publish(new OrderConfirmedEvent(this.orderId));\n    }\n}\n\n// Value Object\n@Embeddable\npublic class CustomerId {\n    private String value;\n    \n    public CustomerId(String value) {\n        if (value == null || value.isEmpty()) {\n            throw new IllegalArgumentException(\"Customer ID cannot be empty\");\n        }\n        this.value = value;\n    }\n    \n    // Immutable, equals based on value\n}"
    },
    {
      "id": 32,
      "question": "What is the Ambassador pattern?",
      "answer": "Ambassador pattern uses a helper service to proxy network requests and provide common connectivity features for client applications.\n\nCore Concept:\n• Ambassador acts as out-of-process proxy\n• Sits between client and remote service\n• Handles connectivity concerns\n• Language and framework agnostic\n\nAmbassador Responsibilities:\n• Monitoring and logging\n• Routing and load balancing\n• Retry logic\n• Circuit breaking\n• Authentication\n• Protocol translation\n• Connection pooling\n\nAmbassador vs Sidecar:\n• Ambassador: Client-side proxy\n• Sidecar: Can be client or server side\n• Ambassador focuses on connectivity\n• Sidecar has broader responsibilities\n\nUse Cases:\n• Legacy application modernization\n• Multi-language environments\n• Standardizing connectivity patterns\n• Adding features without code changes\n• Testing and debugging network issues\n\nBenefits:\n• Separates network concerns from business logic\n• Reusable across applications\n• No application code changes\n• Consistent behavior\n• Easier testing",
      "explanation": "Ambassador pattern deploys a proxy service alongside the client to handle connectivity concerns like retries, circuit breaking, and monitoring without changing application code.",
      "difficulty": "Hard",
      "code": "// Ambassador Pattern Implementation\n@Component\npublic class ServiceAmbassador {\n    \n    private final WebClient webClient;\n    private final CircuitBreaker circuitBreaker;\n    private final RetryPolicy retryPolicy;\n    \n    public ServiceAmbassador() {\n        this.webClient = WebClient.builder().build();\n        this.circuitBreaker = CircuitBreaker.ofDefaults(\"ambassador\");\n        this.retryPolicy = RetryPolicy.ofDefaults(\"ambassador\");\n    }\n    \n    public <T> T call(String url, Class<T> responseType) {\n        return Decorators.ofSupplier(() -> makeRequest(url, responseType))\n            .withCircuitBreaker(circuitBreaker)\n            .withRetry(retryPolicy)\n            .withFallback(Arrays.asList(Exception.class), \n                e -> getDefaultResponse(responseType))\n            .decorate()\n            .get();\n    }\n    \n    private <T> T makeRequest(String url, Class<T> responseType) {\n        // Add monitoring, logging\n        long start = System.currentTimeMillis();\n        try {\n            T response = webClient.get()\n                .uri(url)\n                .retrieve()\n                .bodyToMono(responseType)\n                .block();\n            logMetrics(url, System.currentTimeMillis() - start, true);\n            return response;\n        } catch (Exception e) {\n            logMetrics(url, System.currentTimeMillis() - start, false);\n            throw e;\n        }\n    }\n}"
    },
    {
      "id": 33,
      "question": "How do you handle configuration management in Microservices?",
      "answer": "Configuration management in Microservices requires centralized, dynamic, and environment-specific configuration handling.\n\nConfiguration Types:\n• Application configuration (port, threads)\n• Environment-specific (database URLs)\n• Feature flags\n• Secrets (passwords, API keys)\n• Business configuration\n\nConfiguration Strategies:\n• Centralized Configuration Server\n• Environment Variables\n• Configuration Files\n• Configuration as Code\n• Service Discovery Integration\n\nCentralized Configuration:\n• Single source of truth\n• Dynamic configuration updates\n• Version control\n• Environment separation\n• Audit trail\n\nTools:\n• Spring Cloud Config\n• Consul\n• etcd\n• AWS Systems Manager Parameter Store\n• Kubernetes ConfigMaps and Secrets\n\nBest Practices:\n• Externalize all configuration\n• Never hardcode values\n• Separate secrets from configuration\n• Encrypt sensitive data\n• Support hot reload when possible\n• Version configuration changes\n• Use profiles for environments\n• Provide sensible defaults",
      "explanation": "Microservices configuration management uses centralized servers or tools like Spring Cloud Config to externalize, version control, and dynamically update configuration across all services without redeployment.",
      "difficulty": "Medium",
      "code": "// Spring Cloud Config Server\n@SpringBootApplication\n@EnableConfigServer\npublic class ConfigServerApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(ConfigServerApplication.class, args);\n    }\n}\n\n// application.yml (Config Server)\nspring:\n  cloud:\n    config:\n      server:\n        git:\n          uri: https://github.com/myorg/config-repo\n          default-label: main\n\n// Config Client\n@SpringBootApplication\npublic class UserServiceApplication {\n    @Value(\"${app.max-connections}\")\n    private int maxConnections;\n    \n    @Value(\"${feature.new-ui.enabled}\")\n    private boolean newUiEnabled;\n}\n\n// bootstrap.yml (Config Client)\nspring:\n  application:\n    name: user-service\n  cloud:\n    config:\n      uri: http://config-server:8888\n      fail-fast: true\n\n// Git Repository Structure\n# config-repo/user-service.yml\napp:\n  max-connections: 100\nfeature:\n  new-ui:\n    enabled: false\n\n# config-repo/user-service-production.yml\napp:\n  max-connections: 500\nfeature:\n  new-ui:\n    enabled: true\n\n// Kubernetes ConfigMap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: user-service-config\ndata:\n  application.yml: |\n    app:\n      max-connections: 100\n    server:\n      port: 8080"
    },
    {
      "id": 34,
      "question": "What is the difference between orchestration and choreography in Microservices?",
      "answer": "Orchestration and choreography are two approaches to coordinate workflows across multiple microservices.\n\nOrchestration:\n• Central coordinator controls workflow\n• Orchestrator invokes services in sequence\n• Single point of coordination\n• Command-driven approach\n• Explicit workflow definition\n\nOrchestration Characteristics:\n• Centralized control\n• Clear workflow visibility\n• Easier to understand and debug\n• Single point of failure\n• Orchestrator knows all services\n• Tighter coupling to orchestrator\n\nChoreography:\n• Services coordinate through events\n• No central controller\n• Each service knows what to do on events\n• Event-driven approach\n• Distributed coordination\n\nChoreography Characteristics:\n• Decentralized control\n• Better service autonomy\n• More scalable\n• Complex to understand overall flow\n• No single point of failure\n• Looser coupling\n\nWhen to Use Orchestration:\n• Complex business processes\n• Need centralized monitoring\n• Transactional consistency required\n• Clear workflow steps\n\nWhen to Use Choreography:\n• Simple event flows\n• High scalability needed\n• Service independence important\n• Eventual consistency acceptable",
      "explanation": "Orchestration uses a central coordinator to control service interactions in a command-driven way, while choreography lets services react to events independently without central control, offering better decoupling.",
      "difficulty": "Medium",
      "code": "// Orchestration Example - Saga Orchestrator\n@Service\npublic class OrderOrchestrator {\n    \n    @Autowired\n    private InventoryService inventoryService;\n    @Autowired\n    private PaymentService paymentService;\n    @Autowired\n    private ShippingService shippingService;\n    \n    public OrderResult createOrder(OrderRequest request) {\n        try {\n            // Orchestrator controls the flow\n            String reservationId = inventoryService.reserve(request.getItems());\n            String paymentId = paymentService.charge(request.getPayment());\n            String shipmentId = shippingService.schedule(request.getAddress());\n            \n            return new OrderResult(OrderStatus.CONFIRMED, \n                reservationId, paymentId, shipmentId);\n        } catch (Exception e) {\n            // Orchestrator handles compensation\n            compensate(reservationId, paymentId);\n            return new OrderResult(OrderStatus.FAILED);\n        }\n    }\n}\n\n// Choreography Example - Event-Driven\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private EventPublisher eventPublisher;\n    \n    public void createOrder(OrderRequest request) {\n        Order order = new Order(request);\n        orderRepository.save(order);\n        \n        // Publish event, no direct service calls\n        eventPublisher.publish(new OrderCreatedEvent(order));\n    }\n}\n\n@Service\npublic class InventoryService {\n    \n    @EventListener\n    public void onOrderCreated(OrderCreatedEvent event) {\n        reserveInventory(event.getItems());\n        eventPublisher.publish(new InventoryReservedEvent(event.getOrderId()));\n    }\n}\n\n@Service\npublic class PaymentService {\n    \n    @EventListener\n    public void onInventoryReserved(InventoryReservedEvent event) {\n        processPayment(event.getOrderId());\n        eventPublisher.publish(new PaymentProcessedEvent(event.getOrderId()));\n    }\n}"
    },
    {
      "id": 35,
      "question": "What is Event Sourcing?",
      "answer": "Event Sourcing stores the state of an application as a sequence of events rather than storing just the current state.\n\nCore Concept:\n• All state changes captured as events\n• Events are immutable and append-only\n• Current state derived by replaying events\n• Events are the source of truth\n\nKey Components:\n• Event: Immutable fact about state change\n• Event Store: Database of events\n• Event Stream: Ordered sequence of events\n• Snapshots: Cached state for performance\n• Projections: Read models built from events\n\nBenefits:\n• Complete audit trail\n• Time travel (replay to any point)\n• Event replay for debugging\n• Support for CQRS naturally\n• Better for complex domains\n• Can derive multiple views from events\n\nChallenges:\n• Learning curve\n• Eventual consistency\n• Event versioning\n• Storage requirements\n• Query complexity\n• Cannot delete data easily\n\nWhen to Use:\n• Audit requirements\n• Complex domain logic\n• Temporal queries needed\n• Need to replay events\n• Undo/redo functionality",
      "explanation": "Event Sourcing stores all state changes as immutable events rather than current state, enabling complete audit trails, time travel, and deriving multiple views from the same event stream.",
      "difficulty": "Hard",
      "code": "// Event Sourcing Example\n// Events\npublic class OrderCreatedEvent {\n    private String orderId;\n    private String customerId;\n    private LocalDateTime timestamp;\n    // getters\n}\n\npublic class OrderConfirmedEvent {\n    private String orderId;\n    private LocalDateTime timestamp;\n}\n\npublic class OrderShippedEvent {\n    private String orderId;\n    private String trackingNumber;\n    private LocalDateTime timestamp;\n}\n\n// Aggregate\npublic class Order {\n    private String orderId;\n    private String customerId;\n    private OrderStatus status;\n    private List<DomainEvent> changes = new ArrayList<>();\n    \n    // Command handler\n    public void create(String orderId, String customerId) {\n        apply(new OrderCreatedEvent(orderId, customerId));\n    }\n    \n    public void confirm() {\n        if (status != OrderStatus.PENDING) {\n            throw new IllegalStateException(\"Cannot confirm non-pending order\");\n        }\n        apply(new OrderConfirmedEvent(orderId));\n    }\n    \n    // Event handlers\n    private void apply(OrderCreatedEvent event) {\n        this.orderId = event.getOrderId();\n        this.customerId = event.getCustomerId();\n        this.status = OrderStatus.PENDING;\n        changes.add(event);\n    }\n    \n    private void apply(OrderConfirmedEvent event) {\n        this.status = OrderStatus.CONFIRMED;\n        changes.add(event);\n    }\n    \n    // Rebuild from events\n    public static Order fromEvents(List<DomainEvent> events) {\n        Order order = new Order();\n        events.forEach(order::applyEvent);\n        return order;\n    }\n}\n\n// Event Store\n@Repository\npublic class EventStore {\n    \n    public void save(String aggregateId, List<DomainEvent> events) {\n        events.forEach(event -> {\n            EventRecord record = new EventRecord(\n                aggregateId,\n                event.getClass().getName(),\n                serialize(event),\n                LocalDateTime.now()\n            );\n            repository.save(record);\n        });\n    }\n    \n    public List<DomainEvent> getEvents(String aggregateId) {\n        return repository.findByAggregateId(aggregateId)\n            .stream()\n            .map(this::deserialize)\n            .collect(Collectors.toList());\n    }\n}"
    },
    {
      "id": 36,
      "question": "What is the Anti-Corruption Layer pattern?",
      "answer": "Anti-Corruption Layer (ACL) is a pattern that isolates a system from legacy systems or external services by creating a translation layer.\n\nPurpose:\n• Protect domain model from external influences\n• Translate between different domain models\n• Prevent legacy code pollution\n• Enable gradual migration\n• Maintain clean architecture\n\nImplementation:\n• Facade: Simplifies complex interfaces\n• Adapter: Translates interfaces\n• Translator: Converts data models\n• Gateway: Single point of access\n\nWhen to Use:\n• Integrating with legacy systems\n• Working with third-party services\n• Different domain models\n• Strangler fig migration\n• Preventing model contamination\n\nBenefits:\n• Protects bounded context\n• Reduces coupling\n• Easier to replace external system\n• Domain model stays pure\n• Testable in isolation\n\nChallenges:\n• Additional complexity\n• Performance overhead\n• Maintenance of translation layer\n• Keeping translations in sync",
      "explanation": "Anti-Corruption Layer creates a translation boundary between systems with different domain models, protecting the core domain from external influences and enabling clean integration with legacy or third-party systems.",
      "difficulty": "Hard",
      "code": "// Anti-Corruption Layer Example\n// Legacy System DTOs\npublic class LegacyCustomer {\n    public String cust_id;\n    public String cust_nm;\n    public String addr_ln1;\n    public String addr_ln2;\n    // Old naming conventions\n}\n\n// Domain Model\npublic class Customer {\n    private CustomerId id;\n    private String name;\n    private Address address;\n    // Clean domain model\n}\n\n// Anti-Corruption Layer - Translator\n@Component\npublic class CustomerTranslator {\n    \n    public Customer toDomain(LegacyCustomer legacy) {\n        Address address = new Address(\n            legacy.addr_ln1,\n            legacy.addr_ln2\n        );\n        \n        return new Customer(\n            new CustomerId(legacy.cust_id),\n            legacy.cust_nm,\n            address\n        );\n    }\n    \n    public LegacyCustomer toLegacy(Customer domain) {\n        LegacyCustomer legacy = new LegacyCustomer();\n        legacy.cust_id = domain.getId().getValue();\n        legacy.cust_nm = domain.getName();\n        legacy.addr_ln1 = domain.getAddress().getLine1();\n        legacy.addr_ln2 = domain.getAddress().getLine2();\n        return legacy;\n    }\n}\n\n// Anti-Corruption Layer - Facade\n@Service\npublic class CustomerService {\n    \n    @Autowired\n    private LegacyCustomerClient legacyClient;\n    @Autowired\n    private CustomerTranslator translator;\n    \n    public Customer getCustomer(CustomerId id) {\n        // Call legacy system\n        LegacyCustomer legacy = legacyClient.getCustomer(id.getValue());\n        // Translate to domain model\n        return translator.toDomain(legacy);\n    }\n    \n    public void updateCustomer(Customer customer) {\n        // Translate to legacy format\n        LegacyCustomer legacy = translator.toLegacy(customer);\n        // Call legacy system\n        legacyClient.updateCustomer(legacy);\n    }\n}"
    },
    {
      "id": 37,
      "question": "How do you implement caching in Microservices?",
      "answer": "Caching in Microservices improves performance by storing frequently accessed data closer to the application.\n\nCaching Strategies:\n• Client-Side Caching: Cache in client application\n• Server-Side Caching: Cache in service\n• Gateway Caching: Cache at API Gateway\n• Distributed Caching: Shared cache across services\n• Database Caching: Cache at database level\n\nCache Patterns:\n• Cache-Aside: Application manages cache\n• Read-Through: Cache loads data automatically\n• Write-Through: Cache updates with database\n• Write-Behind: Asynchronous cache updates\n• Refresh-Ahead: Proactive cache refresh\n\nDistributed Cache Solutions:\n• Redis\n• Memcached\n• Hazelcast\n• Apache Ignite\n\nBest Practices:\n• Cache immutable data when possible\n• Set appropriate TTL (Time To Live)\n• Use cache keys wisely\n• Handle cache misses gracefully\n• Invalidate stale data\n• Monitor cache hit rates\n• Consider cache warming\n• Avoid caching sensitive data\n\nCache Invalidation:\n• Time-based expiration\n• Event-based invalidation\n• Manual invalidation\n• LRU (Least Recently Used)",
      "explanation": "Microservices caching uses strategies like cache-aside or read-through with distributed caches like Redis to improve performance by storing frequently accessed data with proper TTL and invalidation mechanisms.",
      "difficulty": "Medium",
      "code": "// Spring Boot with Redis Caching\n@Configuration\n@EnableCaching\npublic class CacheConfig {\n    \n    @Bean\n    public RedisCacheManager cacheManager(RedisConnectionFactory factory) {\n        RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig()\n            .entryTtl(Duration.ofMinutes(10))\n            .serializeKeysWith(RedisSerializationContext.SerializationPair\n                .fromSerializer(new StringRedisSerializer()))\n            .serializeValuesWith(RedisSerializationContext.SerializationPair\n                .fromSerializer(new GenericJackson2JsonRedisSerializer()));\n        \n        return RedisCacheManager.builder(factory)\n            .cacheDefaults(config)\n            .build();\n    }\n}\n\n@Service\npublic class ProductService {\n    \n    @Cacheable(value = \"products\", key = \"#id\")\n    public Product getProduct(String id) {\n        // This method called only on cache miss\n        return productRepository.findById(id)\n            .orElseThrow(() -> new ProductNotFoundException(id));\n    }\n    \n    @CachePut(value = \"products\", key = \"#product.id\")\n    public Product updateProduct(Product product) {\n        return productRepository.save(product);\n    }\n    \n    @CacheEvict(value = \"products\", key = \"#id\")\n    public void deleteProduct(String id) {\n        productRepository.deleteById(id);\n    }\n    \n    @CacheEvict(value = \"products\", allEntries = true)\n    public void clearCache() {\n        // Clear all products from cache\n    }\n}\n\n// application.yml\nspring:\n  redis:\n    host: localhost\n    port: 6379\n  cache:\n    type: redis\n    redis:\n      time-to-live: 600000"
    },
    {
      "id": 38,
      "question": "What is the Outbox pattern?",
      "answer": "Outbox pattern ensures reliable event publishing by storing events in a database table before publishing them to a message broker.\n\nProblem It Solves:\n• Dual write problem (database + message broker)\n• Ensures atomicity between state change and event\n• Prevents data inconsistency\n• Handles message broker failures\n\nHow It Works:\n• Store business data and events in same transaction\n• Background process reads outbox table\n• Publishes events to message broker\n• Marks events as published\n• Handles failures and retries\n\nImplementation Steps:\n• Create outbox table in database\n• Insert event in same transaction as business data\n• Polling or Change Data Capture (CDC) to read events\n• Publish to message broker\n• Update or delete published events\n\nBenefits:\n• Guaranteed event delivery\n• Transactional consistency\n• No lost events\n• Resilient to broker failures\n\nChallenges:\n• Additional table to manage\n• Polling overhead\n• Event ordering considerations\n• Duplicate event handling\n\nAlternatives:\n• Transaction Log Tailing (CDC)\n• Event Sourcing\n• Transactional Messaging",
      "explanation": "Outbox pattern guarantees reliable event publishing by storing events in a database table within the same transaction as business data, then publishing them asynchronously to avoid dual write problems.",
      "difficulty": "Hard",
      "code": "// Outbox Pattern Implementation\n@Entity\n@Table(name = \"outbox\")\npublic class OutboxEvent {\n    @Id\n    private String id;\n    private String aggregateId;\n    private String eventType;\n    private String payload;\n    private LocalDateTime createdAt;\n    private boolean published;\n}\n\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private OrderRepository orderRepository;\n    @Autowired\n    private OutboxRepository outboxRepository;\n    \n    @Transactional\n    public void createOrder(OrderRequest request) {\n        // 1. Save business entity\n        Order order = new Order(request);\n        orderRepository.save(order);\n        \n        // 2. Save event to outbox in same transaction\n        OutboxEvent event = new OutboxEvent(\n            UUID.randomUUID().toString(),\n            order.getId(),\n            \"OrderCreated\",\n            serializeEvent(new OrderCreatedEvent(order)),\n            LocalDateTime.now(),\n            false\n        );\n        outboxRepository.save(event);\n        \n        // Both saved atomically or both rolled back\n    }\n}\n\n// Outbox Publisher - Background Process\n@Component\npublic class OutboxPublisher {\n    \n    @Autowired\n    private OutboxRepository outboxRepository;\n    @Autowired\n    private EventPublisher eventPublisher;\n    \n    @Scheduled(fixedDelay = 1000)\n    @Transactional\n    public void publishEvents() {\n        List<OutboxEvent> unpublished = outboxRepository\n            .findByPublishedFalse(PageRequest.of(0, 100));\n        \n        for (OutboxEvent event : unpublished) {\n            try {\n                // Publish to message broker\n                eventPublisher.publish(\n                    event.getEventType(),\n                    event.getPayload()\n                );\n                \n                // Mark as published\n                event.setPublished(true);\n                outboxRepository.save(event);\n            } catch (Exception e) {\n                // Log and retry later\n                log.error(\"Failed to publish event: {}\", event.getId(), e);\n            }\n        }\n    }\n    \n    @Scheduled(cron = \"0 0 * * * *\")\n    public void cleanupPublishedEvents() {\n        // Delete events older than 7 days\n        LocalDateTime cutoff = LocalDateTime.now().minusDays(7);\n        outboxRepository.deleteByPublishedTrueAndCreatedAtBefore(cutoff);\n    }\n}"
    },
    {
      "id": 39,
      "question": "What is gRPC and when should you use it?",
      "answer": "gRPC is a high-performance RPC framework that uses Protocol Buffers for serialization and HTTP/2 for transport.\n\nKey Features:\n• Binary protocol (Protocol Buffers)\n• HTTP/2 for transport\n• Strongly typed contracts\n• Multiple language support\n• Bidirectional streaming\n• Built-in load balancing\n• Pluggable authentication\n\ngRPC vs REST:\n• Performance: gRPC faster due to binary format\n• Contract: gRPC strongly typed, REST flexible\n• Streaming: gRPC supports streaming, REST limited\n• Browser Support: REST better, gRPC needs proxy\n• Human Readability: REST easier, gRPC binary\n\nCommunication Patterns:\n• Unary: Single request-response\n• Server Streaming: One request, stream of responses\n• Client Streaming: Stream of requests, one response\n• Bidirectional Streaming: Both stream\n\nWhen to Use gRPC:\n• High-performance requirements\n• Internal service communication\n• Real-time streaming needed\n• Polyglot microservices\n• Strong contract enforcement\n\nWhen to Use REST:\n• Public APIs\n• Browser clients\n• Simple CRUD operations\n• Human-readable format needed",
      "explanation": "gRPC is a high-performance RPC framework using Protocol Buffers and HTTP/2, ideal for internal microservice communication requiring speed, streaming, and strong contracts, though less suitable for browser clients.",
      "difficulty": "Medium",
      "code": "// Protocol Buffer Definition\n// user.proto\nsyntax = \"proto3\";\n\npackage user;\n\nservice UserService {\n  rpc GetUser(GetUserRequest) returns (User);\n  rpc ListUsers(ListUsersRequest) returns (stream User);\n  rpc CreateUser(CreateUserRequest) returns (User);\n}\n\nmessage User {\n  string id = 1;\n  string name = 2;\n  string email = 3;\n}\n\nmessage GetUserRequest {\n  string id = 1;\n}\n\nmessage ListUsersRequest {\n  int32 page_size = 1;\n  string page_token = 2;\n}\n\nmessage CreateUserRequest {\n  string name = 1;\n  string email = 2;\n}\n\n// gRPC Server Implementation\n@GrpcService\npublic class UserServiceImpl extends UserServiceGrpc.UserServiceImplBase {\n    \n    @Autowired\n    private UserRepository userRepository;\n    \n    @Override\n    public void getUser(GetUserRequest request, \n                       StreamObserver<User> responseObserver) {\n        User user = userRepository.findById(request.getId())\n            .map(this::toProto)\n            .orElse(User.getDefaultInstance());\n        \n        responseObserver.onNext(user);\n        responseObserver.onCompleted();\n    }\n    \n    @Override\n    public void listUsers(ListUsersRequest request,\n                         StreamObserver<User> responseObserver) {\n        userRepository.findAll().forEach(user -> {\n            responseObserver.onNext(toProto(user));\n        });\n        responseObserver.onCompleted();\n    }\n}\n\n// gRPC Client\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private UserServiceGrpc.UserServiceBlockingStub userServiceStub;\n    \n    public void createOrder(OrderRequest request) {\n        User user = userServiceStub.getUser(\n            GetUserRequest.newBuilder()\n                .setId(request.getUserId())\n                .build()\n        );\n        // Use user info\n    }\n}"
    },
    {
      "id": 40,
      "question": "How do you handle data consistency across Microservices?",
      "answer": "Data consistency in Microservices is challenging due to distributed nature and lack of ACID transactions across services.\n\nConsistency Models:\n• Strong Consistency: Immediate consistency (hard to achieve)\n• Eventual Consistency: Temporary inconsistency acceptable\n• Causal Consistency: Maintains cause-effect relationships\n\nPatterns for Consistency:\n• Saga Pattern: Coordinated local transactions\n• Event Sourcing: Events as source of truth\n• CQRS: Separate read/write models\n• Two-Phase Commit: Distributed transaction (not recommended)\n• Outbox Pattern: Reliable event publishing\n\nEventual Consistency Strategies:\n• Compensating transactions\n• Retry mechanisms\n• Idempotent operations\n• Reconciliation processes\n• Event-driven synchronization\n\nBest Practices:\n• Design for eventual consistency\n• Make operations idempotent\n• Use correlation IDs\n• Implement compensation logic\n• Monitor consistency violations\n• Provide user feedback on async operations\n• Use optimistic locking\n• Version your data\n\nData Duplication:\n• Accept some denormalization\n• Event-driven updates\n• Cache invalidation\n• Background synchronization",
      "explanation": "Microservices achieve data consistency through eventual consistency patterns like Saga and Event Sourcing, accepting temporary inconsistency while ensuring all services eventually reach the same state through coordinated transactions or events.",
      "difficulty": "Hard",
      "code": "// Eventual Consistency with Events\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private EventPublisher eventPublisher;\n    \n    @Transactional\n    public void createOrder(OrderRequest request) {\n        // Local transaction\n        Order order = new Order(request);\n        orderRepository.save(order);\n        \n        // Publish event for other services\n        eventPublisher.publish(new OrderCreatedEvent(\n            order.getId(),\n            order.getCustomerId(),\n            order.getItems()\n        ));\n    }\n}\n\n// Inventory Service maintains its own copy\n@Service\npublic class InventoryService {\n    \n    @EventListener\n    @Transactional\n    public void onOrderCreated(OrderCreatedEvent event) {\n        try {\n            // Update local state\n            reserveInventory(event.getItems());\n            \n            // Publish success event\n            eventPublisher.publish(\n                new InventoryReservedEvent(event.getOrderId())\n            );\n        } catch (InsufficientInventoryException e) {\n            // Publish failure event for compensation\n            eventPublisher.publish(\n                new InventoryReservationFailedEvent(event.getOrderId())\n            );\n        }\n    }\n}\n\n// Idempotent Event Handler\n@Service\npublic class PaymentService {\n    \n    @EventListener\n    @Transactional\n    public void onInventoryReserved(InventoryReservedEvent event) {\n        // Check if already processed (idempotency)\n        if (processedEvents.contains(event.getEventId())) {\n            return;\n        }\n        \n        processPayment(event.getOrderId());\n        processedEvents.add(event.getEventId());\n    }\n}"
    },
    {
      "id": 41,
      "question": "What is the Twelve-Factor App methodology for Microservices?",
      "answer": "The Twelve-Factor App is a methodology for building modern, scalable, and maintainable applications, especially relevant for microservices.\n\nThe 12 Factors:\n• Codebase: One codebase per service, tracked in version control\n• Dependencies: Explicitly declare and isolate dependencies\n• Config: Store configuration in environment variables\n• Backing Services: Treat as attached resources\n• Build, Release, Run: Strictly separate build and run stages\n• Processes: Execute as stateless processes\n• Port Binding: Export services via port binding\n• Concurrency: Scale out via process model\n• Disposability: Fast startup and graceful shutdown\n• Dev/Prod Parity: Keep environments similar\n• Logs: Treat logs as event streams\n• Admin Processes: Run admin tasks as one-off processes\n\nKey Principles:\n• Stateless services\n• Externalized configuration\n• Environment parity\n• Horizontal scalability\n• Disposable processes\n\nBenefits:\n• Cloud-native ready\n• Easy to scale\n• Portable across platforms\n• Continuous deployment friendly",
      "explanation": "Twelve-Factor App methodology provides best practices for building cloud-native microservices, emphasizing statelessness, externalized config, environment parity, and horizontal scalability for maintainable distributed systems.",
      "difficulty": "Medium"
    },
    {
      "id": 42,
      "question": "How do you implement blue-green deployment for Microservices?",
      "answer": "Blue-green deployment maintains two identical production environments to enable zero-downtime deployments with instant rollback capability.\n\nDeployment Process:\n• Blue: Current production environment\n• Green: New version deployment\n• Deploy to green while blue serves traffic\n• Test green environment\n• Switch traffic from blue to green\n• Keep blue as backup for rollback\n\nImplementation Approaches:\n• Load Balancer Switch: Change LB routing\n• DNS Switch: Update DNS records\n• Service Mesh: Update routing rules\n• API Gateway: Update route configuration\n\nBenefits:\n• Zero downtime deployment\n• Instant rollback capability\n• Test in production environment\n• Reduced deployment risk\n• Clear success/failure state\n\nChallenges:\n• Double infrastructure cost\n• Database migration complexity\n• Session handling during switch\n• Resource requirements\n\nBest Practices:\n• Automated health checks\n• Smoke tests before switch\n• Gradual traffic shifting\n• Monitor both environments\n• Database backward compatibility",
      "explanation": "Blue-green deployment runs two identical environments, deploying new versions to the inactive environment and switching traffic instantly, providing zero-downtime updates with easy rollback capability.",
      "difficulty": "Medium",
      "code": "// Kubernetes Blue-Green Deployment\n# Blue Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service-blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-service\n      version: blue\n  template:\n    metadata:\n      labels:\n        app: user-service\n        version: blue\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:1.0\n---\n# Green Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service-green\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-service\n      version: green\n  template:\n    metadata:\n      labels:\n        app: user-service\n        version: green\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:2.0\n---\n# Service (switch by updating selector)\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\nspec:\n  selector:\n    app: user-service\n    version: blue  # Change to 'green' to switch\n  ports:\n  - port: 80\n    targetPort: 8080\n\n# Switch Traffic Script\nkubectl patch service user-service -p '{\"spec\":{\"selector\":{\"version\":\"green\"}}}'\n\n# Rollback if needed\nkubectl patch service user-service -p '{\"spec\":{\"selector\":{\"version\":\"blue\"}}}'"
    },
    {
      "id": 43,
      "question": "What is canary deployment and how does it work?",
      "answer": "Canary deployment gradually rolls out changes to a small subset of users before full deployment, reducing risk by testing in production.\n\nDeployment Process:\n• Deploy new version alongside old version\n• Route small percentage of traffic to new version\n• Monitor metrics and errors\n• Gradually increase traffic if successful\n• Rollback if issues detected\n• Complete rollout when validated\n\nTraffic Distribution:\n• 5% to canary initially\n• Monitor for period (hours/days)\n• Increase to 25%, 50%, 100%\n• Or rollback at any stage\n\nMonitoring Metrics:\n• Error rates\n• Response times\n• Business metrics\n• Resource usage\n• User feedback\n\nBenefits:\n• Lower risk than full deployment\n• Real user testing\n• Early issue detection\n• Minimal impact if problems\n• Can test with specific users\n\nImplementation:\n• Service mesh (Istio, Linkerd)\n• API Gateway routing\n• Feature flags\n• Load balancer rules\n\nChallenges:\n• Complex routing logic\n• Need robust monitoring\n• Longer deployment time\n• Database compatibility",
      "explanation": "Canary deployment gradually shifts traffic from old to new version, starting with a small percentage, allowing real-world validation with minimal risk and easy rollback if issues arise.",
      "difficulty": "Medium",
      "code": "// Istio Canary Deployment\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: user-service\nspec:\n  hosts:\n  - user-service\n  http:\n  - match:\n    - headers:\n        user-type:\n          exact: beta-tester\n    route:\n    - destination:\n        host: user-service\n        subset: v2\n  - route:\n    - destination:\n        host: user-service\n        subset: v1\n      weight: 90\n    - destination:\n        host: user-service\n        subset: v2\n      weight: 10\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: user-service\nspec:\n  host: user-service\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n\n# Gradually increase traffic to v2\n# Phase 1: 10%\n# Phase 2: 25%\n# Phase 3: 50%\n# Phase 4: 100%\n\n# Spring Boot Feature Flag Alternative\n@Service\npublic class FeatureToggleService {\n    \n    public boolean isNewFeatureEnabled(String userId) {\n        // Enable for 10% of users\n        int hash = Math.abs(userId.hashCode());\n        return hash % 100 < 10;\n    }\n}"
    },
    {
      "id": 44,
      "question": "What is API composition pattern?",
      "answer": "API Composition pattern aggregates data from multiple services by invoking them and combining results, typically used in API Gateway or BFF.\n\nHow It Works:\n• Client makes single request\n• Composer calls multiple services\n• Results aggregated and transformed\n• Single response returned to client\n\nImplementation Options:\n• API Gateway aggregation\n• Backend for Frontend (BFF)\n• Dedicated aggregator service\n• Client-side composition (not recommended)\n\nBenefits:\n• Reduced client complexity\n• Fewer round trips from client\n• Consistent data format\n• Centralized transformation logic\n• Better performance for clients\n\nChallenges:\n• Increased coupling in composer\n• Performance bottleneck\n• Error handling complexity\n• Partial failure scenarios\n• Transaction boundaries\n\nBest Practices:\n• Parallel service calls when possible\n• Set appropriate timeouts\n• Handle partial failures gracefully\n• Cache when appropriate\n• Use circuit breakers\n• Consider CQRS for complex queries",
      "explanation": "API Composition pattern combines data from multiple microservices in a single endpoint, reducing client complexity and network calls by aggregating and transforming results before returning to client.",
      "difficulty": "Medium",
      "code": "// API Composition in API Gateway\n@RestController\n@RequestMapping(\"/api/composite\")\npublic class CompositeController {\n    \n    @Autowired\n    private WebClient webClient;\n    \n    @GetMapping(\"/order/{orderId}\")\n    public Mono<OrderDetails> getOrderDetails(@PathVariable String orderId) {\n        // Parallel calls to multiple services\n        Mono<Order> orderMono = webClient.get()\n            .uri(\"http://order-service/orders/{id}\", orderId)\n            .retrieve()\n            .bodyToMono(Order.class);\n            \n        Mono<Customer> customerMono = orderMono.flatMap(order ->\n            webClient.get()\n                .uri(\"http://customer-service/customers/{id}\", order.getCustomerId())\n                .retrieve()\n                .bodyToMono(Customer.class)\n        );\n        \n        Mono<List<Product>> productsMono = orderMono.flatMap(order -> {\n            List<String> productIds = order.getItems()\n                .stream()\n                .map(OrderItem::getProductId)\n                .collect(Collectors.toList());\n                \n            return Flux.fromIterable(productIds)\n                .flatMap(id -> webClient.get()\n                    .uri(\"http://product-service/products/{id}\", id)\n                    .retrieve()\n                    .bodyToMono(Product.class))\n                .collectList();\n        });\n        \n        // Combine results\n        return Mono.zip(orderMono, customerMono, productsMono)\n            .map(tuple -> new OrderDetails(\n                tuple.getT1(),  // Order\n                tuple.getT2(),  // Customer\n                tuple.getT3()   // Products\n            ));\n    }\n}\n\npublic class OrderDetails {\n    private Order order;\n    private Customer customer;\n    private List<Product> products;\n    // Constructor, getters\n}"
    },
    {
      "id": 45,
      "question": "What are idempotent operations and why are they important?",
      "answer": "Idempotent operations produce the same result regardless of how many times they are executed, critical for reliability in distributed systems.\n\nIdempotent Examples:\n• GET requests (read operations)\n• PUT with specific ID (update to specific state)\n• DELETE specific resource\n• Setting absolute values\n\nNon-Idempotent Examples:\n• POST creating resources with auto-generated IDs\n• Operations incrementing counters\n• Appending to lists\n• Operations with side effects\n\nWhy Important:\n• Safe to retry failed operations\n• Handle duplicate messages\n• Network reliability (retries)\n• At-least-once delivery semantics\n• Simplifies error recovery\n\nImplementation Strategies:\n• Use unique request IDs\n• Store processed request IDs\n• Use optimistic locking with versions\n• Design operations as state transitions\n• Use upsert instead of insert\n• Implement idempotency keys\n\nIdempotency Key Pattern:\n• Client generates unique key\n• Server stores key with result\n• Duplicate requests return cached result\n• Keys expire after period",
      "explanation": "Idempotent operations produce the same result when executed multiple times, essential for safe retries and handling duplicate messages in distributed microservices where network failures and at-least-once delivery are common.",
      "difficulty": "Medium",
      "code": "// Idempotency Key Implementation\n@Entity\npublic class IdempotencyRecord {\n    @Id\n    private String idempotencyKey;\n    private String result;\n    private LocalDateTime createdAt;\n    private boolean completed;\n}\n\n@Service\npublic class PaymentService {\n    \n    @Autowired\n    private IdempotencyRepository idempotencyRepo;\n    \n    @Transactional\n    public PaymentResult processPayment(\n            String idempotencyKey, \n            PaymentRequest request) {\n        \n        // Check if already processed\n        Optional<IdempotencyRecord> existing = \n            idempotencyRepo.findById(idempotencyKey);\n            \n        if (existing.isPresent()) {\n            if (existing.get().isCompleted()) {\n                // Return cached result\n                return deserialize(existing.get().getResult());\n            } else {\n                // Previous attempt in progress\n                throw new DuplicateRequestException();\n            }\n        }\n        \n        // Create idempotency record\n        IdempotencyRecord record = new IdempotencyRecord(\n            idempotencyKey,\n            null,\n            LocalDateTime.now(),\n            false\n        );\n        idempotencyRepo.save(record);\n        \n        try {\n            // Process payment\n            PaymentResult result = chargePayment(request);\n            \n            // Update record with result\n            record.setResult(serialize(result));\n            record.setCompleted(true);\n            idempotencyRepo.save(record);\n            \n            return result;\n        } catch (Exception e) {\n            // Delete record to allow retry\n            idempotencyRepo.delete(record);\n            throw e;\n        }\n    }\n}\n\n// Idempotent Update with Version\n@Entity\npublic class Account {\n    @Id\n    private String id;\n    private BigDecimal balance;\n    @Version\n    private Long version;\n    \n    public void setBalance(BigDecimal newBalance) {\n        this.balance = newBalance;  // Idempotent - sets absolute value\n    }\n    \n    public void credit(BigDecimal amount) {\n        this.balance = this.balance.add(amount);  // Not idempotent\n    }\n}"
    },
    {
      "id": 46,
      "question": "What is the difference between synchronous and asynchronous communication?",
      "answer": "Synchronous and asynchronous communication represent different approaches to service interaction in microservices.\n\nSynchronous Communication:\n• Caller waits for response\n• Blocking operation\n• Request-response pattern\n• Immediate result\n• Examples: REST, gRPC\n\nSynchronous Characteristics:\n• Tight temporal coupling\n• Easier to understand and debug\n• Simpler error handling\n• Lower latency for single request\n• Caller blocked during wait\n\nAsynchronous Communication:\n• Caller doesn't wait for response\n• Non-blocking operation\n• Fire-and-forget or callback pattern\n• Eventual result\n• Examples: Message queues, events\n\nAsynchronous Characteristics:\n• Loose temporal coupling\n• Better scalability\n• More resilient to failures\n• Complex error handling\n• Higher throughput\n• Requires message broker\n\nWhen to Use Synchronous:\n• Need immediate response\n• Simple request-response\n• Data consistency critical\n• Simpler flow\n\nWhen to Use Asynchronous:\n• High scalability needed\n• Long-running operations\n• Event notifications\n• Decoupled services",
      "explanation": "Synchronous communication blocks waiting for response with immediate results, while asynchronous communication is non-blocking with eventual results, offering better scalability and resilience at the cost of complexity.",
      "difficulty": "Easy",
      "code": "// Synchronous Communication\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private RestTemplate restTemplate;\n    \n    public Order createOrder(OrderRequest request) {\n        // Synchronous - blocks until response received\n        User user = restTemplate.getForObject(\n            \"http://user-service/users/\" + request.getUserId(),\n            User.class\n        );\n        \n        Inventory inventory = restTemplate.postForObject(\n            \"http://inventory-service/reserve\",\n            request.getItems(),\n            Inventory.class\n        );\n        \n        return new Order(user, inventory);\n    }\n}\n\n// Asynchronous Communication\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n    \n    public void createOrder(OrderRequest request) {\n        // Save order\n        Order order = orderRepository.save(new Order(request));\n        \n        // Publish event - non-blocking\n        OrderCreatedEvent event = new OrderCreatedEvent(order);\n        rabbitTemplate.convertAndSend(\"order.exchange\", \"order.created\", event);\n        \n        // Returns immediately without waiting\n    }\n}\n\n@Service\npublic class InventoryService {\n    \n    @RabbitListener(queues = \"inventory.queue\")\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        // Process asynchronously\n        reserveInventory(event.getItems());\n    }\n}"
    },
    {
      "id": 47,
      "question": "What is the Shared Database anti-pattern?",
      "answer": "Shared Database anti-pattern occurs when multiple microservices directly access the same database, violating service independence.\n\nWhy It's an Anti-Pattern:\n• Tight coupling between services\n• No clear ownership of data\n• Schema changes affect multiple services\n• Difficult to scale independently\n• Can't use different database technologies\n• Breaks microservices principles\n\nProblems Created:\n• Change ripple effects\n• Deployment dependencies\n• Runtime coupling through database\n• Performance bottlenecks\n• Difficult to maintain\n• Testing complexity\n\nCorrect Approach:\n• Database per service pattern\n• Services own their data\n• Communication through APIs\n• Data duplication when needed\n• Event-driven synchronization\n\nMigration Strategies:\n• Identify data ownership\n• Create service-specific schemas\n• Add API layers\n• Gradual data migration\n• Implement data synchronization\n• Use strangler fig pattern\n\nExceptions:\n• Read-only shared reference data\n• Temporary during migration\n• With proper access control",
      "explanation": "Shared Database anti-pattern violates microservices principles by creating tight coupling when multiple services directly access the same database, preventing independent scaling, deployment, and technology choices.",
      "difficulty": "Easy",
      "code": "// Anti-Pattern: Shared Database\n@Service\npublic class OrderService {\n    @Autowired\n    private JdbcTemplate jdbcTemplate;\n    \n    public void createOrder(Order order) {\n        // Direct database access\n        jdbcTemplate.update(\n            \"INSERT INTO orders VALUES (?, ?, ?)\",\n            order.getId(), order.getUserId(), order.getTotal()\n        );\n    }\n}\n\n@Service\npublic class ShippingService {\n    @Autowired\n    private JdbcTemplate jdbcTemplate;  // Same database!\n    \n    public void shipOrder(String orderId) {\n        // Directly accessing orders table\n        jdbcTemplate.update(\n            \"UPDATE orders SET status = 'SHIPPED' WHERE id = ?\",\n            orderId\n        );\n    }\n}\n\n// Correct Pattern: Database per Service\n@Service\npublic class OrderService {\n    @Autowired\n    private OrderRepository orderRepository;  // Own database\n    @Autowired\n    private EventPublisher eventPublisher;\n    \n    public void createOrder(Order order) {\n        orderRepository.save(order);\n        eventPublisher.publish(new OrderCreatedEvent(order));\n    }\n}\n\n@Service\npublic class ShippingService {\n    @Autowired\n    private ShipmentRepository shipmentRepository;  // Own database\n    \n    @EventListener\n    public void onOrderCreated(OrderCreatedEvent event) {\n        // Create shipment in own database\n        Shipment shipment = new Shipment(event.getOrderId());\n        shipmentRepository.save(shipment);\n    }\n    \n    public void updateShipmentStatus(String shipmentId) {\n        // Update own data\n        shipmentRepository.updateStatus(shipmentId, \"SHIPPED\");\n    }\n}"
    },
    {
      "id": 48,
      "question": "What is API Gateway aggregation pattern?",
      "answer": "API Gateway aggregation pattern combines multiple service calls into a single client request, reducing network overhead and simplifying client code.\n\nAggregation Types:\n• Sequential Aggregation: Call services one after another\n• Parallel Aggregation: Call services concurrently\n• Conditional Aggregation: Based on responses\n• Hierarchical Aggregation: Nested service calls\n\nResponsibilities:\n• Route requests to multiple services\n• Aggregate responses\n• Transform data formats\n• Handle partial failures\n• Implement timeouts\n• Apply circuit breakers\n\nBenefits:\n• Reduced client complexity\n• Fewer network round trips\n• Optimized for client needs\n• Centralized error handling\n• Better mobile performance\n• Can cache aggregated results\n\nChallenges:\n• Gateway becomes complex\n• Performance bottleneck risk\n• Additional latency\n• Error handling complexity\n• Testing difficulties\n\nAlternatives:\n• Backend for Frontend (BFF)\n• Client-side aggregation\n• GraphQL\n• CQRS read models",
      "explanation": "API Gateway aggregation combines data from multiple microservices into a single response, reducing client complexity and network calls while providing optimized data tailored to client needs.",
      "difficulty": "Medium",
      "code": "// Sequential Aggregation\n@RestController\npublic class AggregationController {\n    \n    @Autowired\n    private WebClient webClient;\n    \n    @GetMapping(\"/user-profile/{userId}\")\n    public UserProfile getUserProfile(@PathVariable String userId) {\n        // 1. Get user\n        User user = webClient.get()\n            .uri(\"http://user-service/users/{id}\", userId)\n            .retrieve()\n            .bodyToMono(User.class)\n            .block();\n        \n        // 2. Get orders (depends on user)\n        List<Order> orders = webClient.get()\n            .uri(\"http://order-service/orders?userId={id}\", userId)\n            .retrieve()\n            .bodyToFlux(Order.class)\n            .collectList()\n            .block();\n        \n        // 3. Aggregate\n        return new UserProfile(user, orders);\n    }\n}\n\n// Parallel Aggregation with CompletableFuture\n@RestController\npublic class ParallelAggregationController {\n    \n    @GetMapping(\"/dashboard/{userId}\")\n    public Dashboard getDashboard(@PathVariable String userId) {\n        \n        CompletableFuture<User> userFuture = CompletableFuture.supplyAsync(\n            () -> userService.getUser(userId)\n        );\n        \n        CompletableFuture<List<Order>> ordersFuture = CompletableFuture.supplyAsync(\n            () -> orderService.getUserOrders(userId)\n        );\n        \n        CompletableFuture<Wallet> walletFuture = CompletableFuture.supplyAsync(\n            () -> walletService.getWallet(userId)\n        );\n        \n        CompletableFuture<List<Notification>> notificationsFuture = \n            CompletableFuture.supplyAsync(\n                () -> notificationService.getNotifications(userId)\n            );\n        \n        // Wait for all and combine\n        CompletableFuture.allOf(\n            userFuture, ordersFuture, walletFuture, notificationsFuture\n        ).join();\n        \n        return new Dashboard(\n            userFuture.join(),\n            ordersFuture.join(),\n            walletFuture.join(),\n            notificationsFuture.join()\n        );\n    }\n}"
    },
    {
      "id": 49,
      "question": "How do you handle versioning of events in event-driven architecture?",
      "answer": "Event versioning manages changes to event schemas while maintaining backward compatibility in event-driven systems.\n\nVersioning Strategies:\n• Schema Evolution: Add optional fields only\n• Multiple Event Versions: Support old and new simultaneously\n• Event Upcasting: Convert old events to new format\n• Separate Event Types: Create new event types for breaking changes\n\nSchema Evolution Rules:\n• Add optional fields (safe)\n• Never remove fields\n• Never change field types\n• Never make optional fields required\n• Use default values\n• Maintain backward compatibility\n\nEvent Version Indicators:\n• Version field in event\n• Different event names (UserCreatedV2)\n• Schema registry with versions\n• Content type headers\n\nHandling Old Events:\n• Upcasting: Transform to latest version\n• Multi-version handlers: Support multiple versions\n• Weak schema: Ignore unknown fields\n• Event adapters: Convert formats\n\nBest Practices:\n• Use schema registry (Avro, Protobuf)\n• Document all changes\n• Test with old event versions\n• Gradual rollout of changes\n• Keep events immutable\n• Version explicitly",
      "explanation": "Event versioning maintains backward compatibility through schema evolution with optional fields, event upcasting to convert old formats, and supporting multiple versions simultaneously during transitions.",
      "difficulty": "Hard",
      "code": "// Event Versioning with Version Field\npublic class OrderCreatedEvent {\n    private String eventId;\n    private String orderId;\n    private String customerId;\n    private int version;  // Event version\n    \n    // V1 fields\n    private BigDecimal total;\n    \n    // V2 fields (added later, optional)\n    private String currency;  // Optional, defaults to USD\n    private List<TaxInfo> taxes;  // Optional\n}\n\n// Event Handler Supporting Multiple Versions\n@Service\npublic class OrderEventHandler {\n    \n    @EventListener\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        // Upcast to latest version\n        OrderCreatedEvent latestEvent = upcastEvent(event);\n        processOrder(latestEvent);\n    }\n    \n    private OrderCreatedEvent upcastEvent(OrderCreatedEvent event) {\n        if (event.getVersion() == 1) {\n            // Convert V1 to V2\n            event.setCurrency(\"USD\");  // Default value\n            event.setTaxes(Collections.emptyList());\n            event.setVersion(2);\n        }\n        return event;\n    }\n}\n\n// Separate Event Types Approach\npublic class UserRegisteredEventV1 {\n    private String userId;\n    private String email;\n}\n\npublic class UserRegisteredEventV2 {\n    private String userId;\n    private String email;\n    private String phoneNumber;  // New required field\n    private Address address;      // New required field\n}\n\n@Service\npublic class UserEventHandler {\n    \n    @EventListener\n    public void handleV1(UserRegisteredEventV1 event) {\n        // Handle old version\n        processUserRegistration(event.getUserId(), event.getEmail(), null, null);\n    }\n    \n    @EventListener\n    public void handleV2(UserRegisteredEventV2 event) {\n        // Handle new version\n        processUserRegistration(\n            event.getUserId(),\n            event.getEmail(),\n            event.getPhoneNumber(),\n            event.getAddress()\n        );\n    }\n}\n\n// Avro Schema Evolution\n{\n  \"type\": \"record\",\n  \"name\": \"OrderCreated\",\n  \"namespace\": \"com.example.events\",\n  \"fields\": [\n    {\"name\": \"orderId\", \"type\": \"string\"},\n    {\"name\": \"customerId\", \"type\": \"string\"},\n    {\"name\": \"total\", \"type\": \"double\"},\n    {\"name\": \"currency\", \"type\": \"string\", \"default\": \"USD\"},\n    {\"name\": \"taxes\", \"type\": {\"type\": \"array\", \"items\": \"TaxInfo\"}, \"default\": []}\n  ]\n}"
    },
    {
      "id": 50,
      "question": "What is the role of a service registry in Microservices?",
      "answer": "Service registry is a database of available service instances and their locations, enabling dynamic service discovery in microservices.\n\nKey Functions:\n• Store service instance information\n• Register new service instances\n• Deregister unavailable instances\n• Health check monitoring\n• Provide service lookup\n• Load balancing information\n\nRegistration Patterns:\n• Self-Registration: Services register themselves\n• Third-Party Registration: Registrar registers services\n\nService Registry Types:\n• Strongly Consistent: Zookeeper, etcd\n• Eventually Consistent: Consul, Eureka\n• DNS-Based: Kubernetes DNS\n\nStored Information:\n• Service name\n• Host and port\n• Health check URL\n• Metadata (version, tags)\n• Protocol information\n\nPopular Implementations:\n• Netflix Eureka\n• Consul\n• Zookeeper\n• etcd\n• Kubernetes Service Discovery\n\nBest Practices:\n• Implement health checks\n• Use heartbeat mechanism\n• Handle network partitions\n• Cache service locations\n• Implement retry logic\n• Set appropriate TTLs",
      "explanation": "Service registry maintains a dynamic database of available service instances and their network locations, enabling automatic service discovery and load balancing through registration, health checks, and lookup mechanisms.",
      "difficulty": "Easy",
      "code": "// Eureka Service Registration\n@SpringBootApplication\n@EnableEurekaServer\npublic class EurekaServerApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(EurekaServerApplication.class, args);\n    }\n}\n\n// application.yml (Eureka Server)\nserver:\n  port: 8761\neureka:\n  client:\n    registerWithEureka: false\n    fetchRegistry: false\n\n// Service Registration (Client)\n@SpringBootApplication\n@EnableEurekaClient\npublic class UserServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(UserServiceApplication.class, args);\n    }\n}\n\n// application.yml (Service)\nspring:\n  application:\n    name: user-service\neureka:\n  client:\n    serviceUrl:\n      defaultZone: http://localhost:8761/eureka/\n  instance:\n    preferIpAddress: true\n    lease-renewal-interval-in-seconds: 30\n    lease-expiration-duration-in-seconds: 90\n\n// Service Discovery and Load Balancing\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private DiscoveryClient discoveryClient;\n    \n    @Autowired\n    @LoadBalanced\n    private RestTemplate restTemplate;\n    \n    public User getUser(String userId) {\n        // Automatic service discovery and load balancing\n        return restTemplate.getForObject(\n            \"http://user-service/users/\" + userId,\n            User.class\n        );\n    }\n    \n    public List<ServiceInstance> getUserServiceInstances() {\n        // Manual service discovery\n        return discoveryClient.getInstances(\"user-service\");\n    }\n}"
    },
    {
      "id": 51,
      "question": "What is the Scatter-Gather pattern?",
      "answer": "Scatter-Gather pattern broadcasts a request to multiple services and aggregates their responses into a single result.\n\nHow It Works:\n• Scatter: Send request to multiple services simultaneously\n• Gather: Collect and aggregate responses\n• Combine: Merge results into single response\n• Handle: Manage timeouts and partial failures\n\nUse Cases:\n• Price comparison across vendors\n• Searching across multiple data sources\n• Aggregating analytics from multiple services\n• Federated queries\n• Best-offer selection\n\nImplementation Approaches:\n• Parallel async calls with CompletableFuture\n• Reactive streams (Project Reactor)\n• Message broker with reply queues\n• Thread pools for concurrency\n\nBenefits:\n• Faster than sequential calls\n• Comprehensive results\n• Can return best result\n• Fault tolerance (partial results)\n\nChallenges:\n• Timeout management\n• Partial failure handling\n• Result aggregation complexity\n• Performance varies with slowest service\n• Resource consumption",
      "explanation": "Scatter-Gather broadcasts requests to multiple services in parallel, then collects and aggregates their responses, useful for price comparison, search, and scenarios requiring comprehensive results from multiple sources.",
      "difficulty": "Hard",
      "code": "// Scatter-Gather with CompletableFuture\n@Service\npublic class PriceComparisonService {\n    \n    @Autowired\n    private List<PriceProvider> priceProviders;\n    \n    public PriceComparison getBestPrice(String productId) {\n        // Scatter: Send to all providers\n        List<CompletableFuture<PriceQuote>> futures = priceProviders.stream()\n            .map(provider -> CompletableFuture.supplyAsync(() -> \n                provider.getPrice(productId)\n            ))\n            .collect(Collectors.toList());\n        \n        // Gather: Wait with timeout\n        List<PriceQuote> quotes = futures.stream()\n            .map(future -> {\n                try {\n                    return future.get(2, TimeUnit.SECONDS);\n                } catch (TimeoutException e) {\n                    return null;  // Handle timeout\n                } catch (Exception e) {\n                    return null;  // Handle error\n                }\n            })\n            .filter(Objects::nonNull)\n            .collect(Collectors.toList());\n        \n        // Aggregate: Find best price\n        return new PriceComparison(\n            quotes,\n            findBestQuote(quotes)\n        );\n    }\n    \n    private PriceQuote findBestQuote(List<PriceQuote> quotes) {\n        return quotes.stream()\n            .min(Comparator.comparing(PriceQuote::getPrice))\n            .orElse(null);\n    }\n}\n\n// Reactive Scatter-Gather with Project Reactor\n@Service\npublic class SearchService {\n    \n    public Flux<SearchResult> searchAll(String query) {\n        // Scatter to multiple search services\n        Flux<SearchResult> database = searchDatabase(query);\n        Flux<SearchResult> files = searchFiles(query);\n        Flux<SearchResult> external = searchExternalApi(query);\n        \n        // Gather: Merge all results\n        return Flux.merge(database, files, external)\n            .timeout(Duration.ofSeconds(5))\n            .onErrorResume(e -> Flux.empty())\n            .sort(Comparator.comparing(SearchResult::getRelevance)\n                .reversed())\n            .take(20);  // Top 20 results\n    }\n}"
    },
    {
      "id": 52,
      "question": "What is the claim check pattern?",
      "answer": "Claim Check pattern separates large message payloads from the message flow by storing payload externally and passing only a reference.\n\nHow It Works:\n• Store large payload in external storage\n• Send message with reference/claim check\n• Receiver uses claim check to retrieve payload\n• Delete payload after processing\n\nUse Cases:\n• Large file processing\n• Video/image processing\n• Large document handling\n• Binary data transfer\n• Preventing message queue overload\n\nBenefits:\n• Reduced message broker load\n• Better message throughput\n• Doesn't violate message size limits\n• More efficient for large data\n• Can process payload independently\n\nStorage Options:\n• Object storage (S3, Azure Blob)\n• Shared file system\n• Database with blob support\n• Distributed cache\n\nBest Practices:\n• Include metadata in message\n• Set expiration on stored payload\n• Handle storage failures\n• Secure storage access\n• Clean up processed payloads\n• Monitor storage usage",
      "explanation": "Claim Check pattern handles large payloads by storing them externally and sending only a reference in messages, preventing message broker overload while maintaining efficient message flow.",
      "difficulty": "Hard",
      "code": "// Claim Check Pattern Implementation\n@Service\npublic class DocumentProcessingService {\n    \n    @Autowired\n    private S3Client s3Client;\n    @Autowired\n    private MessagePublisher messagePublisher;\n    \n    public void submitDocument(MultipartFile document) {\n        // Generate claim check ID\n        String claimCheckId = UUID.randomUUID().toString();\n        \n        // Store large payload in S3\n        String s3Key = \"documents/\" + claimCheckId;\n        s3Client.putObject(PutObjectRequest.builder()\n            .bucket(\"document-processing\")\n            .key(s3Key)\n            .build(),\n            RequestBody.fromInputStream(\n                document.getInputStream(),\n                document.getSize()\n            )\n        );\n        \n        // Send lightweight message with claim check\n        DocumentSubmittedEvent event = new DocumentSubmittedEvent(\n            claimCheckId,\n            document.getOriginalFilename(),\n            document.getContentType(),\n            document.getSize(),\n            s3Key\n        );\n        \n        messagePublisher.publish(event);\n    }\n}\n\n@Service\npublic class DocumentProcessor {\n    \n    @Autowired\n    private S3Client s3Client;\n    \n    @EventListener\n    public void processDocument(DocumentSubmittedEvent event) {\n        // Retrieve payload using claim check\n        ResponseBytes<GetObjectResponse> s3Object = s3Client.getObjectAsBytes(\n            GetObjectRequest.builder()\n                .bucket(\"document-processing\")\n                .key(event.getS3Key())\n                .build()\n        );\n        \n        byte[] documentContent = s3Object.asByteArray();\n        \n        // Process document\n        processContent(documentContent);\n        \n        // Clean up - delete from storage\n        s3Client.deleteObject(DeleteObjectRequest.builder()\n            .bucket(\"document-processing\")\n            .key(event.getS3Key())\n            .build()\n        );\n    }\n}\n\n// Lightweight Event with Claim Check\npublic class DocumentSubmittedEvent {\n    private String claimCheckId;\n    private String filename;\n    private String contentType;\n    private long size;\n    private String s3Key;  // Claim check\n    private LocalDateTime timestamp;\n}"
    },
    {
      "id": 53,
      "question": "How do you implement graceful shutdown in Microservices?",
      "answer": "Graceful shutdown ensures services complete in-flight requests and release resources properly before terminating.\n\nShutdown Steps:\n• Stop accepting new requests\n• Deregister from service registry\n• Complete in-flight requests\n• Close database connections\n• Flush pending messages\n• Release resources\n• Signal completion\n\nImplementation Aspects:\n• Shutdown hooks\n• Connection draining\n• Timeout configuration\n• Health check updates\n• Load balancer notification\n\nKubernetes Considerations:\n• PreStop hook for cleanup\n• Termination grace period\n• Update readiness probe\n• Handle SIGTERM signal\n\nBest Practices:\n• Set reasonable timeout (30-60 seconds)\n• Log shutdown progress\n• Monitor ongoing requests\n• Reject new requests early\n• Save application state\n• Notify dependent services\n• Handle force shutdown (SIGKILL)\n\nChallenges:\n• Long-running requests\n• Distributed transactions\n• Message processing mid-flight\n• Coordinating with load balancer",
      "explanation": "Graceful shutdown completes in-flight requests, deregisters from service registry, and releases resources before terminating, ensuring no request loss and clean service termination during deployments or scaling.",
      "difficulty": "Medium",
      "code": "// Spring Boot Graceful Shutdown\n// application.yml\nserver:\n  shutdown: graceful\n\nspring:\n  lifecycle:\n    timeout-per-shutdown-phase: 30s\n\n// Custom Shutdown Hook\n@Component\npublic class GracefulShutdown implements ApplicationListener<ContextClosedEvent> {\n    \n    @Autowired\n    private Executor taskExecutor;\n    @Autowired\n    private EurekaClient eurekaClient;\n    \n    @Override\n    public void onApplicationEvent(ContextClosedEvent event) {\n        log.info(\"Starting graceful shutdown...\");\n        \n        // 1. Deregister from service registry\n        eurekaClient.shutdown();\n        log.info(\"Deregistered from Eureka\");\n        \n        // 2. Wait for load balancer to update (5-10 seconds)\n        try {\n            Thread.sleep(10000);\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n        }\n        \n        // 3. Shutdown thread pool\n        if (taskExecutor instanceof ThreadPoolTaskExecutor) {\n            ThreadPoolTaskExecutor pool = (ThreadPoolTaskExecutor) taskExecutor;\n            pool.setWaitForTasksToCompleteOnShutdown(true);\n            pool.setAwaitTerminationSeconds(20);\n            pool.shutdown();\n        }\n        \n        log.info(\"Graceful shutdown completed\");\n    }\n}\n\n// Kubernetes PreStop Hook\napiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:1.0\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"sleep 15\"]\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n      terminationGracePeriodSeconds: 60\n\n// Health Check for Shutdown\n@Component\npublic class ShutdownHealthIndicator implements HealthIndicator {\n    \n    private volatile boolean shuttingDown = false;\n    \n    @PreDestroy\n    public void onShutdown() {\n        shuttingDown = true;\n    }\n    \n    @Override\n    public Health health() {\n        if (shuttingDown) {\n            return Health.outOfService().build();\n        }\n        return Health.up().build();\n    }\n}"
    },
    {
      "id": 54,
      "question": "What is the role of an API Gateway in security?",
      "answer": "API Gateway provides centralized security controls for microservices, acting as the first line of defense.\n\nSecurity Functions:\n• Authentication: Verify client identity\n• Authorization: Check access permissions\n• API key validation\n• Rate limiting and throttling\n• IP whitelisting/blacklisting\n• SSL/TLS termination\n• Request validation and sanitization\n• Security headers injection\n\nAuthentication Approaches:\n• JWT token validation\n• OAuth 2.0 integration\n• API key verification\n• Mutual TLS\n• SAML integration\n\nProtection Features:\n• DDoS protection\n• SQL injection prevention\n• XSS attack prevention\n• Request size limits\n• Bot detection\n• Threat detection\n\nSecurity Headers:\n• CORS configuration\n• Content-Security-Policy\n• X-Frame-Options\n• Strict-Transport-Security\n\nBenefits:\n• Centralized security policy\n• Reduced service complexity\n• Consistent security across services\n• Easier audit and compliance\n• Shield backend services\n\nBest Practices:\n• Use HTTPS only\n• Implement defense in depth\n• Log security events\n• Regular security audits\n• Update security rules",
      "explanation": "API Gateway centralizes security by handling authentication, authorization, rate limiting, and threat protection at the entry point, providing consistent security policies across all microservices while simplifying service implementation.",
      "difficulty": "Medium",
      "code": "// Spring Cloud Gateway Security\n@Configuration\n@EnableWebFluxSecurity\npublic class SecurityConfig {\n    \n    @Bean\n    public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http) {\n        return http\n            .csrf().disable()\n            .authorizeExchange()\n                .pathMatchers(\"/public/**\").permitAll()\n                .pathMatchers(\"/admin/**\").hasRole(\"ADMIN\")\n                .anyExchange().authenticated()\n            .and()\n            .oauth2ResourceServer()\n                .jwt()\n            .and()\n            .build();\n    }\n}\n\n// JWT Validation Filter\n@Component\npublic class JwtAuthenticationFilter implements GatewayFilter {\n    \n    @Autowired\n    private JwtTokenValidator tokenValidator;\n    \n    @Override\n    public Mono<Void> filter(ServerWebExchange exchange, \n                             GatewayFilterChain chain) {\n        String token = extractToken(exchange.getRequest());\n        \n        if (token == null || !tokenValidator.validate(token)) {\n            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);\n            return exchange.getResponse().setComplete();\n        }\n        \n        // Add user context to headers\n        Claims claims = tokenValidator.getClaims(token);\n        ServerHttpRequest request = exchange.getRequest().mutate()\n            .header(\"X-User-Id\", claims.getSubject())\n            .header(\"X-User-Roles\", claims.get(\"roles\", String.class))\n            .build();\n        \n        return chain.filter(exchange.mutate().request(request).build());\n    }\n}\n\n// Rate Limiting\n@Configuration\npublic class RateLimitConfig {\n    \n    @Bean\n    public KeyResolver userKeyResolver() {\n        return exchange -> Mono.just(\n            exchange.getRequest()\n                .getHeaders()\n                .getFirst(\"X-User-Id\")\n        );\n    }\n    \n    @Bean\n    public RouteLocator routes(RouteLocatorBuilder builder) {\n        return builder.routes()\n            .route(\"rate-limited-route\", r -> r.path(\"/api/**\")\n                .filters(f -> f.requestRateLimiter(c -> c\n                    .setRateLimiter(redisRateLimiter())\n                    .setKeyResolver(userKeyResolver())))\n                .uri(\"lb://backend-service\"))\n            .build();\n    }\n}"
    },
    {
      "id": 55,
      "question": "What is the Backends for Frontends (BFF) pattern difference from API Gateway?",
      "answer": "BFF and API Gateway both act as intermediaries but serve different purposes and are often used together.\n\nAPI Gateway:\n• Single entry point for all clients\n• Routing and load balancing\n• Cross-cutting concerns (auth, logging)\n• Technology and platform agnostic\n• Shared across all client types\n• Infrastructure focused\n\nBackend for Frontend (BFF):\n• Separate backend per client type\n• Optimized for specific frontend\n• Client-specific data aggregation\n• Owned by frontend team\n• Different BFF per platform (mobile, web)\n• Application focused\n\nKey Differences:\n• Purpose: Gateway for routing, BFF for optimization\n• Ownership: Gateway by platform team, BFF by frontend team\n• Scope: Gateway shared, BFF specific\n• Complexity: Gateway simpler, BFF has business logic\n• Number: One gateway, multiple BFFs\n\nCombined Architecture:\n• API Gateway handles auth and routing\n• Routes to appropriate BFF\n• BFF aggregates and transforms\n• Backend services remain independent\n\nWhen to Use:\n• Different data needs per client\n• Mobile vs web optimization\n• Multiple frontend teams\n• Complex data aggregation needed",
      "explanation": "API Gateway provides shared infrastructure concerns like authentication and routing for all clients, while BFF creates client-specific backends optimized for each frontend type, often used together in layered architecture.",
      "difficulty": "Medium",
      "code": "// Architecture with both API Gateway and BFF\n\n// API Gateway Layer\n@Configuration\npublic class GatewayRoutes {\n    \n    @Bean\n    public RouteLocator routes(RouteLocatorBuilder builder) {\n        return builder.routes()\n            // Route to Mobile BFF\n            .route(\"mobile-bff\", r -> r\n                .path(\"/mobile/**\")\n                .filters(f -> f\n                    .stripPrefix(1)\n                    .addRequestHeader(\"X-Client-Type\", \"mobile\"))\n                .uri(\"lb://MOBILE-BFF\"))\n            \n            // Route to Web BFF\n            .route(\"web-bff\", r -> r\n                .path(\"/web/**\")\n                .filters(f -> f\n                    .stripPrefix(1)\n                    .addRequestHeader(\"X-Client-Type\", \"web\"))\n                .uri(\"lb://WEB-BFF\"))\n            .build();\n    }\n}\n\n// Mobile BFF - Optimized for mobile\n@RestController\n@RequestMapping(\"/api\")\npublic class MobileBFFController {\n    \n    @GetMapping(\"/dashboard\")\n    public MobileDashboard getDashboard(@RequestParam String userId) {\n        // Lightweight response for mobile\n        UserSummary user = userService.getUserSummary(userId);\n        List<OrderSummary> recentOrders = orderService.getRecentOrders(userId, 3);\n        \n        return new MobileDashboard(user, recentOrders);\n    }\n    \n    @GetMapping(\"/order/{id}\")\n    public MobileOrder getOrder(@PathVariable String id) {\n        // Minimal data for mobile screen\n        return orderService.getMobileOrder(id);\n    }\n}\n\n// Web BFF - Rich data for web\n@RestController\n@RequestMapping(\"/api\")\npublic class WebBFFController {\n    \n    @GetMapping(\"/dashboard\")\n    public WebDashboard getDashboard(@RequestParam String userId) {\n        // Rich data for web with charts and analytics\n        UserProfile user = userService.getUserProfile(userId);\n        List<Order> orders = orderService.getAllOrders(userId);\n        Analytics analytics = analyticsService.getUserAnalytics(userId);\n        Recommendations recs = recommendationService.getRecommendations(userId);\n        \n        return new WebDashboard(user, orders, analytics, recs);\n    }\n    \n    @GetMapping(\"/order/{id}\")\n    public DetailedOrder getOrder(@PathVariable String id) {\n        // Detailed data with full history\n        return orderService.getDetailedOrder(id);\n    }\n}"
    },
    {
      "id": 56,
      "question": "What is the difference between horizontal and vertical scaling in Microservices?",
      "answer": "Horizontal and vertical scaling are two approaches to increasing system capacity with different characteristics.\n\nHorizontal Scaling (Scale Out):\n• Add more instances of same service\n• Distribute load across instances\n• Better fault tolerance\n• Linear cost increase\n• Requires load balancing\n• Stateless services ideal\n• Cloud-native approach\n\nVertical Scaling (Scale Up):\n• Increase resources of existing instance\n• More CPU, RAM, disk\n• Hardware limits exist\n• Single point of failure\n• Simpler to implement\n• No code changes needed\n• Traditional approach\n\nMicroservices Preference:\n• Horizontal scaling preferred\n• Better availability\n• No downtime during scaling\n• Cost effective with cloud\n• Aligns with microservices principles\n\nCombined Approach:\n• Vertical scale within limits\n• Horizontal scale beyond\n• Different strategies per service\n• Cost optimization\n\nConsiderations:\n• Stateless vs stateful services\n• Database scaling strategy\n• Network overhead\n• Load balancer capacity",
      "explanation": "Horizontal scaling adds more service instances for better availability and unlimited growth, while vertical scaling increases instance resources with hardware limits, with microservices typically favoring horizontal scaling for better resilience.",
      "difficulty": "Easy",
      "code": "// Kubernetes Horizontal Pod Autoscaler\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: user-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: user-service\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n\n# Vertical Scaling - Resource Limits\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:1.0\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n\n# Application-level Scaling Configuration\n@Configuration\npublic class ThreadPoolConfig {\n    \n    @Bean\n    public ThreadPoolTaskExecutor taskExecutor() {\n        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n        // Scale thread pool based on load\n        executor.setCorePoolSize(10);\n        executor.setMaxPoolSize(50);\n        executor.setQueueCapacity(100);\n        executor.setThreadNamePrefix(\"async-\");\n        return executor;\n    }\n}"
    },
    {
      "id": 57,
      "question": "How do you handle cascading failures in Microservices?",
      "answer": "Cascading failures occur when one service failure causes dependent services to fail, potentially bringing down entire system.\n\nPrevention Patterns:\n• Circuit Breaker: Stop calling failing services\n• Bulkhead: Isolate resources per dependency\n• Timeout: Limit wait time for responses\n• Retry with Backoff: Smart retry strategy\n• Fallback: Provide alternative responses\n• Rate Limiting: Prevent overload\n\nCircuit Breaker Pattern:\n• Detect failure threshold\n• Open circuit, fail fast\n• Prevent resource exhaustion\n• Allow recovery time\n• Test periodically (half-open)\n\nBulkhead Pattern:\n• Separate thread pools per dependency\n• Limit concurrent requests\n• Prevent one dependency consuming all resources\n• Isolate failures\n\nAdditional Strategies:\n• Load shedding: Reject requests under load\n• Graceful degradation: Reduced functionality\n• Health checks: Early detection\n• Monitoring and alerting\n• Chaos engineering testing\n\nBest Practices:\n• Assume failures will happen\n• Design for failure\n• Test failure scenarios\n• Monitor dependencies\n• Set appropriate timeouts\n• Implement observability",
      "explanation": "Cascading failures are prevented using Circuit Breaker to stop calling failing services, Bulkhead to isolate resources, timeouts to limit waits, and fallbacks to provide alternative responses when dependencies fail.",
      "difficulty": "Hard",
      "code": "// Preventing Cascading Failures with Resilience4j\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private RestTemplate restTemplate;\n    \n    // Circuit Breaker + Timeout + Retry + Bulkhead\n    @CircuitBreaker(name = \"userService\", fallbackMethod = \"getUserFallback\")\n    @TimeLimiter(name = \"userService\")\n    @Retry(name = \"userService\")\n    @Bulkhead(name = \"userService\", type = Bulkhead.Type.THREADPOOL)\n    public CompletableFuture<User> getUser(String userId) {\n        return CompletableFuture.supplyAsync(() ->\n            restTemplate.getForObject(\n                \"http://user-service/users/\" + userId,\n                User.class\n            )\n        );\n    }\n    \n    // Fallback method\n    private CompletableFuture<User> getUserFallback(String userId, Exception ex) {\n        // Return cached data or default\n        User defaultUser = cacheService.getUser(userId)\n            .orElse(new User(userId, \"Unknown User\"));\n        return CompletableFuture.completedFuture(defaultUser);\n    }\n}\n\n// application.yml\nresilience4j:\n  circuitbreaker:\n    instances:\n      userService:\n        slidingWindowSize: 10\n        failureRateThreshold: 50\n        waitDurationInOpenState: 10000\n        permittedNumberOfCallsInHalfOpenState: 3\n  \n  timelimiter:\n    instances:\n      userService:\n        timeoutDuration: 3s\n  \n  retry:\n    instances:\n      userService:\n        maxAttempts: 3\n        waitDuration: 1000\n        enableExponentialBackoff: true\n  \n  bulkhead:\n    instances:\n      userService:\n        maxConcurrentCalls: 10\n  \n  thread-pool-bulkhead:\n    instances:\n      userService:\n        maxThreadPoolSize: 5\n        coreThreadPoolSize: 3\n        queueCapacity: 20"
    },
    {
      "id": 58,
      "question": "What is the purpose of API documentation in Microservices?",
      "answer": "API documentation is critical for microservices communication, enabling teams to understand and consume services effectively.\n\nImportance:\n• Contract definition between services\n• Reduces integration time\n• Self-service for consumers\n• Reduces support questions\n• Enables automated testing\n• Facilitates onboarding\n• Version management\n\nDocumentation Tools:\n• OpenAPI/Swagger: REST API specification\n• AsyncAPI: Async/event-driven APIs\n• gRPC Reflection: gRPC service documentation\n• Postman Collections: Interactive documentation\n• README files: Overview and getting started\n\nWhat to Document:\n• Endpoints and methods\n• Request/response formats\n• Authentication requirements\n• Error codes and messages\n• Rate limits\n• Examples and use cases\n• Versioning information\n• Contact information\n\nBest Practices:\n• Generate from code (Swagger annotations)\n• Keep documentation updated\n• Provide interactive playground\n• Include code examples\n• Document error scenarios\n• Version documentation with API\n• Make easily discoverable\n• Use consistent style",
      "explanation": "API documentation using tools like OpenAPI/Swagger defines contracts between microservices, enabling self-service consumption, reducing integration time, and providing automated testing capabilities with always-current specifications.",
      "difficulty": "Easy",
      "code": "// OpenAPI Documentation with Spring Boot\n@Configuration\npublic class OpenApiConfig {\n    \n    @Bean\n    public OpenAPI customOpenAPI() {\n        return new OpenAPI()\n            .info(new Info()\n                .title(\"User Service API\")\n                .version(\"v1\")\n                .description(\"Microservice for user management\")\n                .contact(new Contact()\n                    .name(\"API Support\")\n                    .email(\"support@example.com\")))\n            .components(new Components()\n                .addSecuritySchemes(\"bearer-jwt\",\n                    new SecurityScheme()\n                        .type(SecurityScheme.Type.HTTP)\n                        .scheme(\"bearer\")\n                        .bearerFormat(\"JWT\")));\n    }\n}\n\n// Controller with Swagger Annotations\n@RestController\n@RequestMapping(\"/api/users\")\n@Tag(name = \"User Management\", description = \"APIs for managing users\")\npublic class UserController {\n    \n    @Operation(\n        summary = \"Get user by ID\",\n        description = \"Returns user details for the given ID\"\n    )\n    @ApiResponses(value = {\n        @ApiResponse(\n            responseCode = \"200\",\n            description = \"User found\",\n            content = @Content(schema = @Schema(implementation = User.class))\n        ),\n        @ApiResponse(\n            responseCode = \"404\",\n            description = \"User not found\"\n        ),\n        @ApiResponse(\n            responseCode = \"401\",\n            description = \"Unauthorized\"\n        )\n    })\n    @GetMapping(\"/{id}\")\n    public ResponseEntity<User> getUser(\n            @Parameter(description = \"User ID\", required = true)\n            @PathVariable String id) {\n        return userService.findById(id)\n            .map(ResponseEntity::ok)\n            .orElse(ResponseEntity.notFound().build());\n    }\n    \n    @Operation(summary = \"Create new user\")\n    @PostMapping\n    public ResponseEntity<User> createUser(\n            @RequestBody @Valid UserRequest request) {\n        User user = userService.create(request);\n        return ResponseEntity.status(HttpStatus.CREATED).body(user);\n    }\n}\n\n// pom.xml\n<dependency>\n    <groupId>org.springdoc</groupId>\n    <artifactId>springdoc-openapi-starter-webmvc-ui</artifactId>\n    <version>2.0.2</version>\n</dependency>\n\n// Access documentation at:\n// http://localhost:8080/swagger-ui.html\n// http://localhost:8080/v3/api-docs"
    },
    {
      "id": 59,
      "question": "What is the Database per Service pattern and its implications?",
      "answer": "Database per Service pattern ensures each microservice has its own private database that cannot be accessed directly by other services.\n\nCore Principles:\n• Each service owns its data\n• No shared database tables\n• Access only through service API\n• Data schema private to service\n• Different database technologies possible\n\nBenefits:\n• Loose coupling between services\n• Independent scaling\n• Technology flexibility (polyglot persistence)\n• Independent schema evolution\n• Fault isolation\n• Clear ownership\n• Team autonomy\n\nChallenges:\n• No ACID transactions across services\n• Complex queries across services\n• Data consistency issues\n• Data duplication\n• Join operations difficult\n• Increased complexity\n\nSolutions:\n• Saga pattern for transactions\n• API composition for queries\n• CQRS for read models\n• Event-driven data synchronization\n• Eventual consistency\n\nPolyglot Persistence:\n• SQL for transactional data\n• NoSQL for scalability\n• Graph DB for relationships\n• Search engines for full-text\n• Cache for performance",
      "explanation": "Database per Service ensures each microservice exclusively owns its data with separate databases, enabling independent scaling and technology choices while requiring patterns like Saga and CQRS to handle distributed data challenges.",
      "difficulty": "Medium",
      "code": "// Service with Own Database\n@Service\npublic class UserService {\n    \n    @Autowired\n    private UserRepository userRepository;  // PostgreSQL\n    \n    public User createUser(UserRequest request) {\n        User user = new User(request);\n        User saved = userRepository.save(user);\n        \n        // Publish event for other services\n        eventPublisher.publish(new UserCreatedEvent(saved));\n        return saved;\n    }\n    \n    public User getUser(String userId) {\n        return userRepository.findById(userId)\n            .orElseThrow(() -> new UserNotFoundException(userId));\n    }\n}\n\n// Different Service, Different Database Type\n@Service\npublic class ProductCatalogService {\n    \n    @Autowired\n    private MongoTemplate mongoTemplate;  // MongoDB\n    \n    public Product createProduct(ProductRequest request) {\n        Product product = new Product(request);\n        mongoTemplate.save(product);\n        \n        eventPublisher.publish(new ProductCreatedEvent(product));\n        return product;\n    }\n}\n\n// Another Service with Different Tech\n@Service\npublic class SearchService {\n    \n    @Autowired\n    private ElasticsearchOperations elasticsearchOps;  // Elasticsearch\n    \n    @EventListener\n    public void onProductCreated(ProductCreatedEvent event) {\n        // Synchronize to search index\n        ProductDocument doc = new ProductDocument(event.getProduct());\n        elasticsearchOps.save(doc);\n    }\n    \n    public List<Product> search(String query) {\n        // Optimized for search\n        return elasticsearchOps.search(query, ProductDocument.class);\n    }\n}\n\n// application.yml - Multiple Datasources\nspring:\n  datasource:\n    url: jdbc:postgresql://localhost:5432/userdb\n    username: user\n    password: pass\n  \n  data:\n    mongodb:\n      uri: mongodb://localhost:27017/productdb\n  \n  elasticsearch:\n    uris: http://localhost:9200"
    },
    {
      "id": 60,
      "question": "How do you implement request tracing across Microservices?",
      "answer": "Request tracing tracks a request's journey through multiple microservices using correlation IDs and distributed tracing tools.\n\nKey Components:\n• Correlation ID: Unique identifier for request\n• Trace ID: Overall transaction identifier\n• Span ID: Individual operation identifier\n• Parent Span: Calling operation\n• Context Propagation: Passing IDs through calls\n\nImplementation Steps:\n• Generate correlation ID at entry point\n• Propagate ID through all service calls\n• Include ID in logs\n• Send trace data to collector\n• Visualize in tracing UI\n\nPropagation Methods:\n• HTTP headers\n• Message headers (queues)\n• MDC (Mapped Diagnostic Context)\n• Thread-local storage\n• Baggage items\n\nTracing Tools:\n• Jaeger\n• Zipkin\n• AWS X-Ray\n• OpenTelemetry\n• Spring Cloud Sleuth\n\nBenefits:\n• End-to-end visibility\n• Performance bottleneck identification\n• Dependency mapping\n• Error troubleshooting\n• Latency analysis\n\nBest Practices:\n• Generate ID early\n• Always propagate\n• Log with correlation ID\n• Sample strategically\n• Monitor overhead",
      "explanation": "Request tracing uses correlation IDs propagated through HTTP and message headers, combined with distributed tracing tools like Jaeger or Zipkin to track requests across microservices for debugging and performance analysis.",
      "difficulty": "Medium",
      "code": "// Spring Cloud Sleuth for Distributed Tracing\n// pom.xml\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-sleuth</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-sleuth-zipkin</artifactId>\n</dependency>\n\n// application.yml\nspring:\n  sleuth:\n    sampler:\n      probability: 1.0  # Sample 100% of requests\n  zipkin:\n    base-url: http://localhost:9411\n    sender:\n      type: web\n\n// Automatic Propagation\n@Service\npublic class OrderService {\n    \n    @Autowired\n    @LoadBalanced\n    private RestTemplate restTemplate;\n    \n    public Order createOrder(OrderRequest request) {\n        // Trace ID automatically propagated\n        User user = restTemplate.getForObject(\n            \"http://user-service/users/\" + request.getUserId(),\n            User.class\n        );\n        \n        // Log includes trace ID\n        log.info(\"Creating order for user: {}\", user.getId());\n        \n        Order order = new Order(request, user);\n        return orderRepository.save(order);\n    }\n}\n\n// Manual Correlation ID Handling\n@Component\npublic class CorrelationInterceptor implements ClientHttpRequestInterceptor {\n    \n    private static final String CORRELATION_ID = \"X-Correlation-ID\";\n    \n    @Override\n    public ClientHttpResponse intercept(\n            HttpRequest request,\n            byte[] body,\n            ClientHttpRequestExecution execution) throws IOException {\n        \n        String correlationId = MDC.get(\"correlationId\");\n        if (correlationId != null) {\n            request.getHeaders().set(CORRELATION_ID, correlationId);\n        }\n        \n        return execution.execute(request, body);\n    }\n}\n\n// Filter to Extract/Generate Correlation ID\n@Component\npublic class CorrelationIdFilter extends OncePerRequestFilter {\n    \n    @Override\n    protected void doFilterInternal(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            FilterChain filterChain) throws ServletException, IOException {\n        \n        String correlationId = request.getHeader(\"X-Correlation-ID\");\n        if (correlationId == null) {\n            correlationId = UUID.randomUUID().toString();\n        }\n        \n        MDC.put(\"correlationId\", correlationId);\n        response.addHeader(\"X-Correlation-ID\", correlationId);\n        \n        try {\n            filterChain.doFilter(request, response);\n        } finally {\n            MDC.clear();\n        }\n    }\n}\n\n// logback-spring.xml\n<pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} [%X{correlationId}] - %msg%n</pattern>"
    },
    {
      "id": 61,
      "question": "What is the purpose of a message broker in Microservices?",
      "answer": "Message broker is middleware that enables asynchronous communication between microservices through message passing.\n\nCore Functions:\n• Message routing and delivery\n• Queue management\n• Publish-subscribe messaging\n• Message persistence\n• Guaranteed delivery\n• Load balancing\n• Message transformation\n\nCommunication Patterns:\n• Point-to-Point: Queue-based messaging\n• Publish-Subscribe: Topic-based broadcasting\n• Request-Reply: Asynchronous RPC\n• Competing Consumers: Load distribution\n\nBenefits:\n• Loose coupling between services\n• Asynchronous communication\n• Better scalability\n• Fault tolerance\n• Load leveling\n• Event-driven architecture\n• Temporal decoupling\n\nPopular Message Brokers:\n• RabbitMQ: Feature-rich, AMQP protocol\n• Apache Kafka: High-throughput streaming\n• AWS SQS/SNS: Managed cloud service\n• Azure Service Bus: Enterprise messaging\n• ActiveMQ: JMS-compliant\n\nWhen to Use:\n• Asynchronous processing\n• Event notifications\n• Load balancing\n• Integration patterns\n• Workflow orchestration",
      "explanation": "Message broker enables asynchronous, decoupled communication between microservices through queues and topics, providing guaranteed delivery, load balancing, and support for event-driven architectures.",
      "difficulty": "Easy",
      "code": "// RabbitMQ Configuration\n@Configuration\npublic class RabbitMQConfig {\n    \n    @Bean\n    public Queue orderQueue() {\n        return new Queue(\"order.queue\", true);  // Durable queue\n    }\n    \n    @Bean\n    public TopicExchange orderExchange() {\n        return new TopicExchange(\"order.exchange\");\n    }\n    \n    @Bean\n    public Binding binding() {\n        return BindingBuilder\n            .bind(orderQueue())\n            .to(orderExchange())\n            .with(\"order.created\");\n    }\n}\n\n// Message Publisher\n@Service\npublic class OrderEventPublisher {\n    \n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n    \n    public void publishOrderCreated(Order order) {\n        OrderCreatedEvent event = new OrderCreatedEvent(\n            order.getId(),\n            order.getCustomerId(),\n            order.getItems(),\n            LocalDateTime.now()\n        );\n        \n        rabbitTemplate.convertAndSend(\n            \"order.exchange\",\n            \"order.created\",\n            event\n        );\n        \n        log.info(\"Published order created event: {}\", order.getId());\n    }\n}\n\n// Message Consumer\n@Service\npublic class InventoryEventListener {\n    \n    @RabbitListener(queues = \"order.queue\")\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        log.info(\"Received order created event: {}\", event.getOrderId());\n        \n        try {\n            inventoryService.reserveItems(event.getItems());\n            log.info(\"Reserved inventory for order: {}\", event.getOrderId());\n        } catch (Exception e) {\n            log.error(\"Failed to reserve inventory\", e);\n            // Message will be requeued or sent to DLQ\n            throw e;\n        }\n    }\n}\n\n// Apache Kafka Example\n@Service\npublic class OrderKafkaProducer {\n    \n    @Autowired\n    private KafkaTemplate<String, OrderCreatedEvent> kafkaTemplate;\n    \n    public void publishOrder(Order order) {\n        OrderCreatedEvent event = new OrderCreatedEvent(order);\n        kafkaTemplate.send(\"order-events\", order.getId(), event);\n    }\n}\n\n@Service\npublic class OrderKafkaConsumer {\n    \n    @KafkaListener(topics = \"order-events\", groupId = \"inventory-service\")\n    public void consume(OrderCreatedEvent event) {\n        processOrder(event);\n    }\n}"
    },
    {
      "id": 62,
      "question": "How do you implement feature flags in Microservices?",
      "answer": "Feature flags (feature toggles) enable or disable functionality without deploying new code, supporting gradual rollouts and A/B testing.\n\nTypes of Feature Flags:\n• Release Toggles: Enable incomplete features\n• Experiment Toggles: A/B testing\n• Ops Toggles: Operational control\n• Permission Toggles: User-based access\n\nImplementation Approaches:\n• Configuration files\n• Database storage\n• Feature flag service (LaunchDarkly, Unleash)\n• Environment variables\n• Distributed configuration\n\nBenefits:\n• Gradual feature rollout\n• Quick rollback without deployment\n• A/B testing capabilities\n• Canary releases\n• User-specific features\n• Reduced deployment risk\n• Decouple deployment from release\n\nBest Practices:\n• Keep flags simple\n• Remove old flags\n• Document flag purpose\n• Monitor flag usage\n• Set expiration dates\n• Limit flag proliferation\n• Test both states\n• Use typed flags\n\nChallenges:\n• Code complexity\n• Testing combinatorial explosion\n• Technical debt\n• Flag lifecycle management",
      "explanation": "Feature flags control functionality dynamically without code deployment, enabling gradual rollouts, A/B testing, and quick rollbacks by toggling features on/off for specific users or percentages through configuration.",
      "difficulty": "Medium",
      "code": "// Simple Feature Flag Service\n@Service\npublic class FeatureFlagService {\n    \n    @Autowired\n    private FeatureFlagRepository flagRepository;\n    \n    public boolean isEnabled(String flagName) {\n        return flagRepository.findByName(flagName)\n            .map(FeatureFlag::isEnabled)\n            .orElse(false);\n    }\n    \n    public boolean isEnabledForUser(String flagName, String userId) {\n        FeatureFlag flag = flagRepository.findByName(flagName)\n            .orElse(null);\n        \n        if (flag == null || !flag.isEnabled()) {\n            return false;\n        }\n        \n        // Percentage rollout\n        if (flag.getPercentage() < 100) {\n            int hash = Math.abs(userId.hashCode() % 100);\n            return hash < flag.getPercentage();\n        }\n        \n        // Whitelist check\n        return flag.getWhitelistedUsers().contains(userId);\n    }\n}\n\n// Using Feature Flags in Code\n@RestController\npublic class UserController {\n    \n    @Autowired\n    private FeatureFlagService featureFlags;\n    \n    @GetMapping(\"/users/{id}\")\n    public UserResponse getUser(@PathVariable String id) {\n        User user = userService.getUser(id);\n        \n        // Check feature flag\n        if (featureFlags.isEnabledForUser(\"enhanced-profile\", id)) {\n            return enhancedUserProfile(user);\n        } else {\n            return basicUserProfile(user);\n        }\n    }\n}\n\n// Spring Cloud Config with Feature Flags\n@Configuration\n@RefreshScope\npublic class FeatureConfig {\n    \n    @Value(\"${features.new-checkout:false}\")\n    private boolean newCheckoutEnabled;\n    \n    @Value(\"${features.recommendations:false}\")\n    private boolean recommendationsEnabled;\n    \n    public boolean isNewCheckoutEnabled() {\n        return newCheckoutEnabled;\n    }\n}\n\n// LaunchDarkly Integration\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private LDClient ldClient;\n    \n    public void processOrder(Order order) {\n        LDUser user = new LDUser.Builder(order.getUserId())\n            .email(order.getUserEmail())\n            .build();\n        \n        boolean useNewPaymentFlow = ldClient.boolVariation(\n            \"new-payment-flow\",\n            user,\n            false\n        );\n        \n        if (useNewPaymentFlow) {\n            newPaymentProcessor.process(order);\n        } else {\n            legacyPaymentProcessor.process(order);\n        }\n    }\n}\n\n// Feature Flag Entity\n@Entity\npublic class FeatureFlag {\n    @Id\n    private String name;\n    private boolean enabled;\n    private int percentage;  // 0-100\n    private Set<String> whitelistedUsers;\n    private LocalDateTime expiresAt;\n}"
    },
    {
      "id": 63,
      "question": "What is the sidecar proxy pattern in service mesh?",
      "answer": "Sidecar proxy pattern deploys a proxy container alongside each service instance to handle network communication and cross-cutting concerns.\n\nSidecar Proxy Functions:\n• Service discovery\n• Load balancing\n• Traffic routing\n• Circuit breaking\n• Retries and timeouts\n• Mutual TLS (mTLS)\n• Metrics collection\n• Distributed tracing\n\nHow It Works:\n• Sidecar intercepts all network traffic\n• Deployed as separate container in same pod\n• Transparent to application\n• Managed by control plane\n• Enforces policies\n\nPopular Sidecar Proxies:\n• Envoy (used by Istio)\n• Linkerd2-proxy\n• NGINX\n• HAProxy\n\nBenefits:\n• No application code changes\n• Language agnostic\n• Centralized policy enforcement\n• Observability built-in\n• Security by default\n• Consistent behavior\n\nChallenges:\n• Resource overhead\n• Increased latency\n• Complexity\n• Debugging difficulty\n• Learning curve\n\nWhen to Use:\n• Large microservices deployment\n• Need consistent policies\n• Security requirements\n• Advanced traffic management",
      "explanation": "Sidecar proxy deploys a network proxy alongside each service to transparently handle service mesh features like load balancing, security, and observability without modifying application code.",
      "difficulty": "Hard",
      "code": "// Istio Sidecar Injection\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    istio-injection: enabled  # Automatic sidecar injection\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\n  namespace: production\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: user-service\n        version: v1\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:1.0\n        ports:\n        - containerPort: 8080\n      # Istio automatically injects envoy sidecar\n\n# Manual Sidecar Injection\napiVersion: v1\nkind: Pod\nmetadata:\n  name: user-service\nspec:\n  containers:\n  # Application container\n  - name: user-service\n    image: user-service:1.0\n    ports:\n    - containerPort: 8080\n  \n  # Envoy sidecar proxy\n  - name: envoy-proxy\n    image: envoyproxy/envoy:v1.24\n    ports:\n    - containerPort: 15001  # Envoy admin\n    - containerPort: 15000  # Envoy stats\n    volumeMounts:\n    - name: envoy-config\n      mountPath: /etc/envoy\n  \n  volumes:\n  - name: envoy-config\n    configMap:\n      name: envoy-config\n\n# Istio Traffic Management with Sidecar\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: user-service\nspec:\n  hosts:\n  - user-service\n  http:\n  - fault:\n      delay:\n        percentage:\n          value: 10\n        fixedDelay: 5s\n    route:\n    - destination:\n        host: user-service\n        subset: v1\n      weight: 90\n    - destination:\n        host: user-service\n        subset: v2\n      weight: 10\n    timeout: 10s\n    retries:\n      attempts: 3\n      perTryTimeout: 2s\n\n# No Application Code Changes Needed!\n@RestController\npublic class UserController {\n    \n    @GetMapping(\"/users/{id}\")\n    public User getUser(@PathVariable String id) {\n        // All traffic management handled by sidecar\n        return userService.getUser(id);\n    }\n}"
    },
    {
      "id": 64,
      "question": "How do you handle data migration in Microservices?",
      "answer": "Data migration in microservices requires careful coordination to maintain service availability and data consistency.\n\nMigration Strategies:\n• Dual Write: Write to both old and new\n• Event-Based: Synchronize via events\n• Batch Migration: Migrate in batches\n• Strangler Pattern: Gradual replacement\n• Blue-Green: Switch after full migration\n\nApproaches:\n• Stop the World: Maintenance window\n• Zero Downtime: Gradual migration\n• Shadow Writing: Write to new without reading\n• Parallel Running: Both systems active\n\nSteps for Zero-Downtime Migration:\n• Implement dual write to old and new\n• Backfill historical data\n• Verify data consistency\n• Switch reads to new database\n• Monitor and validate\n• Remove dual write\n• Decommission old database\n\nBest Practices:\n• Always have rollback plan\n• Test migration process\n• Migrate incrementally\n• Monitor data consistency\n• Use feature flags\n• Keep old data temporarily\n• Document migration steps\n\nChallenges:\n• Data consistency\n• Performance impact\n• Transaction boundaries\n• Rollback complexity\n• Testing difficulty",
      "explanation": "Data migration uses strategies like dual write and event-based synchronization for zero-downtime transitions, gradually moving from old to new databases while maintaining consistency through careful validation and rollback plans.",
      "difficulty": "Hard",
      "code": "// Dual Write Strategy\n@Service\npublic class UserService {\n    \n    @Autowired\n    private LegacyUserRepository legacyRepo;\n    @Autowired\n    private NewUserRepository newRepo;\n    @Autowired\n    private FeatureFlagService featureFlags;\n    \n    @Transactional\n    public User createUser(UserRequest request) {\n        User user = new User(request);\n        \n        // Phase 1: Write to legacy (primary)\n        LegacyUser legacyUser = legacyRepo.save(toLegacy(user));\n        \n        // Dual write to new database\n        if (featureFlags.isEnabled(\"dual-write\")) {\n            try {\n                newRepo.save(user);\n            } catch (Exception e) {\n                log.error(\"Failed to write to new database\", e);\n                // Continue - legacy is primary\n            }\n        }\n        \n        return fromLegacy(legacyUser);\n    }\n    \n    public User getUser(String userId) {\n        // Phase 2: Read from new database if enabled\n        if (featureFlags.isEnabled(\"read-from-new\")) {\n            Optional<User> user = newRepo.findById(userId);\n            if (user.isPresent()) {\n                return user.get();\n            }\n            // Fallback to legacy\n        }\n        \n        // Default: read from legacy\n        return legacyRepo.findById(userId)\n            .map(this::fromLegacy)\n            .orElseThrow(() -> new UserNotFoundException(userId));\n    }\n}\n\n// Data Backfill Job\n@Component\npublic class DataMigrationJob {\n    \n    @Autowired\n    private LegacyUserRepository legacyRepo;\n    @Autowired\n    private NewUserRepository newRepo;\n    \n    @Scheduled(fixedDelay = 60000)\n    public void migrateUsers() {\n        if (!featureFlags.isEnabled(\"run-migration\")) {\n            return;\n        }\n        \n        int batchSize = 100;\n        int offset = 0;\n        \n        while (true) {\n            List<LegacyUser> batch = legacyRepo.findAll(\n                PageRequest.of(offset, batchSize)\n            ).getContent();\n            \n            if (batch.isEmpty()) {\n                break;\n            }\n            \n            // Migrate batch\n            List<User> newUsers = batch.stream()\n                .map(this::convertToNew)\n                .collect(Collectors.toList());\n            \n            newRepo.saveAll(newUsers);\n            log.info(\"Migrated batch of {} users\", batch.size());\n            \n            offset++;\n            \n            // Rate limiting\n            try {\n                Thread.sleep(1000);\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n                break;\n            }\n        }\n    }\n}\n\n// Data Consistency Checker\n@Component\npublic class DataConsistencyChecker {\n    \n    public void verifyConsistency() {\n        List<String> userIds = legacyRepo.findAllIds();\n        \n        int mismatches = 0;\n        for (String userId : userIds) {\n            LegacyUser legacy = legacyRepo.findById(userId).orElse(null);\n            User newUser = newRepo.findById(userId).orElse(null);\n            \n            if (!areEqual(legacy, newUser)) {\n                log.warn(\"Data mismatch for user: {}\", userId);\n                mismatches++;\n            }\n        }\n        \n        log.info(\"Consistency check complete. Mismatches: {}\", mismatches);\n    }\n}"
    },
    {
      "id": 65,
      "question": "What is the purpose of contract testing in Microservices?",
      "answer": "Contract testing verifies that service interactions conform to agreed-upon contracts, preventing integration issues.\n\nTypes of Contract Testing:\n• Consumer-Driven Contracts: Consumer defines expectations\n• Provider Contracts: Provider publishes contract\n• Schema-Based: API schema validation\n\nContract Testing Tools:\n• Pact: Consumer-driven contract testing\n• Spring Cloud Contract: Provider-side contracts\n• Postman Contract Testing\n• OpenAPI validation\n\nConsumer-Driven Contract Flow:\n• Consumer defines expected interactions\n• Generate contract from consumer tests\n• Provider verifies against contract\n• Contract stored in broker/repository\n• Both sides test independently\n\nBenefits:\n• Early detection of breaking changes\n• Independent service testing\n• Clear API contracts\n• Reduces integration testing\n• Fast feedback\n• Documentation as code\n• Version compatibility validation\n\nBest Practices:\n• Test realistic scenarios\n• Keep contracts minimal\n• Version contracts\n• Automate verification\n• Include in CI/CD\n• Store contracts centrally\n• Test backward compatibility",
      "explanation": "Contract testing validates service interactions against agreed contracts using tools like Pact, enabling independent service testing and early detection of breaking changes without requiring integrated test environments.",
      "difficulty": "Medium",
      "code": "// Consumer Contract Test with Pact\n@ExtendWith(PactConsumerTestExt.class)\npublic class OrderServiceContractTest {\n    \n    @Pact(consumer = \"order-service\", provider = \"user-service\")\n    public RequestResponsePact getUserContract(PactDslWithProvider builder) {\n        return builder\n            .given(\"user with id 123 exists\")\n            .uponReceiving(\"get user request\")\n            .path(\"/users/123\")\n            .method(\"GET\")\n            .willRespondWith()\n            .status(200)\n            .headers(Map.of(\"Content-Type\", \"application/json\"))\n            .body(new PactDslJsonBody()\n                .stringValue(\"id\", \"123\")\n                .stringValue(\"name\", \"John Doe\")\n                .stringValue(\"email\", \"john@example.com\"))\n            .toPact();\n    }\n    \n    @Test\n    @PactTestFor(pactMethod = \"getUserContract\")\n    void testGetUser(MockServer mockServer) {\n        // Test consumer against mock provider\n        RestTemplate restTemplate = new RestTemplate();\n        String url = mockServer.getUrl() + \"/users/123\";\n        \n        User user = restTemplate.getForObject(url, User.class);\n        \n        assertThat(user.getId()).isEqualTo(\"123\");\n        assertThat(user.getName()).isEqualTo(\"John Doe\");\n    }\n}\n\n// Provider Contract Verification\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\n@Provider(\"user-service\")\n@PactBroker(host = \"pact-broker.example.com\", port = \"80\")\npublic class UserServiceProviderTest {\n    \n    @LocalServerPort\n    private int port;\n    \n    @Autowired\n    private UserRepository userRepository;\n    \n    @BeforeEach\n    void setUp(PactVerificationContext context) {\n        context.setTarget(new HttpTestTarget(\"localhost\", port));\n    }\n    \n    @State(\"user with id 123 exists\")\n    public void userExists() {\n        // Set up test data\n        User user = new User(\"123\", \"John Doe\", \"john@example.com\");\n        userRepository.save(user);\n    }\n    \n    @TestTemplate\n    @ExtendWith(PactVerificationInvocationContextProvider.class)\n    void verifyPact(PactVerificationContext context) {\n        context.verifyInteraction();\n    }\n}\n\n// Spring Cloud Contract\n// Contract DSL (user-service-contract.groovy)\nContract.make {\n    request {\n        method 'GET'\n        url '/users/123'\n    }\n    response {\n        status 200\n        headers {\n            contentType('application/json')\n        }\n        body([\n            id: '123',\n            name: 'John Doe',\n            email: 'john@example.com'\n        ])\n    }\n}\n\n// Provider Verification (Auto-generated)\n@SpringBootTest\n@AutoConfigureStubRunner(\n    ids = \"com.example:user-service:+:stubs:8080\",\n    stubsMode = StubRunnerProperties.StubsMode.LOCAL\n)\npublic class ContractVerificationTest {\n    // Tests auto-generated from contracts\n}"
    },
    {
      "id": 66,
      "question": "What is the difference between orchestration and coordination in Microservices?",
      "answer": "Orchestration and coordination represent different approaches to managing interactions between microservices.\n\nOrchestration:\n• Central controller manages workflow\n• Explicit control flow\n• Orchestrator knows all participants\n• Command-driven communication\n• Synchronous coordination\n• Easy to understand workflow\n• Single point of control\n\nCoordination:\n• Distributed decision making\n• Implicit control flow\n• Services coordinate independently\n• Event-driven communication\n• Asynchronous coordination\n• Emergent workflow\n• No central controller\n\nOrchestration Characteristics:\n• Workflow engine coordinates\n• Clear sequence of steps\n• Centralized error handling\n• Better for complex processes\n• Easier debugging\n• Potential bottleneck\n\nCoordination Characteristics:\n• Services self-organize\n• Event-based triggers\n• Distributed error handling\n• Better scalability\n• Harder to debug\n• More resilient\n\nWhen to Use:\n• Orchestration: Complex business processes, clear workflow\n• Coordination: Simple flows, high scalability needs\n\nHybrid Approach:\n• Use both patterns\n• Orchestrate complex workflows\n• Coordinate simple interactions",
      "explanation": "Orchestration uses centralized control with explicit workflows managed by a coordinator, while coordination relies on distributed decision-making with services reacting to events independently for emergent behavior.",
      "difficulty": "Hard"
    },
    {
      "id": 67,
      "question": "How do you implement multi-tenancy in Microservices?",
      "answer": "Multi-tenancy allows multiple customers (tenants) to share the same application while keeping their data isolated.\n\nMulti-Tenancy Strategies:\n• Database per Tenant: Complete isolation\n• Schema per Tenant: Shared database, separate schemas\n• Shared Schema: Single database, tenant ID column\n• Hybrid: Mix of approaches\n\nDatabase per Tenant:\n• Complete data isolation\n• Easy tenant migration\n• Higher cost\n• More complex management\n• Best for security-critical applications\n\nSchema per Tenant:\n• Moderate isolation\n• Shared resources\n• Easier backup per tenant\n• Medium complexity\n\nShared Schema:\n• Lowest cost\n• Highest density\n• Row-level security needed\n• Risk of data leakage\n• Simplest to manage\n\nImplementation Considerations:\n• Tenant identification (subdomain, header, JWT)\n• Connection pooling per tenant\n• Caching strategies\n• Data migration\n• Backup and restore\n• Compliance requirements\n\nBest Practices:\n• Always filter by tenant ID\n• Implement tenant context\n• Validate tenant access\n• Monitor per-tenant usage\n• Test cross-tenant isolation",
      "explanation": "Multi-tenancy shares microservices across tenants using strategies like database per tenant for isolation or shared schema with tenant ID filtering, balancing cost, isolation, and complexity based on requirements.",
      "difficulty": "Hard",
      "code": "// Tenant Context Management\n@Component\npublic class TenantContext {\n    \n    private static final ThreadLocal<String> currentTenant = new ThreadLocal<>();\n    \n    public static void setTenant(String tenantId) {\n        currentTenant.set(tenantId);\n    }\n    \n    public static String getTenant() {\n        return currentTenant.get();\n    }\n    \n    public static void clear() {\n        currentTenant.remove();\n    }\n}\n\n// Tenant Identification Filter\n@Component\npublic class TenantFilter extends OncePerRequestFilter {\n    \n    @Override\n    protected void doFilterInternal(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            FilterChain filterChain) throws ServletException, IOException {\n        \n        String tenantId = extractTenantId(request);\n        \n        if (tenantId == null) {\n            response.sendError(HttpStatus.BAD_REQUEST.value(), \"Tenant ID missing\");\n            return;\n        }\n        \n        try {\n            TenantContext.setTenant(tenantId);\n            filterChain.doFilter(request, response);\n        } finally {\n            TenantContext.clear();\n        }\n    }\n    \n    private String extractTenantId(HttpServletRequest request) {\n        // From header\n        String tenantId = request.getHeader(\"X-Tenant-ID\");\n        \n        // From subdomain\n        if (tenantId == null) {\n            String host = request.getServerName();\n            if (host.contains(\".\")) {\n                tenantId = host.substring(0, host.indexOf(\".\"));\n            }\n        }\n        \n        // From JWT token\n        if (tenantId == null) {\n            String token = request.getHeader(\"Authorization\");\n            tenantId = extractTenantFromToken(token);\n        }\n        \n        return tenantId;\n    }\n}\n\n// Database per Tenant - Dynamic DataSource\n@Component\npublic class TenantDataSource {\n    \n    private Map<String, DataSource> tenantDataSources = new ConcurrentHashMap<>();\n    \n    public DataSource getDataSource(String tenantId) {\n        return tenantDataSources.computeIfAbsent(tenantId, this::createDataSource);\n    }\n    \n    private DataSource createDataSource(String tenantId) {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:postgresql://localhost/tenant_\" + tenantId);\n        config.setUsername(\"app\");\n        config.setPassword(\"password\");\n        return new HikariDataSource(config);\n    }\n}\n\n// Shared Schema with Row-Level Security\n@Entity\n@Table(name = \"orders\")\n@FilterDef(name = \"tenantFilter\", parameters = @ParamDef(name = \"tenantId\", type = \"string\"))\n@Filter(name = \"tenantFilter\", condition = \"tenant_id = :tenantId\")\npublic class Order {\n    @Id\n    private String id;\n    \n    @Column(name = \"tenant_id\")\n    private String tenantId;\n    \n    // Other fields\n}\n\n@Repository\npublic interface OrderRepository extends JpaRepository<Order, String> {\n    \n    @Override\n    default List<Order> findAll() {\n        return findByTenantId(TenantContext.getTenant());\n    }\n    \n    List<Order> findByTenantId(String tenantId);\n}"
    },
    {
      "id": 68,
      "question": "What is the role of API composition in handling complex queries?",
      "answer": "API composition aggregates data from multiple microservices to answer complex queries that span service boundaries.\n\nQuery Patterns:\n• Sequential Composition: Call services one after another\n• Parallel Composition: Call services concurrently\n• Nested Composition: Results feed into next query\n• Conditional Composition: Based on intermediate results\n\nImplementation Approaches:\n• API Gateway composition\n• Backend for Frontend (BFF)\n• Dedicated aggregator service\n• CQRS read models\n• GraphQL federation\n\nChallenges:\n• Performance overhead\n• Partial failures\n• Transaction boundaries\n• Data consistency\n• Complex error handling\n• Timeout management\n\nOptimization Strategies:\n• Parallel requests\n• Caching intermediate results\n• Request batching\n• Result streaming\n• Pagination\n• Field selection\n\nAlternatives:\n• CQRS with materialized views\n• Event sourcing with projections\n• Database views (if same DB)\n• Search engine aggregation\n• Data lake queries\n\nBest Practices:\n• Set appropriate timeouts\n• Implement circuit breakers\n• Handle partial results\n• Cache when possible\n• Monitor performance\n• Consider CQRS for complex queries",
      "explanation": "API composition combines data from multiple microservices to answer complex queries by making parallel or sequential service calls, aggregating results, though CQRS with read models may be better for frequently accessed complex queries.",
      "difficulty": "Medium",
      "code": "// Complex Query with API Composition\n@Service\npublic class CustomerDashboardService {\n    \n    @Autowired\n    private WebClient webClient;\n    \n    public Mono<CustomerDashboard> getCustomerDashboard(String customerId) {\n        // Parallel composition of multiple service calls\n        Mono<Customer> customerMono = getCustomer(customerId);\n        Mono<List<Order>> ordersMono = getOrders(customerId);\n        Mono<Wallet> walletMono = getWallet(customerId);\n        Mono<List<Address>> addressesMono = getAddresses(customerId);\n        Mono<Loyalty> loyaltyMono = getLoyalty(customerId);\n        \n        // Combine all results\n        return Mono.zip(\n            customerMono,\n            ordersMono,\n            walletMono,\n            addressesMono,\n            loyaltyMono\n        ).map(tuple -> new CustomerDashboard(\n            tuple.getT1(),  // Customer\n            tuple.getT2(),  // Orders\n            tuple.getT3(),  // Wallet\n            tuple.getT4(),  // Addresses\n            tuple.getT5()   // Loyalty\n        ));\n    }\n    \n    // Nested composition - order details with products\n    public Mono<OrderDetails> getOrderDetails(String orderId) {\n        return getOrder(orderId)\n            .flatMap(order -> {\n                // Get customer based on order\n                Mono<Customer> customerMono = getCustomer(order.getCustomerId());\n                \n                // Get products for order items\n                Mono<List<Product>> productsMono = Flux.fromIterable(order.getItems())\n                    .flatMap(item -> getProduct(item.getProductId()))\n                    .collectList();\n                \n                return Mono.zip(Mono.just(order), customerMono, productsMono)\n                    .map(tuple -> new OrderDetails(\n                        tuple.getT1(),\n                        tuple.getT2(),\n                        tuple.getT3()\n                    ));\n            });\n    }\n    \n    private Mono<Customer> getCustomer(String id) {\n        return webClient.get()\n            .uri(\"http://customer-service/customers/{id}\", id)\n            .retrieve()\n            .bodyToMono(Customer.class)\n            .timeout(Duration.ofSeconds(2))\n            .onErrorResume(e -> Mono.just(new Customer(id, \"Unknown\")));\n    }\n}\n\n// GraphQL API Composition\n@Component\npublic class CustomerDataFetcher implements GraphQLQueryResolver {\n    \n    public CustomerDashboard customer(String id) {\n        // GraphQL automatically handles nested queries\n        return customerDashboardService.getCustomerDashboard(id).block();\n    }\n}\n\n// schema.graphql\ntype Query {\n    customer(id: ID!): CustomerDashboard\n}\n\ntype CustomerDashboard {\n    customer: Customer\n    orders: [Order]\n    wallet: Wallet\n    addresses: [Address]\n    loyalty: Loyalty\n}\n\n// CQRS Alternative - Read Model\n@Document(collection = \"customer_dashboard\")\npublic class CustomerDashboardReadModel {\n    @Id\n    private String customerId;\n    private CustomerData customer;\n    private List<OrderData> recentOrders;\n    private WalletData wallet;\n    private LoyaltyData loyalty;\n    // Pre-aggregated, no composition needed\n}\n\n@EventListener\npublic void onCustomerUpdated(CustomerUpdatedEvent event) {\n    // Update read model asynchronously\n    CustomerDashboardReadModel model = readModelRepo.findById(event.getCustomerId())\n        .orElse(new CustomerDashboardReadModel(event.getCustomerId()));\n    model.updateCustomer(event);\n    readModelRepo.save(model);\n}"
    },
    {
      "id": 69,
      "question": "How do you implement request/response logging in Microservices?",
      "answer": "Request/response logging captures HTTP traffic for debugging, auditing, and monitoring distributed microservices.\n\nLogging Strategies:\n• Interceptor/Filter based logging\n• AOP (Aspect-Oriented Programming)\n• Middleware logging\n• Sidecar proxy logging\n• Centralized logging system\n\nWhat to Log:\n• Request method and path\n• Request headers (sanitized)\n• Request body (if needed)\n• Response status code\n• Response body (if needed)\n• Execution time\n• Correlation ID\n• Timestamp\n• Client IP\n• User ID\n\nBest Practices:\n• Don't log sensitive data (passwords, tokens)\n• Sanitize headers and bodies\n• Use structured logging (JSON)\n• Include correlation IDs\n• Set appropriate log levels\n• Sample in production (not all requests)\n• Be mindful of performance\n• Comply with privacy regulations\n\nLog Levels:\n• DEBUG: Full request/response for development\n• INFO: Basic request info for production\n• WARN: Slow requests, errors\n• ERROR: Failed requests\n\nCentralized Logging:\n• ELK Stack (Elasticsearch, Logstash, Kibana)\n• Splunk\n• CloudWatch Logs\n• Datadog",
      "explanation": "Request/response logging uses filters or interceptors to capture HTTP traffic with correlation IDs and structured formats, sanitizing sensitive data while providing debugging and audit trails in centralized logging systems.",
      "difficulty": "Medium",
      "code": "// Request/Response Logging Filter\n@Component\n@Order(Ordered.HIGHEST_PRECEDENCE)\npublic class RequestResponseLoggingFilter extends OncePerRequestFilter {\n    \n    private static final Set<String> SENSITIVE_HEADERS = Set.of(\n        \"authorization\", \"x-api-key\", \"cookie\"\n    );\n    \n    @Override\n    protected void doFilterInternal(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            FilterChain filterChain) throws ServletException, IOException {\n        \n        long startTime = System.currentTimeMillis();\n        String correlationId = UUID.randomUUID().toString();\n        MDC.put(\"correlationId\", correlationId);\n        \n        // Wrap to capture response\n        ContentCachingRequestWrapper wrappedRequest = \n            new ContentCachingRequestWrapper(request);\n        ContentCachingResponseWrapper wrappedResponse = \n            new ContentCachingResponseWrapper(response);\n        \n        try {\n            // Log request\n            logRequest(wrappedRequest, correlationId);\n            \n            filterChain.doFilter(wrappedRequest, wrappedResponse);\n            \n            // Log response\n            logResponse(wrappedResponse, System.currentTimeMillis() - startTime);\n            \n        } finally {\n            wrappedResponse.copyBodyToResponse();\n            MDC.clear();\n        }\n    }\n    \n    private void logRequest(ContentCachingRequestWrapper request, String correlationId) {\n        Map<String, Object> logData = new HashMap<>();\n        logData.put(\"type\", \"REQUEST\");\n        logData.put(\"correlationId\", correlationId);\n        logData.put(\"method\", request.getMethod());\n        logData.put(\"uri\", request.getRequestURI());\n        logData.put(\"queryString\", request.getQueryString());\n        logData.put(\"clientIp\", request.getRemoteAddr());\n        logData.put(\"headers\", sanitizeHeaders(request));\n        \n        // Log body for POST/PUT (with size limit)\n        if (\"POST\".equals(request.getMethod()) || \"PUT\".equals(request.getMethod())) {\n            byte[] content = request.getContentAsByteArray();\n            if (content.length > 0 && content.length < 10000) {\n                logData.put(\"body\", new String(content, StandardCharsets.UTF_8));\n            }\n        }\n        \n        log.info(\"Request: {}\", new ObjectMapper().writeValueAsString(logData));\n    }\n    \n    private void logResponse(ContentCachingResponseWrapper response, long duration) {\n        Map<String, Object> logData = new HashMap<>();\n        logData.put(\"type\", \"RESPONSE\");\n        logData.put(\"status\", response.getStatus());\n        logData.put(\"durationMs\", duration);\n        \n        // Log response body (with size limit)\n        byte[] content = response.getContentAsByteArray();\n        if (content.length > 0 && content.length < 10000) {\n            logData.put(\"body\", new String(content, StandardCharsets.UTF_8));\n        }\n        \n        if (response.getStatus() >= 400) {\n            log.error(\"Response: {}\", new ObjectMapper().writeValueAsString(logData));\n        } else {\n            log.info(\"Response: {}\", new ObjectMapper().writeValueAsString(logData));\n        }\n    }\n    \n    private Map<String, String> sanitizeHeaders(HttpServletRequest request) {\n        Map<String, String> headers = new HashMap<>();\n        Collections.list(request.getHeaderNames()).forEach(name -> {\n            if (!SENSITIVE_HEADERS.contains(name.toLowerCase())) {\n                headers.put(name, request.getHeader(name));\n            } else {\n                headers.put(name, \"[REDACTED]\");\n            }\n        });\n        return headers;\n    }\n}\n\n// AOP-based Logging\n@Aspect\n@Component\npublic class RestControllerLoggingAspect {\n    \n    @Around(\"@within(org.springframework.web.bind.annotation.RestController)\")\n    public Object logAround(ProceedingJoinPoint joinPoint) throws Throwable {\n        long startTime = System.currentTimeMillis();\n        \n        log.info(\"Entering: {}.{}()\", \n            joinPoint.getSignature().getDeclaringTypeName(),\n            joinPoint.getSignature().getName());\n        \n        try {\n            Object result = joinPoint.proceed();\n            long duration = System.currentTimeMillis() - startTime;\n            log.info(\"Exiting: {}.{}() - Duration: {}ms\",\n                joinPoint.getSignature().getDeclaringTypeName(),\n                joinPoint.getSignature().getName(),\n                duration);\n            return result;\n        } catch (Exception e) {\n            log.error(\"Exception in {}.{}()\",\n                joinPoint.getSignature().getDeclaringTypeName(),\n                joinPoint.getSignature().getName(),\n                e);\n            throw e;\n        }\n    }\n}"
    },
    {
      "id": 70,
      "question": "What is the difference between stateless and stateful microservices?",
      "answer": "Stateless and stateful microservices differ in how they handle data across requests.\n\nStateless Microservices:\n• No session data stored in service\n• Each request contains all necessary information\n• Any instance can handle any request\n• Horizontally scalable\n• No state synchronization needed\n• Session stored externally (database, cache)\n\nStateless Characteristics:\n• Easy to scale\n• Simple load balancing\n• No sticky sessions needed\n• Restart without data loss\n• Cloud-native friendly\n• Preferred for microservices\n\nStateful Microservices:\n• Store session/state information\n• Request routing to same instance\n• Sticky sessions required\n• Scaling more complex\n• State management challenges\n• Potential data loss on restart\n\nStateful Characteristics:\n• Better performance (cached state)\n• Complex load balancing\n• Sticky sessions needed\n• State replication required\n• Harder to scale\n• Used when necessary\n\nExternalizing State:\n• Use Redis for session storage\n• Database for persistent state\n• Distributed cache\n• Message queues for workflows\n\nBest Practices:\n• Design stateless when possible\n• Externalize session state\n• Use tokens (JWT) for auth\n• Avoid in-memory state\n• Plan for instance failures",
      "explanation": "Stateless microservices don't store session data and can scale easily as any instance handles any request, while stateful services maintain state requiring sticky sessions and complex scaling, with stateless being preferred.",
      "difficulty": "Easy",
      "code": "// Stateless Service - Recommended\n@RestController\npublic class OrderController {\n    \n    @Autowired\n    private OrderService orderService;\n    \n    @GetMapping(\"/orders/{id}\")\n    public Order getOrder(\n            @PathVariable String id,\n            @RequestHeader(\"Authorization\") String token) {\n        // No state stored in service\n        // All info from request or external storage\n        String userId = jwtService.getUserId(token);\n        return orderService.getOrder(id, userId);\n    }\n    \n    @PostMapping(\"/orders\")\n    public Order createOrder(\n            @RequestBody OrderRequest request,\n            @RequestHeader(\"Authorization\") String token) {\n        // Each request independent\n        String userId = jwtService.getUserId(token);\n        return orderService.createOrder(request, userId);\n    }\n}\n\n// Session State Externalized to Redis\n@Configuration\npublic class SessionConfig {\n    \n    @Bean\n    public RedisTemplate<String, Object> redisTemplate(\n            RedisConnectionFactory factory) {\n        RedisTemplate<String, Object> template = new RedisTemplate<>();\n        template.setConnectionFactory(factory);\n        return template;\n    }\n}\n\n@Service\npublic class SessionService {\n    \n    @Autowired\n    private RedisTemplate<String, Object> redisTemplate;\n    \n    public void storeSession(String sessionId, Map<String, Object> data) {\n        redisTemplate.opsForHash().putAll(\"session:\" + sessionId, data);\n        redisTemplate.expire(\"session:\" + sessionId, 30, TimeUnit.MINUTES);\n    }\n    \n    public Map<Object, Object> getSession(String sessionId) {\n        return redisTemplate.opsForHash().entries(\"session:\" + sessionId);\n    }\n}\n\n// Stateful Service - Use Only When Necessary\n@RestController\npublic class WebSocketController {\n    \n    // Stateful - maintains WebSocket connections\n    private Map<String, WebSocketSession> sessions = new ConcurrentHashMap<>();\n    \n    @MessageMapping(\"/chat\")\n    public void handleChatMessage(ChatMessage message, WebSocketSession session) {\n        // State maintained for connection duration\n        sessions.put(session.getId(), session);\n        broadcastMessage(message);\n    }\n    \n    // Requires sticky sessions or session replication\n}\n\n// JWT Token for Stateless Auth\npublic class JwtService {\n    \n    public String createToken(String userId) {\n        return Jwts.builder()\n            .setSubject(userId)\n            .claim(\"roles\", Arrays.asList(\"USER\"))\n            .setIssuedAt(new Date())\n            .setExpiration(new Date(System.currentTimeMillis() + 3600000))\n            .signWith(SignatureAlgorithm.HS256, secretKey)\n            .compact();\n    }\n    \n    public String getUserId(String token) {\n        return Jwts.parser()\n            .setSigningKey(secretKey)\n            .parseClaimsJws(token)\n            .getBody()\n            .getSubject();\n    }\n}"
    },
    {
      "id": 71,
      "question": "What is the Backend for Frontend (BFF) pattern and its benefits?",
      "answer": "Backend for Frontend (BFF) creates separate backend services optimized for specific frontend applications or user experiences.\n\nCore Concept:\n• One BFF per frontend type\n• Mobile BFF optimized for mobile constraints\n• Web BFF for rich web applications\n• IoT BFF for embedded devices\n• Third-party BFF for external integrations\n\nBFF Responsibilities:\n• Data aggregation from multiple services\n• Format transformation\n• Response optimization for client\n• Client-specific business logic\n• Authentication and authorization\n• Caching strategies\n• Error handling\n\nBenefits:\n• Optimized responses per client type\n• Reduced over-fetching and under-fetching\n• Frontend team autonomy\n• Independent evolution\n• Better performance\n• Reduced client complexity\n• Security boundaries\n\nChallenges:\n• Code duplication\n• Multiple services to maintain\n• Team coordination\n• Shared logic management\n• Testing complexity\n\nBFF vs API Gateway:\n• BFF: Client-specific optimization\n• API Gateway: Shared infrastructure\n• Often used together\n• Gateway routes to BFFs",
      "explanation": "BFF pattern creates dedicated backend services for each frontend type (mobile, web) to provide optimized data aggregation and transformation, improving performance and enabling independent frontend evolution.",
      "difficulty": "Medium",
      "code": "// Mobile BFF - Lightweight responses\n@RestController\n@RequestMapping(\"/mobile/api\")\npublic class MobileBFFController {\n    \n    @Autowired\n    private UserService userService;\n    @Autowired\n    private OrderService orderService;\n    @Autowired\n    private ProductService productService;\n    \n    @GetMapping(\"/home\")\n    public MobileHomeResponse getHome(@RequestHeader(\"X-User-ID\") String userId) {\n        // Optimized for mobile bandwidth\n        UserSummary user = userService.getUserSummary(userId);\n        List<OrderSummary> recentOrders = orderService.getRecentOrders(userId, 3);\n        List<ProductCard> featured = productService.getFeaturedProducts(5);\n        \n        return MobileHomeResponse.builder()\n            .userName(user.getName())\n            .orderCount(recentOrders.size())\n            .featuredProducts(featured)\n            .build();\n    }\n    \n    @GetMapping(\"/product/{id}\")\n    public MobileProductResponse getProduct(@PathVariable String id) {\n        Product product = productService.getProduct(id);\n        \n        // Minimal data for mobile\n        return MobileProductResponse.builder()\n            .id(product.getId())\n            .name(product.getName())\n            .price(product.getPrice())\n            .thumbnailUrl(product.getThumbnailUrl())  // Small image\n            .rating(product.getAverageRating())\n            .build();\n    }\n}\n\n// Web BFF - Rich responses\n@RestController\n@RequestMapping(\"/web/api\")\npublic class WebBFFController {\n    \n    @GetMapping(\"/home\")\n    public WebHomeResponse getHome(@RequestHeader(\"X-User-ID\") String userId) {\n        // Rich data for web with analytics\n        UserProfile user = userService.getUserProfile(userId);\n        List<Order> orders = orderService.getAllOrders(userId);\n        Analytics analytics = analyticsService.getUserAnalytics(userId);\n        List<Product> recommended = recommendationService.getRecommendations(userId, 20);\n        List<Product> viewed = productService.getRecentlyViewed(userId);\n        \n        return WebHomeResponse.builder()\n            .userProfile(user)\n            .orders(orders)\n            .analytics(analytics)\n            .recommendations(recommended)\n            .recentlyViewed(viewed)\n            .build();\n    }\n    \n    @GetMapping(\"/product/{id}\")\n    public WebProductResponse getProduct(@PathVariable String id) {\n        // Comprehensive product information\n        Product product = productService.getProductDetails(id);\n        List<Review> reviews = reviewService.getReviews(id, 0, 50);\n        List<Product> similar = recommendationService.getSimilarProducts(id, 10);\n        Inventory inventory = inventoryService.getInventory(id);\n        \n        return WebProductResponse.builder()\n            .product(product)\n            .reviews(reviews)\n            .similarProducts(similar)\n            .inventory(inventory)\n            .highResImages(product.getAllImages())  // Full res images\n            .specifications(product.getAllSpecs())\n            .build();\n    }\n}\n\n// Shared Service Layer\n@Service\npublic class UserService {\n    \n    public UserSummary getUserSummary(String userId) {\n        User user = userRepository.findById(userId).orElseThrow();\n        return new UserSummary(user.getId(), user.getName());\n    }\n    \n    public UserProfile getUserProfile(String userId) {\n        User user = userRepository.findById(userId).orElseThrow();\n        return new UserProfile(user);  // Full details\n    }\n}"
    },
    {
      "id": 72,
      "question": "How do you implement security scanning in CI/CD for Microservices?",
      "answer": "Security scanning in CI/CD pipeline identifies vulnerabilities early in the development lifecycle.\n\nTypes of Security Scanning:\n• Static Application Security Testing (SAST)\n• Dynamic Application Security Testing (DAST)\n• Software Composition Analysis (SCA)\n• Container Image Scanning\n• Secret Detection\n• Infrastructure as Code (IaC) scanning\n\nSAST - Source Code Analysis:\n• Scan source code for vulnerabilities\n• Before compilation\n• Tools: SonarQube, Checkmarx, Fortify\n• Detect: SQL injection, XSS, code quality issues\n\nSCA - Dependency Analysis:\n• Scan third-party libraries\n• Known vulnerability databases (CVE)\n• Tools: OWASP Dependency-Check, Snyk, WhiteSource\n• Check for outdated dependencies\n\nContainer Scanning:\n• Scan Docker images\n• Base image vulnerabilities\n• Tools: Trivy, Clair, Anchore\n• Before deployment\n\nSecret Detection:\n• Find hardcoded credentials\n• API keys, passwords\n• Tools: GitLeaks, TruffleHog\n• Prevent commits with secrets\n\nBest Practices:\n• Scan at multiple stages\n• Fail builds on critical issues\n• Regular dependency updates\n• Automated fixes\n• Security as code\n• Track and remediate findings",
      "explanation": "Security scanning integrates SAST for code analysis, SCA for dependency checking, container scanning for images, and secret detection into CI/CD pipelines to identify and prevent vulnerabilities before deployment.",
      "difficulty": "Hard",
      "code": "# Jenkins CI/CD Pipeline with Security Scanning\npipeline {\n    agent any\n    \n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/org/microservice.git'\n            }\n        }\n        \n        stage('Secret Detection') {\n            steps {\n                sh 'gitleaks detect --source . --report-format json --report-path gitleaks-report.json'\n            }\n        }\n        \n        stage('Dependency Check - SCA') {\n            steps {\n                sh 'mvn dependency-check:check'\n                publishHTML([\n                    reportDir: 'target/dependency-check-report',\n                    reportFiles: 'dependency-check-report.html',\n                    reportName: 'OWASP Dependency Check'\n                ])\n            }\n        }\n        \n        stage('SAST - SonarQube') {\n            steps {\n                withSonarQubeEnv('SonarQube') {\n                    sh 'mvn sonar:sonar'\n                }\n            }\n        }\n        \n        stage('Quality Gate') {\n            steps {\n                timeout(time: 1, unit: 'HOURS') {\n                    waitForQualityGate abortPipeline: true\n                }\n            }\n        }\n        \n        stage('Build') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        \n        stage('Build Docker Image') {\n            steps {\n                sh 'docker build -t user-service:${BUILD_NUMBER} .'\n            }\n        }\n        \n        stage('Container Scanning') {\n            steps {\n                sh 'trivy image --severity HIGH,CRITICAL --exit-code 1 user-service:${BUILD_NUMBER}'\n            }\n        }\n        \n        stage('Deploy to Staging') {\n            steps {\n                sh 'kubectl apply -f k8s/staging/'\n            }\n        }\n        \n        stage('DAST - ZAP Scan') {\n            steps {\n                sh '''\n                    docker run -t owasp/zap2docker-stable zap-baseline.py \\\n                    -t http://staging.example.com \\\n                    -r zap-report.html\n                '''\n            }\n        }\n    }\n    \n    post {\n        always {\n            archiveArtifacts artifacts: '**/dependency-check-report.html', allowEmptyArchive: true\n            archiveArtifacts artifacts: 'gitleaks-report.json', allowEmptyArchive: true\n            archiveArtifacts artifacts: 'zap-report.html', allowEmptyArchive: true\n        }\n        failure {\n            emailext subject: 'Security Scan Failed: ${JOB_NAME} - ${BUILD_NUMBER}',\n                    body: 'Check console output for details.',\n                    to: 'security-team@example.com'\n        }\n    }\n}\n\n# GitHub Actions Security Workflow\nname: Security Scan\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Run Trivy vulnerability scanner\n      uses: aquasecurity/trivy-action@master\n      with:\n        scan-type: 'fs'\n        scan-ref: '.'\n        format: 'sarif'\n        output: 'trivy-results.sarif'\n    \n    - name: Upload Trivy results to GitHub Security\n      uses: github/codeql-action/upload-sarif@v2\n      with:\n        sarif_file: 'trivy-results.sarif'\n    \n    - name: Run Snyk to check for vulnerabilities\n      uses: snyk/actions/maven@master\n      env:\n        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n      with:\n        args: --severity-threshold=high\n    \n    - name: GitLeaks Scan\n      uses: gitleaks/gitleaks-action@v2\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}"
    },
    {
      "id": 73,
      "question": "What is service registry and discovery pattern?",
      "answer": "Service registry and discovery enables microservices to find and communicate with each other dynamically without hardcoded addresses.\n\nService Registry:\n• Central database of service instances\n• Stores service locations and metadata\n• Provides lookup capability\n• Monitors service health\n• Updates dynamically\n\nDiscovery Patterns:\n• Client-Side Discovery: Client queries registry\n• Server-Side Discovery: Load balancer queries registry\n• DNS-Based: Service discovery via DNS\n\nClient-Side Discovery:\n• Client queries service registry\n• Client chooses instance (load balancing)\n• Direct connection to service\n• Examples: Eureka, Consul\n\nServer-Side Discovery:\n• Client calls load balancer/router\n• Router queries service registry\n• Router forwards to instance\n• Examples: AWS ELB, Kubernetes Service\n\nRegistration Patterns:\n• Self-Registration: Service registers itself\n• Third-Party Registration: Registrar registers service\n\nBenefits:\n• Dynamic service locations\n• Automatic scaling support\n• Failover handling\n• Load balancing\n• No hardcoded URLs\n\nPopular Implementations:\n• Netflix Eureka\n• Consul\n• Zookeeper\n• etcd\n• Kubernetes DNS",
      "explanation": "Service registry maintains a dynamic database of available service instances while discovery patterns enable services to find each other through client-side queries or server-side load balancer lookups without hardcoded addresses.",
      "difficulty": "Medium",
      "code": "// Eureka Service Registry Setup\n@SpringBootApplication\n@EnableEurekaServer\npublic class ServiceRegistryApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(ServiceRegistryApplication.class, args);\n    }\n}\n\n// application.yml (Registry)\nserver:\n  port: 8761\n\neureka:\n  client:\n    register-with-eureka: false\n    fetch-registry: false\n  server:\n    enable-self-preservation: false\n\n// Service Registration (Self-Registration)\n@SpringBootApplication\n@EnableDiscoveryClient\npublic class UserServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(UserServiceApplication.class, args);\n    }\n}\n\n// application.yml (Service)\nspring:\n  application:\n    name: user-service\n\neureka:\n  client:\n    service-url:\n      defaultZone: http://localhost:8761/eureka/\n    registry-fetch-interval-seconds: 5\n  instance:\n    prefer-ip-address: true\n    lease-renewal-interval-in-seconds: 10\n    lease-expiration-duration-in-seconds: 30\n    instance-id: ${spring.application.name}:${random.value}\n    metadata-map:\n      version: 1.0\n      region: us-east-1\n\n// Client-Side Discovery\n@Configuration\npublic class RestTemplateConfig {\n    \n    @Bean\n    @LoadBalanced\n    public RestTemplate restTemplate() {\n        return new RestTemplate();\n    }\n}\n\n@Service\npublic class OrderService {\n    \n    @Autowired\n    @LoadBalanced\n    private RestTemplate restTemplate;\n    \n    @Autowired\n    private DiscoveryClient discoveryClient;\n    \n    public User getUser(String userId) {\n        // Service name instead of URL\n        return restTemplate.getForObject(\n            \"http://user-service/users/\" + userId,\n            User.class\n        );\n    }\n    \n    public List<ServiceInstance> getUserServiceInstances() {\n        return discoveryClient.getInstances(\"user-service\");\n    }\n}\n\n// Manual Service Discovery\n@Service\npublic class ManualDiscoveryService {\n    \n    @Autowired\n    private EurekaClient eurekaClient;\n    \n    public String getUserServiceUrl() {\n        InstanceInfo instance = eurekaClient.getNextServerFromEureka(\n            \"USER-SERVICE\",\n            false\n        );\n        return instance.getHomePageUrl();\n    }\n}\n\n// Kubernetes Service Discovery\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\nspec:\n  selector:\n    app: user-service\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: ClusterIP\n\n# Services can be accessed via:\n# http://user-service.default.svc.cluster.local\n# or simply: http://user-service"
    },
    {
      "id": 74,
      "question": "How do you implement A/B testing in Microservices?",
      "answer": "A/B testing compares two versions of a feature to determine which performs better, implemented through traffic routing and feature flags.\n\nImplementation Approaches:\n• Feature flags with user segmentation\n• Traffic routing rules (service mesh)\n• API Gateway routing\n• Client-side splits\n• Canary deployments\n\nKey Components:\n• User segmentation\n• Traffic splitting\n• Metrics collection\n• Statistical analysis\n• Decision making\n\nUser Segmentation:\n• Random distribution\n• Percentage-based\n• User attributes (location, device)\n• Consistent experience per user\n• Control and treatment groups\n\nTraffic Splitting Methods:\n• Header-based routing\n• Cookie-based routing\n• User ID hash\n• Query parameter\n• Geographic routing\n\nMetrics to Track:\n• Conversion rates\n• User engagement\n• Response times\n• Error rates\n• Business KPIs\n\nBest Practices:\n• Statistical significance\n• Sufficient sample size\n• Run long enough\n• Monitor both groups\n• Document experiments\n• Clean up after test\n• Gradual rollout of winner",
      "explanation": "A/B testing uses feature flags or service mesh routing to split traffic between versions, tracking metrics to determine which performs better based on statistical analysis of user behavior and business outcomes.",
      "difficulty": "Hard",
      "code": "// A/B Testing with Feature Flags\n@Service\npublic class ABTestingService {\n    \n    @Autowired\n    private FeatureFlagService featureFlags;\n    @Autowired\n    private MetricsService metrics;\n    \n    public CheckoutResponse processCheckout(CheckoutRequest request, String userId) {\n        boolean useNewCheckout = determineVariant(userId, \"new-checkout-flow\");\n        \n        CheckoutResponse response;\n        if (useNewCheckout) {\n            metrics.increment(\"checkout.variant.new\");\n            response = newCheckoutService.process(request);\n        } else {\n            metrics.increment(\"checkout.variant.old\");\n            response = oldCheckoutService.process(request);\n        }\n        \n        // Track conversion\n        if (response.isSuccess()) {\n            String variant = useNewCheckout ? \"new\" : \"old\";\n            metrics.increment(\"checkout.conversion.\" + variant);\n            metrics.recordTiming(\"checkout.duration.\" + variant, response.getDuration());\n        }\n        \n        return response;\n    }\n    \n    private boolean determineVariant(String userId, String experimentName) {\n        // Consistent experience per user\n        int hash = Math.abs((userId + experimentName).hashCode());\n        int bucket = hash % 100;\n        \n        // 50/50 split\n        return bucket < 50;\n    }\n}\n\n// Istio-based A/B Testing\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: checkout-service\nspec:\n  hosts:\n  - checkout-service\n  http:\n  - match:\n    - headers:\n        x-user-group:\n          exact: \"test-group-a\"\n    route:\n    - destination:\n        host: checkout-service\n        subset: v2\n  - route:\n    - destination:\n        host: checkout-service\n        subset: v1\n      weight: 50\n    - destination:\n        host: checkout-service\n        subset: v2\n      weight: 50\n\n# User Segmentation\n@Component\npublic class UserSegmentationService {\n    \n    public String assignUserToGroup(String userId, String experimentId) {\n        // Consistent assignment\n        int hash = Math.abs((userId + experimentId).hashCode());\n        \n        // Store assignment\n        String group = (hash % 2 == 0) ? \"control\" : \"treatment\";\n        experimentRepository.saveAssignment(userId, experimentId, group);\n        \n        return group;\n    }\n    \n    public Map<String, String> getUserAttributes(String userId) {\n        User user = userService.getUser(userId);\n        return Map.of(\n            \"country\", user.getCountry(),\n            \"device_type\", user.getDeviceType(),\n            \"user_segment\", user.getSegment()\n        );\n    }\n}\n\n// Metrics Collection\n@Service\npublic class ExperimentMetricsService {\n    \n    @Autowired\n    private MeterRegistry registry;\n    \n    public void trackExperimentEvent(\n            String experimentId,\n            String variant,\n            String eventType,\n            String userId) {\n        \n        // Increment counter\n        Counter.builder(\"experiment.events\")\n            .tag(\"experiment\", experimentId)\n            .tag(\"variant\", variant)\n            .tag(\"event\", eventType)\n            .register(registry)\n            .increment();\n        \n        // Store detailed event\n        ExperimentEvent event = new ExperimentEvent(\n            experimentId,\n            variant,\n            eventType,\n            userId,\n            LocalDateTime.now()\n        );\n        eventRepository.save(event);\n    }\n    \n    public ExperimentResults getResults(String experimentId) {\n        List<ExperimentEvent> events = eventRepository\n            .findByExperimentId(experimentId);\n        \n        return calculateStatistics(events);\n    }\n}"
    },
    {
      "id": 75,
      "question": "What are the best practices for microservice deployment?",
      "answer": "Microservice deployment requires careful orchestration and automation for reliability and speed.\n\nDeployment Strategies:\n• Blue-Green: Zero downtime with instant rollback\n• Canary: Gradual rollout with monitoring\n• Rolling: Sequential instance updates\n• Recreate: Stop old, start new (downtime)\n• Shadow: Test in production without impact\n\nCI/CD Pipeline:\n• Automated testing (unit, integration, contract)\n• Automated builds\n• Container image creation\n• Security scanning\n• Automated deployment\n• Smoke tests\n• Rollback automation\n\nContainerization:\n• Use Docker for consistency\n• Keep images small\n• Multi-stage builds\n• Scan for vulnerabilities\n• Version images properly\n• Use private registry\n\nOrchestration:\n• Kubernetes for container orchestration\n• Define resource limits\n• Health checks (liveness, readiness)\n• Auto-scaling rules\n• Rolling update strategy\n• Pod disruption budgets\n\nBest Practices:\n• Immutable infrastructure\n• Infrastructure as Code (IaC)\n• Environment parity\n• Automated rollbacks\n• Progressive delivery\n• Monitoring from start\n• Database migration strategy\n• Feature flags for deployment\n• Graceful shutdown\n• Version all artifacts",
      "explanation": "Microservice deployment uses containerization, CI/CD pipelines, orchestration platforms like Kubernetes, and strategies like blue-green or canary deployments to achieve automated, safe, and zero-downtime releases with quick rollback capabilities.",
      "difficulty": "Medium",
      "code": "# Kubernetes Rolling Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\nspec:\n  replicas: 5\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2        # Max pods over desired count\n      maxUnavailable: 1  # Max pods unavailable during update\n  selector:\n    matchLabels:\n      app: user-service\n  template:\n    metadata:\n      labels:\n        app: user-service\n        version: v2\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:v2\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 5\n          failureThreshold: 3\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"sleep 15\"]\n      terminationGracePeriodSeconds: 60\n---\n# HorizontalPodAutoscaler\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: user-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: user-service\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n---\n# PodDisruptionBudget\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: user-service-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: user-service\n\n# GitLab CI/CD Pipeline\nstages:\n  - test\n  - build\n  - deploy\n\nvariables:\n  DOCKER_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA\n\ntest:\n  stage: test\n  script:\n    - mvn test\n    - mvn verify\n\nbuild:\n  stage: build\n  script:\n    - docker build -t $DOCKER_IMAGE .\n    - docker push $DOCKER_IMAGE\n\ndeploy-staging:\n  stage: deploy\n  script:\n    - kubectl set image deployment/user-service user-service=$DOCKER_IMAGE -n staging\n    - kubectl rollout status deployment/user-service -n staging\n  environment:\n    name: staging\n\ndeploy-production:\n  stage: deploy\n  script:\n    - kubectl set image deployment/user-service user-service=$DOCKER_IMAGE -n production\n    - kubectl rollout status deployment/user-service -n production\n  environment:\n    name: production\n  when: manual\n  only:\n    - main"
    },
    {
      "id": 76,
      "question": "How do you implement cross-cutting concerns in Microservices?",
      "answer": "Cross-cutting concerns are functionalities that span multiple services requiring consistent implementation.\n\nCommon Cross-Cutting Concerns:\n• Authentication and authorization\n• Logging and monitoring\n• Distributed tracing\n• Rate limiting\n• Caching\n• Error handling\n• Security\n• Transaction management\n\nImplementation Approaches:\n• API Gateway: Centralized cross-cutting logic\n• Service Mesh: Sidecar proxies handle concerns\n• Shared Libraries: Common code across services\n• AOP (Aspect-Oriented Programming): Cross-cutting at code level\n• Platform Services: Dedicated services for concerns\n\nAPI Gateway Approach:\n• Authentication at entry point\n• Rate limiting centrally\n• Request logging\n• Simple but creates bottleneck\n\nService Mesh Approach:\n• Sidecar handles concerns transparently\n• No code changes needed\n• Consistent across all services\n• Examples: Istio, Linkerd\n\nShared Library Approach:\n• Common code packaged as library\n• Risks coupling\n• Version management challenges\n• Language-specific\n\nBest Practices:\n• Externalize configuration\n• Use consistent patterns\n• Automate where possible\n• Monitor effectiveness\n• Document standards\n• Balance consistency vs autonomy",
      "explanation": "Cross-cutting concerns spanning multiple services are handled through API Gateway for centralized logic, service mesh for transparent sidecar-based implementation, or shared libraries, balancing consistency with service autonomy.",
      "difficulty": "Hard",
      "code": "// API Gateway Handling Cross-Cutting Concerns\n@Configuration\npublic class GatewaySecurityConfig {\n    \n    @Bean\n    public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http) {\n        return http\n            .csrf().disable()\n            .authorizeExchange()\n                .pathMatchers(\"/api/public/**\").permitAll()\n                .anyExchange().authenticated()\n            .and()\n            .oauth2ResourceServer()\n                .jwt()\n            .and()\n            .build();\n    }\n}\n\n@Component\npublic class GlobalLoggingFilter implements GlobalFilter, Ordered {\n    \n    @Override\n    public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) {\n        String correlationId = UUID.randomUUID().toString();\n        long startTime = System.currentTimeMillis();\n        \n        log.info(\"Request: {} {}\", \n            exchange.getRequest().getMethod(),\n            exchange.getRequest().getPath());\n        \n        return chain.filter(exchange)\n            .doFinally(signalType -> {\n                long duration = System.currentTimeMillis() - startTime;\n                log.info(\"Response: {} - Duration: {}ms\",\n                    exchange.getResponse().getStatusCode(),\n                    duration);\n            });\n    }\n    \n    @Override\n    public int getOrder() {\n        return Ordered.HIGHEST_PRECEDENCE;\n    }\n}\n\n// AOP for Cross-Cutting Concerns\n@Aspect\n@Component\npublic class LoggingAspect {\n    \n    @Around(\"@annotation(Loggable)\")\n    public Object logMethodExecution(ProceedingJoinPoint joinPoint) throws Throwable {\n        String methodName = joinPoint.getSignature().getName();\n        String className = joinPoint.getTarget().getClass().getSimpleName();\n        \n        log.info(\"Entering: {}.{}\", className, methodName);\n        long startTime = System.currentTimeMillis();\n        \n        try {\n            Object result = joinPoint.proceed();\n            long duration = System.currentTimeMillis() - startTime;\n            log.info(\"Exiting: {}.{} - Duration: {}ms\", className, methodName, duration);\n            return result;\n        } catch (Exception e) {\n            log.error(\"Exception in {}.{}: {}\", className, methodName, e.getMessage());\n            throw e;\n        }\n    }\n}\n\n@Aspect\n@Component\npublic class AuthorizationAspect {\n    \n    @Before(\"@annotation(secured)\")\n    public void checkAuthorization(JoinPoint joinPoint, Secured secured) {\n        String[] requiredRoles = secured.value();\n        Authentication auth = SecurityContextHolder.getContext().getAuthentication();\n        \n        boolean hasRole = Arrays.stream(requiredRoles)\n            .anyMatch(role -> auth.getAuthorities().stream()\n                .anyMatch(a -> a.getAuthority().equals(role)));\n        \n        if (!hasRole) {\n            throw new AccessDeniedException(\"Insufficient permissions\");\n        }\n    }\n}\n\n// Service Mesh - Istio Configuration\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: user-service\nspec:\n  hosts:\n  - user-service\n  http:\n  - fault:\n      delay:\n        percentage:\n          value: 1\n        fixedDelay: 5s\n    timeout: 10s\n    retries:\n      attempts: 3\n      perTryTimeout: 3s\n      retryOn: 5xx,reset,connect-failure\n    route:\n    - destination:\n        host: user-service\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: user-service-authz\nspec:\n  selector:\n    matchLabels:\n      app: user-service\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/sa/order-service\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/users/*\"]\n\n// Shared Library for Common Concerns\npublic class ServiceUtils {\n    \n    public static void validateRequest(Object request) {\n        Set<ConstraintViolation<Object>> violations = \n            validator.validate(request);\n        if (!violations.isEmpty()) {\n            throw new ValidationException(violations);\n        }\n    }\n    \n    public static String getCurrentUserId() {\n        return SecurityContextHolder.getContext()\n            .getAuthentication()\n            .getName();\n    }\n    \n    public static void auditAction(String action, Object data) {\n        AuditLog log = new AuditLog(\n            getCurrentUserId(),\n            action,\n            data,\n            LocalDateTime.now()\n        );\n        auditRepository.save(log);\n    }\n}"
    },
    {
      "id": 77,
      "question": "What is the role of eventual consistency in Microservices?",
      "answer": "Eventual consistency is a consistency model where data becomes consistent across services over time rather than immediately.\n\nWhy Eventual Consistency:\n• Strong consistency hard in distributed systems\n• CAP theorem: Can't have all three (Consistency, Availability, Partition tolerance)\n• Better availability and performance\n• Services can operate independently\n• Reduced coupling between services\n\nHow It Works:\n• Write to local database succeeds\n• Publish event asynchronously\n• Other services consume events\n• Update their local copies\n• Eventually all copies consistent\n\nConsistency Patterns:\n• Read Your Writes: See your own updates immediately\n• Monotonic Reads: Don't see old data after seeing new\n• Causal Consistency: Cause visible before effect\n\nChallenges:\n• Temporary inconsistency window\n• Complex to reason about\n• Requires compensation logic\n• User experience considerations\n• Difficult debugging\n\nWhen to Use:\n• High availability required\n• Distributed data\n• Cross-service transactions\n• Social media feeds\n• Analytics dashboards\n\nWhen to Avoid:\n• Financial transactions\n• Inventory management\n• Strong consistency required\n• Regulatory requirements",
      "explanation": "Eventual consistency accepts temporary data inconsistency across microservices, using asynchronous event propagation to achieve consistency over time, prioritizing availability and performance over immediate consistency.",
      "difficulty": "Hard",
      "code": "// Eventual Consistency with Events\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private OrderRepository orderRepository;\n    @Autowired\n    private EventPublisher eventPublisher;\n    \n    @Transactional\n    public Order createOrder(OrderRequest request) {\n        // 1. Update local state (consistent immediately)\n        Order order = new Order(request);\n        order.setStatus(OrderStatus.PENDING);\n        orderRepository.save(order);\n        \n        // 2. Publish event (eventual consistency)\n        OrderCreatedEvent event = new OrderCreatedEvent(\n            order.getId(),\n            order.getCustomerId(),\n            order.getItems()\n        );\n        eventPublisher.publish(event);\n        \n        return order;  // Returns before other services updated\n    }\n}\n\n// Inventory Service - Eventually Consistent\n@Service\npublic class InventoryService {\n    \n    @Autowired\n    private InventoryRepository inventoryRepository;\n    \n    @EventListener\n    @Transactional\n    public void onOrderCreated(OrderCreatedEvent event) {\n        // Eventually updated (not immediately)\n        try {\n            event.getItems().forEach(item -> {\n                Inventory inventory = inventoryRepository\n                    .findByProductId(item.getProductId());\n                inventory.reserve(item.getQuantity());\n                inventoryRepository.save(inventory);\n            });\n            \n            // Publish success event\n            eventPublisher.publish(\n                new InventoryReservedEvent(event.getOrderId())\n            );\n        } catch (InsufficientInventoryException e) {\n            // Publish failure event for compensation\n            eventPublisher.publish(\n                new InventoryReservationFailedEvent(event.getOrderId())\n            );\n        }\n    }\n}\n\n// Read Model - Eventually Consistent View\n@Document(collection = \"order_summary\")\npublic class OrderSummary {\n    @Id\n    private String orderId;\n    private String customerName;  // Denormalized from customer service\n    private List<String> productNames;  // Denormalized from product service\n    private BigDecimal total;\n    private OrderStatus status;\n    private LocalDateTime lastUpdated;\n}\n\n@Service\npublic class OrderSummaryService {\n    \n    @EventListener\n    public void onOrderCreated(OrderCreatedEvent event) {\n        // Build summary from multiple sources\n        Customer customer = customerService.getCustomer(event.getCustomerId());\n        List<Product> products = productService.getProducts(event.getProductIds());\n        \n        OrderSummary summary = new OrderSummary();\n        summary.setOrderId(event.getOrderId());\n        summary.setCustomerName(customer.getName());\n        summary.setProductNames(products.stream()\n            .map(Product::getName)\n            .collect(Collectors.toList()));\n        summary.setLastUpdated(LocalDateTime.now());\n        \n        orderSummaryRepository.save(summary);\n    }\n    \n    @EventListener\n    public void onCustomerUpdated(CustomerUpdatedEvent event) {\n        // Update denormalized data\n        List<OrderSummary> orders = orderSummaryRepository\n            .findByCustomerId(event.getCustomerId());\n        \n        orders.forEach(order -> {\n            order.setCustomerName(event.getCustomerName());\n            order.setLastUpdated(LocalDateTime.now());\n        });\n        \n        orderSummaryRepository.saveAll(orders);\n    }\n}\n\n// Handling Eventual Consistency in UI\n@RestController\npublic class OrderController {\n    \n    @PostMapping(\"/orders\")\n    public ResponseEntity<OrderResponse> createOrder(@RequestBody OrderRequest request) {\n        Order order = orderService.createOrder(request);\n        \n        // Return immediately with pending status\n        OrderResponse response = new OrderResponse(\n            order.getId(),\n            OrderStatus.PENDING,\n            \"Order is being processed. You will receive a confirmation shortly.\"\n        );\n        \n        return ResponseEntity.accepted().body(response);\n    }\n    \n    @GetMapping(\"/orders/{id}\")\n    public OrderResponse getOrder(@PathVariable String id) {\n        // May show intermediate states\n        Order order = orderService.getOrder(id);\n        return new OrderResponse(order);\n    }\n}"
    },
    {
      "id": 78,
      "question": "How do you handle backward compatibility in Microservices?",
      "answer": "Backward compatibility ensures new service versions work with existing clients without breaking changes.\n\nBackward Compatible Changes:\n• Adding optional fields\n• Adding new endpoints\n• Adding new optional parameters\n• Expanding enums (with default handling)\n• Relaxing validations\n\nBreaking Changes:\n• Removing fields or endpoints\n• Changing field types\n• Making optional fields required\n• Changing response formats\n• Removing enum values\n• Renaming fields\n\nStrategies:\n• API Versioning: Support multiple versions\n• Expand-Contract Pattern: Add before removing\n• Feature Flags: Gradual rollout\n• Consumer-Driven Contracts: Test compatibility\n• Semantic Versioning: Clear version communication\n\nExpand-Contract Pattern:\n• Expand: Add new functionality\n• Migrate: Update clients gradually\n• Contract: Remove old functionality\n• Three-phase approach\n\nBest Practices:\n• Never break existing contracts\n• Deprecation warnings before removal\n• Maintain compatibility period\n• Test with old clients\n• Document breaking changes\n• Use contract testing\n• Version APIs explicitly\n• Monitor API usage",
      "explanation": "Backward compatibility maintains old API contracts using versioning and expand-contract patterns, allowing gradual client migration by adding new features before removing old ones, avoiding breaking changes.",
      "difficulty": "Hard",
      "code": "// Expand-Contract Pattern - Phase 1: Expand\n@RestController\npublic class UserController {\n    \n    // Old endpoint (keep working)\n    @GetMapping(\"/users/{id}\")\n    public UserV1 getUserV1(@PathVariable String id) {\n        User user = userService.getUser(id);\n        return convertToV1(user);\n    }\n    \n    // New endpoint (add)\n    @GetMapping(\"/v2/users/{id}\")\n    public UserV2 getUserV2(@PathVariable String id) {\n        User user = userService.getUser(id);\n        return convertToV2(user);\n    }\n}\n\n// Phase 2: Support both models\npublic class UserV1 {\n    private String id;\n    private String name;\n    private String email;\n    // Old format\n}\n\npublic class UserV2 {\n    private String id;\n    private String firstName;  // Split from name\n    private String lastName;\n    private String email;\n    private String phoneNumber;  // New field\n}\n\n// Backward Compatible Schema Evolution\n@Entity\npublic class Order {\n    @Id\n    private String id;\n    \n    // Old field (deprecated but kept)\n    @Deprecated\n    @Column(name = \"total\")\n    private BigDecimal total;\n    \n    // New fields (optional)\n    @Column(name = \"subtotal\")\n    private BigDecimal subtotal;\n    \n    @Column(name = \"tax\")\n    private BigDecimal tax;\n    \n    @Column(name = \"shipping\")\n    private BigDecimal shipping;\n    \n    // Maintain backward compatibility\n    public BigDecimal getTotal() {\n        if (total != null) {\n            return total;\n        }\n        // Calculate from new fields\n        return subtotal.add(tax).add(shipping);\n    }\n}\n\n// API Versioning with Content Negotiation\n@RestController\npublic class ProductController {\n    \n    @GetMapping(value = \"/products/{id}\", \n                produces = \"application/vnd.company.v1+json\")\n    public ProductV1 getProductV1(@PathVariable String id) {\n        return productService.getProductV1(id);\n    }\n    \n    @GetMapping(value = \"/products/{id}\",\n                produces = \"application/vnd.company.v2+json\")\n    public ProductV2 getProductV2(@PathVariable String id) {\n        return productService.getProductV2(id);\n    }\n}\n\n// Feature Flag for Gradual Migration\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private FeatureFlagService featureFlags;\n    \n    public OrderResponse createOrder(OrderRequest request, String userId) {\n        if (featureFlags.isEnabledForUser(\"new-order-flow\", userId)) {\n            return createOrderV2(request);\n        } else {\n            return createOrderV1(request);\n        }\n    }\n}\n\n// JSON Backward Compatibility\npublic class OrderDTO {\n    private String id;\n    \n    // Support old field name\n    @JsonProperty(\"customerId\")\n    @JsonAlias({\"customer_id\", \"userId\", \"user_id\"})\n    private String customerId;\n    \n    // Ignore unknown properties (forward compatibility)\n    @JsonIgnoreProperties(ignoreUnknown = true)\n    private Map<String, Object> additionalProperties;\n}\n\n// Contract Testing for Compatibility\n@ExtendWith(PactConsumerTestExt.class)\npublic class BackwardCompatibilityTest {\n    \n    @Pact(consumer = \"order-service\", provider = \"user-service\")\n    public RequestResponsePact testV1Compatibility(PactDslWithProvider builder) {\n        return builder\n            .given(\"user exists with old format\")\n            .uponReceiving(\"get user v1\")\n            .path(\"/users/123\")\n            .method(\"GET\")\n            .willRespondWith()\n            .status(200)\n            .body(new PactDslJsonBody()\n                .stringValue(\"id\", \"123\")\n                .stringValue(\"name\", \"John Doe\")  // Old format\n                .stringValue(\"email\", \"john@example.com\"))\n            .toPact();\n    }\n}"
    },
    {
      "id": 79,
      "question": "What is the role of chaos engineering in Microservices?",
      "answer": "Chaos engineering proactively tests system resilience by intentionally injecting failures to identify weaknesses.\n\nChaos Engineering Principles:\n• Build hypothesis about steady state\n• Vary real-world events (failures)\n• Run experiments in production\n• Automate experiments\n• Minimize blast radius\n\nCommon Failure Scenarios:\n• Service instance crashes\n• Network latency\n• Network partitions\n• Resource exhaustion (CPU, memory)\n• Database failures\n• Message broker failures\n• Cascading failures\n• Traffic spikes\n\nChaos Experiments:\n• Kill random instances (Chaos Monkey)\n• Inject latency\n• Fail entire zones\n• Corrupt network packets\n• Fill disk space\n• Simulate service degradation\n\nTools:\n• Chaos Monkey (Netflix)\n• Gremlin\n• Chaos Toolkit\n• Litmus Chaos\n• Pumba\n• Toxiproxy\n\nBest Practices:\n• Start in staging environment\n• Define blast radius\n• Have rollback plan\n• Monitor during experiments\n• Gradual increase in complexity\n• Run during business hours\n• Document learnings\n• Fix identified issues\n\nBenefits:\n• Identify weaknesses proactively\n• Build confidence in system\n• Improve resilience\n• Validate recovery procedures\n• Reduce mean time to recovery",
      "explanation": "Chaos engineering deliberately injects failures like service crashes or network issues into production or staging to test resilience, identify weaknesses, and validate that circuit breakers, retries, and fallbacks work correctly.",
      "difficulty": "Hard",
      "code": "// Chaos Monkey Configuration\n// application.yml\nchaos:\n  monkey:\n    enabled: true\n    watcher:\n      enabled: true\n    assaults:\n      level: 5\n      latencyActive: true\n      latencyRangeStart: 1000\n      latencyRangeEnd: 5000\n      exceptionsActive: true\n      killApplicationActive: true\n      memoryActive: false\n    \n// Chaos Toolkit Experiment\n{\n  \"title\": \"Service resilience to instance failure\",\n  \"description\": \"Test if order service continues when user service instance fails\",\n  \"steady-state-hypothesis\": {\n    \"title\": \"Orders can be created\",\n    \"probes\": [\n      {\n        \"type\": \"probe\",\n        \"name\": \"order-creation-works\",\n        \"tolerance\": 200,\n        \"provider\": {\n          \"type\": \"http\",\n          \"url\": \"http://order-service/health\",\n          \"timeout\": 3\n        }\n      }\n    ]\n  },\n  \"method\": [\n    {\n      \"type\": \"action\",\n      \"name\": \"terminate-user-service-pod\",\n      \"provider\": {\n        \"type\": \"python\",\n        \"module\": \"chaosk8s.pod.actions\",\n        \"func\": \"terminate_pods\",\n        \"arguments\": {\n          \"label_selector\": \"app=user-service\",\n          \"rand\": true,\n          \"ns\": \"production\"\n        }\n      },\n      \"pauses\": {\n        \"after\": 10\n      }\n    }\n  ],\n  \"rollbacks\": []\n}\n\n// Kubernetes Chaos with Litmus\napiVersion: litmuschaos.io/v1alpha1\nkind: ChaosEngine\nmetadata:\n  name: user-service-chaos\nspec:\n  appinfo:\n    appns: default\n    applabel: 'app=user-service'\n    appkind: deployment\n  chaosServiceAccount: litmus-admin\n  experiments:\n    - name: pod-delete\n      spec:\n        components:\n          env:\n            - name: TOTAL_CHAOS_DURATION\n              value: '30'\n            - name: CHAOS_INTERVAL\n              value: '10'\n            - name: FORCE\n              value: 'false'\n    - name: pod-network-latency\n      spec:\n        components:\n          env:\n            - name: NETWORK_LATENCY\n              value: '2000'\n            - name: TOTAL_CHAOS_DURATION\n              value: '60'\n\n// Simulated Chaos in Code\n@Component\npublic class ChaosFilter extends OncePerRequestFilter {\n    \n    @Value(\"${chaos.enabled:false}\")\n    private boolean chaosEnabled;\n    \n    @Value(\"${chaos.failure-rate:0.1}\")\n    private double failureRate;\n    \n    @Override\n    protected void doFilterInternal(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            FilterChain filterChain) throws ServletException, IOException {\n        \n        if (chaosEnabled && shouldInjectChaos()) {\n            int chaosType = new Random().nextInt(3);\n            \n            switch (chaosType) {\n                case 0:\n                    // Inject latency\n                    try {\n                        Thread.sleep(2000 + new Random().nextInt(3000));\n                    } catch (InterruptedException e) {\n                        Thread.currentThread().interrupt();\n                    }\n                    break;\n                case 1:\n                    // Return error\n                    response.sendError(HttpStatus.INTERNAL_SERVER_ERROR.value(),\n                        \"Chaos: Simulated failure\");\n                    return;\n                case 2:\n                    // Timeout (don't respond)\n                    try {\n                        Thread.sleep(30000);\n                    } catch (InterruptedException e) {\n                        Thread.currentThread().interrupt();\n                    }\n                    break;\n            }\n        }\n        \n        filterChain.doFilter(request, response);\n    }\n    \n    private boolean shouldInjectChaos() {\n        return Math.random() < failureRate;\n    }\n}\n\n// Resilience Validation\n@Test\npublic void testResilienceToServiceFailure() {\n    // Given: System in steady state\n    assertThat(orderService.createOrder(validRequest())).isSuccessful();\n    \n    // When: Kill user service instance\n    chaosService.terminatePod(\"user-service\");\n    \n    // Then: Orders still created (with fallback)\n    await().atMost(10, TimeUnit.SECONDS)\n        .untilAsserted(() -> {\n            OrderResult result = orderService.createOrder(validRequest());\n            assertThat(result.isSuccess()).isTrue();\n        });\n}"
    },
    {
      "id": 80,
      "question": "What are the monitoring and observability best practices for Microservices?",
      "answer": "Monitoring and observability provide visibility into distributed microservices systems for troubleshooting and optimization.\n\nThree Pillars of Observability:\n• Metrics: Numeric measurements over time\n• Logs: Discrete events with context\n• Traces: Request flow across services\n\nKey Metrics to Monitor:\n• Request rate and throughput\n• Error rate and types\n• Latency (p50, p95, p99)\n• Resource usage (CPU, memory, disk)\n• Business metrics\n• Availability and uptime\n• Queue depths\n• Database performance\n\nLogging Best Practices:\n• Structured logging (JSON)\n• Include correlation IDs\n• Appropriate log levels\n• Centralized log aggregation\n• Don't log sensitive data\n• Include context\n\nDistributed Tracing:\n• Track requests across services\n• Identify bottlenecks\n• Tools: Jaeger, Zipkin, X-Ray\n• Sampling strategy\n\nMonitoring Tools:\n• Prometheus + Grafana\n• ELK Stack (Elasticsearch, Logstash, Kibana)\n• Datadog\n• New Relic\n• CloudWatch\n\nBest Practices:\n• Monitor from user perspective\n• Set meaningful alerts\n• Use dashboards effectively\n• Implement SLIs/SLOs\n• Regular review of metrics\n• Correlation between metrics\n• Automate incident response",
      "explanation": "Microservices observability requires collecting metrics, logs, and distributed traces using tools like Prometheus, ELK, and Jaeger to provide comprehensive visibility into system behavior, performance, and failures across services.",
      "difficulty": "Medium",
      "code": "// Prometheus Metrics with Micrometer\n@Configuration\npublic class MetricsConfig {\n    \n    @Bean\n    public MeterRegistryCustomizer<MeterRegistry> metricsCommonTags() {\n        return registry -> registry.config().commonTags(\n            \"application\", \"user-service\",\n            \"environment\", \"production\"\n        );\n    }\n}\n\n@Service\npublic class OrderService {\n    \n    private final Counter orderCounter;\n    private final Timer orderTimer;\n    private final Gauge activeOrders;\n    \n    public OrderService(MeterRegistry registry) {\n        this.orderCounter = Counter.builder(\"orders.created\")\n            .description(\"Total orders created\")\n            .tag(\"status\", \"success\")\n            .register(registry);\n            \n        this.orderTimer = Timer.builder(\"orders.processing.time\")\n            .description(\"Order processing time\")\n            .register(registry);\n            \n        this.activeOrders = Gauge.builder(\"orders.active\", this, \n            service -> orderRepository.countByStatus(OrderStatus.PROCESSING))\n            .register(registry);\n    }\n    \n    public Order createOrder(OrderRequest request) {\n        return orderTimer.recordCallable(() -> {\n            try {\n                Order order = doCreateOrder(request);\n                orderCounter.increment();\n                return order;\n            } catch (Exception e) {\n                Counter.builder(\"orders.created\")\n                    .tag(\"status\", \"failed\")\n                    .tag(\"error\", e.getClass().getSimpleName())\n                    .register(registry)\n                    .increment();\n                throw e;\n            }\n        });\n    }\n}\n\n// Structured Logging\n@Component\npublic class StructuredLogging {\n    \n    public void logOrderCreated(Order order) {\n        Map<String, Object> logData = Map.of(\n            \"event\", \"order.created\",\n            \"orderId\", order.getId(),\n            \"customerId\", order.getCustomerId(),\n            \"amount\", order.getTotal(),\n            \"itemCount\", order.getItems().size(),\n            \"timestamp\", Instant.now()\n        );\n        \n        log.info(\"Order created: {}\", \n            new ObjectMapper().writeValueAsString(logData));\n    }\n}\n\n// Health Checks\n@Component\npublic class CustomHealthIndicator implements HealthIndicator {\n    \n    @Autowired\n    private OrderRepository orderRepository;\n    @Autowired\n    private RestTemplate restTemplate;\n    \n    @Override\n    public Health health() {\n        Map<String, Object> details = new HashMap<>();\n        \n        // Database health\n        try {\n            long count = orderRepository.count();\n            details.put(\"database\", \"UP\");\n            details.put(\"totalOrders\", count);\n        } catch (Exception e) {\n            return Health.down()\n                .withDetail(\"database\", \"DOWN\")\n                .withDetail(\"error\", e.getMessage())\n                .build();\n        }\n        \n        // External service health\n        try {\n            restTemplate.getForEntity(\n                \"http://user-service/actuator/health\",\n                String.class\n            );\n            details.put(\"userService\", \"UP\");\n        } catch (Exception e) {\n            details.put(\"userService\", \"DOWN\");\n        }\n        \n        return Health.up().withDetails(details).build();\n    }\n}\n\n// Grafana Dashboard Configuration (JSON)\n{\n  \"dashboard\": {\n    \"title\": \"Order Service Metrics\",\n    \"panels\": [\n      {\n        \"title\": \"Request Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(orders_created_total[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Error Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(orders_created_total{status='failed'}[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Processing Time (p95)\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, orders_processing_time_seconds_bucket)\"\n          }\n        ]\n      }\n    ]\n  }\n}\n\n// Alert Rules (Prometheus)\ngroups:\n  - name: order-service-alerts\n    rules:\n      - alert: HighErrorRate\n        expr: rate(orders_created_total{status=\"failed\"}[5m]) > 0.05\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"High error rate in order service\"\n          description: \"Error rate is {{ $value }} (>5%)\"\n      \n      - alert: HighLatency\n        expr: histogram_quantile(0.95, orders_processing_time_seconds_bucket) > 2\n        for: 10m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High latency in order processing\"\n          description: \"P95 latency is {{ $value }}s (>2s)\""
    },
    {
      "id": 81,
      "question": "How do you implement API rate limiting and throttling?",
      "answer": "Rate limiting and throttling control request rates to protect services from overload and ensure fair usage.\n\nRate Limiting Algorithms:\n• Fixed Window: Count requests in fixed time windows\n• Sliding Window: More accurate, rolling time window\n• Token Bucket: Tokens added at fixed rate, consumed per request\n• Leaky Bucket: Requests processed at constant rate\n\nImplementation Levels:\n• API Gateway: Centralized rate limiting\n• Service Level: Per-service limits\n• User/Client Level: Per-user quotas\n• Endpoint Level: Different limits per endpoint\n\nRate Limit Criteria:\n• By IP address\n• By API key\n• By user ID\n• By tenant ID\n• By request type\n\nResponse Handling:\n• Return 429 (Too Many Requests)\n• Include Retry-After header\n• Include rate limit headers\n• Provide clear error message\n• Log violations\n\nBest Practices:\n• Set reasonable limits\n• Different tiers for different users\n• Graceful degradation\n• Monitor usage patterns\n• Document limits clearly\n• Provide usage feedback\n• Allow burst traffic\n• Implement backoff strategies",
      "explanation": "Rate limiting uses algorithms like token bucket or sliding window to control request rates at API Gateway or service level, returning 429 status with headers when limits exceeded to protect from overload.",
      "difficulty": "Medium",
      "code": "// Token Bucket Rate Limiter with Bucket4j\n@Service\npublic class RateLimitService {\n    \n    private final Map<String, Bucket> cache = new ConcurrentHashMap<>();\n    \n    public Bucket resolveBucket(String key) {\n        return cache.computeIfAbsent(key, k -> createNewBucket());\n    }\n    \n    private Bucket createNewBucket() {\n        // 100 requests per minute\n        Bandwidth limit = Bandwidth.classic(100, \n            Refill.intervally(100, Duration.ofMinutes(1)));\n        return Bucket.builder()\n            .addLimit(limit)\n            .build();\n    }\n    \n    public Bucket createTieredBucket(String tier) {\n        Bandwidth limit;\n        switch (tier) {\n            case \"premium\":\n                limit = Bandwidth.classic(1000, \n                    Refill.intervally(1000, Duration.ofMinutes(1)));\n                break;\n            case \"standard\":\n                limit = Bandwidth.classic(100, \n                    Refill.intervally(100, Duration.ofMinutes(1)));\n                break;\n            default:\n                limit = Bandwidth.classic(10, \n                    Refill.intervally(10, Duration.ofMinutes(1)));\n        }\n        return Bucket.builder().addLimit(limit).build();\n    }\n}\n\n// Rate Limiting Interceptor\n@Component\npublic class RateLimitInterceptor implements HandlerInterceptor {\n    \n    @Autowired\n    private RateLimitService rateLimitService;\n    \n    @Override\n    public boolean preHandle(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            Object handler) throws IOException {\n        \n        String key = getRateLimitKey(request);\n        Bucket bucket = rateLimitService.resolveBucket(key);\n        \n        ConsumptionProbe probe = bucket.tryConsumeAndReturnRemaining(1);\n        \n        if (probe.isConsumed()) {\n            // Add rate limit headers\n            response.addHeader(\"X-Rate-Limit-Remaining\", \n                String.valueOf(probe.getRemainingTokens()));\n            return true;\n        } else {\n            // Rate limit exceeded\n            long waitForRefill = probe.getNanosToWaitForRefill() / 1_000_000_000;\n            response.setStatus(HttpStatus.TOO_MANY_REQUESTS.value());\n            response.addHeader(\"X-Rate-Limit-Retry-After-Seconds\", \n                String.valueOf(waitForRefill));\n            response.getWriter().write(\n                \"{\\\"error\\\":\\\"Rate limit exceeded. Try again in \" + \n                waitForRefill + \" seconds.\\\"}\"\n            );\n            return false;\n        }\n    }\n    \n    private String getRateLimitKey(HttpServletRequest request) {\n        // By API key\n        String apiKey = request.getHeader(\"X-API-Key\");\n        if (apiKey != null) {\n            return \"api-key:\" + apiKey;\n        }\n        \n        // By user ID\n        String userId = request.getHeader(\"X-User-ID\");\n        if (userId != null) {\n            return \"user:\" + userId;\n        }\n        \n        // By IP\n        return \"ip:\" + request.getRemoteAddr();\n    }\n}\n\n// Redis-based Distributed Rate Limiter\n@Service\npublic class RedisRateLimiter {\n    \n    @Autowired\n    private RedisTemplate<String, String> redisTemplate;\n    \n    public boolean allowRequest(String key, int maxRequests, int windowSeconds) {\n        String redisKey = \"rate-limit:\" + key;\n        long currentTime = System.currentTimeMillis() / 1000;\n        long windowStart = currentTime - windowSeconds;\n        \n        // Remove old entries\n        redisTemplate.opsForZSet().removeRangeByScore(\n            redisKey, 0, windowStart\n        );\n        \n        // Count current requests\n        Long count = redisTemplate.opsForZSet().count(\n            redisKey, windowStart, currentTime\n        );\n        \n        if (count < maxRequests) {\n            // Add new request\n            redisTemplate.opsForZSet().add(\n                redisKey, \n                UUID.randomUUID().toString(), \n                currentTime\n            );\n            redisTemplate.expire(redisKey, windowSeconds, TimeUnit.SECONDS);\n            return true;\n        }\n        \n        return false;\n    }\n}\n\n// Spring Cloud Gateway Rate Limiter\n@Configuration\npublic class GatewayRateLimitConfig {\n    \n    @Bean\n    public KeyResolver userKeyResolver() {\n        return exchange -> Mono.just(\n            exchange.getRequest()\n                .getHeaders()\n                .getFirst(\"X-User-ID\")\n        );\n    }\n    \n    @Bean\n    public RouteLocator routes(RouteLocatorBuilder builder, \n                               RedisRateLimiter rateLimiter) {\n        return builder.routes()\n            .route(\"rate-limited-route\", r -> r\n                .path(\"/api/**\")\n                .filters(f -> f.requestRateLimiter(c -> c\n                    .setRateLimiter(rateLimiter)\n                    .setKeyResolver(userKeyResolver())))\n                .uri(\"lb://backend-service\"))\n            .build();\n    }\n    \n    @Bean\n    public RedisRateLimiter redisRateLimiter() {\n        return new RedisRateLimiter(10, 60); // 10 requests per 60 seconds\n    }\n}"
    },
    {
      "id": 82,
      "question": "What is the role of API Gateway in microservices security?",
      "answer": "API Gateway serves as the security perimeter for microservices, providing centralized authentication, authorization, and threat protection.\n\nSecurity Functions:\n• Authentication: Verify client identity\n• Authorization: Check access permissions\n• SSL/TLS termination\n• API key validation\n• JWT token validation\n• Rate limiting and throttling\n• IP whitelisting/blacklisting\n• Request validation\n• Security headers injection\n\nAuthentication Methods:\n• OAuth 2.0 and OpenID Connect\n• JWT tokens\n• API keys\n• Mutual TLS (mTLS)\n• Basic authentication\n• SAML\n\nThreat Protection:\n• SQL injection prevention\n• XSS attack prevention\n• DDoS protection\n• Bot detection\n• Request size limits\n• Malformed request rejection\n\nSecurity Headers:\n• CORS configuration\n• Content-Security-Policy\n• X-Frame-Options\n• X-Content-Type-Options\n• Strict-Transport-Security\n\nBest Practices:\n• Use HTTPS only\n• Validate all inputs\n• Implement defense in depth\n• Log security events\n• Regular security audits\n• Keep gateway updated\n• Minimize attack surface\n• Use Web Application Firewall (WAF)",
      "explanation": "API Gateway provides centralized security by handling authentication, authorization, SSL termination, and threat protection at the entry point, shielding backend microservices and enforcing consistent security policies.",
      "difficulty": "Hard",
      "code": "// API Gateway Security Configuration\n@Configuration\n@EnableWebFluxSecurity\npublic class SecurityConfig {\n    \n    @Bean\n    public SecurityWebFilterChain securityWebFilterChain(ServerHttpSecurity http) {\n        return http\n            .csrf().disable()\n            .cors().and()\n            .authorizeExchange(exchanges -> exchanges\n                .pathMatchers(\"/api/public/**\").permitAll()\n                .pathMatchers(\"/api/admin/**\").hasRole(\"ADMIN\")\n                .pathMatchers(\"/actuator/health\").permitAll()\n                .anyExchange().authenticated()\n            )\n            .oauth2ResourceServer(oauth2 -> oauth2\n                .jwt(jwt -> jwt.jwtAuthenticationConverter(jwtAuthConverter()))\n            )\n            .headers(headers -> headers\n                .frameOptions().deny()\n                .xssProtection()\n                .contentSecurityPolicy(\"default-src 'self'\")\n            )\n            .build();\n    }\n    \n    @Bean\n    public ReactiveJwtDecoder jwtDecoder() {\n        return ReactiveJwtDecoders.fromIssuerLocation(\n            \"https://auth.example.com/oauth2\"\n        );\n    }\n}\n\n// JWT Token Validation Filter\n@Component\npublic class JwtValidationFilter implements GatewayFilter {\n    \n    @Autowired\n    private JwtTokenValidator tokenValidator;\n    \n    @Override\n    public Mono<Void> filter(ServerWebExchange exchange, \n                             GatewayFilterChain chain) {\n        String token = extractToken(exchange.getRequest());\n        \n        if (token == null) {\n            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);\n            return exchange.getResponse().setComplete();\n        }\n        \n        try {\n            Claims claims = tokenValidator.validateAndParse(token);\n            \n            // Add claims to headers for downstream services\n            ServerHttpRequest request = exchange.getRequest().mutate()\n                .header(\"X-User-Id\", claims.getSubject())\n                .header(\"X-User-Email\", claims.get(\"email\", String.class))\n                .header(\"X-User-Roles\", claims.get(\"roles\", String.class))\n                .build();\n            \n            return chain.filter(exchange.mutate().request(request).build());\n        } catch (JwtException e) {\n            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);\n            return exchange.getResponse().setComplete();\n        }\n    }\n}\n\n// Input Validation Filter\n@Component\npublic class SecurityValidationFilter implements GlobalFilter {\n    \n    private static final int MAX_REQUEST_SIZE = 10 * 1024 * 1024; // 10MB\n    private static final Pattern SQL_INJECTION_PATTERN = \n        Pattern.compile(\".*('|(\\\\-\\\\-)|(;)|(\\\\|\\\\|)|(\\\\*)).*\");\n    \n    @Override\n    public Mono<Void> filter(ServerWebExchange exchange, \n                             GatewayFilterChain chain) {\n        ServerHttpRequest request = exchange.getRequest();\n        \n        // Check request size\n        if (request.getHeaders().getContentLength() > MAX_REQUEST_SIZE) {\n            exchange.getResponse().setStatusCode(HttpStatus.PAYLOAD_TOO_LARGE);\n            return exchange.getResponse().setComplete();\n        }\n        \n        // Validate query parameters\n        for (Map.Entry<String, List<String>> param : \n                request.getQueryParams().entrySet()) {\n            for (String value : param.getValue()) {\n                if (SQL_INJECTION_PATTERN.matcher(value).matches()) {\n                    log.warn(\"Potential SQL injection attempt: {}\", value);\n                    exchange.getResponse().setStatusCode(HttpStatus.BAD_REQUEST);\n                    return exchange.getResponse().setComplete();\n                }\n            }\n        }\n        \n        return chain.filter(exchange);\n    }\n}\n\n// API Key Validation\n@Component\npublic class ApiKeyFilter implements GatewayFilter {\n    \n    @Autowired\n    private ApiKeyService apiKeyService;\n    \n    @Override\n    public Mono<Void> filter(ServerWebExchange exchange, \n                             GatewayFilterChain chain) {\n        String apiKey = exchange.getRequest()\n            .getHeaders()\n            .getFirst(\"X-API-Key\");\n        \n        if (apiKey == null || !apiKeyService.isValid(apiKey)) {\n            exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED);\n            return exchange.getResponse().writeWith(\n                Mono.just(exchange.getResponse()\n                    .bufferFactory()\n                    .wrap(\"{\\\"error\\\":\\\"Invalid API Key\\\"}\".getBytes()))\n            );\n        }\n        \n        // Check API key permissions\n        String path = exchange.getRequest().getPath().value();\n        if (!apiKeyService.hasAccess(apiKey, path)) {\n            exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);\n            return exchange.getResponse().setComplete();\n        }\n        \n        return chain.filter(exchange);\n    }\n}\n\n// CORS Configuration\n@Configuration\npublic class CorsConfig {\n    \n    @Bean\n    public CorsWebFilter corsWebFilter() {\n        CorsConfiguration config = new CorsConfiguration();\n        config.setAllowedOrigins(Arrays.asList(\n            \"https://app.example.com\",\n            \"https://mobile.example.com\"\n        ));\n        config.setAllowedMethods(Arrays.asList(\"GET\", \"POST\", \"PUT\", \"DELETE\"));\n        config.setAllowedHeaders(Arrays.asList(\n            \"Authorization\",\n            \"Content-Type\",\n            \"X-API-Key\"\n        ));\n        config.setMaxAge(3600L);\n        \n        UrlBasedCorsConfigurationSource source = \n            new UrlBasedCorsConfigurationSource();\n        source.registerCorsConfiguration(\"/**\", config);\n        \n        return new CorsWebFilter(source);\n    }\n}"
    },
    {
      "id": 83,
      "question": "How do you handle data synchronization between Microservices?",
      "answer": "Data synchronization keeps data consistent across microservices that maintain their own databases.\n\nSynchronization Patterns:\n• Event-Driven: Publish events on data changes\n• Change Data Capture (CDC): Track database changes\n• API Polling: Periodically check for updates\n• Batch Synchronization: Scheduled bulk updates\n• Message Queue: Guaranteed delivery of changes\n\nEvent-Driven Synchronization:\n• Publish domain events\n• Consumers update local copies\n• Eventual consistency\n• Decoupled services\n• Real-time updates\n\nChange Data Capture:\n• Monitor database transaction log\n• Automatic change detection\n• No application code changes\n• Tools: Debezium, AWS DMS\n\nChallenges:\n• Eventual consistency window\n• Duplicate events\n• Event ordering\n• Partial failures\n• Data conflicts\n• Network partitions\n\nBest Practices:\n• Idempotent consumers\n• Event versioning\n• Conflict resolution strategy\n• Monitor sync lag\n• Handle missing events\n• Maintain data lineage\n• Regular reconciliation\n• Use correlation IDs",
      "explanation": "Data synchronization uses event-driven patterns or CDC to propagate changes across microservices with separate databases, maintaining eventual consistency through asynchronous message passing with idempotent handling and conflict resolution.",
      "difficulty": "Hard",
      "code": "// Event-Driven Synchronization\n@Service\npublic class UserService {\n    \n    @Autowired\n    private UserRepository userRepository;\n    @Autowired\n    private EventPublisher eventPublisher;\n    \n    @Transactional\n    public User updateUser(String userId, UserUpdateRequest request) {\n        User user = userRepository.findById(userId)\n            .orElseThrow(() -> new UserNotFoundException(userId));\n        \n        // Update local data\n        user.setName(request.getName());\n        user.setEmail(request.getEmail());\n        user = userRepository.save(user);\n        \n        // Publish event for synchronization\n        UserUpdatedEvent event = new UserUpdatedEvent(\n            user.getId(),\n            user.getName(),\n            user.getEmail(),\n            user.getVersion(),  // For optimistic locking\n            LocalDateTime.now()\n        );\n        eventPublisher.publish(event);\n        \n        return user;\n    }\n}\n\n// Consumer - Idempotent Event Handler\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private OrderRepository orderRepository;\n    @Autowired\n    private ProcessedEventRepository processedEventRepo;\n    \n    @EventListener\n    @Transactional\n    public void onUserUpdated(UserUpdatedEvent event) {\n        // Check if already processed (idempotency)\n        if (processedEventRepo.exists(event.getEventId())) {\n            log.info(\"Event already processed: {}\", event.getEventId());\n            return;\n        }\n        \n        // Update denormalized user data in orders\n        List<Order> orders = orderRepository.findByCustomerId(event.getUserId());\n        orders.forEach(order -> {\n            order.setCustomerName(event.getName());\n            order.setCustomerEmail(event.getEmail());\n        });\n        orderRepository.saveAll(orders);\n        \n        // Mark event as processed\n        processedEventRepo.save(new ProcessedEvent(\n            event.getEventId(),\n            event.getClass().getSimpleName(),\n            LocalDateTime.now()\n        ));\n        \n        log.info(\"Synchronized user data for {} orders\", orders.size());\n    }\n}\n\n// Change Data Capture with Debezium\n// Debezium connector configuration\n{\n  \"name\": \"user-cdc-connector\",\n  \"config\": {\n    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n    \"database.hostname\": \"localhost\",\n    \"database.port\": \"5432\",\n    \"database.user\": \"debezium\",\n    \"database.password\": \"dbz\",\n    \"database.dbname\": \"userdb\",\n    \"database.server.name\": \"user-server\",\n    \"table.include.list\": \"public.users\",\n    \"transforms\": \"route\",\n    \"transforms.route.type\": \"org.apache.kafka.connect.transforms.RegexRouter\",\n    \"transforms.route.regex\": \"([^.]+)\\\\.([^.]+)\\\\.([^.]+)\",\n    \"transforms.route.replacement\": \"user-events\"\n  }\n}\n\n// CDC Event Consumer\n@Service\npublic class UserCdcConsumer {\n    \n    @KafkaListener(topics = \"user-events\", groupId = \"order-service\")\n    public void handleUserChange(String message) {\n        JsonNode event = objectMapper.readTree(message);\n        String operation = event.get(\"payload\").get(\"op\").asText();\n        \n        switch (operation) {\n            case \"c\": // CREATE\n            case \"u\": // UPDATE\n                JsonNode after = event.get(\"payload\").get(\"after\");\n                syncUserData(\n                    after.get(\"id\").asText(),\n                    after.get(\"name\").asText(),\n                    after.get(\"email\").asText()\n                );\n                break;\n            case \"d\": // DELETE\n                JsonNode before = event.get(\"payload\").get(\"before\");\n                handleUserDeleted(before.get(\"id\").asText());\n                break;\n        }\n    }\n}\n\n// Data Reconciliation Job\n@Component\npublic class DataReconciliationJob {\n    \n    @Scheduled(cron = \"0 0 2 * * *\")  // 2 AM daily\n    public void reconcileUserData() {\n        log.info(\"Starting user data reconciliation\");\n        \n        // Get all users from source\n        List<User> users = userServiceClient.getAllUsers();\n        \n        // Compare with local copies\n        int mismatches = 0;\n        for (User user : users) {\n            List<Order> orders = orderRepository\n                .findByCustomerId(user.getId());\n            \n            for (Order order : orders) {\n                if (!order.getCustomerName().equals(user.getName()) ||\n                    !order.getCustomerEmail().equals(user.getEmail())) {\n                    \n                    order.setCustomerName(user.getName());\n                    order.setCustomerEmail(user.getEmail());\n                    orderRepository.save(order);\n                    mismatches++;\n                }\n            }\n        }\n        \n        log.info(\"Reconciliation complete. Fixed {} mismatches\", mismatches);\n    }\n}"
    },
    {
      "id": 84,
      "question": "What are the best practices for microservice database design?",
      "answer": "Database design for microservices differs from monolithic applications, prioritizing service autonomy and loose coupling.\n\nCore Principles:\n• Database per service pattern\n• Service owns its data exclusively\n• No direct database access from other services\n• Data accessed only through service APIs\n• Different database types per service (polyglot persistence)\n\nSchema Design:\n• Optimize for service boundaries\n• Denormalize when needed\n• Avoid foreign keys across services\n• Use eventual consistency\n• Include audit fields\n• Support versioning\n\nPolyglot Persistence:\n• SQL for transactional data\n• NoSQL for scalability (MongoDB, Cassandra)\n• Document DB for flexible schemas\n• Graph DB for relationships (Neo4j)\n• Search engines for full-text (Elasticsearch)\n• Cache for performance (Redis)\n\nData Partitioning:\n• Horizontal partitioning (sharding)\n• Vertical partitioning by service\n• Service-specific schemas\n• Tenant-based partitioning\n\nBest Practices:\n• Keep transactions within service\n• Use Saga for distributed transactions\n• Implement data replication strategy\n• Plan for data migration\n• Monitor database performance\n• Backup and recovery per service\n• Connection pooling\n• Optimize queries per service needs",
      "explanation": "Microservice database design uses database per service pattern with polyglot persistence, denormalization for performance, eventual consistency across services, and Saga pattern for distributed transactions while avoiding cross-service database access.",
      "difficulty": "Medium",
      "code": "// Database per Service Configuration\n// User Service - PostgreSQL\n@Configuration\n@EnableJpaRepositories(basePackages = \"com.example.user.repository\")\npublic class UserDatabaseConfig {\n    \n    @Bean\n    @Primary\n    @ConfigurationProperties(\"spring.datasource.user\")\n    public DataSource userDataSource() {\n        return DataSourceBuilder.create().build();\n    }\n    \n    @Bean\n    public LocalContainerEntityManagerFactoryBean userEntityManager() {\n        LocalContainerEntityManagerFactoryBean em = \n            new LocalContainerEntityManagerFactoryBean();\n        em.setDataSource(userDataSource());\n        em.setPackagesToScan(\"com.example.user.entity\");\n        em.setJpaVendorAdapter(new HibernateJpaVendorAdapter());\n        return em;\n    }\n}\n\n// application.yml\nspring:\n  datasource:\n    user:\n      url: jdbc:postgresql://localhost:5432/userdb\n      username: userservice\n      password: secret\n    \n// Product Service - MongoDB (Polyglot Persistence)\n@Configuration\npublic class ProductDatabaseConfig {\n    \n    @Bean\n    public MongoClient mongoClient() {\n        return MongoClients.create(\n            \"mongodb://localhost:27017/productdb\"\n        );\n    }\n    \n    @Bean\n    public MongoTemplate mongoTemplate() {\n        return new MongoTemplate(mongoClient(), \"productdb\");\n    }\n}\n\n// Denormalized Data Model\n@Entity\n@Table(name = \"orders\")\npublic class Order {\n    @Id\n    private String id;\n    \n    // Denormalized customer data (from User service)\n    @Column(name = \"customer_id\")\n    private String customerId;\n    \n    @Column(name = \"customer_name\")\n    private String customerName;\n    \n    @Column(name = \"customer_email\")\n    private String customerEmail;\n    \n    // Order-specific data\n    @OneToMany(cascade = CascadeType.ALL)\n    private List<OrderItem> items;\n    \n    private BigDecimal total;\n    private OrderStatus status;\n    \n    // Audit fields\n    @Column(name = \"created_at\")\n    private LocalDateTime createdAt;\n    \n    @Column(name = \"updated_at\")\n    private LocalDateTime updatedAt;\n    \n    @Version\n    private Long version;  // Optimistic locking\n}\n\n// Event-Driven Data Sync\n@Service\npublic class OrderDenormalizationService {\n    \n    @EventListener\n    @Transactional\n    public void onCustomerUpdated(CustomerUpdatedEvent event) {\n        // Update denormalized customer data\n        List<Order> orders = orderRepository\n            .findByCustomerId(event.getCustomerId());\n        \n        orders.forEach(order -> {\n            order.setCustomerName(event.getName());\n            order.setCustomerEmail(event.getEmail());\n            order.setUpdatedAt(LocalDateTime.now());\n        });\n        \n        orderRepository.saveAll(orders);\n    }\n}\n\n// Connection Pooling\n@Configuration\npublic class DatabasePoolConfig {\n    \n    @Bean\n    public HikariConfig hikariConfig() {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:postgresql://localhost:5432/orderdb\");\n        config.setUsername(\"orderservice\");\n        config.setPassword(\"secret\");\n        \n        // Pool settings\n        config.setMaximumPoolSize(20);\n        config.setMinimumIdle(5);\n        config.setConnectionTimeout(30000);\n        config.setIdleTimeout(600000);\n        config.setMaxLifetime(1800000);\n        \n        // Performance\n        config.setAutoCommit(false);\n        config.addDataSourceProperty(\"cachePrepStmts\", \"true\");\n        config.addDataSourceProperty(\"prepStmtCacheSize\", \"250\");\n        config.addDataSourceProperty(\"prepStmtCacheSqlLimit\", \"2048\");\n        \n        return config;\n    }\n    \n    @Bean\n    public DataSource dataSource() {\n        return new HikariDataSource(hikariConfig());\n    }\n}\n\n// Database Migration with Flyway\n// V1__Create_orders_table.sql\nCREATE TABLE orders (\n    id VARCHAR(36) PRIMARY KEY,\n    customer_id VARCHAR(36) NOT NULL,\n    customer_name VARCHAR(255) NOT NULL,\n    customer_email VARCHAR(255) NOT NULL,\n    total DECIMAL(10, 2) NOT NULL,\n    status VARCHAR(50) NOT NULL,\n    created_at TIMESTAMP NOT NULL,\n    updated_at TIMESTAMP NOT NULL,\n    version BIGINT NOT NULL DEFAULT 0\n);\n\nCREATE INDEX idx_orders_customer_id ON orders(customer_id);\nCREATE INDEX idx_orders_status ON orders(status);\nCREATE INDEX idx_orders_created_at ON orders(created_at);"
    },
    {
      "id": 85,
      "question": "How do you implement service mesh in Microservices?",
      "answer": "Service mesh provides infrastructure layer for service-to-service communication with observability, security, and reliability features.\n\nService Mesh Components:\n• Data Plane: Sidecar proxies (Envoy)\n• Control Plane: Policy and configuration management\n• Service discovery\n• Load balancing\n• Circuit breaking\n• Mutual TLS (mTLS)\n• Observability\n\nPopular Service Meshes:\n• Istio: Feature-rich, widely adopted\n• Linkerd: Lightweight, simple\n• Consul Connect: HashiCorp ecosystem\n• AWS App Mesh: AWS-native\n\nKey Features:\n• Traffic Management: Routing, splitting, mirroring\n• Security: mTLS, authorization policies\n• Observability: Metrics, logs, traces\n• Resilience: Retries, timeouts, circuit breakers\n\nImplementation Steps:\n• Install service mesh control plane\n• Enable sidecar injection\n• Configure virtual services\n• Define destination rules\n• Set up security policies\n• Configure observability\n\nBenefits:\n• No code changes needed\n• Consistent policies across services\n• Advanced traffic control\n• Zero-trust security\n• Built-in observability\n\nChallenges:\n• Increased complexity\n• Resource overhead\n• Learning curve\n• Debugging difficulty\n• Performance impact",
      "explanation": "Service mesh uses sidecar proxies like Envoy managed by control plane to transparently handle service communication, providing traffic management, mTLS security, and observability without application code changes.",
      "difficulty": "Hard",
      "code": "# Istio Installation\nistioctl install --set profile=production\n\n# Enable sidecar injection for namespace\nkubectl label namespace default istio-injection=enabled\n\n# Deploy application (sidecar auto-injected)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-service\n  template:\n    metadata:\n      labels:\n        app: user-service\n        version: v1\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:1.0\n        ports:\n        - containerPort: 8080\n# Envoy sidecar injected automatically\n\n---\n# Virtual Service - Traffic Management\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: user-service\nspec:\n  hosts:\n  - user-service\n  http:\n  # Traffic splitting (canary)\n  - match:\n    - headers:\n        user-group:\n          exact: beta\n    route:\n    - destination:\n        host: user-service\n        subset: v2\n  # Weighted routing\n  - route:\n    - destination:\n        host: user-service\n        subset: v1\n      weight: 90\n    - destination:\n        host: user-service\n        subset: v2\n      weight: 10\n    timeout: 10s\n    retries:\n      attempts: 3\n      perTryTimeout: 2s\n      retryOn: 5xx,reset,connect-failure\n\n---\n# Destination Rule - Circuit Breaking\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: user-service\nspec:\n  host: user-service\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 100\n      http:\n        http1MaxPendingRequests: 50\n        http2MaxRequests: 100\n        maxRequestsPerConnection: 2\n    outlierDetection:\n      consecutiveErrors: 5\n      interval: 30s\n      baseEjectionTime: 30s\n      maxEjectionPercent: 50\n      minHealthPercent: 50\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n\n---\n# Mutual TLS Policy\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\nspec:\n  mtls:\n    mode: STRICT\n\n---\n# Authorization Policy\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: user-service-authz\nspec:\n  selector:\n    matchLabels:\n      app: user-service\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/sa/order-service\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/users/*\"]\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/sa/admin-service\"]\n    to:\n    - operation:\n        methods: [\"*\"]\n        paths: [\"*\"]\n\n---\n# Observability - Telemetry\napiVersion: telemetry.istio.io/v1alpha1\nkind: Telemetry\nmetadata:\n  name: mesh-default\nspec:\n  metrics:\n  - providers:\n    - name: prometheus\n  tracing:\n  - providers:\n    - name: jaeger\n    customTags:\n      environment:\n        literal:\n          value: production\n  accessLogging:\n  - providers:\n    - name: envoy\n\n# Query metrics with Prometheus\nrate(istio_requests_total{destination_service=\"user-service\"}[5m])\n\n# View traces in Jaeger\nkubectl port-forward -n istio-system svc/jaeger-query 16686:16686"
    },
    {
      "id": 86,
      "question": "What is the role of API documentation automation in Microservices?",
      "answer": "API documentation automation generates and maintains up-to-date API documentation from code, ensuring accuracy and reducing manual effort.\n\nAutomation Approaches:\n• Code annotations (Swagger/OpenAPI)\n• Contract-first design\n• Code generation from specs\n• Runtime discovery\n• Test-driven documentation\n\nOpenAPI/Swagger Benefits:\n• Auto-generated from code\n• Interactive API explorer\n• Client SDK generation\n• Contract testing support\n• Version tracking\n• Always synchronized with code\n\nDocumentation Tools:\n• Springdoc OpenAPI: Spring Boot integration\n• Swagger Codegen: Generate clients/servers\n• Redoc: Beautiful API docs\n• Postman: Interactive collections\n• AsyncAPI: Async/event-driven APIs\n\nWhat to Document:\n• All endpoints and operations\n• Request/response schemas\n• Authentication requirements\n• Error responses\n• Rate limits\n• Code examples\n• Deprecation notices\n\nBest Practices:\n• Generate from source code\n• Version documentation with API\n• Provide interactive playground\n• Include real examples\n• Document breaking changes\n• Make easily discoverable\n• CI/CD integration\n• Host centrally",
      "explanation": "API documentation automation uses annotations and tools like OpenAPI/Swagger to generate always-current, interactive documentation from code, providing API exploration, client generation, and contract testing capabilities.",
      "difficulty": "Easy"
    },
    {
      "id": 87,
      "question": "How do you implement message deduplication in Microservices?",
      "answer": "Message deduplication prevents processing the same message multiple times in distributed systems with at-least-once delivery guarantees.\n\nDeduplication Strategies:\n• Idempotency keys\n• Message ID tracking\n• State-based deduplication\n• Time-window deduplication\n• Bloom filters\n\nIdempotency Pattern:\n• Generate unique message ID\n• Store processed IDs\n• Check before processing\n• Skip if already processed\n• Handle expiration\n\nImplementation Approaches:\n• Database unique constraints\n• Redis for ID tracking\n• Distributed cache\n• Message broker features\n• Application-level tracking\n\nChallenges:\n• Storage requirements\n• ID expiration policy\n• Distributed coordination\n• Performance impact\n• Clock synchronization\n\nBest Practices:\n• Make operations idempotent\n• Use message broker features\n• Set appropriate TTL\n• Monitor duplicates\n• Log duplicate attempts\n• Handle edge cases\n• Test duplicate scenarios\n• Consider exactly-once semantics",
      "explanation": "Message deduplication tracks processed message IDs using database constraints or Redis to prevent duplicate processing in at-least-once delivery systems, implementing idempotent operations with appropriate TTL and monitoring.",
      "difficulty": "Hard",
      "code": "// Idempotency with Database Unique Constraint\n@Entity\n@Table(name = \"processed_messages\")\npublic class ProcessedMessage {\n    @Id\n    @Column(unique = true)\n    private String messageId;\n    private String eventType;\n    private LocalDateTime processedAt;\n    private LocalDateTime expiresAt;\n}\n\n@Service\npublic class OrderEventHandler {\n    \n    @Autowired\n    private ProcessedMessageRepository processedRepo;\n    @Autowired\n    private OrderService orderService;\n    \n    @Transactional\n    @EventListener\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        String messageId = event.getMessageId();\n        \n        // Check if already processed\n        if (processedRepo.existsByMessageId(messageId)) {\n            log.info(\"Message already processed: {}\", messageId);\n            return;\n        }\n        \n        try {\n            // Create processed message record first\n            ProcessedMessage pm = new ProcessedMessage();\n            pm.setMessageId(messageId);\n            pm.setEventType(\"OrderCreated\");\n            pm.setProcessedAt(LocalDateTime.now());\n            pm.setExpiresAt(LocalDateTime.now().plusDays(7));\n            processedRepo.save(pm);\n            \n            // Process the event\n            orderService.processOrder(event);\n            \n            log.info(\"Successfully processed message: {}\", messageId);\n        } catch (DataIntegrityViolationException e) {\n            // Another instance processed it concurrently\n            log.info(\"Message processed by another instance: {}\", messageId);\n        }\n    }\n}\n\n// Redis-based Deduplication\n@Service\npublic class RedisDeduplicationService {\n    \n    @Autowired\n    private RedisTemplate<String, String> redisTemplate;\n    \n    private static final int TTL_HOURS = 24;\n    \n    public boolean isProcessed(String messageId) {\n        String key = \"processed:\" + messageId;\n        return Boolean.TRUE.equals(redisTemplate.hasKey(key));\n    }\n    \n    public boolean markAsProcessed(String messageId) {\n        String key = \"processed:\" + messageId;\n        Boolean wasAbsent = redisTemplate.opsForValue()\n            .setIfAbsent(key, \"1\", TTL_HOURS, TimeUnit.HOURS);\n        return Boolean.TRUE.equals(wasAbsent);\n    }\n}\n\n@Service\npublic class PaymentEventHandler {\n    \n    @Autowired\n    private RedisDeduplicationService deduplicationService;\n    \n    @KafkaListener(topics = \"payment-events\")\n    public void handlePaymentEvent(PaymentEvent event) {\n        String messageId = event.getMessageId();\n        \n        // Try to mark as processed atomically\n        if (!deduplicationService.markAsProcessed(messageId)) {\n            log.info(\"Duplicate message ignored: {}\", messageId);\n            return;\n        }\n        \n        try {\n            processPayment(event);\n            log.info(\"Payment processed: {}\", messageId);\n        } catch (Exception e) {\n            // Remove from processed set to allow retry\n            redisTemplate.delete(\"processed:\" + messageId);\n            throw e;\n        }\n    }\n}\n\n// Bloom Filter for High-Volume Deduplication\n@Service\npublic class BloomFilterDeduplicationService {\n    \n    private final BloomFilter<String> recentMessages;\n    private final Set<String> confirmedProcessed;\n    \n    public BloomFilterDeduplicationService() {\n        // 1M expected insertions, 1% false positive rate\n        this.recentMessages = BloomFilter.create(\n            Funnels.stringFunnel(Charset.defaultCharset()),\n            1_000_000,\n            0.01\n        );\n        this.confirmedProcessed = new ConcurrentHashMap<>().keySet(ConcurrentHashMap.newKeySet());\n    }\n    \n    public boolean isDuplicate(String messageId) {\n        // Quick check with bloom filter\n        if (!recentMessages.mightContain(messageId)) {\n            // Definitely not seen before\n            recentMessages.put(messageId);\n            return false;\n        }\n        \n        // Bloom filter says might be duplicate, check confirmed set\n        if (confirmedProcessed.contains(messageId)) {\n            return true;\n        }\n        \n        // Add to confirmed\n        confirmedProcessed.add(messageId);\n        return false;\n    }\n}\n\n// Cleanup Job for Expired Messages\n@Component\npublic class MessageCleanupJob {\n    \n    @Autowired\n    private ProcessedMessageRepository processedRepo;\n    \n    @Scheduled(cron = \"0 0 * * * *\")  // Every hour\n    public void cleanupExpiredMessages() {\n        LocalDateTime now = LocalDateTime.now();\n        int deleted = processedRepo.deleteByExpiresAtBefore(now);\n        log.info(\"Cleaned up {} expired message records\", deleted);\n    }\n}\n\n// Kafka Exactly-Once Semantics\n@Configuration\npublic class KafkaConfig {\n    \n    @Bean\n    public ProducerFactory<String, Object> producerFactory() {\n        Map<String, Object> config = new HashMap<>();\n        config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        config.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n        config.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"order-service-\");\n        return new DefaultKafkaProducerFactory<>(config);\n    }\n    \n    @Bean\n    public ConsumerFactory<String, Object> consumerFactory() {\n        Map<String, Object> config = new HashMap<>();\n        config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        config.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, \"read_committed\");\n        return new DefaultKafkaConsumerFactory<>(config);\n    }\n}"
    },
    {
      "id": 88,
      "question": "What are the patterns for handling long-running processes in Microservices?",
      "answer": "Long-running processes require special patterns to handle asynchronous execution, monitoring, and state management.\n\nPatterns for Long-Running Processes:\n• Async Request-Reply: Return immediately, notify on completion\n• Polling: Client polls for status\n• Webhooks: Callback on completion\n• Saga Pattern: Multi-step workflows\n• State Machine: Track process states\n• Job Queue: Background processing\n\nAsync Request-Reply:\n• Accept request, return job ID\n• Process asynchronously\n• Client checks status via job ID\n• Notification on completion\n\nState Management:\n• Store job state in database\n• Track progress\n• Enable resume on failure\n• Provide status updates\n• Support cancellation\n\nImplementation Approaches:\n• Message queues (RabbitMQ, Kafka)\n• Job schedulers (Quartz, Spring Batch)\n• Workflow engines (Temporal, Camunda)\n• Step Functions (AWS)\n\nBest Practices:\n• Return job ID immediately\n• Provide status endpoint\n• Send notifications on completion\n• Implement timeouts\n• Handle failures gracefully\n• Support idempotency\n• Enable monitoring\n• Allow cancellation",
      "explanation": "Long-running processes use async request-reply pattern returning job IDs for status polling, combined with state machines or workflow engines to track progress, handle failures, and notify completion via webhooks or callbacks.",
      "difficulty": "Hard",
      "code": "// Async Request-Reply Pattern\n@RestController\n@RequestMapping(\"/api/reports\")\npublic class ReportController {\n    \n    @Autowired\n    private ReportGenerationService reportService;\n    \n    @PostMapping\n    public ResponseEntity<JobResponse> generateReport(\n            @RequestBody ReportRequest request) {\n        \n        // Create job and return immediately\n        String jobId = UUID.randomUUID().toString();\n        reportService.generateReportAsync(jobId, request);\n        \n        JobResponse response = new JobResponse(\n            jobId,\n            JobStatus.PENDING,\n            \"/api/reports/\" + jobId,\n            null\n        );\n        \n        return ResponseEntity.accepted().body(response);\n    }\n    \n    @GetMapping(\"/{jobId}\")\n    public ResponseEntity<JobStatusResponse> getJobStatus(\n            @PathVariable String jobId) {\n        \n        Job job = reportService.getJob(jobId);\n        if (job == null) {\n            return ResponseEntity.notFound().build();\n        }\n        \n        JobStatusResponse response = new JobStatusResponse(\n            job.getId(),\n            job.getStatus(),\n            job.getProgress(),\n            job.getResult(),\n            job.getError()\n        );\n        \n        return ResponseEntity.ok(response);\n    }\n}\n\n// Job Processing Service\n@Service\npublic class ReportGenerationService {\n    \n    @Autowired\n    private JobRepository jobRepository;\n    @Autowired\n    private TaskExecutor taskExecutor;\n    @Autowired\n    private NotificationService notificationService;\n    \n    public void generateReportAsync(String jobId, ReportRequest request) {\n        // Save job\n        Job job = new Job(\n            jobId,\n            JobType.REPORT_GENERATION,\n            JobStatus.PENDING,\n            request,\n            LocalDateTime.now()\n        );\n        jobRepository.save(job);\n        \n        // Execute asynchronously\n        taskExecutor.execute(() -> processReport(jobId, request));\n    }\n    \n    private void processReport(String jobId, ReportRequest request) {\n        try {\n            // Update status to running\n            updateJobStatus(jobId, JobStatus.RUNNING, 0);\n            \n            // Long-running process\n            ReportData data = fetchData(request);\n            updateJobStatus(jobId, JobStatus.RUNNING, 30);\n            \n            byte[] report = generateReport(data);\n            updateJobStatus(jobId, JobStatus.RUNNING, 60);\n            \n            String reportUrl = uploadReport(report);\n            updateJobStatus(jobId, JobStatus.RUNNING, 90);\n            \n            // Complete\n            Job job = jobRepository.findById(jobId).orElseThrow();\n            job.setStatus(JobStatus.COMPLETED);\n            job.setProgress(100);\n            job.setResult(reportUrl);\n            job.setCompletedAt(LocalDateTime.now());\n            jobRepository.save(job);\n            \n            // Notify client\n            notificationService.notifyJobCompletion(jobId, reportUrl);\n            \n        } catch (Exception e) {\n            // Handle failure\n            Job job = jobRepository.findById(jobId).orElseThrow();\n            job.setStatus(JobStatus.FAILED);\n            job.setError(e.getMessage());\n            jobRepository.save(job);\n            \n            notificationService.notifyJobFailure(jobId, e.getMessage());\n        }\n    }\n}\n\n// Job Entity with State Machine\n@Entity\npublic class Job {\n    @Id\n    private String id;\n    \n    @Enumerated(EnumType.STRING)\n    private JobType type;\n    \n    @Enumerated(EnumType.STRING)\n    private JobStatus status;\n    \n    private int progress;  // 0-100\n    \n    @Column(columnDefinition = \"TEXT\")\n    private String request;\n    \n    private String result;\n    private String error;\n    \n    private LocalDateTime createdAt;\n    private LocalDateTime startedAt;\n    private LocalDateTime completedAt;\n    \n    private int retryCount;\n    private int maxRetries = 3;\n    \n    public boolean canRetry() {\n        return retryCount < maxRetries && \n               status == JobStatus.FAILED;\n    }\n}\n\n// Webhook Notification\n@Service\npublic class NotificationService {\n    \n    @Autowired\n    private RestTemplate restTemplate;\n    \n    public void notifyJobCompletion(String jobId, String result) {\n        String webhookUrl = getWebhookUrl(jobId);\n        if (webhookUrl == null) {\n            return;\n        }\n        \n        JobCompletionEvent event = new JobCompletionEvent(\n            jobId,\n            JobStatus.COMPLETED,\n            result,\n            LocalDateTime.now()\n        );\n        \n        try {\n            restTemplate.postForEntity(\n                webhookUrl,\n                event,\n                Void.class\n            );\n        } catch (Exception e) {\n            log.error(\"Failed to send webhook notification\", e);\n            // Store for retry\n            saveFailedNotification(jobId, webhookUrl, event);\n        }\n    }\n}\n\n// Saga Pattern for Complex Workflows\n@Service\npublic class OrderSagaService {\n    \n    @Autowired\n    private SagaRepository sagaRepository;\n    \n    public String startOrderSaga(OrderRequest request) {\n        String sagaId = UUID.randomUUID().toString();\n        \n        Saga saga = new Saga(\n            sagaId,\n            SagaType.ORDER_PROCESSING,\n            SagaStatus.STARTED\n        );\n        saga.addStep(new SagaStep(\"reserve-inventory\", StepStatus.PENDING));\n        saga.addStep(new SagaStep(\"process-payment\", StepStatus.PENDING));\n        saga.addStep(new SagaStep(\"create-shipment\", StepStatus.PENDING));\n        saga.addStep(new SagaStep(\"send-confirmation\", StepStatus.PENDING));\n        \n        sagaRepository.save(saga);\n        \n        // Start execution\n        executeSaga(sagaId, request);\n        \n        return sagaId;\n    }\n}"
    },
    {
      "id": 89,
      "question": "How do you implement service degradation and fallback patterns?",
      "answer": "Service degradation gracefully reduces functionality when services fail, providing fallback responses instead of complete failures.\n\nDegradation Strategies:\n• Circuit Breaker with Fallback\n• Cached responses\n• Static default responses\n• Simplified functionality\n• Degraded user experience\n• Queue requests for later\n\nFallback Types:\n• Static fallback: Return predefined response\n• Cached fallback: Return last known good state\n• Stubbed fallback: Return minimal valid response\n• Alternative service: Call backup service\n• Empty fallback: Return empty result\n\nGraceful Degradation:\n• Core features remain available\n• Non-essential features disabled\n• Clear user communication\n• Automatic recovery\n• Monitor degradation state\n\nImplementation:\n• Resilience4j fallback methods\n• Spring Cloud Circuit Breaker\n• Service mesh fallback routes\n• Application-level fallbacks\n\nBest Practices:\n• Define fallback priorities\n• Cache responses\n• Set appropriate timeouts\n• Monitor fallback usage\n• Test degradation scenarios\n• Clear user messaging\n• Log degradation events\n• Automatic re-enablement",
      "explanation": "Service degradation uses circuit breakers with fallback methods to return cached, default, or simplified responses when dependencies fail, maintaining core functionality while gracefully degrading non-essential features.",
      "difficulty": "Hard",
      "code": "// Circuit Breaker with Fallback\n@Service\npublic class ProductService {\n    \n    @Autowired\n    private RestTemplate restTemplate;\n    @Autowired\n    private ProductCache productCache;\n    \n    @CircuitBreaker(\n        name = \"productService\",\n        fallbackMethod = \"getProductFallback\"\n    )\n    @TimeLimiter(name = \"productService\")\n    @Retry(name = \"productService\")\n    public CompletableFuture<Product> getProduct(String productId) {\n        return CompletableFuture.supplyAsync(() -> {\n            Product product = restTemplate.getForObject(\n                \"http://product-service/products/\" + productId,\n                Product.class\n            );\n            \n            // Cache successful response\n            productCache.put(productId, product);\n            return product;\n        });\n    }\n    \n    // Fallback with cached data\n    private CompletableFuture<Product> getProductFallback(\n            String productId,\n            Exception ex) {\n        \n        log.warn(\"Product service unavailable, using fallback for: {}\", \n            productId, ex);\n        \n        // Try cache first\n        Product cached = productCache.get(productId);\n        if (cached != null) {\n            cached.setFromCache(true);\n            return CompletableFuture.completedFuture(cached);\n        }\n        \n        // Return stub product\n        Product stub = new Product(\n            productId,\n            \"Product Unavailable\",\n            0.0,\n            \"Service temporarily unavailable\"\n        );\n        stub.setStub(true);\n        return CompletableFuture.completedFuture(stub);\n    }\n}\n\n// Graceful Degradation Service\n@Service\npublic class RecommendationService {\n    \n    @Autowired\n    private RecommendationEngine recommendationEngine;\n    @Autowired\n    private PopularProductsService popularProducts;\n    \n    public List<Product> getRecommendations(String userId) {\n        try {\n            // Try personalized recommendations\n            return recommendationEngine.getPersonalized(userId);\n        } catch (ServiceUnavailableException e) {\n            log.warn(\"Recommendation engine unavailable, degrading to popular products\");\n            \n            // Fallback to popular products (degraded but functional)\n            return popularProducts.getTopProducts(10);\n        } catch (Exception e) {\n            log.error(\"All recommendation services failed\", e);\n            \n            // Final fallback: empty list\n            return Collections.emptyList();\n        }\n    }\n}\n\n// Feature Toggle for Degradation\n@Service\npublic class FeatureDegradationService {\n    \n    @Autowired\n    private FeatureFlagService featureFlags;\n    @Autowired\n    private MetricsService metrics;\n    \n    private Map<String, DegradationLevel> featureStates = new ConcurrentHashMap<>();\n    \n    public enum DegradationLevel {\n        FULL,      // All features enabled\n        PARTIAL,   // Non-essential features disabled\n        MINIMAL,   // Only core features\n        OFFLINE    // Service unavailable\n    }\n    \n    public boolean isFeatureAvailable(String feature) {\n        DegradationLevel level = getCurrentDegradationLevel();\n        \n        switch (level) {\n            case FULL:\n                return true;\n            case PARTIAL:\n                return !isNonEssential(feature);\n            case MINIMAL:\n                return isCoreFeature(feature);\n            case OFFLINE:\n                return false;\n            default:\n                return false;\n        }\n    }\n    \n    @Scheduled(fixedRate = 60000)\n    public void evaluateDegradation() {\n        // Check system health\n        double errorRate = metrics.getErrorRate();\n        double latency = metrics.getAverageLatency();\n        int activeUsers = metrics.getActiveUsers();\n        \n        DegradationLevel newLevel;\n        \n        if (errorRate < 0.01 && latency < 200) {\n            newLevel = DegradationLevel.FULL;\n        } else if (errorRate < 0.05 && latency < 500) {\n            newLevel = DegradationLevel.PARTIAL;\n        } else if (errorRate < 0.10) {\n            newLevel = DegradationLevel.MINIMAL;\n        } else {\n            newLevel = DegradationLevel.OFFLINE;\n        }\n        \n        if (newLevel != getCurrentDegradationLevel()) {\n            log.warn(\"Degradation level changed to: {}\", newLevel);\n            setDegradationLevel(newLevel);\n        }\n    }\n}\n\n// Tiered Fallback Chain\n@Service\npublic class SearchService {\n    \n    public List<Product> search(String query) {\n        // Tier 1: Try primary search service\n        try {\n            return elasticsearchService.search(query);\n        } catch (Exception e) {\n            log.warn(\"Elasticsearch unavailable: {}\", e.getMessage());\n        }\n        \n        // Tier 2: Try secondary search\n        try {\n            return databaseSearchService.search(query);\n        } catch (Exception e) {\n            log.warn(\"Database search failed: {}\", e.getMessage());\n        }\n        \n        // Tier 3: Try cached results\n        List<Product> cached = searchCache.get(query);\n        if (cached != null && !cached.isEmpty()) {\n            log.info(\"Returning cached search results\");\n            return cached;\n        }\n        \n        // Tier 4: Return popular products\n        log.warn(\"All search services unavailable, returning popular products\");\n        return popularProductsService.getTopProducts(20);\n    }\n}\n\n// application.yml - Resilience4j Configuration\nresilience4j:\n  circuitbreaker:\n    instances:\n      productService:\n        slidingWindowSize: 10\n        minimumNumberOfCalls: 5\n        failureRateThreshold: 50\n        waitDurationInOpenState: 30000\n        permittedNumberOfCallsInHalfOpenState: 3\n        automaticTransitionFromOpenToHalfOpenEnabled: true\n  \n  timelimiter:\n    instances:\n      productService:\n        timeoutDuration: 3s\n  \n  retry:\n    instances:\n      productService:\n        maxAttempts: 3\n        waitDuration: 1000\n        retryExceptions:\n          - java.net.ConnectException\n          - java.net.SocketTimeoutException"
    },
    {
      "id": 90,
      "question": "What are the key differences between REST and gRPC in Microservices?",
      "answer": "REST and gRPC are communication protocols with different characteristics suited for different scenarios.\n\nREST (REpresentational State Transfer):\n• Text-based (JSON/XML)\n• HTTP/1.1 protocol\n• Request-response pattern\n• Wide browser support\n• Human-readable\n• Easy debugging\n• Flexible and loosely typed\n\ngRPC (Google Remote Procedure Call):\n• Binary (Protocol Buffers)\n• HTTP/2 protocol\n• Multiple patterns (unary, streaming)\n• Limited browser support\n• Compact binary format\n• Strongly typed\n• Higher performance\n\nPerformance:\n• gRPC faster (binary, HTTP/2)\n• gRPC smaller payload size\n• gRPC supports multiplexing\n• REST easier to cache\n\nUse Cases for REST:\n• Public APIs\n• Browser-based clients\n• Simple CRUD operations\n• When human readability important\n• Microservices with diverse tech stacks\n\nUse Cases for gRPC:\n• Internal microservice communication\n• High-performance requirements\n• Real-time streaming\n• Polyglot environments\n• Strong contract enforcement\n\nStreaming Support:\n• REST: Limited (SSE, WebSockets)\n• gRPC: Native bidirectional streaming",
      "explanation": "REST uses text-based JSON over HTTP/1.1 for human-readable, browser-friendly APIs, while gRPC uses binary Protocol Buffers over HTTP/2 for high-performance, strongly-typed internal microservice communication with streaming support.",
      "difficulty": "Medium",
      "code": "// REST API Example\n@RestController\n@RequestMapping(\"/api/users\")\npublic class UserController {\n    \n    @GetMapping(\"/{id}\")\n    public ResponseEntity<UserDTO> getUser(@PathVariable String id) {\n        User user = userService.getUser(id);\n        return ResponseEntity.ok(new UserDTO(user));\n    }\n    \n    @PostMapping\n    public ResponseEntity<UserDTO> createUser(@RequestBody CreateUserRequest request) {\n        User user = userService.createUser(request);\n        return ResponseEntity\n            .status(HttpStatus.CREATED)\n            .body(new UserDTO(user));\n    }\n}\n\n// REST Request/Response (JSON)\n// GET /api/users/123\n{\n  \"id\": \"123\",\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n  \"createdAt\": \"2024-01-01T10:00:00Z\"\n}\n\n// gRPC Protocol Buffer Definition\n// user.proto\nsyntax = \"proto3\";\n\npackage user;\n\nservice UserService {\n  rpc GetUser(GetUserRequest) returns (User);\n  rpc CreateUser(CreateUserRequest) returns (User);\n  rpc ListUsers(ListUsersRequest) returns (stream User);\n  rpc StreamUsers(stream GetUserRequest) returns (stream User);\n}\n\nmessage User {\n  string id = 1;\n  string name = 2;\n  string email = 3;\n  int64 created_at = 4;\n}\n\nmessage GetUserRequest {\n  string id = 1;\n}\n\nmessage CreateUserRequest {\n  string name = 1;\n  string email = 2;\n}\n\nmessage ListUsersRequest {\n  int32 page_size = 1;\n  string page_token = 2;\n}\n\n// gRPC Server Implementation\n@GrpcService\npublic class UserGrpcService extends UserServiceGrpc.UserServiceImplBase {\n    \n    @Autowired\n    private UserService userService;\n    \n    @Override\n    public void getUser(GetUserRequest request,\n                       StreamObserver<User> responseObserver) {\n        User user = userService.getUser(request.getId());\n        responseObserver.onNext(toProto(user));\n        responseObserver.onCompleted();\n    }\n    \n    @Override\n    public void listUsers(ListUsersRequest request,\n                         StreamObserver<User> responseObserver) {\n        // Server streaming\n        List<User> users = userService.getAllUsers();\n        users.forEach(user -> {\n            responseObserver.onNext(toProto(user));\n        });\n        responseObserver.onCompleted();\n    }\n    \n    @Override\n    public StreamObserver<GetUserRequest> streamUsers(\n            StreamObserver<User> responseObserver) {\n        // Bidirectional streaming\n        return new StreamObserver<GetUserRequest>() {\n            @Override\n            public void onNext(GetUserRequest request) {\n                User user = userService.getUser(request.getId());\n                responseObserver.onNext(toProto(user));\n            }\n            \n            @Override\n            public void onCompleted() {\n                responseObserver.onCompleted();\n            }\n            \n            @Override\n            public void onError(Throwable t) {\n                responseObserver.onError(t);\n            }\n        };\n    }\n}\n\n// gRPC Client\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private UserServiceGrpc.UserServiceBlockingStub userServiceStub;\n    \n    public void createOrder(OrderRequest request) {\n        // Synchronous call\n        User user = userServiceStub.getUser(\n            GetUserRequest.newBuilder()\n                .setId(request.getUserId())\n                .build()\n        );\n        \n        processOrder(user, request);\n    }\n}\n\n// Performance Comparison\n// REST: ~500 bytes JSON payload\n// gRPC: ~50 bytes binary payload (10x smaller)\n\n// REST: HTTP/1.1 (one request per connection)\n// gRPC: HTTP/2 (multiplexed, multiple requests per connection)"
    },
    {
      "id": 91,
      "question": "How do you implement distributed caching in Microservices?",
      "answer": "Distributed caching shares cache across multiple service instances for consistency, performance, and scalability.\n\nDistributed Cache Solutions:\n• Redis: In-memory data store\n• Memcached: Distributed memory caching\n• Hazelcast: In-memory data grid\n• Apache Ignite: Distributed database and cache\n\nCaching Strategies:\n• Cache-Aside: Application manages cache\n• Read-Through: Cache loads from source\n• Write-Through: Write to cache and source\n• Write-Behind: Async write to source\n• Refresh-Ahead: Proactive refresh\n\nCache Patterns:\n• Per-service cache: Independent caches\n• Shared cache: Common cache cluster\n• Hierarchical cache: L1 (local) + L2 (distributed)\n\nKey Considerations:\n• Cache key design\n• TTL (Time To Live) settings\n• Eviction policies (LRU, LFU)\n• Cache invalidation strategy\n• Data serialization\n• Cache warming\n\nBest Practices:\n• Cache immutable data when possible\n• Set appropriate TTLs\n• Handle cache misses gracefully\n• Monitor hit rates\n• Implement fallback to source\n• Use consistent hashing\n• Plan for cache failures\n• Version cached data",
      "explanation": "Distributed caching uses solutions like Redis to share cached data across microservice instances, implementing strategies like cache-aside with proper TTL, invalidation, and fallback mechanisms for performance and consistency.",
      "difficulty": "Medium",
      "code": "// Redis Configuration\n@Configuration\n@EnableCaching\npublic class RedisCacheConfig {\n    \n    @Bean\n    public RedisConnectionFactory redisConnectionFactory() {\n        RedisStandaloneConfiguration config = \n            new RedisStandaloneConfiguration(\"localhost\", 6379);\n        return new LettuceConnectionFactory(config);\n    }\n    \n    @Bean\n    public CacheManager cacheManager(RedisConnectionFactory factory) {\n        RedisCacheConfiguration config = RedisCacheConfiguration\n            .defaultCacheConfig()\n            .entryTtl(Duration.ofMinutes(10))\n            .serializeKeysWith(\n                RedisSerializationContext.SerializationPair\n                    .fromSerializer(new StringRedisSerializer())\n            )\n            .serializeValuesWith(\n                RedisSerializationContext.SerializationPair\n                    .fromSerializer(new GenericJackson2JsonRedisSerializer())\n            );\n        \n        Map<String, RedisCacheConfiguration> cacheConfigurations = Map.of(\n            \"products\", config.entryTtl(Duration.ofHours(1)),\n            \"users\", config.entryTtl(Duration.ofMinutes(30)),\n            \"sessions\", config.entryTtl(Duration.ofMinutes(15))\n        );\n        \n        return RedisCacheManager.builder(factory)\n            .cacheDefaults(config)\n            .withInitialCacheConfigurations(cacheConfigurations)\n            .build();\n    }\n    \n    @Bean\n    public RedisTemplate<String, Object> redisTemplate(\n            RedisConnectionFactory factory) {\n        RedisTemplate<String, Object> template = new RedisTemplate<>();\n        template.setConnectionFactory(factory);\n        template.setKeySerializer(new StringRedisSerializer());\n        template.setValueSerializer(new GenericJackson2JsonRedisSerializer());\n        return template;\n    }\n}\n\n// Cache-Aside Pattern\n@Service\npublic class ProductService {\n    \n    @Autowired\n    private ProductRepository productRepository;\n    \n    @Cacheable(value = \"products\", key = \"#id\")\n    public Product getProduct(String id) {\n        log.info(\"Loading product from database: {}\", id);\n        return productRepository.findById(id)\n            .orElseThrow(() -> new ProductNotFoundException(id));\n    }\n    \n    @CachePut(value = \"products\", key = \"#product.id\")\n    public Product updateProduct(Product product) {\n        return productRepository.save(product);\n    }\n    \n    @CacheEvict(value = \"products\", key = \"#id\")\n    public void deleteProduct(String id) {\n        productRepository.deleteById(id);\n    }\n    \n    @CacheEvict(value = \"products\", allEntries = true)\n    public void clearProductCache() {\n        log.info(\"Clearing all products from cache\");\n    }\n}\n\n// Manual Cache Management\n@Service\npublic class UserService {\n    \n    @Autowired\n    private RedisTemplate<String, User> redisTemplate;\n    @Autowired\n    private UserRepository userRepository;\n    \n    private static final String CACHE_PREFIX = \"user:\";\n    private static final long TTL_MINUTES = 30;\n    \n    public User getUser(String userId) {\n        // Try cache first\n        String cacheKey = CACHE_PREFIX + userId;\n        User cached = redisTemplate.opsForValue().get(cacheKey);\n        \n        if (cached != null) {\n            log.debug(\"Cache hit for user: {}\", userId);\n            return cached;\n        }\n        \n        // Cache miss - load from database\n        log.info(\"Cache miss for user: {}, loading from DB\", userId);\n        User user = userRepository.findById(userId)\n            .orElseThrow(() -> new UserNotFoundException(userId));\n        \n        // Store in cache\n        redisTemplate.opsForValue().set(\n            cacheKey,\n            user,\n            TTL_MINUTES,\n            TimeUnit.MINUTES\n        );\n        \n        return user;\n    }\n    \n    public void invalidateUserCache(String userId) {\n        String cacheKey = CACHE_PREFIX + userId;\n        redisTemplate.delete(cacheKey);\n        log.info(\"Invalidated cache for user: {}\", userId);\n    }\n}\n\n// Event-Driven Cache Invalidation\n@Service\npublic class CacheInvalidationService {\n    \n    @Autowired\n    private CacheManager cacheManager;\n    \n    @EventListener\n    public void onProductUpdated(ProductUpdatedEvent event) {\n        Cache cache = cacheManager.getCache(\"products\");\n        if (cache != null) {\n            cache.evict(event.getProductId());\n            log.info(\"Invalidated product cache: {}\", event.getProductId());\n        }\n    }\n    \n    @EventListener\n    public void onInventoryUpdated(InventoryUpdatedEvent event) {\n        // Invalidate related caches\n        invalidateProductCache(event.getProductId());\n        invalidateProductListCache();\n    }\n}\n\n// Hierarchical Caching (L1 + L2)\n@Service\npublic class MultiLevelCacheService {\n    \n    // L1: Local cache (Caffeine)\n    private final Cache<String, Product> localCache;\n    \n    // L2: Distributed cache (Redis)\n    @Autowired\n    private RedisTemplate<String, Product> redisTemplate;\n    \n    @Autowired\n    private ProductRepository productRepository;\n    \n    public MultiLevelCacheService() {\n        this.localCache = Caffeine.newBuilder()\n            .maximumSize(1000)\n            .expireAfterWrite(5, TimeUnit.MINUTES)\n            .build();\n    }\n    \n    public Product getProduct(String productId) {\n        // L1: Check local cache\n        Product product = localCache.getIfPresent(productId);\n        if (product != null) {\n            log.debug(\"L1 cache hit: {}\", productId);\n            return product;\n        }\n        \n        // L2: Check Redis\n        product = redisTemplate.opsForValue().get(\"product:\" + productId);\n        if (product != null) {\n            log.debug(\"L2 cache hit: {}\", productId);\n            localCache.put(productId, product);  // Promote to L1\n            return product;\n        }\n        \n        // Load from database\n        log.info(\"Cache miss, loading from DB: {}\", productId);\n        product = productRepository.findById(productId)\n            .orElseThrow(() -> new ProductNotFoundException(productId));\n        \n        // Store in both caches\n        localCache.put(productId, product);\n        redisTemplate.opsForValue().set(\n            \"product:\" + productId,\n            product,\n            1,\n            TimeUnit.HOURS\n        );\n        \n        return product;\n    }\n}\n\n// Cache Monitoring\n@Component\npublic class CacheMetrics {\n    \n    @Autowired\n    private CacheManager cacheManager;\n    @Autowired\n    private MeterRegistry meterRegistry;\n    \n    @Scheduled(fixedRate = 60000)\n    public void recordCacheMetrics() {\n        cacheManager.getCacheNames().forEach(cacheName -> {\n            Cache cache = cacheManager.getCache(cacheName);\n            if (cache instanceof RedisCache) {\n                // Record metrics\n                meterRegistry.gauge(\n                    \"cache.size\",\n                    Tags.of(\"cache\", cacheName),\n                    cache,\n                    c -> getCacheSize(c)\n                );\n            }\n        });\n    }\n}"
    },
    {
      "id": 92,
      "question": "What are the patterns for handling file uploads in Microservices?",
      "answer": "File upload handling in microservices requires patterns that support scalability, reliability, and separation of concerns.\n\nUpload Patterns:\n• Direct Upload: Client uploads to service\n• Presigned URL: Client uploads directly to storage\n• Chunked Upload: Large files in chunks\n• Claim Check: Store file, pass reference\n• Async Processing: Background file processing\n\nStorage Options:\n• Object Storage: S3, Azure Blob, GCS\n• Network File System: NFS\n• Database: For small files\n• CDN: For distribution\n\nPresigned URL Pattern:\n• Service generates presigned URL\n• Client uploads directly to storage\n• Service receives notification\n• Reduces service load\n• Better performance\n\nChunked Upload:\n• Split large files\n• Upload chunks in parallel\n• Resume on failure\n• Assemble after upload\n\nBest Practices:\n• Validate file type and size\n• Scan for viruses\n• Generate unique filenames\n• Set appropriate permissions\n• Implement rate limiting\n• Use CDN for serving\n• Clean up abandoned uploads\n• Monitor storage usage",
      "explanation": "File uploads use presigned URLs for direct client-to-storage uploads or chunked uploads for large files, with async processing and claim check pattern to decouple storage from processing while ensuring scalability.",
      "difficulty": "Medium",
      "code": "// Presigned URL Upload Pattern\n@RestController\n@RequestMapping(\"/api/files\")\npublic class FileUploadController {\n    \n    @Autowired\n    private S3Client s3Client;\n    @Autowired\n    private FileMetadataService metadataService;\n    \n    @PostMapping(\"/upload-url\")\n    public ResponseEntity<PresignedUrlResponse> getUploadUrl(\n            @RequestParam String filename,\n            @RequestParam String contentType) {\n        \n        // Generate unique file key\n        String fileKey = UUID.randomUUID().toString() + \"/\" + filename;\n        String bucket = \"user-uploads\";\n        \n        // Create presigned URL for upload\n        PutObjectRequest putRequest = PutObjectRequest.builder()\n            .bucket(bucket)\n            .key(fileKey)\n            .contentType(contentType)\n            .build();\n        \n        PutObjectPresignRequest presignRequest = PutObjectPresignRequest.builder()\n            .signatureDuration(Duration.ofMinutes(15))\n            .putObjectRequest(putRequest)\n            .build();\n        \n        PresignedPutObjectRequest presignedRequest = \n            s3Presigner.presignPutObject(presignRequest);\n        \n        // Save metadata\n        FileMetadata metadata = new FileMetadata(\n            fileKey,\n            filename,\n            contentType,\n            FileStatus.PENDING,\n            LocalDateTime.now()\n        );\n        metadataService.save(metadata);\n        \n        PresignedUrlResponse response = new PresignedUrlResponse(\n            presignedRequest.url().toString(),\n            fileKey,\n            Duration.ofMinutes(15)\n        );\n        \n        return ResponseEntity.ok(response);\n    }\n    \n    @PostMapping(\"/confirm\")\n    public ResponseEntity<Void> confirmUpload(\n            @RequestParam String fileKey) {\n        \n        // Verify file exists\n        HeadObjectRequest headRequest = HeadObjectRequest.builder()\n            .bucket(\"user-uploads\")\n            .key(fileKey)\n            .build();\n        \n        try {\n            s3Client.headObject(headRequest);\n            \n            // Update metadata\n            metadataService.updateStatus(fileKey, FileStatus.UPLOADED);\n            \n            // Trigger async processing\n            fileProcessingService.processFile(fileKey);\n            \n            return ResponseEntity.ok().build();\n        } catch (NoSuchKeyException e) {\n            return ResponseEntity.notFound().build();\n        }\n    }\n}\n\n// Direct Upload with Multipart\n@RestController\npublic class DirectUploadController {\n    \n    @Autowired\n    private S3Client s3Client;\n    \n    @PostMapping(\"/api/files/direct\")\n    public ResponseEntity<FileUploadResponse> uploadFile(\n            @RequestParam(\"file\") MultipartFile file,\n            @RequestHeader(\"X-User-ID\") String userId) {\n        \n        // Validate file\n        validateFile(file);\n        \n        // Generate file key\n        String fileKey = String.format(\n            \"%s/%s/%s\",\n            userId,\n            LocalDate.now(),\n            UUID.randomUUID() + \"-\" + file.getOriginalFilename()\n        );\n        \n        try {\n            // Upload to S3\n            PutObjectRequest putRequest = PutObjectRequest.builder()\n                .bucket(\"user-uploads\")\n                .key(fileKey)\n                .contentType(file.getContentType())\n                .metadata(Map.of(\n                    \"user-id\", userId,\n                    \"original-filename\", file.getOriginalFilename()\n                ))\n                .build();\n            \n            s3Client.putObject(\n                putRequest,\n                RequestBody.fromInputStream(\n                    file.getInputStream(),\n                    file.getSize()\n                )\n            );\n            \n            // Generate public URL\n            String fileUrl = String.format(\n                \"https://%s.s3.amazonaws.com/%s\",\n                \"user-uploads\",\n                fileKey\n            );\n            \n            FileUploadResponse response = new FileUploadResponse(\n                fileKey,\n                fileUrl,\n                file.getSize()\n            );\n            \n            return ResponseEntity.ok(response);\n            \n        } catch (IOException e) {\n            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)\n                .build();\n        }\n    }\n    \n    private void validateFile(MultipartFile file) {\n        // Size check\n        if (file.getSize() > 10 * 1024 * 1024) {  // 10MB\n            throw new FileTooLargeException(\"File exceeds 10MB limit\");\n        }\n        \n        // Type check\n        String contentType = file.getContentType();\n        List<String> allowedTypes = Arrays.asList(\n            \"image/jpeg\",\n            \"image/png\",\n            \"application/pdf\"\n        );\n        if (!allowedTypes.contains(contentType)) {\n            throw new InvalidFileTypeException(\n                \"File type not allowed: \" + contentType\n            );\n        }\n    }\n}\n\n// Chunked Upload for Large Files\n@Service\npublic class ChunkedUploadService {\n    \n    @Autowired\n    private S3Client s3Client;\n    \n    private Map<String, String> activeUploads = new ConcurrentHashMap<>();\n    \n    public ChunkUploadResponse initiateChunkedUpload(\n            String filename,\n            long totalSize) {\n        \n        String fileKey = UUID.randomUUID().toString() + \"/\" + filename;\n        \n        // Initiate multipart upload\n        CreateMultipartUploadRequest createRequest = \n            CreateMultipartUploadRequest.builder()\n                .bucket(\"user-uploads\")\n                .key(fileKey)\n                .build();\n        \n        CreateMultipartUploadResponse response = \n            s3Client.createMultipartUpload(createRequest);\n        \n        activeUploads.put(fileKey, response.uploadId());\n        \n        return new ChunkUploadResponse(\n            fileKey,\n            response.uploadId(),\n            calculateChunkCount(totalSize)\n        );\n    }\n    \n    public void uploadChunk(\n            String fileKey,\n            String uploadId,\n            int chunkNumber,\n            byte[] chunkData) {\n        \n        UploadPartRequest uploadRequest = UploadPartRequest.builder()\n            .bucket(\"user-uploads\")\n            .key(fileKey)\n            .uploadId(uploadId)\n            .partNumber(chunkNumber)\n            .build();\n        \n        s3Client.uploadPart(\n            uploadRequest,\n            RequestBody.fromBytes(chunkData)\n        );\n    }\n    \n    public String completeUpload(String fileKey, String uploadId) {\n        // List all parts\n        ListPartsRequest listRequest = ListPartsRequest.builder()\n            .bucket(\"user-uploads\")\n            .key(fileKey)\n            .uploadId(uploadId)\n            .build();\n        \n        List<CompletedPart> completedParts = new ArrayList<>();\n        s3Client.listParts(listRequest).parts().forEach(part -> {\n            completedParts.add(CompletedPart.builder()\n                .partNumber(part.partNumber())\n                .eTag(part.eTag())\n                .build());\n        });\n        \n        // Complete multipart upload\n        CompleteMultipartUploadRequest completeRequest = \n            CompleteMultipartUploadRequest.builder()\n                .bucket(\"user-uploads\")\n                .key(fileKey)\n                .uploadId(uploadId)\n                .multipartUpload(CompletedMultipartUpload.builder()\n                    .parts(completedParts)\n                    .build())\n                .build();\n        \n        s3Client.completeMultipartUpload(completeRequest);\n        activeUploads.remove(fileKey);\n        \n        return fileKey;\n    }\n}\n\n// Async File Processing\n@Service\npublic class FileProcessingService {\n    \n    @Autowired\n    private S3Client s3Client;\n    \n    @Async\n    public CompletableFuture<Void> processFile(String fileKey) {\n        try {\n            // Download file\n            GetObjectRequest getRequest = GetObjectRequest.builder()\n                .bucket(\"user-uploads\")\n                .key(fileKey)\n                .build();\n            \n            ResponseBytes<GetObjectResponse> objectBytes = \n                s3Client.getObjectAsBytes(getRequest);\n            \n            byte[] data = objectBytes.asByteArray();\n            \n            // Process file (virus scan, thumbnail generation, etc.)\n            virusScanService.scan(data);\n            \n            if (isImage(fileKey)) {\n                generateThumbnail(fileKey, data);\n            }\n            \n            // Update metadata\n            metadataService.updateStatus(fileKey, FileStatus.PROCESSED);\n            \n            return CompletableFuture.completedFuture(null);\n        } catch (Exception e) {\n            log.error(\"File processing failed: {}\", fileKey, e);\n            metadataService.updateStatus(fileKey, FileStatus.FAILED);\n            throw new CompletionException(e);\n        }\n    }\n}"
    },
    {
      "id": 93,
      "question": "How do you implement API gateway aggregation and composition?",
      "answer": "API Gateway aggregation combines responses from multiple services into a single client response, reducing network calls and client complexity.\n\nAggregation Patterns:\n• Parallel Aggregation: Concurrent service calls\n• Sequential Aggregation: Dependent service calls\n• Partial Aggregation: Continue on partial failures\n• Conditional Aggregation: Based on response data\n\nComposition Strategies:\n• Server-side composition at gateway\n• Backend for Frontend (BFF) pattern\n• GraphQL federation\n• Response merging and transformation\n\nImplementation:\n• WebClient for reactive calls\n• CompletableFuture for parallel calls\n• Circuit breakers for resilience\n• Timeouts for each call\n• Fallback responses\n\nBenefits:\n• Reduced client round trips\n• Simplified client logic\n• Optimized network usage\n• Centralized error handling\n• Better mobile performance\n\nChallenges:\n• Increased gateway complexity\n• Timeout management\n• Partial failure handling\n• Response size management\n• Caching strategy\n\nBest Practices:\n• Use parallel calls when possible\n• Set appropriate timeouts\n• Handle partial failures gracefully\n• Cache aggregated responses\n• Monitor performance\n• Version aggregated APIs",
      "explanation": "API Gateway aggregation makes parallel calls to multiple microservices using WebClient or CompletableFuture, combines responses, and returns unified data to clients, reducing network overhead while handling timeouts and partial failures.",
      "difficulty": "Hard",
      "code": "// Parallel Aggregation with WebClient\n@RestController\n@RequestMapping(\"/api/aggregate\")\npublic class AggregationController {\n    \n    @Autowired\n    private WebClient webClient;\n    \n    @GetMapping(\"/user-dashboard/{userId}\")\n    public Mono<UserDashboard> getUserDashboard(@PathVariable String userId) {\n        // Parallel calls to multiple services\n        Mono<User> userMono = getUserInfo(userId);\n        Mono<List<Order>> ordersMono = getUserOrders(userId);\n        Mono<Wallet> walletMono = getUserWallet(userId);\n        Mono<List<Notification>> notificationsMono = getUserNotifications(userId);\n        \n        // Combine all responses\n        return Mono.zip(userMono, ordersMono, walletMono, notificationsMono)\n            .map(tuple -> new UserDashboard(\n                tuple.getT1(),  // User\n                tuple.getT2(),  // Orders\n                tuple.getT3(),  // Wallet\n                tuple.getT4()   // Notifications\n            ))\n            .timeout(Duration.ofSeconds(5))\n            .onErrorResume(e -> {\n                log.error(\"Dashboard aggregation failed\", e);\n                return Mono.just(getDefaultDashboard(userId));\n            });\n    }\n    \n    private Mono<User> getUserInfo(String userId) {\n        return webClient.get()\n            .uri(\"http://user-service/users/{id}\", userId)\n            .retrieve()\n            .bodyToMono(User.class)\n            .timeout(Duration.ofSeconds(2))\n            .onErrorResume(e -> Mono.just(getDefaultUser(userId)));\n    }\n    \n    private Mono<List<Order>> getUserOrders(String userId) {\n        return webClient.get()\n            .uri(\"http://order-service/orders?userId={id}\", userId)\n            .retrieve()\n            .bodyToFlux(Order.class)\n            .collectList()\n            .timeout(Duration.ofSeconds(2))\n            .onErrorResume(e -> Mono.just(Collections.emptyList()));\n    }\n}\n\n// Sequential Aggregation (Dependent Calls)\n@RestController\npublic class SequentialAggregationController {\n    \n    @GetMapping(\"/api/order-details/{orderId}\")\n    public Mono<OrderDetails> getOrderDetails(@PathVariable String orderId) {\n        return getOrder(orderId)\n            .flatMap(order -> {\n                // Get customer based on order\n                Mono<Customer> customerMono = \n                    getCustomer(order.getCustomerId());\n                \n                // Get products for order items\n                Mono<List<Product>> productsMono = \n                    getProductsForOrder(order);\n                \n                // Get shipping info\n                Mono<ShippingInfo> shippingMono = \n                    getShippingInfo(order.getId());\n                \n                // Combine all\n                return Mono.zip(\n                    Mono.just(order),\n                    customerMono,\n                    productsMono,\n                    shippingMono\n                ).map(tuple -> new OrderDetails(\n                    tuple.getT1(),\n                    tuple.getT2(),\n                    tuple.getT3(),\n                    tuple.getT4()\n                ));\n            });\n    }\n}\n\n// CompletableFuture-based Aggregation\n@Service\npublic class SyncAggregationService {\n    \n    @Autowired\n    private RestTemplate restTemplate;\n    @Autowired\n    private ExecutorService executorService;\n    \n    public ProductDetails getProductDetails(String productId) {\n        // Start parallel calls\n        CompletableFuture<Product> productFuture = \n            CompletableFuture.supplyAsync(() ->\n                restTemplate.getForObject(\n                    \"http://product-service/products/\" + productId,\n                    Product.class\n                ),\n                executorService\n            );\n        \n        CompletableFuture<List<Review>> reviewsFuture = \n            CompletableFuture.supplyAsync(() ->\n                Arrays.asList(restTemplate.getForObject(\n                    \"http://review-service/reviews?productId=\" + productId,\n                    Review[].class\n                )),\n                executorService\n            );\n        \n        CompletableFuture<Inventory> inventoryFuture = \n            CompletableFuture.supplyAsync(() ->\n                restTemplate.getForObject(\n                    \"http://inventory-service/inventory/\" + productId,\n                    Inventory.class\n                ),\n                executorService\n            );\n        \n        CompletableFuture<PriceInfo> priceFuture = \n            CompletableFuture.supplyAsync(() ->\n                restTemplate.getForObject(\n                    \"http://pricing-service/prices/\" + productId,\n                    PriceInfo.class\n                ),\n                executorService\n            );\n        \n        // Wait for all and combine\n        try {\n            CompletableFuture.allOf(\n                productFuture,\n                reviewsFuture,\n                inventoryFuture,\n                priceFuture\n            ).get(5, TimeUnit.SECONDS);\n            \n            return new ProductDetails(\n                productFuture.get(),\n                reviewsFuture.get(),\n                inventoryFuture.get(),\n                priceFuture.get()\n            );\n        } catch (TimeoutException e) {\n            // Handle timeout, return partial data\n            return buildPartialProductDetails(\n                productFuture,\n                reviewsFuture,\n                inventoryFuture,\n                priceFuture\n            );\n        } catch (Exception e) {\n            throw new AggregationException(\"Failed to aggregate product data\", e);\n        }\n    }\n    \n    private ProductDetails buildPartialProductDetails(\n            CompletableFuture<Product> productFuture,\n            CompletableFuture<List<Review>> reviewsFuture,\n            CompletableFuture<Inventory> inventoryFuture,\n            CompletableFuture<PriceInfo> priceFuture) {\n        \n        Product product = getOrDefault(productFuture, new Product());\n        List<Review> reviews = getOrDefault(reviewsFuture, Collections.emptyList());\n        Inventory inventory = getOrDefault(inventoryFuture, new Inventory());\n        PriceInfo price = getOrDefault(priceFuture, new PriceInfo());\n        \n        return new ProductDetails(product, reviews, inventory, price);\n    }\n    \n    private <T> T getOrDefault(CompletableFuture<T> future, T defaultValue) {\n        try {\n            if (future.isDone() && !future.isCompletedExceptionally()) {\n                return future.get();\n            }\n        } catch (Exception e) {\n            log.warn(\"Failed to get future result\", e);\n        }\n        return defaultValue;\n    }\n}"
    },
    {
      "id": 94,
      "question": "What are the strategies for handling database schema evolution in Microservices?",
      "answer": "Database schema evolution manages changes to database structure in production while maintaining service availability and data integrity.\n\nEvolution Strategies:\n• Expand-Contract Pattern: Add before removing\n• Parallel Change: Support both old and new\n• Blue-Green Database: Separate databases\n• Backward Compatible Changes: Non-breaking changes\n• Database Migration Tools: Automated migrations\n\nExpand-Contract Pattern:\n• Phase 1 (Expand): Add new columns/tables\n• Phase 2 (Migrate): Dual write to old and new\n• Phase 3 (Contract): Remove old columns/tables\n• Gradual, safe approach\n\nBackward Compatible Changes:\n• Add nullable columns\n• Add new tables\n• Add indexes\n• Extend varchar lengths\n• Add default values\n\nBreaking Changes:\n• Remove columns\n• Change column types\n• Add NOT NULL constraints\n• Rename columns/tables\n• Change relationships\n\nMigration Tools:\n• Flyway: Version-based migrations\n• Liquibase: XML/JSON based\n• Database-specific tools\n\nBest Practices:\n• Version all schema changes\n• Test migrations thoroughly\n• Rollback capability\n• Monitor migration performance\n• Backup before migrations\n• Separate DDL from DML\n• Use transactions when possible",
      "explanation": "Database schema evolution uses expand-contract pattern to add new schema before removing old, migration tools like Flyway for versioned changes, and backward compatibility to support gradual service updates without downtime.",
      "difficulty": "Hard",
      "code": "// Flyway Configuration\n@Configuration\npublic class FlywayConfig {\n    \n    @Bean\n    public Flyway flyway(DataSource dataSource) {\n        Flyway flyway = Flyway.configure()\n            .dataSource(dataSource)\n            .locations(\"classpath:db/migration\")\n            .baselineOnMigrate(true)\n            .validateOnMigrate(true)\n            .outOfOrder(false)\n            .load();\n        \n        flyway.migrate();\n        return flyway;\n    }\n}\n\n// Migration Files\n// V1__Create_users_table.sql\nCREATE TABLE users (\n    id VARCHAR(36) PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    email VARCHAR(255) NOT NULL UNIQUE,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n);\n\n// V2__Add_user_status.sql (Backward compatible)\nALTER TABLE users\nADD COLUMN status VARCHAR(50) DEFAULT 'ACTIVE';\n\n// V3__Split_user_name.sql (Expand-Contract - Expand phase)\nALTER TABLE users\nADD COLUMN first_name VARCHAR(255),\nADD COLUMN last_name VARCHAR(255);\n\n-- Migrate existing data\nUPDATE users\nSET first_name = SUBSTRING_INDEX(name, ' ', 1),\n    last_name = SUBSTRING_INDEX(name, ' ', -1)\nWHERE first_name IS NULL;\n\n// Application Code - Dual Write Phase\n@Entity\n@Table(name = \"users\")\npublic class User {\n    @Id\n    private String id;\n    \n    // Old field (kept for backward compatibility)\n    @Deprecated\n    @Column(name = \"name\")\n    private String name;\n    \n    // New fields\n    @Column(name = \"first_name\")\n    private String firstName;\n    \n    @Column(name = \"last_name\")\n    private String lastName;\n    \n    // Maintain both representations\n    @PrePersist\n    @PreUpdate\n    public void syncNameFields() {\n        // Dual write: update both old and new\n        if (firstName != null && lastName != null) {\n            this.name = firstName + \" \" + lastName;\n        } else if (name != null && firstName == null) {\n            String[] parts = name.split(\" \", 2);\n            this.firstName = parts[0];\n            this.lastName = parts.length > 1 ? parts[1] : \"\";\n        }\n    }\n}\n\n// V4__Remove_old_name_column.sql (Contract phase)\n-- Only after all services updated\nALTER TABLE users\nDROP COLUMN name;\n\n// Rollback Script\n// R__Rollback_split_name.sql\nALTER TABLE users\nADD COLUMN name VARCHAR(255);\n\nUPDATE users\nSET name = CONCAT(first_name, ' ', last_name)\nWHERE name IS NULL;\n\nALTER TABLE users\nDROP COLUMN first_name,\nDROP COLUMN last_name;\n\n// Feature Flag for Schema Changes\n@Service\npublic class UserService {\n    \n    @Autowired\n    private FeatureFlagService featureFlags;\n    \n    public User getUser(String userId) {\n        User user = userRepository.findById(userId).orElseThrow();\n        \n        if (featureFlags.isEnabled(\"use-split-name\")) {\n            // Use new schema\n            return new UserDTO(\n                user.getId(),\n                user.getFirstName(),\n                user.getLastName()\n            );\n        } else {\n            // Use old schema\n            return new UserDTO(\n                user.getId(),\n                user.getName()\n            );\n        }\n    }\n}\n\n// Zero-Downtime Deployment Strategy\n// 1. Deploy code supporting both schemas\n// 2. Run migration to add new columns\n// 3. Enable feature flag gradually\n// 4. Monitor for issues\n// 5. Once stable, deploy code using only new schema\n// 6. Run migration to remove old columns\n\n// Migration Monitoring\n@Component\npublic class MigrationMonitor {\n    \n    @Autowired\n    private Flyway flyway;\n    @Autowired\n    private MeterRegistry meterRegistry;\n    \n    @EventListener(ApplicationReadyEvent.class)\n    public void checkMigrationStatus() {\n        MigrationInfo[] migrations = flyway.info().all();\n        \n        for (MigrationInfo info : migrations) {\n            meterRegistry.gauge(\n                \"flyway.migration.status\",\n                Tags.of(\n                    \"version\", info.getVersion().toString(),\n                    \"state\", info.getState().toString()\n                ),\n                info.getState().isApplied() ? 1 : 0\n            );\n        }\n    }\n}"
    },
    {
      "id": 95,
      "question": "How do you implement correlation and causation tracking in Microservices?",
      "answer": "Correlation and causation tracking links related events and requests across distributed microservices for debugging and analysis.\n\nTracking Concepts:\n• Correlation ID: Links related requests\n• Causation ID: Parent request identifier\n• Session ID: User session tracking\n• Transaction ID: Business transaction tracking\n• Span ID: Distributed tracing identifier\n\nImplementation:\n• Generate ID at entry point\n• Propagate through headers\n• Include in all logs\n• Store in MDC (Mapped Diagnostic Context)\n• Pass through message queues\n• Include in database records\n\nID Propagation:\n• HTTP headers (X-Correlation-ID)\n• Message headers (Kafka, RabbitMQ)\n• Thread-local storage (MDC)\n• Async context propagation\n\nCausation Tracking:\n• Track parent-child relationships\n• Record event sequences\n• Build dependency graphs\n• Identify root causes\n\nTools:\n• Spring Cloud Sleuth\n• OpenTelemetry\n• Jaeger/Zipkin\n• Custom implementation\n\nBest Practices:\n• Generate IDs early\n• Always propagate IDs\n• Include in all log statements\n• Store in audit tables\n• Index for quick search\n• Set expiration policies\n• Monitor ID coverage",
      "explanation": "Correlation tracking uses unique IDs generated at entry point and propagated through HTTP headers, message queues, and MDC to link related requests and events across microservices for debugging and tracing.",
      "difficulty": "Medium",
      "code": "// Correlation ID Filter\n@Component\n@Order(Ordered.HIGHEST_PRECEDENCE)\npublic class CorrelationIdFilter extends OncePerRequestFilter {\n    \n    private static final String CORRELATION_ID_HEADER = \"X-Correlation-ID\";\n    private static final String CAUSATION_ID_HEADER = \"X-Causation-ID\";\n    \n    @Override\n    protected void doFilterInternal(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            FilterChain filterChain) throws ServletException, IOException {\n        \n        // Get or generate correlation ID\n        String correlationId = request.getHeader(CORRELATION_ID_HEADER);\n        if (correlationId == null) {\n            correlationId = UUID.randomUUID().toString();\n        }\n        \n        // Get causation ID (parent request ID)\n        String causationId = request.getHeader(CAUSATION_ID_HEADER);\n        \n        // Store in MDC\n        MDC.put(\"correlationId\", correlationId);\n        if (causationId != null) {\n            MDC.put(\"causationId\", causationId);\n        }\n        \n        // Add to response headers\n        response.addHeader(CORRELATION_ID_HEADER, correlationId);\n        \n        try {\n            filterChain.doFilter(request, response);\n        } finally {\n            MDC.clear();\n        }\n    }\n}\n\n// RestTemplate Interceptor for Propagation\n@Component\npublic class CorrelationIdInterceptor implements ClientHttpRequestInterceptor {\n    \n    @Override\n    public ClientHttpResponse intercept(\n            HttpRequest request,\n            byte[] body,\n            ClientHttpRequestExecution execution) throws IOException {\n        \n        // Propagate correlation ID\n        String correlationId = MDC.get(\"correlationId\");\n        if (correlationId != null) {\n            request.getHeaders().set(\"X-Correlation-ID\", correlationId);\n        }\n        \n        // Set causation ID to current request ID\n        String currentRequestId = UUID.randomUUID().toString();\n        request.getHeaders().set(\"X-Causation-ID\", currentRequestId);\n        \n        return execution.execute(request, body);\n    }\n}\n\n// Message Queue Correlation\n@Service\npublic class OrderEventPublisher {\n    \n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n    \n    public void publishOrderCreated(Order order) {\n        OrderCreatedEvent event = new OrderCreatedEvent(order);\n        \n        // Create message with correlation headers\n        Message message = MessageBuilder\n            .withBody(serialize(event))\n            .setHeader(\"correlationId\", MDC.get(\"correlationId\"))\n            .setHeader(\"causationId\", UUID.randomUUID().toString())\n            .setHeader(\"timestamp\", System.currentTimeMillis())\n            .build();\n        \n        rabbitTemplate.send(\"order.exchange\", \"order.created\", message);\n        \n        log.info(\"Published order created event: {}\", order.getId());\n    }\n}\n\n@Service\npublic class InventoryEventListener {\n    \n    @RabbitListener(queues = \"inventory.queue\")\n    public void handleOrderCreated(Message message) {\n        try {\n            // Extract correlation headers\n            String correlationId = \n                (String) message.getMessageProperties().getHeaders().get(\"correlationId\");\n            String causationId = \n                (String) message.getMessageProperties().getHeaders().get(\"causationId\");\n            \n            // Set in MDC\n            MDC.put(\"correlationId\", correlationId);\n            MDC.put(\"causationId\", causationId);\n            \n            // Process event\n            OrderCreatedEvent event = deserialize(message.getBody());\n            processOrder(event);\n            \n            log.info(\"Processed order created event\");\n        } finally {\n            MDC.clear();\n        }\n    }\n}\n\n// Audit Entity with Correlation\n@Entity\npublic class AuditLog {\n    @Id\n    private String id;\n    private String correlationId;\n    private String causationId;\n    private String userId;\n    private String action;\n    private String entityType;\n    private String entityId;\n    private LocalDateTime timestamp;\n    \n    @Column(columnDefinition = \"TEXT\")\n    private String details;\n}\n\n@Service\npublic class AuditService {\n    \n    @Autowired\n    private AuditLogRepository auditRepository;\n    \n    public void logAction(String action, String entityType, String entityId) {\n        AuditLog log = new AuditLog();\n        log.setId(UUID.randomUUID().toString());\n        log.setCorrelationId(MDC.get(\"correlationId\"));\n        log.setCausationId(MDC.get(\"causationId\"));\n        log.setAction(action);\n        log.setEntityType(entityType);\n        log.setEntityId(entityId);\n        log.setTimestamp(LocalDateTime.now());\n        \n        auditRepository.save(log);\n    }\n    \n    public List<AuditLog> getRelatedEvents(String correlationId) {\n        return auditRepository.findByCorrelationIdOrderByTimestamp(correlationId);\n    }\n}\n\n// Logback Configuration\n// logback-spring.xml\n<configuration>\n    <appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder>\n            <pattern>\n                %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} \n                [correlationId=%X{correlationId}] [causationId=%X{causationId}] \n                - %msg%n\n            </pattern>\n        </encoder>\n    </appender>\n    \n    <appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <file>logs/application.log</file>\n        <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\">\n            <includeMdcKeyName>correlationId</includeMdcKeyName>\n            <includeMdcKeyName>causationId</includeMdcKeyName>\n        </encoder>\n    </appender>\n</configuration>\n\n// Elasticsearch Query for Related Events\nGET /logs/_search\n{\n  \"query\": {\n    \"term\": {\n      \"correlationId\": \"abc-123-def\"\n    }\n  },\n  \"sort\": [\n    { \"timestamp\": \"asc\" }\n  ]\n}"
    },
    {
      "id": 96,
      "question": "What are the strategies for handling service-to-service authentication?",
      "answer": "Service-to-service authentication ensures that only authorized services can communicate with each other.\n\nAuthentication Methods:\n• Mutual TLS (mTLS): Certificate-based authentication\n• JWT Tokens: Signed tokens between services\n• API Keys: Shared secrets\n• OAuth 2.0 Client Credentials: Standard protocol\n• Service Mesh: Built-in authentication\n\nmTLS Authentication:\n• Each service has certificate\n• Certificates validated on connection\n• Strong cryptographic security\n• Automatic with service mesh\n• No application code changes\n\nJWT Service Tokens:\n• Service requests token from auth service\n• Token includes service identity and permissions\n• Short-lived tokens\n• Propagated in request headers\n• Validated by receiving service\n\nService Mesh Approach:\n• Transparent mTLS between services\n• Managed by sidecar proxies\n• Automatic certificate rotation\n• Policy-based authorization\n• No code changes needed\n\nBest Practices:\n• Use strong authentication\n• Rotate credentials regularly\n• Implement least privilege\n• Monitor authentication failures\n• Use service mesh when possible\n• Separate user and service auth\n• Log all authentication events",
      "explanation": "Service-to-service authentication uses mTLS for certificate-based verification, JWT tokens for identity propagation, or service mesh for transparent authentication, ensuring only authorized services can communicate.",
      "difficulty": "Hard",
      "code": "// JWT Service-to-Service Authentication\n@Service\npublic class ServiceAuthenticationService {\n    \n    @Value(\"${service.auth.url}\")\n    private String authServiceUrl;\n    \n    @Value(\"${service.name}\")\n    private String serviceName;\n    \n    @Value(\"${service.secret}\")\n    private String serviceSecret;\n    \n    private String cachedToken;\n    private LocalDateTime tokenExpiry;\n    \n    public String getServiceToken() {\n        if (cachedToken != null && LocalDateTime.now().isBefore(tokenExpiry)) {\n            return cachedToken;\n        }\n        \n        // Request new token\n        ServiceTokenRequest request = new ServiceTokenRequest(\n            serviceName,\n            serviceSecret\n        );\n        \n        ServiceTokenResponse response = restTemplate.postForObject(\n            authServiceUrl + \"/token\",\n            request,\n            ServiceTokenResponse.class\n        );\n        \n        cachedToken = response.getToken();\n        tokenExpiry = LocalDateTime.now().plusMinutes(response.getExpiresIn() / 60 - 5);\n        \n        return cachedToken;\n    }\n}\n\n// Service Token Interceptor\n@Component\npublic class ServiceTokenInterceptor implements ClientHttpRequestInterceptor {\n    \n    @Autowired\n    private ServiceAuthenticationService authService;\n    \n    @Override\n    public ClientHttpResponse intercept(\n            HttpRequest request,\n            byte[] body,\n            ClientHttpRequestExecution execution) throws IOException {\n        \n        // Add service token to request\n        String token = authService.getServiceToken();\n        request.getHeaders().set(\"Authorization\", \"Bearer \" + token);\n        request.getHeaders().set(\"X-Service-Name\", \"order-service\");\n        \n        return execution.execute(request, body);\n    }\n}\n\n// Service Token Validation\n@Component\npublic class ServiceTokenValidator {\n    \n    @Value(\"${jwt.secret}\")\n    private String jwtSecret;\n    \n    public boolean validateServiceToken(String token) {\n        try {\n            Claims claims = Jwts.parser()\n                .setSigningKey(jwtSecret)\n                .parseClaimsJws(token)\n                .getBody();\n            \n            // Verify it's a service token\n            String type = claims.get(\"type\", String.class);\n            if (!\"service\".equals(type)) {\n                return false;\n            }\n            \n            // Check service permissions\n            List<String> permissions = claims.get(\"permissions\", List.class);\n            return permissions != null && !permissions.isEmpty();\n            \n        } catch (JwtException e) {\n            log.error(\"Invalid service token\", e);\n            return false;\n        }\n    }\n    \n    public String getServiceName(String token) {\n        Claims claims = Jwts.parser()\n            .setSigningKey(jwtSecret)\n            .parseClaimsJws(token)\n            .getBody();\n        return claims.get(\"service_name\", String.class);\n    }\n}\n\n// mTLS Configuration\n@Configuration\npublic class MutualTLSConfig {\n    \n    @Bean\n    public SSLContext sslContext() throws Exception {\n        // Load client certificate\n        KeyStore keyStore = KeyStore.getInstance(\"PKCS12\");\n        keyStore.load(\n            new FileInputStream(\"client-cert.p12\"),\n            \"password\".toCharArray()\n        );\n        \n        // Load trusted CA certificates\n        KeyStore trustStore = KeyStore.getInstance(\"JKS\");\n        trustStore.load(\n            new FileInputStream(\"truststore.jks\"),\n            \"password\".toCharArray()\n        );\n        \n        // Initialize SSL context\n        KeyManagerFactory keyManagerFactory = \n            KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());\n        keyManagerFactory.init(keyStore, \"password\".toCharArray());\n        \n        TrustManagerFactory trustManagerFactory = \n            TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());\n        trustManagerFactory.init(trustStore);\n        \n        SSLContext sslContext = SSLContext.getInstance(\"TLS\");\n        sslContext.init(\n            keyManagerFactory.getKeyManagers(),\n            trustManagerFactory.getTrustManagers(),\n            new SecureRandom()\n        );\n        \n        return sslContext;\n    }\n    \n    @Bean\n    public RestTemplate mtlsRestTemplate(SSLContext sslContext) {\n        CloseableHttpClient httpClient = HttpClients.custom()\n            .setSSLContext(sslContext)\n            .build();\n        \n        HttpComponentsClientHttpRequestFactory factory = \n            new HttpComponentsClientHttpRequestFactory(httpClient);\n        \n        return new RestTemplate(factory);\n    }\n}\n\n// Istio Service Mesh mTLS\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: production\nspec:\n  mtls:\n    mode: STRICT\n\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: order-service-authz\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: order-service\n  rules:\n  - from:\n    - source:\n        principals:\n        - \"cluster.local/ns/production/sa/user-service\"\n        - \"cluster.local/ns/production/sa/payment-service\"\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/orders/*\"]\n\n# No application code changes needed with service mesh!"
    },
    {
      "id": 97,
      "question": "How do you implement request validation in Microservices?",
      "answer": "Request validation ensures incoming data meets requirements before processing, preventing invalid data propagation.\n\nValidation Layers:\n• API Gateway: First line validation\n• Service Layer: Business rule validation\n• Domain Layer: Entity validation\n• Database Layer: Constraint validation\n\nValidation Types:\n• Schema Validation: Structure and types\n• Business Rule Validation: Domain rules\n• Security Validation: Authentication, authorization\n• Format Validation: Email, phone, dates\n• Range Validation: Min, max values\n\nImplementation:\n• Bean Validation (JSR-303)\n• Custom validators\n• OpenAPI validation\n• JSON Schema validation\n\nError Handling:\n• Return 400 Bad Request\n• Provide detailed error messages\n• List all validation errors\n• Don't expose internal details\n• Log validation failures\n\nBest Practices:\n• Validate early (fail fast)\n• Provide clear error messages\n• Validate at multiple layers\n• Use standard annotations\n• Custom validators for complex rules\n• Test validation thoroughly\n• Version validation rules\n• Monitor validation failures",
      "explanation": "Request validation uses Bean Validation annotations and custom validators at API Gateway and service layers to ensure data integrity, returning 400 with detailed error messages when validation fails.",
      "difficulty": "Easy",
      "code": "// Bean Validation Annotations\npublic class CreateUserRequest {\n    \n    @NotBlank(message = \"Name is required\")\n    @Size(min = 2, max = 100, message = \"Name must be between 2 and 100 characters\")\n    private String name;\n    \n    @NotBlank(message = \"Email is required\")\n    @Email(message = \"Invalid email format\")\n    private String email;\n    \n    @NotBlank(message = \"Password is required\")\n    @Pattern(\n        regexp = \"^(?=.*[0-9])(?=.*[a-z])(?=.*[A-Z])(?=.*[@#$%^&+=])(?=\\\\S+$).{8,}$\",\n        message = \"Password must be at least 8 characters with uppercase, lowercase, number and special character\"\n    )\n    private String password;\n    \n    @NotNull(message = \"Date of birth is required\")\n    @Past(message = \"Date of birth must be in the past\")\n    private LocalDate dateOfBirth;\n    \n    @NotNull\n    @Min(value = 18, message = \"Must be at least 18 years old\")\n    @Max(value = 120, message = \"Age must be less than 120\")\n    private Integer age;\n    \n    @Valid\n    @NotNull(message = \"Address is required\")\n    private Address address;\n}\n\n// Controller with Validation\n@RestController\n@RequestMapping(\"/api/users\")\n@Validated\npublic class UserController {\n    \n    @PostMapping\n    public ResponseEntity<UserResponse> createUser(\n            @Valid @RequestBody CreateUserRequest request) {\n        User user = userService.createUser(request);\n        return ResponseEntity.status(HttpStatus.CREATED)\n            .body(new UserResponse(user));\n    }\n    \n    @GetMapping(\"/{id}\")\n    public ResponseEntity<UserResponse> getUser(\n            @PathVariable \n            @Pattern(regexp = \"^[a-zA-Z0-9-]+$\", message = \"Invalid user ID format\")\n            String id) {\n        User user = userService.getUser(id);\n        return ResponseEntity.ok(new UserResponse(user));\n    }\n}\n\n// Global Exception Handler\n@RestControllerAdvice\npublic class ValidationExceptionHandler {\n    \n    @ExceptionHandler(MethodArgumentNotValidException.class)\n    public ResponseEntity<ValidationErrorResponse> handleValidationErrors(\n            MethodArgumentNotValidException ex) {\n        \n        List<ValidationError> errors = ex.getBindingResult()\n            .getFieldErrors()\n            .stream()\n            .map(error -> new ValidationError(\n                error.getField(),\n                error.getDefaultMessage(),\n                error.getRejectedValue()\n            ))\n            .collect(Collectors.toList());\n        \n        ValidationErrorResponse response = new ValidationErrorResponse(\n            \"Validation failed\",\n            errors\n        );\n        \n        return ResponseEntity\n            .status(HttpStatus.BAD_REQUEST)\n            .body(response);\n    }\n    \n    @ExceptionHandler(ConstraintViolationException.class)\n    public ResponseEntity<ValidationErrorResponse> handleConstraintViolation(\n            ConstraintViolationException ex) {\n        \n        List<ValidationError> errors = ex.getConstraintViolations()\n            .stream()\n            .map(violation -> new ValidationError(\n                violation.getPropertyPath().toString(),\n                violation.getMessage(),\n                violation.getInvalidValue()\n            ))\n            .collect(Collectors.toList());\n        \n        ValidationErrorResponse response = new ValidationErrorResponse(\n            \"Validation failed\",\n            errors\n        );\n        \n        return ResponseEntity\n            .status(HttpStatus.BAD_REQUEST)\n            .body(response);\n    }\n}\n\n// Custom Validator\n@Constraint(validatedBy = UniqueEmailValidator.class)\n@Target({ElementType.FIELD})\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface UniqueEmail {\n    String message() default \"Email already exists\";\n    Class<?>[] groups() default {};\n    Class<? extends Payload>[] payload() default {};\n}\n\n@Component\npublic class UniqueEmailValidator implements ConstraintValidator<UniqueEmail, String> {\n    \n    @Autowired\n    private UserRepository userRepository;\n    \n    @Override\n    public boolean isValid(String email, ConstraintValidatorContext context) {\n        if (email == null) {\n            return true;\n        }\n        return !userRepository.existsByEmail(email);\n    }\n}\n\n// Business Rule Validation\n@Service\npublic class OrderValidationService {\n    \n    public void validateOrder(OrderRequest request) {\n        List<String> errors = new ArrayList<>();\n        \n        // Business rule validations\n        if (request.getItems().isEmpty()) {\n            errors.add(\"Order must contain at least one item\");\n        }\n        \n        if (request.getTotal().compareTo(BigDecimal.ZERO) <= 0) {\n            errors.add(\"Order total must be greater than zero\");\n        }\n        \n        BigDecimal calculatedTotal = request.getItems().stream()\n            .map(item -> item.getPrice().multiply(\n                BigDecimal.valueOf(item.getQuantity())))\n            .reduce(BigDecimal.ZERO, BigDecimal::add);\n        \n        if (!calculatedTotal.equals(request.getTotal())) {\n            errors.add(\"Order total does not match item totals\");\n        }\n        \n        if (!errors.isEmpty()) {\n            throw new ValidationException(errors);\n        }\n    }\n}"
    },
    {
      "id": 98,
      "question": "What are the best practices for microservice error handling?",
      "answer": "Error handling in microservices requires consistent patterns across services for reliability and debugging.\n\nError Types:\n• Client Errors (4xx): Invalid requests\n• Server Errors (5xx): Internal failures\n• Network Errors: Timeouts, connection failures\n• Business Errors: Domain rule violations\n• Validation Errors: Invalid input\n\nError Response Structure:\n• Error code\n• Error message\n• Error details\n• Timestamp\n• Request ID/Correlation ID\n• Stack trace (development only)\n\nHTTP Status Codes:\n• 400: Bad Request\n• 401: Unauthorized\n• 403: Forbidden\n• 404: Not Found\n• 409: Conflict\n• 422: Unprocessable Entity\n• 500: Internal Server Error\n• 503: Service Unavailable\n\nError Propagation:\n• Don't expose internal errors\n• Transform errors at boundaries\n• Aggregate errors from multiple services\n• Preserve correlation IDs\n• Log at error source\n\nRetry Strategy:\n• Retry transient errors\n• Don't retry client errors\n• Use exponential backoff\n• Set retry limits\n• Implement circuit breakers\n\nBest Practices:\n• Standardize error responses\n• Log all errors\n• Monitor error rates\n• Use appropriate status codes\n• Provide actionable messages\n• Hide sensitive details\n• Include correlation IDs",
      "explanation": "Microservices error handling uses standardized error response structures with appropriate HTTP status codes, logging with correlation IDs, retry strategies for transient errors, and error transformation at service boundaries.",
      "difficulty": "Medium",
      "code": "// Standard Error Response\npublic class ErrorResponse {\n    private String errorCode;\n    private String message;\n    private String details;\n    private LocalDateTime timestamp;\n    private String path;\n    private String correlationId;\n    private List<ValidationError> validationErrors;\n}\n\n// Global Exception Handler\n@RestControllerAdvice\npublic class GlobalExceptionHandler {\n    \n    @ExceptionHandler(ResourceNotFoundException.class)\n    public ResponseEntity<ErrorResponse> handleResourceNotFound(\n            ResourceNotFoundException ex,\n            HttpServletRequest request) {\n        \n        ErrorResponse error = ErrorResponse.builder()\n            .errorCode(\"RESOURCE_NOT_FOUND\")\n            .message(ex.getMessage())\n            .timestamp(LocalDateTime.now())\n            .path(request.getRequestURI())\n            .correlationId(MDC.get(\"correlationId\"))\n            .build();\n        \n        return ResponseEntity\n            .status(HttpStatus.NOT_FOUND)\n            .body(error);\n    }\n    \n    @ExceptionHandler(BusinessException.class)\n    public ResponseEntity<ErrorResponse> handleBusinessException(\n            BusinessException ex,\n            HttpServletRequest request) {\n        \n        ErrorResponse error = ErrorResponse.builder()\n            .errorCode(ex.getErrorCode())\n            .message(ex.getMessage())\n            .details(ex.getDetails())\n            .timestamp(LocalDateTime.now())\n            .path(request.getRequestURI())\n            .correlationId(MDC.get(\"correlationId\"))\n            .build();\n        \n        return ResponseEntity\n            .status(HttpStatus.UNPROCESSABLE_ENTITY)\n            .body(error);\n    }\n    \n    @ExceptionHandler(Exception.class)\n    public ResponseEntity<ErrorResponse> handleGenericException(\n            Exception ex,\n            HttpServletRequest request) {\n        \n        log.error(\"Unhandled exception\", ex);\n        \n        ErrorResponse error = ErrorResponse.builder()\n            .errorCode(\"INTERNAL_ERROR\")\n            .message(\"An unexpected error occurred\")\n            .timestamp(LocalDateTime.now())\n            .path(request.getRequestURI())\n            .correlationId(MDC.get(\"correlationId\"))\n            .build();\n        \n        return ResponseEntity\n            .status(HttpStatus.INTERNAL_SERVER_ERROR)\n            .body(error);\n    }\n}\n\n// Custom Business Exceptions\npublic class BusinessException extends RuntimeException {\n    private final String errorCode;\n    private final String details;\n    \n    public BusinessException(String errorCode, String message, String details) {\n        super(message);\n        this.errorCode = errorCode;\n        this.details = details;\n    }\n}\n\npublic class InsufficientBalanceException extends BusinessException {\n    public InsufficientBalanceException(BigDecimal required, BigDecimal available) {\n        super(\n            \"INSUFFICIENT_BALANCE\",\n            \"Insufficient balance for transaction\",\n            String.format(\"Required: %s, Available: %s\", required, available)\n        );\n    }\n}\n\n// Error Handling with Circuit Breaker\n@Service\npublic class OrderService {\n    \n    @CircuitBreaker(name = \"userService\", fallbackMethod = \"getUserFallback\")\n    @Retry(name = \"userService\")\n    public User getUser(String userId) {\n        try {\n            return restTemplate.getForObject(\n                \"http://user-service/users/\" + userId,\n                User.class\n            );\n        } catch (HttpClientErrorException e) {\n            // Don't retry client errors\n            if (e.getStatusCode() == HttpStatus.NOT_FOUND) {\n                throw new UserNotFoundException(userId);\n            }\n            throw e;\n        } catch (HttpServerErrorException e) {\n            // Retry server errors\n            log.warn(\"User service error, will retry\", e);\n            throw e;\n        }\n    }\n    \n    private User getUserFallback(String userId, Exception ex) {\n        log.error(\"User service unavailable, using fallback\", ex);\n        return new User(userId, \"Unknown User\");\n    }\n}\n\n// Logging with Error Context\n@Aspect\n@Component\npublic class ErrorLoggingAspect {\n    \n    @AfterThrowing(pointcut = \"@within(org.springframework.web.bind.annotation.RestController)\",\n                   throwing = \"ex\")\n    public void logError(JoinPoint joinPoint, Exception ex) {\n        Map<String, Object> errorContext = new HashMap<>();\n        errorContext.put(\"method\", joinPoint.getSignature().getName());\n        errorContext.put(\"class\", joinPoint.getTarget().getClass().getSimpleName());\n        errorContext.put(\"args\", joinPoint.getArgs());\n        errorContext.put(\"exception\", ex.getClass().getSimpleName());\n        errorContext.put(\"message\", ex.getMessage());\n        errorContext.put(\"correlationId\", MDC.get(\"correlationId\"));\n        \n        log.error(\"Error occurred: {}\", errorContext, ex);\n        \n        // Send to monitoring system\n        metricsService.recordError(ex.getClass().getSimpleName());\n    }\n}"
    },
    {
      "id": 99,
      "question": "How do you implement request throttling and backpressure in Microservices?",
      "answer": "Throttling and backpressure protect services from overload by controlling request rates and providing feedback when overwhelmed.\n\nThrottling Strategies:\n• Rate Limiting: Maximum requests per time window\n• Concurrent Request Limiting: Maximum parallel requests\n• Queue-based: Buffer requests up to limit\n• Adaptive: Adjust based on system load\n• Token Bucket: Consume tokens per request\n\nBackpressure Patterns:\n• Reactive Backpressure: Signal upstream to slow down\n• Queue with Rejection: Reject when queue full\n• Load Shedding: Drop low-priority requests\n• Circuit Breaking: Stop accepting requests\n\nImplementation Levels:\n• API Gateway: Entry point throttling\n• Service Level: Per-service limits\n• Database Level: Connection pool limits\n• Message Queue: Consumer rate limiting\n\nBest Practices:\n• Set realistic limits\n• Prioritize requests\n• Graceful degradation\n• Return 429 or 503\n• Include Retry-After header\n• Monitor queue depths\n• Alert on rejections\n• Test under load\n\nReactive Backpressure:\n• Project Reactor\n• Spring WebFlux\n• RxJava\n• Automatic flow control",
      "explanation": "Throttling limits request rates using algorithms like token bucket while backpressure signals upstream services to slow down when overwhelmed, implemented through reactive streams or queue-based systems with proper rejection handling.",
      "difficulty": "Hard",
      "code": "// Concurrent Request Limiting\n@Component\npublic class ConcurrencyLimitFilter extends OncePerRequestFilter {\n    \n    private final Semaphore semaphore;\n    private final int maxConcurrentRequests;\n    \n    public ConcurrencyLimitFilter(\n            @Value(\"${server.max-concurrent-requests:100}\") int maxRequests) {\n        this.maxConcurrentRequests = maxRequests;\n        this.semaphore = new Semaphore(maxRequests);\n    }\n    \n    @Override\n    protected void doFilterInternal(\n            HttpServletRequest request,\n            HttpServletResponse response,\n            FilterChain filterChain) throws ServletException, IOException {\n        \n        boolean acquired = false;\n        try {\n            acquired = semaphore.tryAcquire(100, TimeUnit.MILLISECONDS);\n            \n            if (!acquired) {\n                response.setStatus(HttpStatus.SERVICE_UNAVAILABLE.value());\n                response.getWriter().write(\n                    \"{\\\"error\\\":\\\"Too many concurrent requests\\\"}\"  );\n                return;\n            }\n            \n            filterChain.doFilter(request, response);\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            response.setStatus(HttpStatus.SERVICE_UNAVAILABLE.value());\n        } finally {\n            if (acquired) {\n                semaphore.release();\n            }\n        }\n    }\n}\n\n// Reactive Backpressure with WebFlux\n@RestController\npublic class ReactiveController {\n    \n    @Autowired\n    private DataService dataService;\n    \n    @GetMapping(value = \"/stream\", produces = MediaType.TEXT_EVENT_STREAM_VALUE)\n    public Flux<Data> streamData() {\n        return dataService.getData()\n            // Backpressure: Buffer up to 100, drop oldest\n            .onBackpressureBuffer(100, BufferOverflowStrategy.DROP_OLDEST)\n            // Or drop latest\n            //.onBackpressureDrop()\n            // Or emit error\n            //.onBackpressureError()\n            .delayElements(Duration.ofMillis(100));\n    }\n}\n\n// Adaptive Throttling\n@Service\npublic class AdaptiveThrottlingService {\n    \n    private final AtomicInteger activeRequests = new AtomicInteger(0);\n    private final AtomicInteger rejectedRequests = new AtomicInteger(0);\n    \n    @Autowired\n    private MeterRegistry meterRegistry;\n    \n    private volatile int currentLimit = 100;\n    \n    public boolean allowRequest() {\n        int active = activeRequests.get();\n        \n        // Adaptive limit based on system health\n        double cpuUsage = getSystemCpuUsage();\n        double memoryUsage = getMemoryUsage();\n        \n        if (cpuUsage > 0.8 || memoryUsage > 0.8) {\n            currentLimit = Math.max(50, currentLimit - 10);\n        } else if (cpuUsage < 0.5 && memoryUsage < 0.5) {\n            currentLimit = Math.min(200, currentLimit + 10);\n        }\n        \n        if (active >= currentLimit) {\n            rejectedRequests.incrementAndGet();\n            meterRegistry.counter(\"requests.rejected\").increment();\n            return false;\n        }\n        \n        activeRequests.incrementAndGet();\n        return true;\n    }\n    \n    public void releaseRequest() {\n        activeRequests.decrementAndGet();\n    }\n}\n\n// Queue-based Throttling\n@Service\npublic class QueueBasedThrottling {\n    \n    private final BlockingQueue<Runnable> requestQueue;\n    private final ExecutorService executorService;\n    \n    public QueueBasedThrottling() {\n        this.requestQueue = new LinkedBlockingQueue<>(1000);\n        this.executorService = new ThreadPoolExecutor(\n            10,  // core threads\n            50,  // max threads\n            60, TimeUnit.SECONDS,\n            requestQueue,\n            new ThreadPoolExecutor.CallerRunsPolicy()  // Backpressure\n        );\n    }\n    \n    public CompletableFuture<Response> submitRequest(Request request) {\n        if (requestQueue.remainingCapacity() < 100) {\n            // Queue almost full, reject\n            throw new ServiceUnavailableException(\n                \"Service overloaded, try again later\"\n            );\n        }\n        \n        return CompletableFuture.supplyAsync(\n            () -> processRequest(request),\n            executorService\n        );\n    }\n}\n\n// Message Queue Consumer Backpressure\n@Service\npublic class ThrottledMessageConsumer {\n    \n    @RabbitListener(\n        queues = \"order.queue\",\n        concurrency = \"5-10\",\n        containerFactory = \"throttledListenerFactory\"\n    )\n    public void handleOrder(Order order) {\n        processOrder(order);\n    }\n}\n\n@Configuration\npublic class RabbitMQConfig {\n    \n    @Bean\n    public SimpleRabbitListenerContainerFactory throttledListenerFactory(\n            ConnectionFactory connectionFactory) {\n        \n        SimpleRabbitListenerContainerFactory factory = \n            new SimpleRabbitListenerContainerFactory();\n        factory.setConnectionFactory(connectionFactory);\n        \n        // Throttling settings\n        factory.setPrefetchCount(10);  // Limit unacked messages\n        factory.setConcurrentConsumers(5);\n        factory.setMaxConcurrentConsumers(10);\n        \n        return factory;\n    }\n}"
    },
    {
      "id": 100,
      "question": "What are the key considerations for microservice monitoring and alerting?",
      "answer": "Monitoring and alerting provide visibility into microservices health, performance, and business metrics for proactive issue detection.\n\nMonitoring Layers:\n• Infrastructure: CPU, memory, disk, network\n• Application: Request rates, latency, errors\n• Business: Transactions, revenue, user actions\n• Dependencies: Database, cache, external services\n\nKey Metrics (RED Method):\n• Rate: Requests per second\n• Errors: Error rate percentage\n• Duration: Response time (p50, p95, p99)\n\nKey Metrics (USE Method):\n• Utilization: Resource usage percentage\n• Saturation: Queue depths, wait times\n• Errors: Error counts and rates\n\nAlertingguidelines:\n• Alert on symptoms, not causes\n• Reduce alert fatigue\n• Clear actionable alerts\n• Appropriate severity levels\n• Include runbook links\n• Escalation policies\n\nService Level Objectives:\n• Define SLIs (Service Level Indicators)\n• Set SLOs (Service Level Objectives)\n• Calculate error budgets\n• Alert on SLO violations\n\nBest Practices:\n• Monitor from user perspective\n• Use dashboards effectively\n• Implement distributed tracing\n• Log aggregation\n• Anomaly detection\n• Capacity planning\n• Regular review and tuning\n• Document monitoring strategy",
      "explanation": "Microservices monitoring tracks RED metrics (Rate, Errors, Duration) and USE metrics using tools like Prometheus and Grafana, with alerts based on SLO violations and symptoms rather than causes to reduce alert fatigue.",
      "difficulty": "Medium",
      "code": "// Prometheus Metrics Instrumentation\n@Service\npublic class OrderService {\n    \n    private final Counter orderCounter;\n    private final Timer orderTimer;\n    private final Gauge activeOrders;\n    private final DistributionSummary orderValue;\n    \n    public OrderService(MeterRegistry registry) {\n        // Rate metric\n        this.orderCounter = Counter.builder(\"orders.created\")\n            .description(\"Total orders created\")\n            .tags(\"service\", \"order-service\")\n            .register(registry);\n        \n        // Duration metric\n        this.orderTimer = Timer.builder(\"orders.processing.time\")\n            .description(\"Order processing duration\")\n            .publishPercentiles(0.5, 0.95, 0.99)\n            .register(registry);\n        \n        // Utilization metric\n        this.activeOrders = Gauge.builder(\"orders.active\", \n            this, service -> getActiveOrderCount())\n            .description(\"Currently active orders\")\n            .register(registry);\n        \n        // Business metric\n        this.orderValue = DistributionSummary.builder(\"orders.value\")\n            .description(\"Order value distribution\")\n            .baseUnit(\"dollars\")\n            .register(registry);\n    }\n    \n    public Order createOrder(OrderRequest request) {\n        return orderTimer.recordCallable(() -> {\n            try {\n                Order order = processOrder(request);\n                \n                // Record metrics\n                orderCounter.increment();\n                orderValue.record(order.getTotal().doubleValue());\n                \n                return order;\n            } catch (Exception e) {\n                Counter.builder(\"orders.errors\")\n                    .tag(\"error.type\", e.getClass().getSimpleName())\n                    .register(registry)\n                    .increment();\n                throw e;\n            }\n        });\n    }\n}\n\n// SLO-based Alerting\n// prometheus-rules.yml\ngroups:\n  - name: slo-alerts\n    interval: 30s\n    rules:\n      # Error rate SLO: < 1% errors\n      - alert: HighErrorRate\n        expr: |\n          sum(rate(http_requests_total{status=~\"5..\"}[5m]))\n          /\n          sum(rate(http_requests_total[5m]))\n          > 0.01\n        for: 5m\n        labels:\n          severity: critical\n          slo: error-rate\n        annotations:\n          summary: \"Error rate exceeds SLO (>1%)\"\n          description: \"Current error rate: {{ $value | humanizePercentage }}\"\n          runbook: \"https://wiki.example.com/runbooks/high-error-rate\"\n      \n      # Latency SLO: p95 < 500ms\n      - alert: HighLatency\n        expr: |\n          histogram_quantile(0.95,\n            sum(rate(http_request_duration_seconds_bucket[5m])) by (le)\n          ) > 0.5\n        for: 10m\n        labels:\n          severity: warning\n          slo: latency\n        annotations:\n          summary: \"P95 latency exceeds SLO (>500ms)\"\n          description: \"Current P95: {{ $value }}s\"\n      \n      # Availability SLO: > 99.9%\n      - alert: LowAvailability\n        expr: |\n          (sum(rate(http_requests_total{status!~\"5..\"}[1h]))\n          /\n          sum(rate(http_requests_total[1h])))\n          < 0.999\n        for: 5m\n        labels:\n          severity: critical\n          slo: availability\n        annotations:\n          summary: \"Service availability below SLO (<99.9%)\"\n\n// Custom Health Indicator\n@Component\npublic class BusinessHealthIndicator implements HealthIndicator {\n    \n    @Autowired\n    private OrderRepository orderRepository;\n    @Autowired\n    private MetricsService metricsService;\n    \n    @Override\n    public Health health() {\n        Map<String, Object> details = new HashMap<>();\n        \n        // Check business metrics\n        long recentOrders = orderRepository\n            .countByCreatedAtAfter(LocalDateTime.now().minusMinutes(5));\n        details.put(\"recentOrders\", recentOrders);\n        \n        double errorRate = metricsService.getErrorRate();\n        details.put(\"errorRate\", errorRate);\n        \n        double avgLatency = metricsService.getAverageLatency();\n        details.put(\"avgLatency\", avgLatency);\n        \n        // Determine health based on SLOs\n        if (errorRate > 0.05) {\n            return Health.down()\n                .withDetails(details)\n                .withDetail(\"reason\", \"Error rate exceeds threshold\")\n                .build();\n        }\n        \n        if (avgLatency > 1000) {\n            return Health.degraded()\n                .withDetails(details)\n                .withDetail(\"reason\", \"High latency detected\")\n                .build();\n        }\n        \n        return Health.up().withDetails(details).build();\n    }\n}\n\n// Grafana Dashboard JSON (excerpt)\n{\n  \"dashboard\": {\n    \"title\": \"Microservices Overview\",\n    \"panels\": [\n      {\n        \"title\": \"Request Rate\",\n        \"targets\": [{\n          \"expr\": \"sum(rate(http_requests_total[5m])) by (service)\"\n        }]\n      },\n      {\n        \"title\": \"Error Rate\",\n        \"targets\": [{\n          \"expr\": \"sum(rate(http_requests_total{status=~\\\"5..\\\"}[5m])) / sum(rate(http_requests_total[5m]))\"\n        }]\n      },\n      {\n        \"title\": \"Response Time (P95)\",\n        \"targets\": [{\n          \"expr\": \"histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))\"\n        }]\n      }\n    ]\n  }\n}"
    }
  ]
}
