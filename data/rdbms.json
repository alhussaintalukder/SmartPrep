{
  "topic": "RDBMS",
  "questions": [
    {
      "id": 1,
      "question": "What is RDBMS and how does it differ from DBMS?",
      "answer": "RDBMS (Relational Database Management System):\n• Stores data in tables (rows and columns)\n• Relationships between tables via keys\n• Follows ACID properties\n• Uses SQL for queries\n• Examples: MySQL, PostgreSQL, Oracle\n\nDBMS (Database Management System):\n• Generic term for any database system\n• Can be hierarchical, network, or relational\n• May not enforce relationships\n• May not support ACID\n\nKey difference: RDBMS is a type of DBMS that specifically uses relational model.",
      "explanation": "RDBMS is a specialized DBMS that organizes data into related tables with enforced relationships and ACID compliance.",
      "difficulty": "Easy"
    },
    {
      "id": 2,
      "question": "What are ACID properties in databases?",
      "answer": "ACID ensures reliable transaction processing:\n\nA - Atomicity:\n• All or nothing execution\n• Transaction fully completes or fully rolls back\n• No partial updates\n\nC - Consistency:\n• Database remains in valid state\n• All constraints satisfied\n• Data integrity maintained\n\nI - Isolation:\n• Concurrent transactions don't interfere\n• Each transaction independent\n• Prevents dirty reads, lost updates\n\nD - Durability:\n• Committed changes permanent\n• Survives system failures\n• Written to persistent storage",
      "explanation": "ACID properties guarantee that database transactions are processed reliably, ensuring data integrity even in case of errors or failures.",
      "difficulty": "Medium",
      "code": "-- Atomicity example\nBEGIN TRANSACTION;\n  UPDATE accounts SET balance = balance - 100 WHERE id = 1;\n  UPDATE accounts SET balance = balance + 100 WHERE id = 2;\n  -- Both succeed or both rollback\nCOMMIT;\n-- If any fails, ROLLBACK happens automatically\n\n-- Consistency example\nALTER TABLE orders ADD CONSTRAINT check_total CHECK (total >= 0);\n-- Cannot insert negative totals\n\n-- Isolation example (prevents dirty reads)\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN TRANSACTION;\n  SELECT balance FROM accounts WHERE id = 1;\n  -- Other transactions cannot see uncommitted changes\nCOMMIT;\n\n-- Durability example\nCOMMIT; -- Changes written to disk immediately\n-- Even if system crashes, data persists"
    },
    {
      "id": 3,
      "question": "What are the different types of keys in RDBMS?",
      "answer": "Primary Key:\n• Uniquely identifies each record\n• Cannot be NULL\n• Only one per table\n• Automatically indexed\n\nForeign Key:\n• Links two tables\n• References primary key of another table\n• Enforces referential integrity\n• Can be NULL\n\nCandidate Key:\n• Potential primary keys\n• All satisfy uniqueness + non-null\n\nComposite Key:\n• Combination of multiple columns\n• Together form unique identifier\n\nUnique Key:\n• Ensures column uniqueness\n• Can have NULL (once)\n• Multiple unique keys allowed\n\nSuper Key:\n• Set of attributes that uniquely identify\n• Can have redundant attributes",
      "explanation": "Keys ensure data integrity, establish relationships, and enable efficient data retrieval in relational databases.",
      "difficulty": "Medium",
      "code": "-- Primary Key\nCREATE TABLE users (\n    id INT PRIMARY KEY AUTO_INCREMENT,\n    email VARCHAR(255) NOT NULL,\n    username VARCHAR(50) NOT NULL\n);\n\n-- Foreign Key\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    user_id INT,\n    total DECIMAL(10,2),\n    FOREIGN KEY (user_id) REFERENCES users(id)\n        ON DELETE CASCADE\n        ON UPDATE CASCADE\n);\n\n-- Composite Key\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    PRIMARY KEY (order_id, product_id),\n    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n);\n\n-- Unique Key\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    email VARCHAR(255) UNIQUE,\n    ssn VARCHAR(11) UNIQUE,\n    name VARCHAR(100)\n);\n\n-- Candidate Keys example\n-- In users table: id, email, username all could be primary keys\n-- Chosen primary key: id\n-- Other candidate keys can be UNIQUE"
    },
    {
      "id": 4,
      "question": "What is normalization and what are its normal forms?",
      "answer": "Normalization eliminates redundancy and anomalies:\n\n1NF (First Normal Form):\n• Atomic values (no repeating groups)\n• Each column single value\n• Unique column names\n\n2NF (Second Normal Form):\n• Must be in 1NF\n• No partial dependencies\n• All non-key attributes depend on entire primary key\n\n3NF (Third Normal Form):\n• Must be in 2NF\n• No transitive dependencies\n• Non-key attributes depend only on primary key\n\nBCNF (Boyce-Codd Normal Form):\n• Stricter version of 3NF\n• Every determinant is a candidate key\n\n4NF & 5NF:\n• Handle multi-valued and join dependencies\n• Rarely used in practice",
      "explanation": "Normalization is the process of organizing data to minimize redundancy and dependency, improving data integrity and query performance.",
      "difficulty": "Hard",
      "code": "-- Unnormalized table\nCREATE TABLE orders_bad (\n    order_id INT,\n    customer_name VARCHAR(100),\n    customer_email VARCHAR(255),\n    product_names TEXT, -- Violates 1NF (multiple values)\n    quantities TEXT\n);\n\n-- 1NF: Atomic values\nCREATE TABLE orders_1nf (\n    order_id INT,\n    customer_name VARCHAR(100),\n    customer_email VARCHAR(255),\n    product_name VARCHAR(100),\n    quantity INT\n);\n\n-- 2NF: Remove partial dependencies\nCREATE TABLE customers (\n    customer_id INT PRIMARY KEY,\n    customer_name VARCHAR(100),\n    customer_email VARCHAR(255)\n);\n\nCREATE TABLE orders_2nf (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date DATE,\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    PRIMARY KEY (order_id, product_id)\n);\n\n-- 3NF: Remove transitive dependencies\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(100),\n    category_id INT,\n    price DECIMAL(10,2)\n);\n\nCREATE TABLE categories (\n    category_id INT PRIMARY KEY,\n    category_name VARCHAR(50)\n);\n-- Now category_name depends only on category_id, not transitively on product_id\n\n-- Benefits:\n-- ✅ No data redundancy\n-- ✅ Easy to update\n-- ✅ No anomalies\n-- ✅ Data integrity maintained"
    },
    {
      "id": 5,
      "question": "What are the different types of SQL joins?",
      "answer": "INNER JOIN:\n• Returns matching rows from both tables\n• Most common join\n\nLEFT JOIN (LEFT OUTER JOIN):\n• All rows from left table\n• Matching rows from right table\n• NULL if no match\n\nRIGHT JOIN (RIGHT OUTER JOIN):\n• All rows from right table\n• Matching rows from left table\n• NULL if no match\n\nFULL JOIN (FULL OUTER JOIN):\n• All rows from both tables\n• NULL where no match\n\nCROSS JOIN:\n• Cartesian product\n• Every row from first table with every row from second\n\nSELF JOIN:\n• Table joined with itself\n• Uses aliases",
      "explanation": "Joins combine rows from two or more tables based on related columns, enabling complex queries across normalized data.",
      "difficulty": "Medium",
      "code": "-- Sample tables\nCREATE TABLE employees (id INT, name VARCHAR(100), dept_id INT);\nCREATE TABLE departments (id INT, dept_name VARCHAR(100));\n\nINSERT INTO employees VALUES (1, 'John', 1), (2, 'Jane', 2), (3, 'Bob', NULL);\nINSERT INTO departments VALUES (1, 'IT'), (2, 'HR'), (3, 'Finance');\n\n-- INNER JOIN (matching records only)\nSELECT e.name, d.dept_name\nFROM employees e\nINNER JOIN departments d ON e.dept_id = d.id;\n-- Result: John-IT, Jane-HR (Bob excluded)\n\n-- LEFT JOIN (all employees, even without department)\nSELECT e.name, d.dept_name\nFROM employees e\nLEFT JOIN departments d ON e.dept_id = d.id;\n-- Result: John-IT, Jane-HR, Bob-NULL\n\n-- RIGHT JOIN (all departments, even without employees)\nSELECT e.name, d.dept_name\nFROM employees e\nRIGHT JOIN departments d ON e.dept_id = d.id;\n-- Result: John-IT, Jane-HR, NULL-Finance\n\n-- FULL JOIN (all records from both)\nSELECT e.name, d.dept_name\nFROM employees e\nFULL OUTER JOIN departments d ON e.dept_id = d.id;\n-- Result: John-IT, Jane-HR, Bob-NULL, NULL-Finance\n\n-- CROSS JOIN (Cartesian product)\nSELECT e.name, d.dept_name\nFROM employees e\nCROSS JOIN departments d;\n-- Result: 3 employees × 3 departments = 9 rows\n\n-- SELF JOIN (employees and their managers)\nCREATE TABLE employees2 (id INT, name VARCHAR(100), manager_id INT);\nSELECT e1.name AS employee, e2.name AS manager\nFROM employees2 e1\nLEFT JOIN employees2 e2 ON e1.manager_id = e2.id;"
    },
    {
      "id": 6,
      "question": "What is an index and what are its types?",
      "answer": "Index speeds up data retrieval:\n\nClustered Index:\n• Determines physical order of data\n• Only one per table\n• Created automatically on primary key\n• Faster for range queries\n\nNon-Clustered Index:\n• Separate structure from data\n• Multiple allowed per table\n• Contains pointer to actual data\n• Good for lookups\n\nUnique Index:\n• Ensures column uniqueness\n• Can be clustered or non-clustered\n\nComposite Index:\n• Index on multiple columns\n• Order matters\n\nFull-Text Index:\n• For text searching\n• Supports LIKE, CONTAINS\n\nTrade-offs:\n• Faster SELECT\n• Slower INSERT/UPDATE/DELETE",
      "explanation": "Indexes are database structures that improve query performance by creating fast access paths to data.",
      "difficulty": "Medium",
      "code": "-- Create non-clustered index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Create unique index\nCREATE UNIQUE INDEX idx_users_username ON users(username);\n\n-- Create composite index\nCREATE INDEX idx_orders_user_date ON orders(user_id, order_date);\n-- Efficient for: WHERE user_id = ? AND order_date = ?\n-- Also works for: WHERE user_id = ?\n-- NOT efficient for: WHERE order_date = ? (order matters!)\n\n-- Create full-text index\nCREATE FULLTEXT INDEX idx_posts_content ON posts(title, content);\n\n-- View indexes\nSHOW INDEX FROM users;\n\n-- Drop index\nDROP INDEX idx_users_email ON users;\n\n-- Analyze index usage\nEXPLAIN SELECT * FROM users WHERE email = 'john@example.com';\n-- Shows if index is used\n\n-- Index best practices:\n-- ✅ Index foreign keys\n-- ✅ Index columns in WHERE, JOIN, ORDER BY\n-- ✅ Use composite indexes for common query patterns\n-- ❌ Don't over-index (slows writes)\n-- ❌ Don't index small tables\n-- ❌ Don't index columns with low cardinality"
    },
    {
      "id": 7,
      "question": "What are database transactions and isolation levels?",
      "answer": "Transaction: Group of operations executed as single unit\n\nIsolation Levels:\n\nREAD UNCOMMITTED (Level 0):\n• Transactions can read uncommitted changes\n• Dirty reads possible\n• Fastest, least safe\n\nREAD COMMITTED (Level 1):\n• Only committed data visible\n• Prevents dirty reads\n• Non-repeatable reads possible\n• Default in most databases\n\nREPEATABLE READ (Level 2):\n• Same data returned for same query\n• Prevents non-repeatable reads\n• Phantom reads possible\n• MySQL default\n\nSERIALIZABLE (Level 3):\n• Complete isolation\n• No dirty, non-repeatable, phantom reads\n• Slowest, safest\n\nProblems:\n• Dirty read: Reading uncommitted data\n• Non-repeatable read: Data changes between reads\n• Phantom read: New rows appear",
      "explanation": "Isolation levels control the degree to which transactions are isolated from each other, balancing consistency with performance.",
      "difficulty": "Hard",
      "code": "-- Start transaction\nBEGIN TRANSACTION;\n-- or\nSTART TRANSACTION;\n\n-- Perform operations\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n\n-- Commit (save changes)\nCOMMIT;\n\n-- Rollback (undo changes)\nROLLBACK;\n\n-- Set isolation level\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\n\n-- Example: Dirty read problem (READ UNCOMMITTED)\n-- Transaction 1:\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nBEGIN;\nSELECT balance FROM accounts WHERE id = 1; -- Reads 500\n-- Transaction 2 updates but doesn't commit\n-- Transaction 1 reads again: sees 400 (uncommitted!)\n-- Transaction 2 rolls back\nCOMMIT;\n-- Wrong data was read!\n\n-- Solution: READ COMMITTED\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN;\nSELECT balance FROM accounts WHERE id = 1; -- Reads 500\n-- Transaction 2 updates but doesn't commit\n-- Transaction 1 reads again: still sees 500 (committed value)\nCOMMIT;\n\n-- Savepoints for partial rollback\nBEGIN;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nSAVEPOINT sp1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n-- Error occurred, rollback to savepoint\nROLLBACK TO SAVEPOINT sp1;\n-- First update preserved, second undone\nCOMMIT;"
    },
    {
      "id": 8,
      "question": "What are stored procedures and their advantages?",
      "answer": "Stored Procedure: Precompiled SQL code stored in database\n\nAdvantages:\n• Performance: Compiled once, executed many times\n• Network traffic: Single call vs multiple queries\n• Security: Grant execute permission, not direct table access\n• Reusability: Called from multiple applications\n• Maintainability: Update logic in one place\n• Business logic: Encapsulate complex operations\n\nDisadvantages:\n• Debugging difficulty\n• Version control challenges\n• Database vendor lock-in\n• Limited testing tools\n• Can become complex\n\nBest practices:\n• Use for complex operations\n• Keep logic simple\n• Document parameters\n• Use transactions\n• Handle errors properly",
      "explanation": "Stored procedures are reusable, precompiled SQL routines that execute on the database server, improving performance and security.",
      "difficulty": "Medium",
      "code": "-- Create stored procedure\nDELIMITER //\nCREATE PROCEDURE TransferMoney(\n    IN from_account INT,\n    IN to_account INT,\n    IN amount DECIMAL(10,2),\n    OUT status VARCHAR(50)\n)\nBEGIN\n    DECLARE current_balance DECIMAL(10,2);\n    DECLARE EXIT HANDLER FOR SQLEXCEPTION\n    BEGIN\n        ROLLBACK;\n        SET status = 'ERROR: Transaction failed';\n    END;\n    \n    START TRANSACTION;\n    \n    -- Check balance\n    SELECT balance INTO current_balance\n    FROM accounts\n    WHERE account_id = from_account\n    FOR UPDATE;\n    \n    IF current_balance < amount THEN\n        SET status = 'ERROR: Insufficient funds';\n        ROLLBACK;\n    ELSE\n        -- Deduct from source\n        UPDATE accounts\n        SET balance = balance - amount\n        WHERE account_id = from_account;\n        \n        -- Add to destination\n        UPDATE accounts\n        SET balance = balance + amount\n        WHERE account_id = to_account;\n        \n        COMMIT;\n        SET status = 'SUCCESS';\n    END IF;\nEND //\nDELIMITER ;\n\n-- Call stored procedure\nCALL TransferMoney(1, 2, 100.00, @result);\nSELECT @result;\n\n-- Stored procedure with cursor\nDELIMITER //\nCREATE PROCEDURE ProcessOrders()\nBEGIN\n    DECLARE done INT DEFAULT FALSE;\n    DECLARE order_id INT;\n    DECLARE order_total DECIMAL(10,2);\n    \n    DECLARE order_cursor CURSOR FOR\n        SELECT id, total FROM orders WHERE status = 'PENDING';\n    \n    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;\n    \n    OPEN order_cursor;\n    \n    read_loop: LOOP\n        FETCH order_cursor INTO order_id, order_total;\n        IF done THEN\n            LEAVE read_loop;\n        END IF;\n        \n        -- Process order\n        UPDATE orders SET status = 'PROCESSED' WHERE id = order_id;\n    END LOOP;\n    \n    CLOSE order_cursor;\nEND //\nDELIMITER ;\n\n-- Drop stored procedure\nDROP PROCEDURE IF EXISTS TransferMoney;"
    },
    {
      "id": 9,
      "question": "What are views and materialized views?",
      "answer": "View:\n• Virtual table based on SELECT query\n• No data storage (query executed each time)\n• Always shows current data\n• Can simplify complex queries\n• Can restrict data access\n\nMaterialized View:\n• Physical copy of query result\n• Stores data on disk\n• Must be refreshed to update\n• Faster queries (no computation)\n• Uses storage space\n\nUse cases:\n• Security: Hide sensitive columns\n• Simplification: Abstract complexity\n• Backward compatibility: Maintain old schema\n• Performance: Precompute aggregations (materialized)\n\nUpdatable views:\n• Simple SELECT\n• No joins, GROUP BY, DISTINCT\n• Direct column mapping",
      "explanation": "Views provide logical abstraction over tables, while materialized views cache query results for performance.",
      "difficulty": "Medium",
      "code": "-- Create simple view\nCREATE VIEW active_users AS\nSELECT id, username, email, created_at\nFROM users\nWHERE status = 'ACTIVE';\n\n-- Use view\nSELECT * FROM active_users;\n-- Equivalent to:\n-- SELECT id, username, email, created_at FROM users WHERE status = 'ACTIVE';\n\n-- Create complex view with joins\nCREATE VIEW order_summary AS\nSELECT \n    o.order_id,\n    u.username,\n    u.email,\n    o.order_date,\n    SUM(oi.quantity * oi.price) AS total\nFROM orders o\nJOIN users u ON o.user_id = u.id\nJOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id, u.username, u.email, o.order_date;\n\n-- Updatable view example\nCREATE VIEW employee_basic AS\nSELECT id, name, email, department\nFROM employees;\n\n-- Can UPDATE through view\nUPDATE employee_basic SET department = 'IT' WHERE id = 1;\n\n-- Create view with check option\nCREATE VIEW high_salary_employees AS\nSELECT * FROM employees WHERE salary > 50000\nWITH CHECK OPTION;\n\n-- This will fail (violates WHERE condition)\nINSERT INTO high_salary_employees (name, salary)\nVALUES ('John', 40000);\n\n-- Materialized view (PostgreSQL syntax)\nCREATE MATERIALIZED VIEW sales_summary AS\nSELECT \n    product_id,\n    DATE_TRUNC('month', order_date) AS month,\n    SUM(quantity) AS total_quantity,\n    SUM(total) AS total_revenue\nFROM orders\nGROUP BY product_id, DATE_TRUNC('month', order_date);\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW sales_summary;\n\n-- Query materialized view (fast!)\nSELECT * FROM sales_summary WHERE month = '2024-01-01';\n\n-- Drop view\nDROP VIEW IF EXISTS active_users;"
    },
    {
      "id": 10,
      "question": "What are triggers and when should they be used?",
      "answer": "Trigger: Automated action executed in response to database events\n\nTypes:\n• BEFORE trigger: Executes before event\n• AFTER trigger: Executes after event\n• INSTEAD OF trigger: Replaces event\n\nEvents:\n• INSERT\n• UPDATE\n• DELETE\n\nUse cases:\n• Audit logging\n• Data validation\n• Derived column updates\n• Enforcing business rules\n• Maintaining summary tables\n\nBest practices:\n• Keep simple and fast\n• Avoid complex logic\n• Document thoroughly\n• Be careful with cascading triggers\n• Consider alternative solutions\n\nCautions:\n• Hidden logic (hard to debug)\n• Performance impact\n• Can cause unexpected behavior\n• Difficult to test",
      "explanation": "Triggers automatically execute defined actions when specific database events occur, useful for maintaining data integrity and audit trails.",
      "difficulty": "Medium",
      "code": "-- BEFORE INSERT trigger (validation)\nDELIMITER //\nCREATE TRIGGER before_user_insert\nBEFORE INSERT ON users\nFOR EACH ROW\nBEGIN\n    -- Validate email format\n    IF NEW.email NOT LIKE '%@%' THEN\n        SIGNAL SQLSTATE '45000'\n        SET MESSAGE_TEXT = 'Invalid email format';\n    END IF;\n    \n    -- Set created_at\n    IF NEW.created_at IS NULL THEN\n        SET NEW.created_at = NOW();\n    END IF;\n    \n    -- Convert username to lowercase\n    SET NEW.username = LOWER(NEW.username);\nEND;//\nDELIMITER ;\n\n-- AFTER INSERT trigger (audit log)\nDELIMITER //\nCREATE TRIGGER after_user_insert\nAFTER INSERT ON users\nFOR EACH ROW\nBEGIN\n    INSERT INTO audit_log (table_name, action, record_id, timestamp)\n    VALUES ('users', 'INSERT', NEW.id, NOW());\nEND;//\nDELIMITER ;\n\n-- BEFORE UPDATE trigger\nDELIMITER //\nCREATE TRIGGER before_user_update\nBEFORE UPDATE ON users\nFOR EACH ROW\nBEGIN\n    -- Prevent changing username\n    IF NEW.username != OLD.username THEN\n        SIGNAL SQLSTATE '45000'\n        SET MESSAGE_TEXT = 'Username cannot be changed';\n    END IF;\n    \n    -- Update modified timestamp\n    SET NEW.updated_at = NOW();\nEND;//\nDELIMITER ;\n\n-- AFTER DELETE trigger (soft delete)\nDELIMITER //\nCREATE TRIGGER after_user_delete\nAFTER DELETE ON users\nFOR EACH ROW\nBEGIN\n    -- Archive deleted user\n    INSERT INTO deleted_users\n    SELECT *, NOW() AS deleted_at\n    FROM (SELECT OLD.*) AS old_data;\nEND;//\nDELIMITER ;\n\n-- Maintain summary table\nDELIMITER //\nCREATE TRIGGER update_order_count\nAFTER INSERT ON orders\nFOR EACH ROW\nBEGIN\n    UPDATE user_statistics\n    SET order_count = order_count + 1,\n        total_spent = total_spent + NEW.total\n    WHERE user_id = NEW.user_id;\nEND;//\nDELIMITER ;\n\n-- View triggers\nSHOW TRIGGERS;\n\n-- Drop trigger\nDROP TRIGGER IF EXISTS before_user_insert;"
    },
    {
      "id": 11,
      "question": "What are constraints in SQL?",
      "answer": "Constraints enforce data integrity rules:\n\nPRIMARY KEY:\n• Unique + NOT NULL\n• One per table\n• Automatically indexed\n\nFOREIGN KEY:\n• References another table\n• Referential integrity\n• CASCADE options\n\nUNIQUE:\n• No duplicate values\n• NULL allowed (once)\n• Multiple per table\n\nNOT NULL:\n• Column must have value\n• Cannot insert NULL\n\nCHECK:\n• Custom validation rule\n• Boolean expression\n\nDEFAULT:\n• Automatic value if not specified\n\nBenefits:\n• Data integrity\n• Prevents invalid data\n• Self-documenting\n• Performance (indexes)",
      "explanation": "Constraints are rules enforced by the database to maintain data accuracy and integrity automatically.",
      "difficulty": "Easy",
      "code": "-- NOT NULL constraint\nCREATE TABLE users (\n    id INT PRIMARY KEY,\n    email VARCHAR(255) NOT NULL,\n    username VARCHAR(50) NOT NULL\n);\n\n-- UNIQUE constraint\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    email VARCHAR(255) UNIQUE,\n    ssn VARCHAR(11) UNIQUE NOT NULL\n);\n\n-- CHECK constraint\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    price DECIMAL(10,2) CHECK (price > 0),\n    quantity INT CHECK (quantity >= 0),\n    category VARCHAR(50) CHECK (category IN ('Electronics', 'Clothing', 'Food'))\n);\n\n-- DEFAULT constraint\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    status VARCHAR(20) DEFAULT 'PENDING',\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n    is_active BOOLEAN DEFAULT TRUE\n);\n\n-- FOREIGN KEY with CASCADE\nCREATE TABLE order_items (\n    id INT PRIMARY KEY,\n    order_id INT,\n    product_id INT,\n    quantity INT NOT NULL,\n    FOREIGN KEY (order_id) REFERENCES orders(id)\n        ON DELETE CASCADE\n        ON UPDATE CASCADE,\n    FOREIGN KEY (product_id) REFERENCES products(id)\n        ON DELETE RESTRICT\n        ON UPDATE RESTRICT\n);\n\n-- Add constraint to existing table\nALTER TABLE users\nADD CONSTRAINT chk_age CHECK (age >= 18);\n\n-- Drop constraint\nALTER TABLE users\nDROP CONSTRAINT chk_age;\n\n-- Named constraints for better error messages\nCREATE TABLE accounts (\n    id INT PRIMARY KEY,\n    balance DECIMAL(10,2),\n    CONSTRAINT chk_positive_balance CHECK (balance >= 0),\n    CONSTRAINT chk_reasonable_balance CHECK (balance <= 1000000)\n);"
    },
    {
      "id": 12,
      "question": "What is denormalization and when should it be used?",
      "answer": "Denormalization: Intentionally introducing redundancy\n\nReasons:\n• Improve query performance\n• Reduce complex joins\n• Optimize read-heavy workloads\n• Simplify queries\n\nTechniques:\n• Store computed values\n• Duplicate data across tables\n• Add redundant columns\n• Pre-join tables\n• Materialized views\n\nTrade-offs:\n✅ Faster reads\n✅ Simpler queries\n✅ Better performance\n❌ Data redundancy\n❌ Harder to maintain\n❌ Slower writes\n❌ Risk of inconsistency\n\nWhen to use:\n• Read >> Write operations\n• Complex joins hurt performance\n• Real-time reporting\n• Analytics/OLAP systems\n\nWhen to avoid:\n• Write-heavy applications\n• Strong consistency required\n• Storage constraints",
      "explanation": "Denormalization sacrifices normalization principles to improve query performance, introducing controlled redundancy.",
      "difficulty": "Hard",
      "code": "-- Normalized design\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT,\n    order_date DATE\n);\n\nCREATE TABLE order_items (\n    id INT PRIMARY KEY,\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    price DECIMAL(10,2)\n);\n\n-- Query requires JOIN and SUM\nSELECT \n    o.id,\n    o.order_date,\n    SUM(oi.quantity * oi.price) AS total\nFROM orders o\nJOIN order_items oi ON o.id = oi.order_id\nGROUP BY o.id, o.order_date;\n\n-- Denormalized: Add total column\nCREATE TABLE orders_denorm (\n    id INT PRIMARY KEY,\n    user_id INT,\n    order_date DATE,\n    total DECIMAL(10,2),  -- Precomputed total\n    item_count INT        -- Precomputed count\n);\n\n-- Now simple query (no JOIN)\nSELECT id, order_date, total\nFROM orders_denorm;\n\n-- Maintain with trigger\nDELIMITER //\nCREATE TRIGGER update_order_total\nAFTER INSERT ON order_items\nFOR EACH ROW\nBEGIN\n    UPDATE orders_denorm\n    SET \n        total = (SELECT SUM(quantity * price) \n                 FROM order_items \n                 WHERE order_id = NEW.order_id),\n        item_count = (SELECT COUNT(*) \n                      FROM order_items \n                      WHERE order_id = NEW.order_id)\n    WHERE id = NEW.order_id;\nEND;//\nDELIMITER ;\n\n-- Example: User stats denormalization\nCREATE TABLE users_denorm (\n    id INT PRIMARY KEY,\n    username VARCHAR(50),\n    email VARCHAR(255),\n    -- Denormalized stats\n    total_orders INT DEFAULT 0,\n    total_spent DECIMAL(10,2) DEFAULT 0,\n    last_order_date DATE,\n    average_order_value DECIMAL(10,2)\n);\n\n-- Periodic update of denormalized data\nUPDATE users_denorm u\nSET \n    total_orders = (SELECT COUNT(*) FROM orders WHERE user_id = u.id),\n    total_spent = (SELECT COALESCE(SUM(total), 0) FROM orders WHERE user_id = u.id),\n    last_order_date = (SELECT MAX(order_date) FROM orders WHERE user_id = u.id),\n    average_order_value = (SELECT AVG(total) FROM orders WHERE user_id = u.id);"
    },
    {
      "id": 13,
      "question": "What is the difference between DELETE, TRUNCATE, and DROP?",
      "answer": "DELETE:\n• Removes specific rows\n• WHERE clause supported\n• Can be rolled back (with transaction)\n• Triggers fired\n• Slower (row-by-row)\n• Logs each deletion\n• Identity counter NOT reset\n\nTRUNCATE:\n• Removes all rows\n• No WHERE clause\n• Cannot be rolled back (in most DBs)\n• Triggers NOT fired\n• Fast (deallocates pages)\n• Minimal logging\n• Resets identity counter\n• Requires ALTER permission\n\nDROP:\n• Removes entire table/database\n• Structure + data deleted\n• Cannot be rolled back\n• Frees all resources\n• Requires DROP permission\n\nWhen to use:\n• DELETE: Remove specific records\n• TRUNCATE: Clear all data, keep structure\n• DROP: Remove table completely",
      "explanation": "DELETE removes rows, TRUNCATE empties a table, and DROP removes the table entirely - each with different performance and recovery characteristics.",
      "difficulty": "Medium",
      "code": "-- DELETE: Remove specific rows\nDELETE FROM users WHERE status = 'INACTIVE';\n-- Logs each row, fires triggers, can rollback\n\nBEGIN TRANSACTION;\nDELETE FROM users WHERE id = 1;\nROLLBACK;  -- Can undo\n\n-- DELETE all rows (slow for large tables)\nDELETE FROM temp_table;\n\n-- TRUNCATE: Remove all rows (fast)\nTRUNCATE TABLE temp_table;\n-- Fast, minimal logging, resets auto-increment\n\n-- Cannot use WHERE\n-- TRUNCATE TABLE users WHERE status = 'INACTIVE';  -- ERROR!\n\n-- Cannot rollback in most databases\nBEGIN TRANSACTION;\nTRUNCATE TABLE temp_table;\nROLLBACK;  -- Doesn't restore data (MySQL/SQL Server)\n\n-- DROP: Remove entire table\nDROP TABLE temp_table;\n-- Table no longer exists\n\n-- Performance comparison for 1 million rows:\n-- DELETE FROM table;           -- ~30 seconds\n-- TRUNCATE TABLE table;        -- <1 second\n-- DROP TABLE table;            -- <1 second\n\n-- Identity/Auto-increment behavior\nCREATE TABLE test (id INT AUTO_INCREMENT PRIMARY KEY, value VARCHAR(50));\nINSERT INTO test (value) VALUES ('A'), ('B'), ('C');\n-- ids: 1, 2, 3\n\n-- After DELETE\nDELETE FROM test;\nINSERT INTO test (value) VALUES ('D');\n-- id: 4 (continues sequence)\n\n-- After TRUNCATE\nTRUNCATE TABLE test;\nINSERT INTO test (value) VALUES ('E');\n-- id: 1 (resets sequence)\n\n-- Truncate with foreign key (must disable)\nSET FOREIGN_KEY_CHECKS = 0;\nTRUNCATE TABLE parent_table;\nSET FOREIGN_KEY_CHECKS = 1;\n\n-- Or cascade delete\nDELETE FROM parent_table;  -- Cascades to child tables"
    },
    {
      "id": 14,
      "question": "What are aggregate functions and window functions?",
      "answer": "Aggregate Functions:\n• Compute single value from multiple rows\n• Used with GROUP BY\n• Collapse rows\n\nCommon functions:\n• COUNT() - Count rows\n• SUM() - Total of values\n• AVG() - Average\n• MAX/MIN() - Maximum/minimum\n• GROUP_CONCAT() - Concatenate strings\n\nWindow Functions:\n• Perform calculation across row sets\n• Don't collapse rows\n• Access to other rows\n\nCommon functions:\n• ROW_NUMBER() - Sequential number\n• RANK() - Rank with gaps\n• DENSE_RANK() - Rank without gaps\n• LAG/LEAD() - Previous/next row\n• FIRST_VALUE/LAST_VALUE()\n\nKey difference:\n• Aggregate: Many rows → one row\n• Window: Calculation per row, access to set",
      "explanation": "Aggregate functions summarize data into single values, while window functions perform calculations across related rows without collapsing them.",
      "difficulty": "Hard",
      "code": "-- Aggregate Functions\nSELECT \n    COUNT(*) AS total_users,\n    COUNT(DISTINCT country) AS unique_countries,\n    AVG(age) AS average_age,\n    SUM(balance) AS total_balance,\n    MAX(created_at) AS latest_user,\n    MIN(created_at) AS oldest_user\nFROM users;\n\n-- With GROUP BY\nSELECT \n    department,\n    COUNT(*) AS employee_count,\n    AVG(salary) AS avg_salary,\n    MAX(salary) AS max_salary\nFROM employees\nGROUP BY department;\n\n-- HAVING (filter after aggregation)\nSELECT \n    department,\n    AVG(salary) AS avg_salary\nFROM employees\nGROUP BY department\nHAVING AVG(salary) > 50000;\n\n-- Window Functions\n-- ROW_NUMBER: Unique sequential number\nSELECT \n    name,\n    salary,\n    ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num\nFROM employees;\n\n-- RANK: Ranking with gaps for ties\nSELECT \n    name,\n    salary,\n    RANK() OVER (ORDER BY salary DESC) AS rank\nFROM employees;\n-- If two have same salary: 1, 2, 2, 4, 5...\n\n-- DENSE_RANK: No gaps\nSELECT \n    name,\n    salary,\n    DENSE_RANK() OVER (ORDER BY salary DESC) AS dense_rank\nFROM employees;\n-- If two have same salary: 1, 2, 2, 3, 4...\n\n-- PARTITION BY: Group window calculations\nSELECT \n    name,\n    department,\n    salary,\n    AVG(salary) OVER (PARTITION BY department) AS dept_avg_salary,\n    salary - AVG(salary) OVER (PARTITION BY department) AS diff_from_avg\nFROM employees;\n\n-- LAG/LEAD: Access previous/next rows\nSELECT \n    order_date,\n    total,\n    LAG(total, 1) OVER (ORDER BY order_date) AS previous_total,\n    LEAD(total, 1) OVER (ORDER BY order_date) AS next_total,\n    total - LAG(total, 1) OVER (ORDER BY order_date) AS change\nFROM orders;\n\n-- Running total\nSELECT \n    order_date,\n    total,\n    SUM(total) OVER (ORDER BY order_date) AS running_total\nFROM orders;\n\n-- Top N per group\nWITH RankedSales AS (\n    SELECT \n        product_id,\n        sale_date,\n        amount,\n        ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY amount DESC) AS rn\n    FROM sales\n)\nSELECT * FROM RankedSales WHERE rn <= 3;"
    },
    {
      "id": 15,
      "question": "What are CTEs (Common Table Expressions) and how are they useful?",
      "answer": "CTE: Temporary named result set for single query\n\nSyntax:\nWITH cte_name AS (\n    SELECT ...\n)\nSELECT * FROM cte_name;\n\nBenefits:\n• Improved readability\n• Recursive queries\n• Code reuse within query\n• Break complex query into steps\n• Alternative to subqueries/temp tables\n\nTypes:\n1. Simple CTE: Named subquery\n2. Recursive CTE: Self-referencing\n3. Multiple CTEs: Chain together\n\nvs Subqueries:\n• More readable\n• Can reference multiple times\n• Can be recursive\n\nvs Temp Tables:\n• No explicit create/drop\n• Query-scoped\n• Cannot index\n\nvs Views:\n• Query-scoped only\n• No permanent storage",
      "explanation": "CTEs provide a way to write more readable and maintainable SQL by breaking complex queries into named, reusable components.",
      "difficulty": "Medium",
      "code": "-- Simple CTE\nWITH high_value_customers AS (\n    SELECT \n        user_id,\n        SUM(total) AS total_spent\n    FROM orders\n    GROUP BY user_id\n    HAVING SUM(total) > 10000\n)\nSELECT \n    u.username,\n    u.email,\n    hvc.total_spent\nFROM users u\nJOIN high_value_customers hvc ON u.id = hvc.user_id;\n\n-- Multiple CTEs\nWITH \nmonthly_sales AS (\n    SELECT \n        DATE_TRUNC('month', order_date) AS month,\n        SUM(total) AS monthly_total\n    FROM orders\n    GROUP BY DATE_TRUNC('month', order_date)\n),\naverage_monthly AS (\n    SELECT AVG(monthly_total) AS avg_total\n    FROM monthly_sales\n)\nSELECT \n    ms.month,\n    ms.monthly_total,\n    am.avg_total,\n    ms.monthly_total - am.avg_total AS difference\nFROM monthly_sales ms\nCROSS JOIN average_monthly am;\n\n-- Recursive CTE: Hierarchical data\nWITH RECURSIVE employee_hierarchy AS (\n    -- Base case: Top-level employees\n    SELECT \n        id,\n        name,\n        manager_id,\n        1 AS level,\n        CAST(name AS VARCHAR(1000)) AS path\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case: Employees with managers\n    SELECT \n        e.id,\n        e.name,\n        e.manager_id,\n        eh.level + 1,\n        CONCAT(eh.path, ' -> ', e.name)\n    FROM employees e\n    JOIN employee_hierarchy eh ON e.manager_id = eh.id\n)\nSELECT * FROM employee_hierarchy ORDER BY level, name;\n\n-- Recursive CTE: Generate series\nWITH RECURSIVE numbers AS (\n    SELECT 1 AS n\n    UNION ALL\n    SELECT n + 1 FROM numbers WHERE n < 100\n)\nSELECT * FROM numbers;\n\n-- Find all descendants\nWITH RECURSIVE subordinates AS (\n    SELECT id, name, manager_id\n    FROM employees\n    WHERE id = 5  -- Starting employee\n    \n    UNION ALL\n    \n    SELECT e.id, e.name, e.manager_id\n    FROM employees e\n    JOIN subordinates s ON e.manager_id = s.id\n)\nSELECT * FROM subordinates;\n\n-- Complex example: Sales analysis\nWITH \nproduct_sales AS (\n    SELECT \n        p.id,\n        p.name,\n        SUM(oi.quantity) AS units_sold,\n        SUM(oi.quantity * oi.price) AS revenue\n    FROM products p\n    JOIN order_items oi ON p.id = oi.product_id\n    GROUP BY p.id, p.name\n),\nranked_products AS (\n    SELECT \n        *,\n        RANK() OVER (ORDER BY revenue DESC) AS revenue_rank\n    FROM product_sales\n)\nSELECT * FROM ranked_products WHERE revenue_rank <= 10;"
    },
    {
      "id": 16,
      "question": "What is query optimization and how does EXPLAIN work?",
      "answer": "Query optimization: Process of finding most efficient execution plan\n\nEXPLAIN:\n• Shows query execution plan\n• Reveals how database executes query\n• Identifies performance issues\n\nKey metrics:\n• Type: JOIN type (ALL, index, range, ref, eq_ref)\n• Possible_keys: Available indexes\n• Key: Index actually used\n• Rows: Estimated rows examined\n• Extra: Additional information\n\nOptimization techniques:\n• Add indexes on WHERE/JOIN columns\n• Avoid SELECT *\n• Use LIMIT when possible\n• Optimize JOIN order\n• Avoid functions on indexed columns\n• Use covering indexes\n• Partition large tables\n\nBad signs in EXPLAIN:\n• Type: ALL (full table scan)\n• High row count\n• Using filesort/temporary\n• NULL in key column",
      "explanation": "EXPLAIN analyzes query execution plans to identify bottlenecks and optimization opportunities in SQL queries.",
      "difficulty": "Hard",
      "code": "-- Basic EXPLAIN\nEXPLAIN SELECT * FROM users WHERE email = 'test@example.com';\n\n-- EXPLAIN output interpretation:\n-- +----+-------------+-------+------+---------------+------+---------+------+------+-------------+\n-- | id | select_type | table | type | possible_keys | key  | key_len | ref  | rows | Extra       |\n-- +----+-------------+-------+------+---------------+------+---------+------+------+-------------+\n-- |  1 | SIMPLE      | users | ALL  | NULL          | NULL | NULL    | NULL | 1000 | Using where |\n-- +----+-------------+-------+------+---------------+------+---------+------+------+-------------+\n-- Type=ALL means full table scan (BAD for large tables)\n\n-- Add index and check again\nCREATE INDEX idx_email ON users(email);\nEXPLAIN SELECT * FROM users WHERE email = 'test@example.com';\n-- Now type=ref, key=idx_email (GOOD)\n\n-- EXPLAIN ANALYZE (actual execution)\nEXPLAIN ANALYZE\nSELECT u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n\n-- Problematic query\nEXPLAIN SELECT * FROM orders\nWHERE YEAR(order_date) = 2024;  -- Function on indexed column!\n-- Can't use index on order_date\n\n-- Optimized version\nEXPLAIN SELECT * FROM orders\nWHERE order_date >= '2024-01-01' AND order_date < '2025-01-01';\n-- Now uses index\n\n-- Covering index example\nCREATE INDEX idx_user_email_name ON users(email, name);\n\n-- This query uses only the index (no table access)\nEXPLAIN SELECT name FROM users WHERE email = 'test@example.com';\n-- Extra: Using index (excellent!)\n\n-- JOIN ordering matters\n-- Bad: Large table first\nEXPLAIN SELECT *\nFROM orders o\nJOIN users u ON o.user_id = u.id\nWHERE u.email = 'specific@example.com';\n\n-- Better: Filter first, then join\nEXPLAIN SELECT *\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.email = 'specific@example.com';\n\n-- Subquery vs JOIN\n-- Subquery (may execute multiple times)\nEXPLAIN SELECT *\nFROM users\nWHERE id IN (SELECT user_id FROM orders WHERE total > 1000);\n\n-- JOIN (often faster)\nEXPLAIN SELECT DISTINCT u.*\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.total > 1000;\n\n-- Identify missing indexes\nSELECT \n    table_name,\n    index_name,\n    column_name,\n    seq_in_index\nFROM information_schema.statistics\nWHERE table_schema = 'your_database'\nORDER BY table_name, index_name, seq_in_index;"
    },
    {
      "id": 17,
      "question": "What are database locks and locking mechanisms?",
      "answer": "Locks: Mechanisms to control concurrent access\n\nLock Types:\n\nBy Scope:\n• Row-level: Lock individual rows\n• Page-level: Lock data pages\n• Table-level: Lock entire table\n• Database-level: Lock whole database\n\nBy Mode:\n• Shared (S): Multiple reads allowed\n• Exclusive (X): No other locks allowed\n• Update (U): Prevents deadlocks\n• Intent locks (IS, IX): Indicate lower-level locks\n\nLock Granularity:\n• Fine (row): High concurrency, overhead\n• Coarse (table): Low concurrency, less overhead\n\nLocking Strategies:\n• Pessimistic: Lock before modification\n• Optimistic: Check before commit\n\nProblems:\n• Lock contention\n• Deadlocks\n• Lock escalation\n• Performance degradation",
      "explanation": "Database locks prevent data inconsistencies during concurrent access by controlling which transactions can read or modify data.",
      "difficulty": "Hard",
      "code": "-- Explicit locking (MySQL)\n-- Shared lock (read lock)\nSELECT * FROM accounts WHERE id = 1 LOCK IN SHARE MODE;\n-- Multiple transactions can read, none can write\n\n-- Exclusive lock (write lock)\nSELECT * FROM accounts WHERE id = 1 FOR UPDATE;\n-- No other transaction can read or write\n\n-- Transaction with locking\nSTART TRANSACTION;\nSELECT balance FROM accounts WHERE id = 1 FOR UPDATE;\n-- Row is locked, other transactions wait\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nCOMMIT;\n-- Lock released\n\n-- Table-level locking\nLOCK TABLES accounts WRITE;\nUPDATE accounts SET balance = balance * 1.05;\nUNLOCK TABLES;\n\n-- Pessimistic locking example\nBEGIN TRANSACTION;\n-- Lock the row immediately\nSELECT quantity FROM inventory WHERE product_id = 10 FOR UPDATE;\n-- Other transactions wait here if they try to lock\nUPDATE inventory SET quantity = quantity - 1 WHERE product_id = 10;\nCOMMIT;\n\n-- Optimistic locking with version column\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    price DECIMAL(10,2),\n    version INT DEFAULT 0\n);\n\n-- Read with version\nSELECT id, name, price, version FROM products WHERE id = 1;\n-- Returns: id=1, name='Widget', price=10.00, version=5\n\n-- Update only if version matches (no one else modified)\nUPDATE products\nSET name = 'New Widget', price = 12.00, version = version + 1\nWHERE id = 1 AND version = 5;\n-- If affected rows = 0, someone else modified it\n\n-- Deadlock example\n-- Transaction 1:\nBEGIN;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;  -- Locks account 1\n-- ... some delay ...\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;  -- Waits for account 2\n\n-- Transaction 2 (simultaneously):\nBEGIN;\nUPDATE accounts SET balance = balance - 50 WHERE id = 2;   -- Locks account 2\n-- ... some delay ...\nUPDATE accounts SET balance = balance + 50 WHERE id = 1;   -- Waits for account 1 (DEADLOCK!)\n\n-- Prevent deadlock: Always lock in same order\n-- Both transactions:\nBEGIN;\nSELECT * FROM accounts WHERE id IN (1, 2) ORDER BY id FOR UPDATE;\n-- Locks acquired in consistent order\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\nCOMMIT;\n\n-- Set lock timeout (PostgreSQL)\nSET lock_timeout = '5s';\nSELECT * FROM accounts WHERE id = 1 FOR UPDATE;\n-- Throws error if lock not acquired in 5 seconds\n\n-- Check locks (MySQL)\nSELECT * FROM information_schema.INNODB_LOCKS;\nSELECT * FROM information_schema.INNODB_LOCK_WAITS;\n\n-- Show deadlocks\nSHOW ENGINE INNODB STATUS;\n\n-- Lock escalation example\n-- Many row locks may escalate to table lock\nUPDATE large_table SET status = 'processed' WHERE date = '2024-01-01';\n-- If too many rows, DB may escalate to table lock"
    },
    {
      "id": 18,
      "question": "What are deadlocks and how can they be prevented?",
      "answer": "Deadlock: Two+ transactions waiting for each other's locks\n\nConditions (all must be true):\n1. Mutual exclusion: Resource held exclusively\n2. Hold and wait: Holding while requesting more\n3. No preemption: Can't force release\n4. Circular wait: Circular dependency chain\n\nDetection:\n• Database monitors for cycles\n• Timeout mechanisms\n• Deadlock victim selection\n• Transaction rollback\n\nPrevention strategies:\n\n1. Lock ordering:\n   • Always acquire locks in same order\n   • Sort resource IDs before locking\n\n2. Lock timeout:\n   • Set maximum wait time\n   • Retry transaction\n\n3. Keep transactions short:\n   • Minimize lock duration\n   • Commit quickly\n\n4. Use appropriate isolation:\n   • Lower level if possible\n   • READ COMMITTED vs SERIALIZABLE\n\n5. Avoid user interaction:\n   • Don't wait for input while holding locks",
      "explanation": "Deadlocks occur when transactions create circular dependencies on locks. Prevention requires careful lock ordering and transaction design.",
      "difficulty": "Hard",
      "code": "-- Deadlock scenario\n-- Transaction A:\nBEGIN TRANSACTION;\nUPDATE users SET balance = balance - 100 WHERE id = 1;  -- Lock user 1\n-- Wait 1 second\nUPDATE users SET balance = balance + 100 WHERE id = 2;  -- Wait for user 2 lock\nCOMMIT;\n\n-- Transaction B (concurrent):\nBEGIN TRANSACTION;\nUPDATE users SET balance = balance - 50 WHERE id = 2;   -- Lock user 2\n-- Wait 1 second\nUPDATE users SET balance = balance + 50 WHERE id = 1;   -- Wait for user 1 lock (DEADLOCK!)\nCOMMIT;\n-- One transaction will be chosen as victim and rolled back\n\n-- Solution 1: Lock ordering\n-- Both transactions acquire locks in ID order\nBEGIN TRANSACTION;\n-- Lock resources in ascending ID order\nSELECT * FROM users WHERE id IN (1, 2) ORDER BY id FOR UPDATE;\nUPDATE users SET balance = balance - 100 WHERE id = 1;\nUPDATE users SET balance = balance + 100 WHERE id = 2;\nCOMMIT;\n\n-- Solution 2: Lock all resources upfront\nBEGIN TRANSACTION;\nSELECT * FROM accounts WHERE id IN (1, 2, 3, 5) ORDER BY id FOR UPDATE;\n-- Now safe to update in any order\nUPDATE accounts SET balance = balance - 100 WHERE id = 3;\nUPDATE accounts SET balance = balance + 50 WHERE id = 1;\nUPDATE accounts SET balance = balance + 50 WHERE id = 5;\nCOMMIT;\n\n-- Solution 3: Set lock timeout\nSET SESSION innodb_lock_wait_timeout = 5;  -- 5 seconds\n\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\nCOMMIT;\n-- If can't acquire lock in 5s, transaction rolled back\n\n-- Solution 4: Retry logic with exponential backoff\nDELIMITER //\nCREATE PROCEDURE transfer_money(\n    IN from_id INT,\n    IN to_id INT,\n    IN amount DECIMAL(10,2)\n)\nBEGIN\n    DECLARE max_retries INT DEFAULT 3;\n    DECLARE retry_count INT DEFAULT 0;\n    DECLARE deadlock_found BOOLEAN DEFAULT FALSE;\n    \n    retry_loop: LOOP\n        BEGIN\n            DECLARE EXIT HANDLER FOR 1213  -- Deadlock error code\n            BEGIN\n                SET deadlock_found = TRUE;\n            END;\n            \n            START TRANSACTION;\n            \n            -- Lock in order to prevent deadlock\n            IF from_id < to_id THEN\n                SELECT balance FROM accounts WHERE id = from_id FOR UPDATE;\n                SELECT balance FROM accounts WHERE id = to_id FOR UPDATE;\n            ELSE\n                SELECT balance FROM accounts WHERE id = to_id FOR UPDATE;\n                SELECT balance FROM accounts WHERE id = from_id FOR UPDATE;\n            END IF;\n            \n            UPDATE accounts SET balance = balance - amount WHERE id = from_id;\n            UPDATE accounts SET balance = balance + amount WHERE id = to_id;\n            \n            COMMIT;\n            LEAVE retry_loop;\n        END;\n        \n        IF deadlock_found THEN\n            SET retry_count = retry_count + 1;\n            IF retry_count >= max_retries THEN\n                SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Max retries exceeded';\n            END IF;\n            -- Exponential backoff\n            DO SLEEP(POWER(2, retry_count) * 0.1);\n            SET deadlock_found = FALSE;\n        END IF;\n    END LOOP;\nEND//\nDELIMITER ;\n\n-- Monitor deadlocks\nSHOW ENGINE INNODB STATUS;\n-- Look for LATEST DETECTED DEADLOCK section\n\n-- PostgreSQL: Check for deadlocks\nSELECT * FROM pg_stat_database WHERE datname = 'your_database';\n-- Check deadlocks column\n\n-- Best practices\n-- 1. Keep transactions short\nBEGIN;\n-- Do all reads\nSELECT * FROM accounts WHERE id = 1;\nSELECT * FROM accounts WHERE id = 2;\n-- Quick updates\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\nCOMMIT;  -- Release locks quickly\n\n-- 2. Avoid user interaction in transactions\n-- BAD:\nBEGIN;\nSELECT * FROM accounts WHERE id = 1 FOR UPDATE;\n-- Wait for user input... (holding lock!)\n-- ... user takes 5 minutes ...\nUPDATE accounts SET notes = @user_input WHERE id = 1;\nCOMMIT;\n\n-- GOOD:\n-- Get user input first\nSET @user_input = 'some value';\nBEGIN;\nUPDATE accounts SET notes = @user_input WHERE id = 1;\nCOMMIT;"
    },
    {
      "id": 19,
      "question": "What is database partitioning and types of partitioning?",
      "answer": "Partitioning: Dividing large table into smaller pieces\n\nBenefits:\n• Improved query performance\n• Easier maintenance\n• Faster data loading/deletion\n• Parallel query execution\n• Archive old data easily\n\nTypes:\n\n1. Horizontal Partitioning (Sharding):\n• Split by rows\n• Each partition has same columns\n• Examples: by date, region, customer_id\n\n2. Vertical Partitioning:\n• Split by columns\n• Each partition has subset of columns\n• Separate frequently/rarely accessed columns\n\n3. Range Partitioning:\n• Based on value range\n• Example: dates, IDs\n\n4. List Partitioning:\n• Based on discrete values\n• Example: country, status\n\n5. Hash Partitioning:\n• Hash function distributes data\n• Even distribution\n\n6. Composite Partitioning:\n• Combination of methods\n• Example: Range-hash",
      "explanation": "Partitioning divides large tables into smaller, more manageable pieces while maintaining logical unity, improving performance and maintenance.",
      "difficulty": "Hard",
      "code": "-- Range Partitioning by date\nCREATE TABLE orders (\n    id INT,\n    user_id INT,\n    order_date DATE,\n    total DECIMAL(10,2)\n)\nPARTITION BY RANGE (YEAR(order_date)) (\n    PARTITION p2020 VALUES LESS THAN (2021),\n    PARTITION p2021 VALUES LESS THAN (2022),\n    PARTITION p2022 VALUES LESS THAN (2023),\n    PARTITION p2023 VALUES LESS THAN (2024),\n    PARTITION p2024 VALUES LESS THAN (2025),\n    PARTITION p_future VALUES LESS THAN MAXVALUE\n);\n\n-- Query automatically uses relevant partition\nSELECT * FROM orders WHERE order_date >= '2023-01-01' AND order_date < '2024-01-01';\n-- Only scans p2023 partition\n\n-- List Partitioning by region\nCREATE TABLE customers (\n    id INT,\n    name VARCHAR(100),\n    country VARCHAR(50)\n)\nPARTITION BY LIST COLUMNS(country) (\n    PARTITION p_north_america VALUES IN ('USA', 'Canada', 'Mexico'),\n    PARTITION p_europe VALUES IN ('UK', 'Germany', 'France', 'Spain'),\n    PARTITION p_asia VALUES IN ('China', 'Japan', 'India', 'Korea'),\n    PARTITION p_other VALUES IN (DEFAULT)\n);\n\n-- Hash Partitioning (even distribution)\nCREATE TABLE users (\n    id INT,\n    username VARCHAR(50),\n    email VARCHAR(255)\n)\nPARTITION BY HASH(id)\nPARTITIONS 4;  -- Creates 4 partitions\n\n-- Key Partitioning (MySQL specific, similar to hash)\nCREATE TABLE sessions (\n    session_id VARCHAR(128),\n    user_id INT,\n    created_at DATETIME\n)\nPARTITION BY KEY(session_id)\nPARTITIONS 8;\n\n-- Composite Partitioning (Range + Hash)\nCREATE TABLE logs (\n    id BIGINT,\n    log_date DATE,\n    message TEXT\n)\nPARTITION BY RANGE (YEAR(log_date))\nSUBPARTITION BY HASH(id) SUBPARTITIONS 4 (\n    PARTITION p2023 VALUES LESS THAN (2024),\n    PARTITION p2024 VALUES LESS THAN (2025)\n);\n-- Creates 8 total partitions (2 ranges * 4 hash subpartitions)\n\n-- Manage partitions\n-- Add new partition\nALTER TABLE orders\nADD PARTITION (PARTITION p2025 VALUES LESS THAN (2026));\n\n-- Drop old partition (fast data deletion)\nALTER TABLE orders DROP PARTITION p2020;\n-- Much faster than: DELETE FROM orders WHERE YEAR(order_date) = 2020;\n\n-- Archive partition\nALTER TABLE orders\nEXCHANGE PARTITION p2020 WITH TABLE orders_2020_archive;\n\n-- View partition information\nSELECT \n    table_name,\n    partition_name,\n    partition_method,\n    partition_expression,\n    table_rows\nFROM information_schema.partitions\nWHERE table_schema = 'your_database'\n  AND table_name = 'orders';\n\n-- Vertical Partitioning example (manual)\n-- Original table:\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    description TEXT,\n    price DECIMAL(10,2),\n    weight DECIMAL(10,2),\n    dimensions VARCHAR(50),\n    large_image BLOB,\n    thumbnail BLOB\n);\n\n-- Split into frequently and rarely accessed\nCREATE TABLE products_main (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    price DECIMAL(10,2),\n    thumbnail BLOB\n);\n\nCREATE TABLE products_details (\n    product_id INT PRIMARY KEY,\n    description TEXT,\n    weight DECIMAL(10,2),\n    dimensions VARCHAR(50),\n    large_image BLOB,\n    FOREIGN KEY (product_id) REFERENCES products_main(id)\n);\n\n-- Partition pruning example\nEXPLAIN PARTITIONS\nSELECT * FROM orders WHERE order_date = '2023-05-15';\n-- Shows which partitions are scanned"
    },
    {
      "id": 20,
      "question": "What is database replication and its types?",
      "answer": "Replication: Copying data across multiple database servers\n\nBenefits:\n• High availability\n• Load distribution\n• Disaster recovery\n• Geographic distribution\n• Read scalability\n\nTypes:\n\n1. Master-Slave (Primary-Replica):\n• One master (writes)\n• Multiple slaves (reads)\n• Async or sync replication\n• Simple, scalable reads\n\n2. Master-Master (Multi-Master):\n• Multiple masters (writes)\n• Bidirectional replication\n• Conflict resolution needed\n• Higher availability\n\n3. Cascading Replication:\n• Slaves replicate from other slaves\n• Reduces master load\n\n4. Statement vs Row-based:\n• Statement: Replicate SQL\n• Row: Replicate data changes\n• Mixed: Both\n\nChallenges:\n• Replication lag\n• Conflict resolution\n• Consistency guarantees\n• Network partitions",
      "explanation": "Database replication maintains copies of data across multiple servers to improve availability, performance, and disaster recovery capabilities.",
      "difficulty": "Hard",
      "code": "-- MySQL Master-Slave Replication Setup\n\n-- On Master:\n-- 1. Configure my.cnf\n-- [mysqld]\n-- server-id = 1\n-- log-bin = mysql-bin\n-- binlog-format = ROW\n-- bind-address = 0.0.0.0\n\n-- 2. Create replication user\nCREATE USER 'repl'@'%' IDENTIFIED BY 'password';\nGRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';\nFLUSH PRIVILEGES;\n\n-- 3. Get master status\nSHOW MASTER STATUS;\n-- Note: File and Position values\n\n-- On Slave:\n-- 1. Configure my.cnf\n-- [mysqld]\n-- server-id = 2\n-- relay-log = mysql-relay\n-- read_only = 1\n\n-- 2. Configure replication\nCHANGE MASTER TO\n    MASTER_HOST='master_ip',\n    MASTER_USER='repl',\n    MASTER_PASSWORD='password',\n    MASTER_LOG_FILE='mysql-bin.000001',\n    MASTER_LOG_POS=123;\n\n-- 3. Start slave\nSTART SLAVE;\n\n-- 4. Check status\nSHOW SLAVE STATUS\\G\n-- Look for:\n-- Slave_IO_Running: Yes\n-- Slave_SQL_Running: Yes\n-- Seconds_Behind_Master: 0 (or small number)\n\n-- PostgreSQL Replication (Streaming)\n-- On Primary:\n-- postgresql.conf\n-- wal_level = replica\n-- max_wal_senders = 3\n-- wal_keep_size = 64MB\n\n-- pg_hba.conf\n-- host replication repl_user replica_ip/32 md5\n\n-- Create replication user\nCREATE ROLE repl_user WITH REPLICATION LOGIN PASSWORD 'password';\n\n-- On Standby:\n-- Create standby.signal file\n-- touch standby.signal\n\n-- postgresql.conf\n-- primary_conninfo = 'host=primary_ip port=5432 user=repl_user password=password'\n-- hot_standby = on\n\n-- Application code: Read from replicas\n-- Write operations - use master\nfunction executeWrite(sql, params) {\n    return masterConnection.execute(sql, params);\n}\n\n-- Read operations - use slave with fallback\nfunction executeRead(sql, params) {\n    try {\n        return slaveConnection.execute(sql, params);\n    } catch (error) {\n        // Fallback to master if slave unavailable\n        return masterConnection.execute(sql, params);\n    }\n}\n\n-- Handle replication lag\n-- Check lag\nSELECT \n    UNIX_TIMESTAMP() - UNIX_TIMESTAMP(MAX(timestamp)) AS lag_seconds\nFROM replication_heartbeat;\n\n-- If critical read, use master\nif (requireFreshData && replicationLag > maxAcceptableLag) {\n    return masterConnection.execute(sql, params);\n} else {\n    return slaveConnection.execute(sql, params);\n}\n\n-- Master-Master replication (MySQL)\n-- Server 1:\n-- my.cnf\n-- server-id = 1\n-- log-bin = mysql-bin\n-- auto-increment-increment = 2\n-- auto-increment-offset = 1\n-- binlog-do-db = mydb\n\nCHANGE MASTER TO\n    MASTER_HOST='server2_ip',\n    MASTER_USER='repl',\n    MASTER_PASSWORD='password';\nSTART SLAVE;\n\n-- Server 2:\n-- my.cnf\n-- server-id = 2\n-- log-bin = mysql-bin\n-- auto-increment-increment = 2\n-- auto-increment-offset = 2\n-- binlog-do-db = mydb\n\nCHANGE MASTER TO\n    MASTER_HOST='server1_ip',\n    MASTER_USER='repl',\n    MASTER_PASSWORD='password';\nSTART SLAVE;\n\n-- Conflict detection\nCREATE TABLE sync_status (\n    table_name VARCHAR(64),\n    last_sync TIMESTAMP,\n    conflicts INT DEFAULT 0\n);\n\n-- Monitor replication health\nSELECT \n    @@hostname AS server,\n    @@server_id AS server_id,\n    @@read_only AS read_only_mode;\n\nSHOW SLAVE STATUS\\G\n\n-- Switchover/Failover process\n-- 1. Stop writes to master\nSET GLOBAL read_only = 1;\n\n-- 2. Ensure slave caught up\nSHOW SLAVE STATUS\\G  -- Check Seconds_Behind_Master = 0\n\n-- 3. Promote slave to master\nSTOP SLAVE;\nRESET SLAVE ALL;\nSET GLOBAL read_only = 0;\n\n-- 4. Point application to new master"
    },
    {
      "id": 21,
      "question": "What is database sharding and when should it be used?",
      "answer": "Sharding: Horizontal partitioning across multiple databases/servers\n\nKey concepts:\n• Each shard = independent database\n• Data divided by shard key\n• Distributed across servers\n• No single point of failure\n\nSharding strategies:\n\n1. Range-based:\n• Shard by value ranges\n• Example: user_id 1-1M → Shard1, 1M-2M → Shard2\n• Pros: Simple, range queries easy\n• Cons: Uneven distribution\n\n2. Hash-based:\n• Hash function determines shard\n• Example: hash(user_id) % num_shards\n• Pros: Even distribution\n• Cons: Range queries difficult\n\n3. Geographic:\n• By location\n• Example: US → Shard1, EU → Shard2\n• Pros: Latency optimization\n• Cons: Load imbalance\n\n4. Directory-based:\n• Lookup service maps key → shard\n• Pros: Flexible\n• Cons: Lookup overhead\n\nWhen to use:\n• Data > single server capacity\n• High write throughput\n• Geographic distribution needed\n\nChallenges:\n• Cross-shard queries\n• Distributed transactions\n• Rebalancing data\n• Application complexity",
      "explanation": "Sharding distributes data across multiple database servers to scale beyond single-server limits, enabling massive scale at the cost of complexity.",
      "difficulty": "Hard",
      "code": "-- Application-level sharding logic\n\n-- 1. Hash-based sharding\nfunction getShardId(userId) {\n    const NUM_SHARDS = 4;\n    return userId % NUM_SHARDS;\n}\n\nfunction getUserShard(userId) {\n    const shardId = getShardId(userId);\n    return dbConnections[shardId];\n}\n\n// Usage\nconst userId = 12345;\nconst shard = getUserShard(userId);\nconst user = await shard.query('SELECT * FROM users WHERE id = ?', [userId]);\n\n-- 2. Range-based sharding\nfunction getShardByRange(userId) {\n    if (userId <= 1000000) return shards[0];        // Shard 0: 1-1M\n    if (userId <= 2000000) return shards[1];        // Shard 1: 1M-2M\n    if (userId <= 3000000) return shards[2];        // Shard 2: 2M-3M\n    return shards[3];                               // Shard 3: 3M+\n}\n\n-- 3. Geographic sharding\nconst SHARD_MAP = {\n    'US': 'shard-us',\n    'EU': 'shard-eu',\n    'ASIA': 'shard-asia'\n};\n\nfunction getShardByLocation(country) {\n    const region = getRegion(country);  // Maps country to region\n    return connections[SHARD_MAP[region]];\n}\n\n-- 4. Directory-based sharding\nCREATE TABLE shard_directory (\n    user_id INT PRIMARY KEY,\n    shard_id INT NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    INDEX idx_shard (shard_id)\n);\n\nfunction getShardFromDirectory(userId) {\n    const result = await directoryDb.query(\n        'SELECT shard_id FROM shard_directory WHERE user_id = ?',\n        [userId]\n    );\n    return shards[result.shard_id];\n}\n\n-- Cross-shard query example\n// BAD: This doesn't work with sharding\nSELECT u.name, COUNT(o.id) as order_count\nFROM users u\nJOIN orders o ON u.id = o.user_id\nGROUP BY u.id;\n\n// GOOD: Application-level aggregation\nasync function getUserOrderCounts() {\n    const results = [];\n    \n    // Query each shard\n    for (const shard of allShards) {\n        const shardResults = await shard.query(`\n            SELECT u.name, COUNT(o.id) as order_count\n            FROM users u\n            JOIN orders o ON u.id = o.user_id\n            GROUP BY u.id\n        `);\n        results.push(...shardResults);\n    }\n    \n    // Merge results\n    return results;\n}\n\n-- Shard rebalancing\n-- Step 1: Create new shard\nCREATE DATABASE shard_4;\n\n-- Step 2: Update shard directory\nUPDATE shard_directory\nSET shard_id = 4\nWHERE user_id BETWEEN 3000001 AND 4000000;\n\n-- Step 3: Migrate data\nINSERT INTO shard_4.users\nSELECT * FROM shard_3.users\nWHERE id BETWEEN 3000001 AND 4000000;\n\n-- Step 4: Verify migration\nSELECT COUNT(*) FROM shard_4.users;\n\n-- Step 5: Delete from old shard\nDELETE FROM shard_3.users\nWHERE id BETWEEN 3000001 AND 4000000;\n\n-- Consistent hashing (better for rebalancing)\nclass ConsistentHash {\n    constructor(shards) {\n        this.ring = new Map();\n        this.sortedKeys = [];\n        \n        // Add virtual nodes for each shard\n        shards.forEach(shard => {\n            for (let i = 0; i < 150; i++) {  // 150 virtual nodes per shard\n                const hash = this.hash(`${shard}-${i}`);\n                this.ring.set(hash, shard);\n            }\n        });\n        \n        this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n    }\n    \n    getShard(key) {\n        if (this.ring.size === 0) return null;\n        \n        const hash = this.hash(key);\n        \n        // Find first node >= hash\n        for (const nodeHash of this.sortedKeys) {\n            if (nodeHash >= hash) {\n                return this.ring.get(nodeHash);\n            }\n        }\n        \n        // Wrap around to first node\n        return this.ring.get(this.sortedKeys[0]);\n    }\n    \n    hash(key) {\n        // Simple hash function (use better one in production)\n        let hash = 0;\n        for (let i = 0; i < key.length; i++) {\n            hash = ((hash << 5) - hash) + key.charCodeAt(i);\n            hash |= 0;\n        }\n        return Math.abs(hash);\n    }\n}\n\n-- Monitoring sharded database\nSELECT \n    shard_id,\n    COUNT(*) as user_count,\n    AVG(data_size) as avg_size\nFROM shard_directory\nGROUP BY shard_id;\n\n-- Check shard health\nfor (const [shardId, shard] of Object.entries(shards)) {\n    const stats = await shard.query('SELECT COUNT(*) as count, SUM(data_length) as size FROM information_schema.tables');\n    console.log(`Shard ${shardId}:`, stats);\n}"
    },
    {
      "id": 22,
      "question": "What is the CAP theorem?",
      "answer": "CAP Theorem: In distributed database, can only guarantee 2 of 3:\n\nC - Consistency:\n• All nodes see same data simultaneously\n• Every read gets latest write\n• Strong consistency guarantee\n\nA - Availability:\n• Every request gets response (success/failure)\n• System operational even if nodes fail\n• No downtime\n\nP - Partition Tolerance:\n• System works despite network failures\n• Can handle message loss\n• Must choose between C or A during partition\n\nTrade-offs:\n\nCA (Consistency + Availability):\n• No partition tolerance\n• Single-site databases\n• Examples: Traditional RDBMS (single server)\n• Unrealistic for distributed systems\n\nCP (Consistency + Partition Tolerance):\n• Sacrifice availability  \n• Block requests during partition\n• Examples: MongoDB, HBase, Redis\n• Use when data accuracy critical\n\nAP (Availability + Partition Tolerance):\n• Sacrifice consistency\n• Always respond (may be stale)\n• Examples: Cassandra, DynamoDB, CouchDB\n• Use when uptime critical\n\nIn practice:\n• Can't avoid partitions in distributed systems\n• Must choose: CP or AP\n• Often use eventual consistency",
      "explanation": "CAP theorem states distributed databases can only provide two of: Consistency, Availability, and Partition Tolerance simultaneously.",
      "difficulty": "Hard",
      "code": "-- CP System Example (Consistency + Partition Tolerance)\n-- MongoDB with majority write concern\n\n// Write must be acknowledged by majority\ndb.users.insertOne(\n    { name: 'John', email: 'john@example.com' },\n    { writeConcern: { w: 'majority', j: true, wtimeout: 5000 } }\n);\n\n// During partition:\n// - If can't reach majority: Write fails (sacrifices availability)\n// - Ensures consistency: No split brain\n\n-- AP System Example (Availability + Partition Tolerance)\n-- Cassandra with ANY consistency level\n\n// Write succeeds if ANY node receives it\nINSERT INTO users (id, name, email)\nVALUES (uuid(), 'John', 'john@example.com')\nUSING CONSISTENCY ANY;\n\n// During partition:\n// - Write always succeeds (high availability)\n// - May be inconsistent across nodes temporarily\n// - Eventually consistent\n\n-- Demonstrating CAP trade-offs\n\n-- Scenario: 2-node distributed database\n-- Node A and Node B, replicating data\n\n-- BEFORE partition: All fine\nnode_A.execute(\"UPDATE balance SET amount = 100 WHERE user_id = 1\");\nnode_B.replicate();  // Gets update\n// Both nodes consistent ✓\n\n-- DURING partition: Network split\n-- Nodes can't communicate\n\n-- Option 1: CP - Maintain Consistency\nif (!canReachOtherNode()) {\n    throw new Error('Cannot ensure consistency, rejecting write');\n}\n// ✓ Consistent (rejects writes)\n// ✗ Not available (returns errors)\n\n-- Option 2: AP - Maintain Availability\nif (!canReachOtherNode()) {\n    // Accept write anyway\n    node_A.execute(\"UPDATE balance SET amount = 100 WHERE user_id = 1\");\n    // Queue for later replication\n}\n// ✓ Available (accepts writes)\n// ✗ Inconsistent (nodes diverge)\n\n-- Eventual Consistency (AP systems)\n-- Accept divergence, sync later\n\nCREATE TABLE users (\n    id UUID PRIMARY KEY,\n    name TEXT,\n    email TEXT,\n    version BIGINT,\n    last_modified TIMESTAMP\n);\n\n-- Conflict resolution strategies:\n\n-- 1. Last Write Wins (LWW)\nUPDATE users\nSET name = 'John Updated',\n    last_modified = NOW(),\n    version = version + 1\nWHERE id = @id\n  AND last_modified < @new_timestamp;\n\n-- 2. Version vectors (track causality)\nclass VersionVector {\n    constructor() {\n        this.vector = {}; // {nodeId: version}\n    }\n    \n    increment(nodeId) {\n        this.vector[nodeId] = (this.vector[nodeId] || 0) + 1;\n    }\n    \n    happensBefore(other) {\n        // Check if this ≤ other\n        for (const [node, version] of Object.entries(this.vector)) {\n            if ((other.vector[node] || 0) < version) {\n                return false;\n            }\n        }\n        return true;\n    }\n    \n    isConcurrent(other) {\n        return !this.happensBefore(other) && !other.happensBefore(this);\n    }\n}\n\n-- 3. Application-level merge\nfunction mergeConflictingUpdates(local, remote) {\n    if (local.timestamp > remote.timestamp) {\n        return local;  // Keep local\n    } else if (local.timestamp < remote.timestamp) {\n        return remote;  // Take remote\n    } else {\n        // Same timestamp - custom merge logic\n        return {\n            ...local,\n            ...remote,\n            conflicts: [local, remote]  // Store both\n        };\n    }\n}\n\n-- PostgreSQL: Synchronous replication (CP)\n-- postgresql.conf\nsynchronous_commit = on\nsynchronous_standby_names = 'standby1,standby2'\n\n-- Must wait for standby acknowledgment\n-- Sacrifices availability if standby unreachable\n\n-- MySQL: Asynchronous replication (AP)\n-- Master doesn't wait for slave\n-- Slaves may lag behind\n-- High availability, eventual consistency\n\n-- Application strategy: Read-your-writes consistency\nfunction updateUser(userId, data) {\n    // Write to primary\n    await primary.query('UPDATE users SET data = ? WHERE id = ?', [data, userId]);\n    \n    // Add to tracking\n    recentWrites.set(userId, Date.now());\n}\n\nfunction getUser(userId) {\n    // Check if recently written\n    if (recentWrites.has(userId)) {\n        // Read from primary (consistent)\n        return primary.query('SELECT * FROM users WHERE id = ?', [userId]);\n    } else {\n        // Read from replica (may be stale, but available)\n        return replica.query('SELECT * FROM users WHERE id = ?', [userId]);\n    }\n}"
    },
    {
      "id": 23,
      "question": "What is BASE (Basically Available, Soft state, Eventually consistent)?",
      "answer": "BASE: Alternative to ACID for distributed systems\n\nB - Basically Available:\n• System appears to work most of the time\n• Partial failures tolerated\n• May have degraded functionality\n• Prioritizes availability over consistency\n\nS - Soft State:\n• State may change over time\n• Even without input (due to replication)\n• No consistency guarantees\n• System doesn't have to be write-consistent\n\nE - Eventually Consistent:\n• Given enough time, all replicas converge\n• Temporary inconsistencies acceptable\n• Updates propagate asynchronously\n• Eventually all reads return same value\n\nACID vs BASE:\n\nACID (Traditional RDBMS):\n• Strong consistency\n• Immediate consistency\n• Pessimistic\n• Availability sacrificed\n• Vertical scaling\n\nBASE (NoSQL/Distributed):\n• Weak consistency\n• Stale data acceptable\n• Optimistic\n• Consistency sacrificed\n• Horizontal scaling\n\nWhen to use BASE:\n• High availability required\n• Massive scale needed\n• Temporary inconsistency acceptable\n• Social media, caching, analytics",
      "explanation": "BASE is a consistency model for distributed systems that prioritizes availability and partition tolerance over immediate consistency.",
      "difficulty": "Medium",
      "code": "-- BASE in action: Social media likes\n\n-- Traditional ACID approach\nBEGIN TRANSACTION;\nUPDATE posts SET like_count = like_count + 1 WHERE id = 123;\nINSERT INTO likes (user_id, post_id) VALUES (456, 123);\nCOMMIT;\n-- Locks, slow, doesn't scale\n\n-- BASE approach\n// 1. Accept like immediately (Basically Available)\nawait redis.incr(`post:${postId}:likes`);  // Fast, in-memory\nawait queue.publish('like_event', { userId, postId, timestamp });\nreturn { success: true };  // Respond immediately\n\n// 2. Process asynchronously (Soft state)\nqueue.subscribe('like_event', async (event) => {\n    try {\n        await db.query(\n            'INSERT INTO likes (user_id, post_id, created_at) VALUES (?, ?, ?)',\n            [event.userId, event.postId, event.timestamp]\n        );\n    } catch (error) {\n        // Retry later\n        await queue.publishDelayed('like_event', event, 60000);\n    }\n});\n\n// 3. Sync periodically (Eventually Consistent)\nsetInterval(async () => {\n    // Reconcile counts\n    const redisCount = await redis.get(`post:${postId}:likes`);\n    const dbCount = await db.query(\n        'SELECT COUNT(*) as count FROM likes WHERE post_id = ?',\n        [postId]\n    );\n    \n    if (redisCount !== dbCount.count) {\n        // Update database to match redis\n        await db.query(\n            'UPDATE posts SET like_count = ? WHERE id = ?',\n            [redisCount, postId]\n        );\n    }\n}, 60000);  // Every minute\n\n-- Shopping cart example (BASE)\nCREATE TABLE cart_items (\n    id INT PRIMARY KEY AUTO_INCREMENT,\n    user_id INT,\n    product_id INT,\n    quantity INT,\n    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    synced_at TIMESTAMP NULL  -- Track sync status\n);\n\n-- Add to cart (immediate response)\napp.post('/cart/add', async (req, res) => {\n    const { userId, productId, quantity } = req.body;\n    \n    // 1. Update local cache (fast)\n    await cache.hset(\n        `cart:${userId}`,\n        productId,\n        quantity\n    );\n    \n    // 2. Queue for database sync\n    await queue.add('sync_cart', {\n        userId,\n        productId,\n        quantity,\n        timestamp: Date.now()\n    });\n    \n    // 3. Respond immediately\n    res.json({ success: true });\n});\n\n-- Background sync worker\nqueue.process('sync_cart', async (job) => {\n    const { userId, productId, quantity, timestamp } = job.data;\n    \n    await db.query(`\n        INSERT INTO cart_items (user_id, product_id, quantity, added_at, synced_at)\n        VALUES (?, ?, ?, FROM_UNIXTIME(?), NOW())\n        ON DUPLICATE KEY UPDATE\n            quantity = VALUES(quantity),\n            synced_at = NOW()\n    `, [userId, productId, quantity, timestamp / 1000]);\n});\n\n-- Read cart (with eventual consistency)\napp.get('/cart/:userId', async (req, res) => {\n    const { userId } = req.params;\n    \n    // Try cache first (may be more up-to-date)\n    let cart = await cache.hgetall(`cart:${userId}`);\n    \n    if (!cart || Object.keys(cart).length === 0) {\n        // Fallback to database\n        const items = await db.query(\n            'SELECT product_id, quantity FROM cart_items WHERE user_id = ?',\n            [userId]\n        );\n        cart = Object.fromEntries(\n            items.map(item => [item.product_id, item.quantity])\n        );\n        \n        // Populate cache\n        if (Object.keys(cart).length > 0) {\n            await cache.hmset(`cart:${userId}`, cart);\n            await cache.expire(`cart:${userId}`, 3600);\n        }\n    }\n    \n    res.json({ cart });\n});\n\n-- Inventory management (BASE)\n-- Accept order optimistically\nCREATE TABLE inventory (\n    product_id INT PRIMARY KEY,\n    available INT,\n    reserved INT,\n    last_updated TIMESTAMP\n);\n\nCREATE TABLE reservations (\n    id INT PRIMARY KEY AUTO_INCREMENT,\n    product_id INT,\n    quantity INT,\n    user_id INT,\n    status ENUM('pending', 'confirmed', 'cancelled'),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Reserve inventory (optimistic)\nINSERT INTO reservations (product_id, quantity, user_id, status)\nVALUES (?, ?, ?, 'pending');\n\nUPDATE inventory\nSET reserved = reserved + ?\nWHERE product_id = ?;\n\n-- Background reconciliation\nCREATE EVENT reconcile_inventory\nON SCHEDULE EVERY 5 MINUTE\nDO\n  UPDATE inventory i\n  SET reserved = (\n      SELECT COALESCE(SUM(quantity), 0)\n      FROM reservations r\n      WHERE r.product_id = i.product_id\n        AND r.status = 'pending'\n  ),\n  last_updated = NOW();\n\n-- Conflict resolution (application-level)\nfunction resolveInventoryConflict(localValue, remoteValue) {\n    // Custom merge logic\n    return {\n        available: Math.min(localValue.available, remoteValue.available),  // Conservative\n        reserved: Math.max(localValue.reserved, remoteValue.reserved),\n        last_updated: new Date()\n    };\n}"
    },
    {
      "id": 24,
      "question": "What are execution plans and how to analyze them?",
      "answer": "Execution Plan: Step-by-step roadmap of query execution\n\nShows:\n• Tables accessed\n• Indexes used\n• Join methods\n• Estimated costs\n• Row counts\n• Sort operations\n• Temporary tables\n\nTypes:\n• Estimated: Before execution\n• Actual: After execution (with real stats)\n\nKey operations:\n\nTable Scan: Full table read (slow)\nIndex Scan: Read entire index\nIndex Seek: Use index efficiently (fast)\nNested Loop: Join method for small datasets\nHash Join: Join with hash table\nMerge Join: Join sorted datasets\nSort: Order results\nFilter: WHERE clause application\n\nRed flags:\n• Table scans on large tables\n• Missing indexes\n• Implicit conversions\n• High estimated costs\n• Many rows processed\n• Using temporary/filesort\n\nAnalysis steps:\n1. Get execution plan\n2. Identify bottlenecks\n3. Check index usage\n4. Compare estimated vs actual\n5. Optimize accordingly",
      "explanation": "Execution plans reveal how databases process queries, enabling identification of performance bottlenecks and optimization opportunities.",
      "difficulty": "Hard",
      "code": "-- Get execution plan (MySQL)\nEXPLAIN SELECT u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01'\nGROUP BY u.id, u.name;\n\n-- Extended information\nEXPLAIN FORMAT=JSON\nSELECT * FROM users WHERE email = 'test@example.com';\n\n-- Actual execution with statistics\nEXPLAIN ANALYZE\nSELECT * FROM orders WHERE total > 1000;\n\n-- PostgreSQL: Detailed execution plan\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE)\nSELECT u.*, COUNT(o.id)\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id;\n\n-- Sample output interpretation:\n/*\n+----+-------------+-------+------+---------------+----------+---------+------+------+----------+\n| id | select_type | table | type | possible_keys | key      | key_len | ref  | rows | Extra    |\n+----+-------------+-------+------+---------------+----------+---------+------+------+----------+\n|  1 | SIMPLE      | users | ALL  | NULL          | NULL     | NULL    | NULL | 1000 | Using... |\n+----+-------------+-------+------+---------------+----------+---------+------+------+----------+\n\ntype=ALL → Full table scan (BAD!)\nrows=1000 → Examining 1000 rows\nkey=NULL → No index used\nExtra=Using where → Filtering after scan\n*/\n\n-- Problematic query analysis\nEXPLAIN SELECT * FROM orders\nWHERE DATE(order_date) = '2024-01-15';  -- Function prevents index use\n\n-- Better: Let index be used\nEXPLAIN SELECT * FROM orders\nWHERE order_date >= '2024-01-15' AND order_date < '2024-01-16';\n\n-- Analyze different JOIN strategies\n-- Nested Loop Join (good for small datasets)\nEXPLAIN SELECT *\nFROM small_table s\nJOIN large_table l ON s.id = l.small_id\nWHERE s.id = 5;\n-- Plan: Index seek on small_table, nested loop with index seek on large_table\n\n-- Hash Join (good for large datasets)\nEXPLAIN SELECT *\nFROM large_table1 l1\nJOIN large_table2 l2 ON l1.ref_id = l2.id;\n-- Plan: Hash join (builds hash table for one table)\n\n-- Subquery performance\n-- Bad: Correlated subquery (executes for each row)\nEXPLAIN SELECT u.name,\n    (SELECT COUNT(*) FROM orders WHERE user_id = u.id) as order_count\nFROM users u;\n-- Plan: Shows DEPENDENT SUBQUERY (executes per row)\n\n-- Better: JOIN\nEXPLAIN SELECT u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n-- Plan: Single join operation\n\n-- Index usage comparison\nCREATE TABLE test_table (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    age INT,\n    city VARCHAR(50),\n    INDEX idx_age (age),\n    INDEX idx_city (city),\n    INDEX idx_age_city (age, city)\n);\n\n-- Uses single-column index\nEXPLAIN SELECT * FROM test_table WHERE age = 25;\n-- key: idx_age\n\n-- Uses composite index (best)\nEXPLAIN SELECT * FROM test_table WHERE age = 25 AND city = 'NYC';\n-- key: idx_age_city (covering index)\n\n-- Can't use index efficiently\nEXPLAIN SELECT * FROM test_table WHERE city = 'NYC' AND age = 25;\n-- key: idx_city or idx_age (depends on selectivity)\n\n-- Analyze index statistics\nANALYZE TABLE test_table;\n\n-- Check index cardinality\nSHOW INDEX FROM test_table;\n\n-- Find unused indexes\nSELECT \n    t.table_schema,\n    t.table_name,\n    t.index_name,\n    t.index_type\nFROM information_schema.statistics t\nLEFT JOIN performance_schema.table_io_waits_summary_by_index_usage p\n    ON t.table_schema = p.object_schema\n    AND t.table_name = p.object_name\n    AND t.index_name = p.index_name\nWHERE t.table_schema NOT IN ('mysql', 'performance_schema', 'information_schema')\n  AND p.index_name IS NULL\n  AND t.index_name != 'PRIMARY';\n\n-- Cost-based analysis\nEXPLAIN FORMAT=JSON\nSELECT * FROM large_table WHERE indexed_column = 'value';\n-- Look at \"cost_info\": {\"read_cost\": \"1.00\", \"eval_cost\": \"0.20\", ...}\n\n-- Compare query performance\nSET profiling = 1;\n\nSELECT * FROM users WHERE email = 'test@example.com';\nSELECT * FROM users WHERE id = 123;\n\nSHOW PROFILES;\n-- Shows duration of each query\n\nSHOW PROFILE FOR QUERY 1;\n-- Detailed breakdown of query execution time\n\n-- Optimize based on execution plan\n-- Before:\nEXPLAIN SELECT * FROM orders WHERE YEAR(order_date) = 2024;\n-- Type: ALL, Extra: Using where\n\n-- After optimization:\nCREATE INDEX idx_order_date ON orders(order_date);\nEXPLAIN SELECT * FROM orders \nWHERE order_date >= '2024-01-01' AND order_date < '2025-01-01';\n-- Type: range, key: idx_order_date"
    },
    {
      "id": 25,
      "question": "What is connection pooling and why is it important?",
      "answer": "Connection Pooling: Maintain cache of database connections for reuse\n\nWithout pooling:\n• Create connection for each request\n• Expensive: TCP handshake, authentication, resource allocation\n• Close after use\n• High overhead\n\nWith pooling:\n• Maintain pool of open connections\n• Reuse existing connections\n• Return to pool after use\n• Minimal overhead\n\nBenefits:\n• Performance: Faster than creating new\n• Resource management: Limit concurrent connections\n• Connection reuse: Reduce overhead\n• Scalability: Handle more requests\n• Failover support: Health checks, retry logic\n\nKey parameters:\n• minPoolSize: Minimum connections\n• maxPoolSize: Maximum connections\n• connectionTimeout: Wait time for connection\n• idleTimeout: Close idle connections\n• maxLifetime: Maximum connection age\n• leakDetectionThreshold: Detect connection leaks\n\nBest practices:\n• Set appropriate pool size\n• Always return connections\n• Monitor pool statistics\n• Handle connection failures\n• Use timeouts\n• Close pool on shutdown",
      "explanation": "Connection pooling improves performance by reusing database connections instead of creating new ones for each request.",
      "difficulty": "Medium",
      "code": "-- Node.js with mysql2 pool\nconst mysql = require('mysql2/promise');\n\nconst pool = mysql.createPool({\n    host: 'localhost',\n    user: 'dbuser',\n    password: 'password',\n    database: 'mydb',\n    waitForConnections: true,\n    connectionLimit: 10,          // Max connections\n    maxIdle: 10,                  // Max idle connections\n    idleTimeout: 60000,           // Idle timeout (1 min)\n    queueLimit: 0,                // No limit on queue\n    enableKeepAlive: true,\n    keepAliveInitialDelay: 0\n});\n\n// Use connection from pool\nasync function getUser(userId) {\n    // Get connection from pool\n    const connection = await pool.getConnection();\n    \n    try {\n        const [rows] = await connection.query(\n            'SELECT * FROM users WHERE id = ?',\n            [userId]\n        );\n        return rows[0];\n    } finally {\n        // Return to pool (IMPORTANT!)\n        connection.release();\n    }\n}\n\n// Or use pool directly (auto-release)\nasync function getUserSimple(userId) {\n    const [rows] = await pool.query(\n        'SELECT * FROM users WHERE id = ?',\n        [userId]\n    );\n    return rows[0];\n}\n\n-- Java with HikariCP\nimport com.zaxxer.hikari.HikariConfig;\nimport com.zaxxer.hikari.HikariDataSource;\n\npublic class DatabaseConfig {\n    private static HikariDataSource dataSource;\n    \n    static {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:mysql://localhost:3306/mydb\");\n        config.setUsername(\"dbuser\");\n        config.setPassword(\"password\");\n        \n        // Pool configuration\n        config.setMinimumIdle(5);           // Min idle connections\n        config.setMaximumPoolSize(10);      // Max pool size\n        config.setConnectionTimeout(30000); // 30 seconds\n        config.setIdleTimeout(600000);      // 10 minutes\n        config.setMaxLifetime(1800000);     // 30 minutes\n        config.setLeakDetectionThreshold(60000);  // 1 minute\n        \n        // Performance\n        config.setAutoCommit(true);\n        config.addDataSourceProperty(\"cachePrepStmts\", \"true\");\n        config.addDataSourceProperty(\"prepStmtCacheSize\", \"250\");\n        config.addDataSourceProperty(\"prepStmtCacheSqlLimit\", \"2048\");\n        \n        dataSource = new HikariDataSource(config);\n    }\n    \n    public static Connection getConnection() throws SQLException {\n        return dataSource.getConnection();\n    }\n}\n\n// Usage\npublic User getUser(int userId) throws SQLException {\n    String sql = \"SELECT * FROM users WHERE id = ?\";\n    \n    try (Connection conn = DatabaseConfig.getConnection();\n         PreparedStatement stmt = conn.prepareStatement(sql)) {\n        \n        stmt.setInt(1, userId);\n        ResultSet rs = stmt.executeQuery();\n        \n        if (rs.next()) {\n            return new User(\n                rs.getInt(\"id\"),\n                rs.getString(\"name\"),\n                rs.getString(\"email\")\n            );\n        }\n    }  // Connection auto-returned to pool\n    \n    return null;\n}\n\n-- Python with psycopg2\nimport psycopg2\nfrom psycopg2 import pool\n\nclass Database:\n    def __init__(self):\n        self.connection_pool = psycopg2.pool.SimpleConnectionPool(\n            minconn=1,\n            maxconn=10,\n            host='localhost',\n            database='mydb',\n            user='dbuser',\n            password='password'\n        )\n    \n    def get_user(self, user_id):\n        conn = None\n        try:\n            # Get connection from pool\n            conn = self.connection_pool.getconn()\n            cursor = conn.cursor()\n            \n            cursor.execute('SELECT * FROM users WHERE id = %s', (user_id,))\n            user = cursor.fetchone()\n            \n            return user\n        finally:\n            if conn:\n                # Return to pool\n                self.connection_pool.putconn(conn)\n    \n    def close_all(self):\n        self.connection_pool.closeall()\n\n-- C# with ADO.NET (built-in pooling)\nusing System.Data.SqlClient;\n\npublic class DatabaseHelper\n{\n    // Connection string with pooling parameters\n    private static string connectionString = \n        \"Server=localhost;Database=mydb;User Id=dbuser;Password=password;\" +\n        \"Min Pool Size=5;\" +\n        \"Max Pool Size=100;\" +\n        \"Connection Timeout=30;\" +\n        \"Pooling=true;\";\n    \n    public static User GetUser(int userId)\n    {\n        using (var connection = new SqlConnection(connectionString))\n        {\n            connection.Open();\n            \n            using (var command = new SqlCommand(\n                \"SELECT * FROM users WHERE id = @userId\", connection))\n            {\n                command.Parameters.AddWithValue(\"@userId\", userId);\n                \n                using (var reader = command.ExecuteReader())\n                {\n                    if (reader.Read())\n                    {\n                        return new User\n                        {\n                            Id = reader.GetInt32(0),\n                            Name = reader.GetString(1),\n                            Email = reader.GetString(2)\n                        };\n                    }\n                }\n            }\n        }  // Connection returned to pool\n        \n        return null;\n    }\n}\n\n-- Monitor connection pool\n// Node.js\nsetInterval(() => {\n    console.log('Pool stats:', {\n        total: pool.pool._allConnections.length,\n        active: pool.pool._allConnections.length - pool.pool._freeConnections.length,\n        idle: pool.pool._freeConnections.length\n    });\n}, 10000);\n\n// Java HikariCP\nHikariPoolMXBean poolMXBean = dataSource.getHikariPoolMXBean();\nSystem.out.println(\"Active: \" + poolMXBean.getActiveConnections());\nSystem.out.println(\"Idle: \" + poolMXBean.getIdleConnections());\nSystem.out.println(\"Total: \" + poolMXBean.getTotalConnections());\nSystem.out.println(\"Threads waiting: \" + poolMXBean.getThreadsAwaitingConnection());\n\n-- Connection leak detection\n// If connection not returned, logs warning after threshold\nconfig.setLeakDetectionThreshold(60000);  // 1 minute\n\n// Proper pattern to avoid leaks\ntry (Connection conn = pool.getConnection()) {\n    // Use connection\n}  // Auto-closed (returned to pool)"
    },
    {
      "id": 26,
      "question": "What are database cursors and when should they be used?",
      "answer": "Cursor: Database object to traverse result set row-by-row\n\nTypes:\n\nImplicit: Automatically created by DB\nExplicit: Declared by programmer\n\nServer-side: Executes on database server\nClient-side: Result set sent to client\n\nForward-only: One direction (fast)\nScrollable: Move in any direction (slow)\n\nRead-only: Cannot modify\nUpdatable: Can UPDATE/DELETE current row\n\nLifecycle:\n1. DECLARE: Create cursor\n2. OPEN: Execute query\n3. FETCH: Retrieve rows\n4. CLOSE: Release resources\n5. DEALLOCATE: Remove cursor\n\nUse cases:\n• Row-by-row processing required\n• Complex business logic per row\n• Cannot express in set-based SQL\n• Procedural operations\n\nDrawbacks:\n• Slow (row-by-row vs set-based)\n• Resource intensive\n• Locks held longer\n• Network overhead\n\nAlternatives:\n• Set-based operations (preferred)\n• Temp tables\n• CTEs\n• Window functions",
      "explanation": "Cursors enable row-by-row processing of query results but should be avoided when set-based operations can achieve the same result.",
      "difficulty": "Medium",
      "code": "-- SQL Server cursor example\nDECLARE @user_id INT, @balance DECIMAL(10,2);\n\n-- Declare cursor\nDECLARE user_cursor CURSOR FOR\nSELECT id, balance\nFROM accounts\nWHERE balance > 1000;\n\n-- Open cursor\nOPEN user_cursor;\n\n-- Fetch first row\nFETCH NEXT FROM user_cursor INTO @user_id, @balance;\n\n-- Loop through results\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    -- Process each row\n    IF @balance > 5000\n    BEGIN\n        UPDATE accounts\n        SET tier = 'PREMIUM'\n        WHERE id = @user_id;\n    END\n    ELSE\n    BEGIN\n        UPDATE accounts\n        SET tier = 'STANDARD'\n        WHERE id = @user_id;\n    END\n    \n    -- Fetch next row\n    FETCH NEXT FROM user_cursor INTO @user_id, @balance;\nEND\n\n-- Clean up\nCLOSE user_cursor;\nDEALLOCATE user_cursor;\n\n-- Better: Set-based approach (much faster)\nUPDATE accounts\nSET tier = CASE\n    WHEN balance > 5000 THEN 'PREMIUM'\n    ELSE 'STANDARD'\n    END\nWHERE balance > 1000;\n\n-- MySQL cursor in stored procedure\nDELIMITER //\n\nCREATE PROCEDURE process_orders()\nBEGIN\n    DECLARE done INT DEFAULT FALSE;\n    DECLARE v_order_id INT;\n    DECLARE v_total DECIMAL(10,2);\n    DECLARE v_discount DECIMAL(5,2);\n    \n    -- Declare cursor\n    DECLARE order_cursor CURSOR FOR\n        SELECT id, total\n        FROM orders\n        WHERE status = 'PENDING';\n    \n    -- Declare handler for end of cursor\n    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;\n    \n    OPEN order_cursor;\n    \n    read_loop: LOOP\n        FETCH order_cursor INTO v_order_id, v_total;\n        \n        IF done THEN\n            LEAVE read_loop;\n        END IF;\n        \n        -- Calculate discount based on total\n        SET v_discount = CASE\n            WHEN v_total > 1000 THEN 0.10\n            WHEN v_total > 500 THEN 0.05\n            ELSE 0.00\n        END;\n        \n        -- Apply discount\n        UPDATE orders\n        SET \n            discount = v_discount,\n            final_total = total * (1 - v_discount),\n            status = 'PROCESSED'\n        WHERE id = v_order_id;\n    END LOOP;\n    \n    CLOSE order_cursor;\nEND//\n\nDELIMITER ;\n\n-- PostgreSQL cursor\nDO $$\nDECLARE\n    rec RECORD;\n    cur CURSOR FOR\n        SELECT id, name, salary\n        FROM employees\n        WHERE department = 'Sales';\nBEGIN\n    OPEN cur;\n    \n    LOOP\n        FETCH cur INTO rec;\n        EXIT WHEN NOT FOUND;\n        \n        -- Process record\n        RAISE NOTICE 'Employee: %, Salary: %', rec.name, rec.salary;\n        \n        -- Update with 10% raise\n        UPDATE employees\n        SET salary = salary * 1.10\n        WHERE id = rec.id;\n    END LOOP;\n    \n    CLOSE cur;\nEND $$;\n\n-- Scrollable cursor (can move backwards)\nDECLARE @name VARCHAR(100);\n\nDECLARE user_scroll_cursor SCROLL CURSOR FOR\nSELECT name FROM users ORDER BY name;\n\nOPEN user_scroll_cursor;\n\n-- Fetch first\nFETCH FIRST FROM user_scroll_cursor INTO @name;\n\n-- Fetch last\nFETCH LAST FROM user_scroll_cursor INTO @name;\n\n-- Fetch previous\nFETCH PRIOR FROM user_scroll_cursor INTO @name;\n\n-- Fetch specific position\nFETCH ABSOLUTE 5 FROM user_scroll_cursor INTO @name;\n\n-- Fetch relative\nFETCH RELATIVE -2 FROM user_scroll_cursor INTO @name;\n\nCLOSE user_scroll_cursor;\nDEALLOCATE user_scroll_cursor;\n\n-- Updatable cursor\nDECLARE product_cursor CURSOR FOR\nSELECT id, price\nFROM products\nWHERE price < 10\nFOR UPDATE;  -- Makes cursor updatable\n\nOPEN product_cursor;\n\nFETCH NEXT FROM product_cursor INTO @product_id, @price;\n\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    -- Update current row\n    UPDATE products\n    SET price = price * 1.20\n    WHERE CURRENT OF product_cursor;  -- Updates current cursor position\n    \n    FETCH NEXT FROM product_cursor INTO @product_id, @price;\nEND\n\nCLOSE product_cursor;\nDEALLOCATE product_cursor;\n\n-- Performance comparison\n-- Cursor approach (slow): ~10 seconds for 100k rows\nDECLARE @id INT;\nDECLARE cursor1 CURSOR FOR SELECT id FROM large_table;\nOPEN cursor1;\nFETCH NEXT FROM cursor1 INTO @id;\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    UPDATE large_table SET processed = 1 WHERE id = @id;\n    FETCH NEXT FROM cursor1 INTO @id;\nEND\nCLOSE cursor1;\nDEALLOCATE cursor1;\n\n-- Set-based approach (fast): <1 second for 100k rows\nUPDATE large_table SET processed = 1;\n\n-- When cursor is actually needed\n-- Complex calculation that can't be expressed in SQL\nDECLARE interest_cursor CURSOR FOR\nSELECT id, balance, rate, compound_frequency\nFROM accounts;\n\nOPEN interest_cursor;\n\nfETCH NEXT FROM interest_cursor INTO @id, @balance, @rate, @frequency;\n\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    -- Complex compound interest calculation\n    DECLARE @new_balance DECIMAL(15,2);\n    SET @new_balance = @balance * POWER(1 + @rate / @frequency, @frequency);\n    \n    UPDATE accounts\n    SET balance = @new_balance,\n        last_interest_date = GETDATE()\n    WHERE id = @id;\n    \n    FETCH NEXT FROM interest_cursor INTO @id, @balance, @rate, @frequency;\nEND\n\nCLOSE interest_cursor;\nDEALLOCATE interest_cursor;"
    },
    {
      "id": 27,
      "question": "What is query caching and how does it work?",
      "answer": "Query Caching: Store query results for reuse\n\nTypes:\n\n1. Query Result Cache:\n• Cache entire result set\n• Key: SQL query text\n• Invalidated on data changes\n• MySQL Query Cache (deprecated in 8.0)\n\n2. Buffer Pool/Page Cache:\n• Cache data pages in memory\n• InnoDB Buffer Pool\n• PostgreSQL Shared Buffers\n• Reduces disk I/O\n\n3. Prepared Statement Cache:\n• Cache execution plans\n• Reuse for similar queries\n• Faster execution\n\n4. Application-level Cache:\n• Redis, Memcached\n• Cache results in app\n• Full control over invalidation\n\nBenefits:\n• Faster response times\n• Reduced CPU usage\n• Lower database load\n• Better scalability\n\nChallenges:\n• Cache invalidation\n• Stale data\n• Memory usage\n• Hit rate optimization\n\nBest practices:\n• Cache read-heavy queries\n• Set appropriate TTL\n• Implement cache warming\n• Monitor hit rates\n• Handle cache misses gracefully",
      "explanation": "Query caching stores query results in memory to avoid expensive database operations for repeated queries.",
      "difficulty": "Medium",
      "code": "-- MySQL Query Cache (deprecated in 8.0)\nSHOW VARIABLES LIKE 'query_cache%';\n\n-- Enable query cache\nSET GLOBAL query_cache_type = ON;\nSET GLOBAL query_cache_size = 1048576;  -- 1MB\n\n-- Check cache status\nSHOW STATUS LIKE 'Qcache%';\n-- Qcache_hits: Queries served from cache\n-- Qcache_inserts: Queries added to cache\n-- Qcache_not_cached: Non-cacheable queries\n\n-- Queries that aren't cached:\n-- - Non-deterministic functions (NOW(), RAND())\n-- - User-defined functions\n-- - Stored procedures\n-- - Queries with certain keywords\n\nSELECT * FROM users;  -- Can be cached\nSELECT *, NOW() FROM users;  -- NOT cached (NOW() is non-deterministic)\n\n-- Force no caching for specific query\nSELECT SQL_NO_CACHE * FROM users WHERE id = 1;\n\n-- Application-level caching with Redis\nconst Redis = require('ioredis');\nconst redis = new Redis();\n\nasync function getUser(userId) {\n    const cacheKey = `user:${userId}`;\n    \n    // Try cache first\n    let user = await redis.get(cacheKey);\n    \n    if (user) {\n        console.log('Cache hit');\n        return JSON.parse(user);\n    }\n    \n    console.log('Cache miss');\n    // Query database\n    const [rows] = await db.query('SELECT * FROM users WHERE id = ?', [userId]);\n    user = rows[0];\n    \n    if (user) {\n        // Store in cache (TTL: 1 hour)\n        await redis.setex(cacheKey, 3600, JSON.stringify(user));\n    }\n    \n    return user;\n}\n\n-- Cache-aside pattern\nasync function getProduct(productId) {\n    const key = `product:${productId}`;\n    \n    // 1. Check cache\n    let product = await cache.get(key);\n    if (product) return product;\n    \n    // 2. Query database\n    product = await db.query(\n        'SELECT * FROM products WHERE id = ?',\n        [productId]\n    );\n    \n    // 3. Update cache\n    await cache.set(key, product, { ttl: 3600 });\n    \n    return product;\n}\n\n-- Read-through cache\nasync function getUserWithReadThrough(userId) {\n    const key = `user:${userId}`;\n    \n    return await cache.wrap(key, async () => {\n        // This function only called on cache miss\n        const [rows] = await db.query(\n            'SELECT * FROM users WHERE id = ?',\n            [userId]\n        );\n        return rows[0];\n    }, { ttl: 3600 });\n}\n\n-- Write-through cache (update cache on write)\nasync function updateUser(userId, data) {\n    // 1. Update database\n    await db.query(\n        'UPDATE users SET name = ?, email = ? WHERE id = ?',\n        [data.name, data.email, userId]\n    );\n    \n    // 2. Update cache\n    const key = `user:${userId}`;\n    await cache.set(key, data, { ttl: 3600 });\n    \n    return data;\n}\n\n-- Write-behind/Write-back cache (async write)\nconst writeQueue = [];\n\nasync function updateUserAsync(userId, data) {\n    const key = `user:${userId}`;\n    \n    // 1. Update cache immediately\n    await cache.set(key, data, { ttl: 3600 });\n    \n    // 2. Queue database write\n    writeQueue.push({ userId, data });\n    \n    // 3. Background process handles writes\n    return data;\n}\n\nsetInterval(async () => {\n    while (writeQueue.length > 0) {\n        const { userId, data } = writeQueue.shift();\n        await db.query(\n            'UPDATE users SET name = ?, email = ? WHERE id = ?',\n            [data.name, data.email, userId]\n        );\n    }\n}, 1000);  // Flush every second\n\n-- Cache invalidation strategies\n\n-- 1. TTL-based (Time To Live)\nawait cache.set('key', value, { ttl: 300 });  // 5 minutes\n\n-- 2. Explicit invalidation\nasync function deleteUser(userId) {\n    // Delete from database\n    await db.query('DELETE FROM users WHERE id = ?', [userId]);\n    \n    // Invalidate cache\n    await cache.del(`user:${userId}`);\n}\n\n-- 3. Tag-based invalidation\nawait cache.set('user:1', data, { tags: ['users', 'premium'] });\nawait cache.set('user:2', data, { tags: ['users'] });\n\n// Invalidate all users\nawait cache.invalidateTags(['users']);\n\n-- 4. Version-based invalidation\nconst version = await cache.get('users:version') || 0;\nconst key = `users:list:v${version}`;\n\nlet users = await cache.get(key);\nif (!users) {\n    users = await db.query('SELECT * FROM users');\n    await cache.set(key, users);\n}\n\n// On update: increment version\nasync function updateUsers() {\n    await db.query('UPDATE users SET ...');\n    await cache.incr('users:version');  // Invalidates all caches\n}\n\n-- Cache warming (preload cache)\nasync function warmCache() {\n    console.log('Warming cache...');\n    \n    // Load popular products\n    const products = await db.query(\n        'SELECT * FROM products ORDER BY views DESC LIMIT 100'\n    );\n    \n    for (const product of products) {\n        await cache.set(\n            `product:${product.id}`,\n            product,\n            { ttl: 3600 }\n        );\n    }\n    \n    console.log('Cache warmed');\n}\n\n-- Monitor cache performance\nasync function getCacheStats() {\n    const info = await redis.info('stats');\n    const lines = info.split('\\r\\n');\n    const stats = {};\n    \n    lines.forEach(line => {\n        const [key, value] = line.split(':');\n        if (key && value) stats[key] = value;\n    });\n    \n    const hitRate = stats.keyspace_hits / \n        (stats.keyspace_hits + stats.keyspace_misses);\n    \n    return {\n        hits: stats.keyspace_hits,\n        misses: stats.keyspace_misses,\n        hitRate: (hitRate * 100).toFixed(2) + '%'\n    };\n}\n\n-- Multi-level caching\nclass CacheManager {\n    constructor() {\n        this.l1Cache = new Map();  // In-memory (fast, small)\n        this.l2Cache = redis;       // Redis (medium, larger)\n    }\n    \n    async get(key) {\n        // L1 cache (fastest)\n        if (this.l1Cache.has(key)) {\n            return this.l1Cache.get(key);\n        }\n        \n        // L2 cache (Redis)\n        const value = await this.l2Cache.get(key);\n        if (value) {\n            // Promote to L1\n            this.l1Cache.set(key, JSON.parse(value));\n            return JSON.parse(value);\n        }\n        \n        return null;\n    }\n    \n    async set(key, value, ttl) {\n        // Set in both caches\n        this.l1Cache.set(key, value);\n        await this.l2Cache.setex(key, ttl, JSON.stringify(value));\n    }\n}"
    },
    {
      "id": 28,
      "question": "What are SQL injection attacks and how to prevent them?",
      "answer": "SQL Injection: Malicious SQL code injected through user input\n\nHow it works:\n• Attacker inputs SQL syntax\n• Poorly constructed queries execute it\n• Bypass authentication/access unauthorized data\n• Modify/delete data\n• Execute admin operations\n\nAttack examples:\n• Authentication bypass: `' OR '1'='1`\n• Union attacks: `' UNION SELECT * FROM users--`\n• Blind SQL injection: Time-based, boolean-based\n• Second-order injection: Store and execute later\n\nPrevention methods:\n\n1. Prepared Statements/Parameterized Queries:\n• Separate SQL code from data\n• Database handles escaping\n• Most effective defense\n\n2. Stored Procedures:\n• Pre-compiled SQL\n• Limit direct SQL execution\n\n3. Input Validation:\n• Whitelist acceptable input\n• Reject suspicious patterns\n• Type checking\n\n4. Escape User Input:\n• Last resort\n• Database-specific escaping\n\n5. Least Privilege:\n• Limited database permissions\n• Separate accounts per function\n\n6. WAF: Web Application Firewall",
      "explanation": "SQL injection exploits poor input handling to execute malicious SQL. Parameterized queries are the primary defense.",
      "difficulty": "Hard",
      "code": "-- VULNERABLE: String concatenation\n-- User input: ' OR '1'='1\nconst username = req.body.username;  // ' OR '1'='1\nconst password = req.body.password;\n\nconst query = \"SELECT * FROM users WHERE username = '\" + username + \n              \"' AND password = '\" + password + \"'\";\n// Results in: SELECT * FROM users WHERE username = '' OR '1'='1' AND password = ''\n// Always true! Returns all users.\n\n-- ATTACK: Delete data\n-- Input: '; DROP TABLE users--\nconst query = \"SELECT * FROM products WHERE id = \" + productId;\n// Results in: SELECT * FROM products WHERE id = 1; DROP TABLE users--\n\n-- ATTACK: Union-based data extraction\n-- Input: ' UNION SELECT username, password, NULL FROM users--\nconst query = \"SELECT name, price, description FROM products WHERE id = '\" + id + \"'\";\n// Extracts user credentials\n\n-- SAFE: Parameterized queries (Node.js)\nconst username = req.body.username;\nconst password = req.body.password;\n\n// Placeholder prevents SQL injection\nconst query = 'SELECT * FROM users WHERE username = ? AND password = ?';\nconst [rows] = await db.query(query, [username, password]);\n// Database properly escapes values\n\n-- SAFE: Prepared statements (Java)\nString sql = \"SELECT * FROM users WHERE username = ? AND password = ?\";\nPreparedStatement stmt = connection.prepareStatement(sql);\nstmt.setString(1, username);  // Automatically escaped\nstmt.setString(2, password);\nResultSet rs = stmt.executeQuery();\n\n-- SAFE: Named parameters (C#)\nstring sql = \"SELECT * FROM users WHERE username = @username AND password = @password\";\nusing (SqlCommand cmd = new SqlCommand(sql, connection))\n{\n    cmd.Parameters.AddWithValue(\"@username\", username);\n    cmd.Parameters.AddWithValue(\"@password\", password);\n    SqlDataReader reader = cmd.ExecuteReader();\n}\n\n-- SAFE: Parameterized queries (Python)\nimport psycopg2\n\nsql = \"SELECT * FROM users WHERE username = %s AND password = %s\"\ncursor.execute(sql, (username, password))\n\n-- SAFE: ORM (prevents SQL injection)\n// Sequelize (Node.js)\nconst user = await User.findOne({\n    where: {\n        username: username,\n        password: password\n    }\n});\n\n// Entity Framework (C#)\nvar user = dbContext.Users\n    .Where(u => u.Username == username && u.Password == password)\n    .FirstOrDefault();\n\n-- Input validation\nfunction validateUsername(username) {\n    // Only alphanumeric and underscore\n    if (!/^[a-zA-Z0-9_]{3,20}$/.test(username)) {\n        throw new Error('Invalid username format');\n    }\n    return username;\n}\n\nfunction validateId(id) {\n    const numId = parseInt(id, 10);\n    if (isNaN(numId) || numId < 1) {\n        throw new Error('Invalid ID');\n    }\n    return numId;\n}\n\n-- Whitelist validation\nfunction validateSortField(field) {\n    const allowedFields = ['name', 'price', 'date'];\n    if (!allowedFields.includes(field)) {\n        throw new Error('Invalid sort field');\n    }\n    return field;\n}\n\nconst sortField = validateSortField(req.query.sort);\nconst query = `SELECT * FROM products ORDER BY ${sortField}`;\n// Safe because field is from whitelist\n\n-- Stored procedures (safer, but not immune)\nCREATE PROCEDURE GetUser\n    @Username NVARCHAR(50),\n    @Password NVARCHAR(50)\nAS\nBEGIN\n    SELECT * FROM users\n    WHERE username = @Username AND password = @Password;\nEND;\n\n-- Call stored procedure\nusing (SqlCommand cmd = new SqlCommand(\"GetUser\", connection))\n{\n    cmd.CommandType = CommandType.StoredProcedure;\n    cmd.Parameters.AddWithValue(\"@Username\", username);\n    cmd.Parameters.AddWithValue(\"@Password\", password);\n    SqlDataReader reader = cmd.ExecuteReader();\n}\n\n-- Escaping (last resort, prefer parameterized queries)\nconst mysql = require('mysql2');\nconst escaped = mysql.escape(userInput);\nconst query = `SELECT * FROM users WHERE username = ${escaped}`;\n\n-- Least privilege principle\n-- Create limited user for application\nCREATE USER 'app_user'@'localhost' IDENTIFIED BY 'password';\n\n-- Grant only necessary permissions\nGRANT SELECT, INSERT, UPDATE ON mydb.users TO 'app_user'@'localhost';\nGRANT SELECT, INSERT ON mydb.orders TO 'app_user'@'localhost';\n-- NO DELETE, DROP, or admin privileges\n\n-- Separate users for different functions\nCREATE USER 'app_read'@'localhost' IDENTIFIED BY 'password';\nGRANT SELECT ON mydb.* TO 'app_read'@'localhost';\n\nCREATE USER 'app_write'@'localhost' IDENTIFIED BY 'password';\nGRANT SELECT, INSERT, UPDATE ON mydb.* TO 'app_write'@'localhost';\n\n-- Detect SQL injection attempts\nfunction detectSQLInjection(input) {\n    const suspiciousPatterns = [\n        /('\\s*(OR|AND)\\s*'\\s*=\\s*')/i,  -- ' OR '1'='1\n        /(;\\s*(DROP|DELETE|UPDATE|INSERT))/i,  -- ; DROP TABLE\n        /(UNION\\s+SELECT)/i,\n        /(--)|(#)|(\\*\\/)/,  -- Comments\n        /(xp_cmdshell|sp_executesql)/i  -- System procedures\n    ];\n    \n    for (const pattern of suspiciousPatterns) {\n        if (pattern.test(input)) {\n            console.warn('SQL injection attempt detected:', input);\n            return true;\n        }\n    }\n    return false;\n}\n\nfunction secureEndpoint(req, res) {\n    const username = req.body.username;\n    \n    if (detectSQLInjection(username)) {\n        return res.status(400).json({ error: 'Invalid input' });\n    }\n    \n    // Process request\n}\n\n-- Testing for SQL injection vulnerabilities\n-- Test inputs:\n' OR '1'='1\n' OR '1'='1' --\n' OR '1'='1' /*\n'; DROP TABLE users--\n1' UNION SELECT NULL, username, password FROM users--\n1' AND 1=0 UNION SELECT NULL, table_name FROM information_schema.tables--\nadmin'--\nadmin' #\nadmin'/*"
    },
    {
      "id": 29,
      "question": "What is database backup and recovery?",
      "answer": "Backup: Copy of data for recovery after failure\n\nBackup Types:\n\n1. Full Backup:\n• Complete database copy\n• Largest size, slowest\n• Simple restoration\n• Base for incremental/differential\n\n2. Incremental Backup:\n• Changes since last backup (any type)\n• Smallest size, fastest\n• Requires full + all incrementals to restore\n• Complex restoration\n\n3. Differential Backup:\n• Changes since last full backup\n• Medium size\n• Requires full + latest differential\n• Moderate restoration complexity\n\nBackup Methods:\n• Physical: Copy database files\n• Logical: Export SQL statements (mysqldump)\n• Hot/Online: During operation\n• Cold/Offline: Database stopped\n• Warm: Read-only mode\n\nRecovery Types:\n• Crash recovery: From unexpected failure\n• Point-in-time recovery (PITR): Restore to specific time\n• Disaster recovery: Site failure\n\nRTO: Recovery Time Objective - Max downtime\nRPO: Recovery Point Objective - Max data loss\n\nBest practices:\n• Regular automated backups\n• Test restores regularly  \n• Off-site/cloud storage\n• Encrypt backups\n• Monitor backup success\n• Document procedures",
      "explanation": "Database backup and recovery ensure data can be restored after failures, with different strategies balancing speed, size, and complexity.",
      "difficulty": "Hard",
      "code": "-- MySQL Full Backup (logical)\n-- Using mysqldump\nmysqldump -u root -p --all-databases > full_backup.sql\n\n-- Backup specific database\nmysqldump -u root -p mydatabase > mydatabase_backup.sql\n\n-- Backup with compression\nmysqldump -u root -p mydatabase | gzip > mydatabase_backup.sql.gz\n\n-- Backup specific tables\nmysqldump -u root -p mydatabase users orders > tables_backup.sql\n\n-- Consistent backup (locks tables)\nmysqldump -u root -p --single-transaction mydatabase > backup.sql\n\n-- Restore from mysqldump\nmysql -u root -p mydatabase < mydatabase_backup.sql\n\n-- MySQL Physical Backup (faster for large DBs)\n-- Copy data directory (requires shutdown or lock)\ncp -r /var/lib/mysql /backup/mysql_backup\n\n-- Using Percona XtraBackup (hot backup)\nxtrabackup --backup --target-dir=/backup/full_backup\n\n-- Prepare backup for restore\nxtrabackup --prepare --target-dir=/backup/full_backup\n\n-- Restore\nxtrabackup --copy-back --target-dir=/backup/full_backup\n\n-- PostgreSQL Backup\n-- Logical backup\npg_dump -U postgres mydatabase > mydatabase_backup.sql\n\n-- All databases\npg_dumpall -U postgres > all_databases.sql\n\n-- Compressed backup\npg_dump -U postgres -Fc mydatabase > mydatabase_backup.dump\n\n-- Restore\npg_restore -U postgres -d mydatabase mydatabase_backup.dump\n\n-- PostgreSQL Physical Backup\n-- Base backup (hot)\npg_basebackup -U postgres -D /backup/base_backup -Ft -z -P\n\n-- Point-in-time Recovery (PITR)\n-- Enable WAL archiving (postgresql.conf)\nwal_level = replica\narchive_mode = on\narchive_command = 'cp %p /backup/wal/%f'\n\n-- Create base backup\npg_basebackup -U postgres -D /backup/base_backup -Fp\n\n-- Restore to specific time\n-- 1. Stop PostgreSQL\n-- 2. Replace data directory with backup\nrm -rf /var/lib/postgresql/data\ncp -r /backup/base_backup /var/lib/postgresql/data\n\n-- 3. Create recovery.conf\nrestore_command = 'cp /backup/wal/%f %p'\nrecovery_target_time = '2024-01-15 10:30:00'\n\n-- 4. Start PostgreSQL\n\n-- Automated backup script\n#!/bin/bash\n\n# Configuration\nDB_USER=\"root\"\nDB_PASS=\"password\"\nDB_NAME=\"mydatabase\"\nBACKUP_DIR=\"/backup/mysql\"\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_FILE=\"${BACKUP_DIR}/${DB_NAME}_${DATE}.sql.gz\"\n\n# Retention (keep 7 days)\nRETENTION_DAYS=7\n\n# Create backup directory\nmkdir -p ${BACKUP_DIR}\n\n# Perform backup\necho \"Starting backup at $(date)\"\nmysqldump -u ${DB_USER} -p${DB_PASS} ${DB_NAME} | gzip > ${BACKUP_FILE}\n\n# Check if successful\nif [ $? -eq 0 ]; then\n    echo \"Backup successful: ${BACKUP_FILE}\"\n    \n    # Delete old backups\n    find ${BACKUP_DIR} -name \"${DB_NAME}_*.sql.gz\" -mtime +${RETENTION_DAYS} -delete\n    echo \"Old backups cleaned up\"\n    \n    # Upload to cloud (optional)\n    aws s3 cp ${BACKUP_FILE} s3://mybucket/backups/\n    \n    # Send notification\n    echo \"Backup completed successfully\" | mail -s \"Database Backup Success\" admin@example.com\nelse\n    echo \"Backup failed!\"\n    echo \"Backup failed for ${DB_NAME}\" | mail -s \"Database Backup FAILED\" admin@example.com\n    exit 1\nfi\n\n-- Schedule with cron\n-- crontab -e\n-- Daily backup at 2 AM\n0 2 * * * /path/to/backup_script.sh\n\n-- Incremental backup strategy\n-- Full backup on Sunday\n0 2 * * 0 /scripts/full_backup.sh\n\n-- Incremental backups Monday-Saturday\n0 2 * * 1-6 /scripts/incremental_backup.sh\n\n-- Backup verification script\n#!/bin/bash\n\nBACKUP_FILE=\"$1\"\n\n# Create test database\nmysql -u root -p -e \"CREATE DATABASE IF NOT EXISTS test_restore;\"\n\n# Restore backup to test database\nzcat ${BACKUP_FILE} | mysql -u root -p test_restore\n\nif [ $? -eq 0 ]; then\n    echo \"Backup validation successful\"\n    \n    # Run integrity checks\n    mysqlcheck -u root -p test_restore --all-databases\n    \n    # Drop test database\n    mysql -u root -p -e \"DROP DATABASE test_restore;\"\nelse\n    echo \"Backup validation FAILED!\"\n    exit 1\nfi\n\n-- SQL Server Backup\n-- Full backup\nBACKUP DATABASE mydatabase\nTO DISK = 'C:\\Backups\\mydatabase_full.bak'\nWITH FORMAT, COMPRESSION;\n\n-- Differential backup\nBACKUP DATABASE mydatabase\nTO DISK = 'C:\\Backups\\mydatabase_diff.bak'\nWITH DIFFERENTIAL, COMPRESSION;\n\n-- Transaction log backup\nBACKUP LOG mydatabase\nTO DISK = 'C:\\Backups\\mydatabase_log.trn';\n\n-- Restore sequence\n-- 1. Restore full backup\nRESTORE DATABASE mydatabase\nFROM DISK = 'C:\\Backups\\mydatabase_full.bak'\nWITH NORECOVERY;\n\n-- 2. Restore differential\nRESTORE DATABASE mydatabase\nFROM DISK = 'C:\\Backups\\mydatabase_diff.bak'\nWITH NORECOVERY;\n\n-- 3. Restore transaction logs\nRESTORE LOG mydatabase\nFROM DISK = 'C:\\Backups\\mydatabase_log.trn'\nWITH RECOVERY;\n\n-- Point-in-time restore\nRESTORE LOG mydatabase\nFROM DISK = 'C:\\Backups\\mydatabase_log.trn'\nWITH RECOVERY, STOPAT = '2024-01-15 10:30:00';\n\n-- Disaster recovery test procedure\n-- 1. Simulate failure\n-- 2. Restore from backup\n-- 3. Verify data integrity\n-- 4. Check application functionality\n-- 5. Measure recovery time\n-- 6. Document issues\n-- 7. Update procedures\n\n-- Monitor backup status\nSELECT \n    database_name,\n    backup_start_date,\n    backup_finish_date,\n    DATEDIFF(SECOND, backup_start_date, backup_finish_date) as duration_seconds,\n    backup_size / 1024 / 1024 as size_mb,\n    type\nFROM msdb.dbo.backupset\nWHERE backup_start_date > DATEADD(DAY, -7, GETDATE())\nORDER BY backup_start_date DESC;"
    },
    {
      "id": 30,
      "question": "What are database statistics and how do they affect query performance?",
      "answer": "Statistics: Metadata about data distribution in tables/indexes\n\nWhat statistics track:\n• Row counts in tables\n• Distinct value counts (cardinality)\n• Data distribution (histograms)\n• Index selectivity\n• NULL counts\n• Average row size\n\nHow used:\n• Query optimizer makes decisions\n• Chooses indexes to use\n• Determines JOIN order\n• Estimates costs\n• Selects execution plans\n\nWhen statistics are stale:\n• Poor execution plans chosen\n• Full table scans instead of indexes\n• Inefficient JOIN orders\n• Slow queries\n• Unpredictable performance\n\nMaintaining statistics:\n\nAuto-update:\n• Database automatically updates\n• After significant changes\n• Threshold-based triggers\n\nManual update:\n• After bulk operations\n• Before complex queries\n• During maintenance windows\n\nBest practices:\n• Keep statistics current\n• Update after bulk loads\n• Monitor stale statistics\n• Consider sampling rate\n• Update before important queries",
      "explanation": "Database statistics help the query optimizer make informed decisions about execution plans by providing data distribution information.",
      "difficulty": "Hard",
      "code": "-- MySQL: View statistics\nSHOW INDEX FROM users;\n-- Shows: Cardinality (estimate of distinct values)\n\nSHOW TABLE STATUS LIKE 'users';\n-- Shows: Rows, Avg_row_length, Data_length\n\n-- Update statistics manually\nANALYZE TABLE users;\nANALYZE TABLE orders, products;  -- Multiple tables\n\n-- View column statistics (MySQL 8.0+)\nSELECT \n    table_name,\n    column_name,\n    histogram\nFROM information_schema.COLUMN_STATISTICS;\n\n-- Create histogram for better statistics\nANALYZE TABLE users UPDATE HISTOGRAM ON age, country;\n\n-- Drop histogram\nANALYZE TABLE users DROP HISTOGRAM ON age;\n\n-- PostgreSQL: View statistics\nSELECT \n    schemaname,\n    tablename,\n    n_live_tup,     -- Estimated row count\n    n_dead_tup,     -- Dead tuples\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables\nWHERE tablename = 'users';\n\n-- View column statistics\nSELECT \n    tablename,\n    attname,\n    n_distinct,     -- Distinct values (-1 = unique, positive = estimate, negative = fraction)\n    correlation     -- Physical/logical order correlation (-1 to 1)\nFROM pg_stats\nWHERE tablename = 'users';\n\n-- Update statistics\nANALYZE users;\nANALYZE;  -- All tables\n\n-- Verbose output\nANALYZE VERBOSE users;\n\n-- Set statistics target (more accurate but slower)\nALTER TABLE users ALTER COLUMN email SET STATISTICS 1000;\n-- Default is 100, max is 10000\n\n-- SQL Server: View statistics\nDBCC SHOW_STATISTICS('users', 'idx_email');\n-- Shows:\n-- - Header: Rows, rows sampled, steps, density\n-- - Density vector\n-- - Histogram\n\n-- View all statistics for table\nSELECT \n    s.name AS stats_name,\n    s.auto_created,\n    s.user_created,\n    s.no_recompute,\n    sp.last_updated,\n    sp.rows,\n    sp.rows_sampled,\n    sp.modification_counter  -- Changes since last update\nFROM sys.stats s\nJOIN sys.stats_properties sp ON s.object_id = sp.object_id AND s.stats_id = sp.stats_id\nWHERE s.object_id = OBJECT_ID('users');\n\n-- Update statistics\nUPDATE STATISTICS users;\n\n-- Update with full scan (more accurate)\nUPDATE STATISTICS users WITH FULLSCAN;\n\n-- Update specific index statistics\nUPDATE STATISTICS users idx_email;\n\n-- Update with sample percentage\nUPDATE STATISTICS users WITH SAMPLE 50 PERCENT;\n\n-- Auto-update statistics (database level)\nALTER DATABASE [mydatabase] SET AUTO_UPDATE_STATISTICS ON;\nALTER DATABASE [mydatabase] SET AUTO_UPDATE_STATISTICS_ASYNC ON;\n\n-- Create statistics manually\nCREATE STATISTICS stat_user_age ON users(age);\nCREATE STATISTICS stat_user_age_country ON users(age, country);  -- Multi-column\n\n-- Drop statistics\nDROP STATISTICS users.stat_user_age;\n\n-- Example: Impact of stale statistics\n-- Table has 1 million rows\n-- Statistics show 1000 rows (outdated)\n-- Query: SELECT * FROM users WHERE status = 'active';\n\n-- With stale stats:\n-- Optimizer thinks: 1000 rows total, maybe 100 active\n-- Chooses: Table scan (thinks it's small)\n-- Actual: 900,000 active users, very slow!\n\n-- After updating statistics:\nANALYZE TABLE users;\n-- Optimizer knows: 1 million rows, 900,000 active\n-- Chooses: Index seek (if index exists)\n-- Much faster!\n\n-- Monitor statistics age\n-- MySQL\nSELECT \n    table_name,\n    update_time,\n    TIMESTAMPDIFF(DAY, update_time, NOW()) as days_old\nFROM information_schema.tables\nWHERE table_schema = 'mydatabase'\n  AND update_time IS NOT NULL\nORDER BY days_old DESC;\n\n-- PostgreSQL\nSELECT \n    schemaname,\n    tablename,\n    last_analyze,\n    NOW() - last_analyze as time_since_analyze,\n    n_mod_since_analyze  -- Modifications since last analyze\nFROM pg_stat_user_tables\nWHERE n_mod_since_analyze > 10000  -- Many changes\n   OR last_analyze < NOW() - INTERVAL '7 days'  -- Old statistics\nORDER BY n_mod_since_analyze DESC;\n\n-- SQL Server: Find stale statistics\nSELECT \n    OBJECT_NAME(s.object_id) AS table_name,\n    s.name AS stats_name,\n    sp.last_updated,\n    sp.rows,\n    sp.rows_sampled,\n    sp.modification_counter,\n    sp.modification_counter * 100.0 / sp.rows AS pct_modified\nFROM sys.stats s\nJOIN sys.stats_properties sp ON s.object_id = sp.object_id AND s.stats_id = sp.stats_id\nWHERE sp.modification_counter > 10000  -- Many changes\n   OR sp.modification_counter * 100.0 / sp.rows > 20  -- >20% changed\nORDER BY sp.modification_counter DESC;\n\n-- Automated statistics maintenance\n-- Script to update statistics for busy tables\nDECLARE @tableName NVARCHAR(128);\n\nDECLARE tableCursor CURSOR FOR\nSELECT DISTINCT OBJECT_NAME(s.object_id)\nFROM sys.stats s\nJOIN sys.stats_properties sp ON s.object_id = sp.object_id AND s.stats_id = sp.stats_id\nWHERE sp.modification_counter * 100.0 / NULLIF(sp.rows, 0) > 10;\n\nOPEN tableCursor;\nFETCH NEXT FROM tableCursor INTO @tableName;\n\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    PRINT 'Updating statistics for ' + @tableName;\n    EXEC('UPDATE STATISTICS ' + @tableName + ' WITH FULLSCAN');\n    FETCH NEXT FROM tableCursor INTO @tableName;\nEND\n\nCLOSE tableCursor;\nDEALLOCATE tableCursor;\n\n-- PostgreSQL: Auto-vacuum settings (includes auto-analyze)\n-- postgresql.conf\nautovacuum = on\nautovacuum_analyze_scale_factor = 0.1  -- Analyze after 10% changes\nautovacuum_analyze_threshold = 50      -- Min 50 row changes\n\n-- Per-table settings\nALTER TABLE users SET (\n    autovacuum_analyze_scale_factor = 0.05,  -- More frequent\n    autovacuum_analyze_threshold = 100\n);\n\n-- Schedule statistics updates (cron)\n-- Daily at 3 AM\n0 3 * * * psql -d mydatabase -c \"ANALYZE;\" >> /var/log/pgsql/analyze.log 2>&1"
    },
    {
      "id": 31,
      "question": "What are database transactions and ACID properties in detail?",
      "answer": "Transaction: Unit of work that must complete entirely or not at all\n\nACID in depth:\n\nAtomicity:\n• All-or-nothing execution\n• If any operation fails, entire transaction rolls back\n• Implemented via: Write-Ahead Log (WAL), undo logs\n• Example: Bank transfer - debit and credit both succeed or both fail\n\nConsistency:\n• Database moves from one valid state to another\n• All constraints remain satisfied\n• Triggers and cascades execute\n• Example: Total money in system unchanged after transfer\n\nIsolation:\n• Concurrent transactions don't interfere\n• Each transaction sees consistent snapshot\n• Levels: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE\n• Example: Two people can't book same seat\n\nDurability:\n• Committed changes persist (even after crash)\n• Written to non-volatile storage\n• Implemented via: WAL, fsync, replication\n• Example: Payment recorded even if server crashes after commit\n\nTransaction states:\n• Active: Executing\n• Partially Committed: After final operation\n• Committed: Successfully completed\n• Failed: Cannot proceed\n• Aborted: Rolled back",
      "explanation": "ACID properties ensure database transactions are processed reliably, maintaining data integrity even during failures and concurrent access.",
      "difficulty": "Hard",
      "code": "-- Transaction basics\nSTART TRANSACTION;  -- or BEGIN\n\nINSERT INTO accounts (user_id, balance) VALUES (1, 1000);\nUPDATE accounts SET balance = balance - 100 WHERE user_id = 1;\nINSERT INTO transactions (from_account, amount) VALUES (1, 100);\n\nCOMMIT;  -- Make permanent\n-- or ROLLBACK;  -- Undo changes\n\n-- Atomicity example: Bank transfer\nSTART TRANSACTION;\n\n-- Debit from account A\nUPDATE accounts\nSET balance = balance - 500\nWHERE account_id = 1 AND balance >= 500;\n\n-- Check if debit successful\nIF ROW_COUNT() = 0 THEN\n    ROLLBACK;  -- Insufficient funds\n    SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Insufficient funds';\nEND IF;\n\n-- Credit to account B\nUPDATE accounts\nSET balance = balance + 500\nWHERE account_id = 2;\n\n-- Record transaction\nINSERT INTO transaction_log (from_account, to_account, amount, timestamp)\nVALUES (1, 2, 500, NOW());\n\nCOMMIT;  -- Either all succeed or all fail\n\n-- Consistency example: Maintaining invariants\nSTART TRANSACTION;\n\n-- Check constraint will be validated\nINSERT INTO inventory (product_id, quantity)\nVALUES (1, -5);  -- Fails if CHECK (quantity >= 0) exists\n\n-- Foreign key maintained\nINSERT INTO order_items (order_id, product_id)\nVALUES (999, 1);  -- Fails if order 999 doesn't exist\n\nCOMMIT;\n-- If any constraint violated, transaction aborts\n\n-- Isolation example: Preventing dirty reads\n-- Transaction 1:\nSTART TRANSACTION;\nUPDATE accounts SET balance = 5000 WHERE account_id = 1;\n-- Transaction not yet committed\nSLEEP(10);\nROLLBACK;  -- Oops, mistake\n\n-- Transaction 2 (concurrent):\nSTART TRANSACTION;\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;  -- Won't see uncommitted changes\nSELECT balance FROM accounts WHERE account_id = 1;\n-- Returns original balance, not 5000\nCOMMIT;\n\n-- Durability example: Crash recovery\nSTART TRANSACTION;\nINSERT INTO orders (user_id, total) VALUES (1, 100);\nCOMMIT;\n-- Server crashes here\n-- After restart, order still exists (written to disk)\n\n-- Isolation levels demonstration\n\n-- READ UNCOMMITTED (Dirty reads possible)\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nSTART TRANSACTION;\nSELECT * FROM accounts WHERE account_id = 1;\n-- May see uncommitted changes from other transactions\nCOMMIT;\n\n-- READ COMMITTED (Default in most DBs)\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nSTART TRANSACTION;\nSELECT * FROM accounts WHERE account_id = 1;  -- balance = 1000\n-- Another transaction commits: UPDATE accounts SET balance = 2000 WHERE account_id = 1\nSELECT * FROM accounts WHERE account_id = 1;  -- balance = 2000 (non-repeatable read)\nCOMMIT;\n\n-- REPEATABLE READ (Prevents non-repeatable reads)\nSET TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nSTART TRANSACTION;\nSELECT * FROM accounts WHERE account_id = 1;  -- balance = 1000\n-- Another transaction commits: UPDATE accounts SET balance = 2000 WHERE account_id = 1\nSELECT * FROM accounts WHERE account_id = 1;  -- balance = 1000 (consistent snapshot)\nCOMMIT;\n\n-- SERIALIZABLE (Strictest)\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nSTART TRANSACTION;\nSELECT COUNT(*) FROM accounts WHERE balance > 1000;  -- Result: 5\n-- Another transaction commits: INSERT INTO accounts VALUES (10, 1500)\nSELECT COUNT(*) FROM accounts WHERE balance > 1000;  -- Result: 5 (prevents phantom reads)\nCOMMIT;\n\n-- Savepoints: Partial rollback\nSTART TRANSACTION;\n\nINSERT INTO users (name) VALUES ('Alice');\nSAVEPOINT user_inserted;\n\nINSERT INTO accounts (user_id, balance) VALUES (LAST_INSERT_ID(), 1000);\nSAVEPOINT account_inserted;\n\nINSERT INTO transactions (account_id, amount) VALUES (LAST_INSERT_ID(), 100);\n-- Oops, error\n\nROLLBACK TO SAVEPOINT account_inserted;  -- Keep user, undo account and transaction\n-- or: ROLLBACK TO SAVEPOINT user_inserted;  -- Undo everything except user\n-- or: ROLLBACK;  -- Undo everything\n\nCOMMIT;\n\n-- Application-level transaction management\n\n// Node.js/TypeScript\nasync function transferMoney(\n    fromAccountId: number,\n    toAccountId: number,\n    amount: number\n) {\n    const connection = await pool.getConnection();\n    \n    try {\n        await connection.beginTransaction();\n        \n        // Debit\n        const [result] = await connection.query(\n            'UPDATE accounts SET balance = balance - ? WHERE account_id = ? AND balance >= ?',\n            [amount, fromAccountId, amount]\n        );\n        \n        if (result.affectedRows === 0) {\n            throw new Error('Insufficient funds');\n        }\n        \n        // Credit\n        await connection.query(\n            'UPDATE accounts SET balance = balance + ? WHERE account_id = ?',\n            [amount, toAccountId]\n        );\n        \n        // Log\n        await connection.query(\n            'INSERT INTO transaction_log (from_account, to_account, amount) VALUES (?, ?, ?)',\n            [fromAccountId, toAccountId, amount]\n        );\n        \n        await connection.commit();\n        console.log('Transfer successful');\n    } catch (error) {\n        await connection.rollback();\n        console.error('Transfer failed, rolled back:', error);\n        throw error;\n    } finally {\n        connection.release();\n    }\n}\n\n// Java\npublic void transferMoney(int fromAccount, int toAccount, BigDecimal amount) \n        throws SQLException {\n    Connection conn = null;\n    try {\n        conn = dataSource.getConnection();\n        conn.setAutoCommit(false);  // Start transaction\n        \n        // Debit\n        PreparedStatement debitStmt = conn.prepareStatement(\n            \"UPDATE accounts SET balance = balance - ? WHERE account_id = ? AND balance >= ?\"\n        );\n        debitStmt.setBigDecimal(1, amount);\n        debitStmt.setInt(2, fromAccount);\n        debitStmt.setBigDecimal(3, amount);\n        \n        if (debitStmt.executeUpdate() == 0) {\n            throw new InsufficientFundsException();\n        }\n        \n        // Credit\n        PreparedStatement creditStmt = conn.prepareStatement(\n            \"UPDATE accounts SET balance = balance + ? WHERE account_id = ?\"\n        );\n        creditStmt.setBigDecimal(1, amount);\n        creditStmt.setInt(2, toAccount);\n        creditStmt.executeUpdate();\n        \n        conn.commit();  // Success\n    } catch (Exception e) {\n        if (conn != null) {\n            conn.rollback();  // Failure\n        }\n        throw e;\n    } finally {\n        if (conn != null) {\n            conn.setAutoCommit(true);\n            conn.close();\n        }\n    }\n}\n\n-- Two-phase commit (distributed transactions)\n-- Phase 1: Prepare\nPREPARE TRANSACTION 'tx_12345';\n\n-- Phase 2: Commit or rollback\nCOMMIT PREPARED 'tx_12345';\n-- or: ROLLBACK PREPARED 'tx_12345';\n\n-- Monitor long-running transactions\nSELECT \n    trx_id,\n    trx_state,\n    trx_started,\n    TIMESTAMPDIFF(SECOND, trx_started, NOW()) as duration_seconds,\n    trx_rows_locked,\n    trx_tables_locked\nFROM information_schema.innodb_trx\nWHERE trx_started < DATE_SUB(NOW(), INTERVAL 60 SECOND)\nORDER BY trx_started;\n\n-- Kill long transaction\nKILL CONNECTION <process_id>;"
    },
    {
      "id": 32,
      "question": "What are subqueries and their types?",
      "answer": "Subquery: Query nested inside another query\n\nTypes by result:\n\n1. Scalar Subquery:\n• Returns single value (1 row × 1 column)\n• Used with =, >, <, etc.\n• Example: SELECT * WHERE price > (SELECT AVG(price) FROM products)\n\n2. Row Subquery:\n• Returns single row (1 row × N columns)\n• Used with =, IN\n• Example: WHERE (col1, col2) = (SELECT col1, col2 FROM...)\n\n3. Column Subquery:\n• Returns single column (N rows × 1 column)\n• Used with IN, ANY, ALL\n• Example: WHERE id IN (SELECT user_id FROM...)\n\n4. Table Subquery:\n• Returns table (N rows × M columns)\n• Used in FROM clause\n• Example: SELECT * FROM (SELECT...) AS subquery\n\nTypes by behavior:\n\nIndependent: Execute once, independent of outer query\nCorrelated: Execute for each row of outer query, references outer query\n\nClauses:\n• SELECT: Inline values\n• FROM: Derived tables\n• WHERE: Filtering\n• HAVING: Group filtering\n\nPerformance:\n• Independent: Usually faster\n• Correlated: Slower (executes multiple times)\n• Consider JOINs as alternative",
      "explanation": "Subqueries are nested queries that provide flexibility in SQL but require careful use to avoid performance problems.",
      "difficulty": "Medium",
      "code": "-- Scalar subquery: Single value\nSELECT \n    name,\n    price,\n    (SELECT AVG(price) FROM products) AS avg_price,\n    price - (SELECT AVG(price) FROM products) AS diff_from_avg\nFROM products;\n\n-- WHERE clause scalar subquery\nSELECT name, price\nFROM products\nWHERE price > (SELECT AVG(price) FROM products);\n\n-- Column subquery with IN\nSELECT name, email\nFROM users\nWHERE id IN (SELECT user_id FROM orders WHERE total > 1000);\n\n-- NOT IN\nSELECT name\nFROM products\nWHERE id NOT IN (SELECT product_id FROM order_items);\n\n-- Column subquery with ANY\nSELECT name, salary\nFROM employees\nWHERE salary > ANY (SELECT salary FROM employees WHERE department = 'Sales');\n-- Salary greater than at least one Sales employee\n\n-- Column subquery with ALL\nSELECT name, salary\nFROM employees\nWHERE salary > ALL (SELECT salary FROM employees WHERE department = 'Sales');\n-- Salary greater than every Sales employee\n\n-- EXISTS (checks existence)\nSELECT name\nFROM users u\nWHERE EXISTS (\n    SELECT 1\n    FROM orders o\n    WHERE o.user_id = u.id\n      AND o.total > 1000\n);\n-- Returns users who have orders over $1000\n\n-- NOT EXISTS\nSELECT name\nFROM products p\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM order_items oi\n    WHERE oi.product_id = p.id\n);\n-- Products never ordered\n\n-- Table subquery in FROM clause (derived table)\nSELECT \n    dept_name,\n    avg_salary\nFROM (\n    SELECT \n        department AS dept_name,\n        AVG(salary) AS avg_salary\n    FROM employees\n    GROUP BY department\n) AS dept_averages\nWHERE avg_salary > 50000;\n\n-- Correlated subquery: References outer query\nSELECT \n    e1.name,\n    e1.salary,\n    e1.department\nFROM employees e1\nWHERE salary > (\n    SELECT AVG(salary)\n    FROM employees e2\n    WHERE e2.department = e1.department  -- Correlated: references e1\n);\n-- Employees earning above their department average\n\n-- Correlated with EXISTS\nSELECT \n    p.name AS product_name,\n    p.price\nFROM products p\nWHERE EXISTS (\n    SELECT 1\n    FROM order_items oi\n    WHERE oi.product_id = p.id  -- Correlated\n      AND oi.quantity > 10\n);\n-- Products ordered in quantities > 10\n\n-- Multiple nesting levels\nSELECT name\nFROM users\nWHERE id IN (\n    SELECT user_id\n    FROM orders\n    WHERE total > (\n        SELECT AVG(total)\n        FROM orders\n        WHERE status = 'COMPLETED'\n    )\n);\n\n-- Subquery in SELECT (inline)\nSELECT \n    u.name,\n    u.email,\n    (SELECT COUNT(*) FROM orders WHERE user_id = u.id) AS total_orders,\n    (SELECT SUM(total) FROM orders WHERE user_id = u.id) AS total_spent\nFROM users u;\n\n-- Subquery in HAVING\nSELECT \n    department,\n    AVG(salary) AS avg_salary\nFROM employees\nGROUP BY department\nHAVING AVG(salary) > (\n    SELECT AVG(salary)\n    FROM employees\n);\n-- Departments with above-average salaries\n\n-- Subquery vs JOIN performance\n-- Subquery (may be slow)\nSELECT name\nFROM users\nWHERE id IN (SELECT user_id FROM orders);\n\n-- Equivalent JOIN (often faster)\nSELECT DISTINCT u.name\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id;\n\n-- Correlated subquery (slow: executes for each row)\nSELECT \n    p.name,\n    (\n        SELECT MAX(order_date)\n        FROM orders o\n        JOIN order_items oi ON o.id = oi.order_id\n        WHERE oi.product_id = p.id  -- Correlated\n    ) AS last_ordered\nFROM products p;\n\n-- Better: LEFT JOIN (single execution)\nSELECT \n    p.name,\n    MAX(o.order_date) AS last_ordered\nFROM products p\nLEFT JOIN order_items oi ON p.id = oi.product_id\nLEFT JOIN orders o ON oi.order_id = o.id\nGROUP BY p.id, p.name;\n\n-- Complex example: Top 3 products per category\nSELECT \n    category,\n    product_name,\n    sales\nFROM (\n    SELECT \n        category,\n        name AS product_name,\n        total_sales AS sales,\n        (\n            SELECT COUNT(*)\n            FROM products p2\n            WHERE p2.category = p1.category\n              AND p2.total_sales > p1.total_sales\n        ) + 1 AS rank\n    FROM products p1\n) AS ranked\nWHERE rank <= 3\nORDER BY category, rank;\n\n-- Better: Window function\nSELECT \n    category,\n    name AS product_name,\n    total_sales AS sales\nFROM (\n    SELECT \n        category,\n        name,\n        total_sales,\n        ROW_NUMBER() OVER (PARTITION BY category ORDER BY total_sales DESC) AS rank\n    FROM products\n) AS ranked\nWHERE rank <= 3;\n\n-- Subquery with UPDATE\nUPDATE products\nSET price = price * 1.1\nWHERE id IN (\n    SELECT product_id\n    FROM order_items\n    GROUP BY product_id\n    HAVING SUM(quantity) > 100\n);\n\n-- Subquery with DELETE\nDELETE FROM users\nWHERE id NOT IN (\n    SELECT DISTINCT user_id\n    FROM orders\n    WHERE order_date > DATE_SUB(NOW(), INTERVAL 1 YEAR)\n);\n-- Delete inactive users (no orders in last year)\n\n-- Subquery with INSERT\nINSERT INTO archived_orders (order_id, user_id, total, order_date)\nSELECT id, user_id, total, order_date\nFROM orders\nWHERE order_date < DATE_SUB(NOW(), INTERVAL 2 YEAR);\n\n-- NULL handling in subqueries\n-- Be careful with NOT IN and NULL\nSELECT name\nFROM products\nWHERE id NOT IN (SELECT product_id FROM reviews);\n-- If reviews.product_id has NULL, returns no rows!\n\n-- Better: Use NOT EXISTS\nSELECT name\nFROM products p\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM reviews r\n    WHERE r.product_id = p.id\n);"
    },
    {
      "id": 33,
      "question": "What is a composite index and covering index?",
      "answer": "Composite Index: Index on multiple columns\n\nKey concepts:\n• Column order matters significantly\n• Leftmost prefix rule applies\n• Can satisfy multiple query patterns\n\nLeftmost prefix:\n• Index on (A, B, C)\n• Can be used for:\n  - WHERE A = x\n  - WHERE A = x AND B = y\n  - WHERE A = x AND B = y AND C = z\n• Cannot be used for:\n  - WHERE B = y (skips A)\n  - WHERE C = z (skips A and B)\n\nColumn ordering:\n• Most selective first (high cardinality)\n• Equality before range\n• Most frequently queried first\n\nCovering Index: Index contains all columns needed by query\n\nBenefits:\n• No table lookup required\n• Read only from index (Index-only scan)\n• Significantly faster\n• Reduced I/O\n\nTrade-offs:\n• Larger index size\n• Slower writes (maintain extra data)\n• More disk space\n• Memory usage\n\nWhen to use:\n• Read-heavy workloads\n• Frequently executed queries\n• Large tables where table access expensive",
      "explanation": "Composite indexes optimize queries on multiple columns, while covering indexes include all query columns to avoid table access.",
      "difficulty": "Hard",
      "code": "-- Composite index creation\nCREATE INDEX idx_user_city_age ON users(city, age);\n\n-- Queries that can use this index:\n-- 1. Both columns\nSELECT * FROM users WHERE city = 'NYC' AND age = 25;\n-- Uses index efficiently\n\n-- 2. Leftmost column only\nSELECT * FROM users WHERE city = 'NYC';\n-- Uses index (city is leftmost)\n\n-- 3. Leftmost + range\nSELECT * FROM users WHERE city = 'NYC' AND age > 21;\n-- Uses index\n\n-- Queries that CANNOT use this index efficiently:\n-- 1. Non-leftmost column\nSELECT * FROM users WHERE age = 25;\n-- Cannot use index (skips city)\n\n-- 2. Wrong order\nSELECT * FROM users WHERE age = 25 AND city = 'NYC';\n-- Optimizer may rewrite, but generally less efficient\n\n-- Proper ordering: Equality before range\nCREATE INDEX idx_orders_status_date \nON orders(status, order_date);\n\nSELECT * FROM orders\nWHERE status = 'PENDING'\n  AND order_date >= '2024-01-01';\n-- Efficient: status (equality) first, then date (range)\n\n-- Wrong order:\nCREATE INDEX idx_orders_date_status \nON orders(order_date, status);\n\nSELECT * FROM orders\nWHERE status = 'PENDING'\n  AND order_date >= '2024-01-01';\n-- Less efficient: range first, equality second\n\n-- Covering index example\n-- Query:\nSELECT user_id, name, email\nFROM users\nWHERE city = 'NYC' AND age > 21;\n\n-- Non-covering index\nCREATE INDEX idx_city_age ON users(city, age);\n-- Process:\n-- 1. Find matching rows in index\n-- 2. For each match, look up user_id, name, email in table\n-- Two steps: index search + table lookup\n\n-- Covering index\nCREATE INDEX idx_city_age_covering \nON users(city, age, user_id, name, email);\n-- Process:\n-- 1. Find matching rows in index\n-- 2. Read user_id, name, email directly from index\n-- One step: index-only scan\n\nEXPLAIN SELECT user_id, name, email\nFROM users\nWHERE city = 'NYC' AND age > 21;\n-- Extra: Using index (covering index used)\n\n-- Include columns (PostgreSQL, SQL Server)\n-- Better than adding to index key\nCREATE INDEX idx_orders_status_covering\nON orders(status, order_date)\nINCLUDE (total, user_id);\n-- status and order_date are key columns\n-- total and user_id are included but not part of key\n-- Benefit: Covering index without affecting sort order\n\n-- MySQL: Implicit covering via InnoDB\n-- InnoDB includes primary key in all secondary indexes\nCREATE TABLE users (\n    id INT PRIMARY KEY,  -- Clustered index\n    email VARCHAR(255),\n    name VARCHAR(100),\n    INDEX idx_email (email)\n);\n\n-- This query is automatically covered:\nSELECT id FROM users WHERE email = 'test@example.com';\n-- idx_email includes id implicitly (primary key)\n\n-- Composite index best practices\n\n-- 1. Cardinality: High to low\nCREATE INDEX idx_user_filters\nON users(email, country, age);\n-- email: Unique (high cardinality)\n-- country: ~200 values (medium)\n-- age: ~100 values (low)\n\n-- 2. Equality before range\nCREATE INDEX idx_orders_lookup\nON orders(user_id, status, order_date);\n\nSELECT * FROM orders\nWHERE user_id = 123          -- Equality\n  AND status = 'COMPLETED'   -- Equality\n  AND order_date >= '2024-01-01';  -- Range\n\n-- 3. Query patterns\n-- If you have these queries:\n-- Q1: WHERE city = ? AND age = ?\n-- Q2: WHERE city = ? AND age = ? AND status = ?\n-- Q3: WHERE city = ?\n\n-- One index covers all:\nCREATE INDEX idx_city_age_status \nON users(city, age, status);\n\n-- 4. Index prefix\nCREATE INDEX idx_user_name_prefix ON users(last_name(10));\n-- Only first 10 characters indexed (saves space)\n\n-- Analyze index usage\n-- MySQL\nEXPLAIN SELECT *\nFROM users\nWHERE city = 'NYC' AND age = 25;\n-- Check \"key\" column for index used\n\n-- View index statistics\nSELECT \n    table_name,\n    index_name,\n    seq_in_index,\n    column_name,\n    cardinality\nFROM information_schema.statistics\nWHERE table_schema = 'mydatabase'\n  AND table_name = 'users'\nORDER BY table_name, index_name, seq_in_index;\n\n-- PostgreSQL: Check index usage\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,  -- Number of index scans\n    idx_tup_read,  -- Tuples read from index\n    idx_tup_fetch  -- Tuples fetched from table\nFROM pg_stat_user_indexes\nWHERE tablename = 'users';\n\n-- Covering index verification\nEXPLAIN (ANALYZE, BUFFERS)\nSELECT id, name, email\nFROM users\nWHERE city = 'NYC';\n-- Look for \"Index Only Scan\" (PostgreSQL)\n-- Or \"Using index\" (MySQL)\n\n-- Composite index with multiple strategies\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    category VARCHAR(50),\n    price DECIMAL(10,2),\n    brand VARCHAR(50),\n    rating DECIMAL(3,2),\n    stock INT,\n    name VARCHAR(200)\n);\n\n-- Index for: category filter + price sort\nCREATE INDEX idx_category_price \nON products(category, price);\n\n-- Index for: category + brand filter\nCREATE INDEX idx_category_brand \nON products(category, brand);\n\n-- Covering index for common query\nCREATE INDEX idx_category_price_covering\nON products(category, price, id, name, stock);\n\nSELECT id, name, price, stock\nFROM products\nWHERE category = 'Electronics'\n  AND price BETWEEN 100 AND 500\nORDER BY price;\n-- Fully covered by idx_category_price_covering\n\n-- When too many indexes hurt\n-- Each index:\n-- - Takes disk space\n-- - Slows INSERT/UPDATE/DELETE\n-- - Uses memory\n-- Balance between read and write performance"
    },
    {
      "id": 34,
      "question": "What are database constraints in detail?",
      "answer": "Constraints: Rules enforcing data integrity\n\nPRIMARY KEY:\n• Uniquely identifies each row\n• Cannot be NULL\n• One per table\n• Automatically creates unique index\n• Can be single or composite column\n\nFOREIGN KEY:\n• Links tables (referential integrity)\n• Value must exist in referenced table\n• Can be NULL (unless NOT NULL)\n• Cascade actions:\n  - CASCADE: Propagate changes\n  - SET NULL: Set to NULL\n  - SET DEFAULT: Set to default value\n  - RESTRICT/NO ACTION: Prevent change\n\nUNIQUE:\n• No duplicate values\n• Can have multiple per table\n• NULL allowed (implementation varies)\n• Creates index automatically\n\nCHECK:\n• Custom validation rule\n• Boolean expression\n• Row-level checking  \n• Complex business rules\n\nNOT NULL:\n• Column must have value\n• Cannot insert/update to NULL\n• Simple but essential\n\nDEFAULT:\n• Automatic value if not specified\n• Can be literal or function\n• Applied on INSERT\n\nBenefits:\n• Data integrity\n• Self-documenting\n• Database-enforced (reliable)\n• Error prevention",
      "explanation": "Database constraints enforce data integrity rules at the database level, ensuring consistent and valid data across all applications.",
      "difficulty": "Medium",
      "code": "-- PRIMARY KEY\nCREATE TABLE users (\n    id INT PRIMARY KEY,  -- Simple primary key\n    username VARCHAR(50) UNIQUE NOT NULL,\n    email VARCHAR(255) UNIQUE NOT NULL\n);\n\n-- Alternative syntax\nCREATE TABLE users (\n    id INT,\n    username VARCHAR(50),\n    PRIMARY KEY (id)\n);\n\n-- Composite primary key\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    PRIMARY KEY (order_id, product_id)  -- Both columns together must be unique\n);\n\n-- FOREIGN KEY with CASCADE\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT,\n    total DECIMAL(10,2),\n    FOREIGN KEY (user_id) REFERENCES users(id)\n        ON DELETE CASCADE  -- Delete orders when user deleted\n        ON UPDATE CASCADE  -- Update order.user_id when user.id changes\n);\n\n-- FOREIGN KEY with different actions\nCREATE TABLE posts (\n    id INT PRIMARY KEY,\n    user_id INT,\n    title VARCHAR(200),\n    FOREIGN KEY (user_id) REFERENCES users(id)\n        ON DELETE SET NULL  --Set posts.user_id = NULL when user deleted\n        ON UPDATE RESTRICT  -- Prevent changing user.id if posts exist\n);\n\n-- FOREIGN KEY with SET DEFAULT\nCREATE TABLE comments (\n    id INT PRIMARY KEY,\n    post_id INT DEFAULT 1,  -- Default post ID\n    content TEXT,\n    FOREIGN KEY (post_id) REFERENCES posts(id)\n        ON DELETE SET DEFAULT  -- Set to default when post deleted\n);\n\n-- UNIQUE constraint\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    email VARCHAR(255) UNIQUE,  -- Column level\n    ssn VARCHAR(11),\n    phone VARCHAR(20),\n    CONSTRAINT uq_ssn UNIQUE (ssn),  -- Table level with name\n    CONSTRAINT uq_phone UNIQUE (phone)\n);\n\n-- Composite UNIQUE\nCREATE TABLE reservations (\n    id INT PRIMARY KEY,\n    room_id INT,\n    date DATE,\n    user_id INT,\n    UNIQUE (room_id, date)  -- Same room can't be booked twice on same date\n);\n\n-- CHECK constraints\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    price DECIMAL(10,2) CHECK (price > 0),  -- Price must be positive\n    discount_pct DECIMAL(5,2) CHECK (discount_pct >= 0 AND discount_pct <= 100),\n    stock INT CHECK (stock >= 0),\n    category VARCHAR(50) CHECK (category IN ('Electronics', 'Clothing', 'Food'))\n);\n\n-- Named CHECK constraints\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    age INT,\n    salary DECIMAL(10,2),\n    hire_date DATE,\n    CONSTRAINT chk_age CHECK (age >= 18 AND age <= 65),\n    CONSTRAINT chk_salary CHECK (salary > 0),\n    CONSTRAINT chk_hire_date CHECK (hire_date <= CURRENT_DATE)\n);\n\n-- Multi-column CHECK\nCREATE TABLE price_ranges (\n    id INT PRIMARY KEY,\n    min_price DECIMAL(10,2),\n    max_price DECIMAL(10,2),\n    CONSTRAINT chk_price_range CHECK (min_price < max_price)\n);\n\n-- Complex CHECK constraint\nCREATE TABLE accounts (\n    id INT PRIMARY KEY,\n    account_type VARCHAR(20) CHECK (account_type IN ('CHECKING', 'SAVINGS', 'LOAN')),\n    balance DECIMAL(15,2),\n    credit_limit DECIMAL(15,2),\n    CONSTRAINT chk_balance_logic CHECK (\n        (account_type IN ('CHECKING', 'SAVINGS') AND balance >= 0) OR\n        (account_type = 'LOAN' AND balance <= 0)\n    )\n);\n\n-- NOT NULL constraint\nCREATE TABLE customers (\n    id INT PRIMARY KEY,\n    first_name VARCHAR(50) NOT NULL,\n    last_name VARCHAR(50) NOT NULL,\n    email VARCHAR(255) NOT NULL,\n    phone VARCHAR(20)  -- Optional, NULL allowed\n);\n\n-- DEFAULT constraint\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT,\n    status VARCHAR(20) DEFAULT 'PENDING',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    modified_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    is_paid BOOLEAN DEFAULT FALSE,\n    shipping_cost DECIMAL(10,2) DEFAULT 0.00\n);\n\n-- Adding constraints to existing table\n-- Add PRIMARY KEY\nALTER TABLE legacy_users\nADD PRIMARY KEY (id);\n\n-- Add FOREIGN KEY\nALTER TABLE orders\nADD CONSTRAINT fk_user\nFOREIGN KEY (user_id) REFERENCES users(id)\nON DELETE CASCADE;\n\n-- Add UNIQUE\nALTER TABLE users\nADD CONSTRAINT uq_email UNIQUE (email);\n\n-- Add CHECK\nALTER TABLE products\nADD CONSTRAINT chk_positive_price CHECK (price > 0);\n\n-- Add NOT NULL\nALTER TABLE users\nMODIFY email VARCHAR(255) NOT NULL;\n\n-- Removing constraints\n-- Drop FOREIGN KEY\nALTER TABLE orders\nDROP FOREIGN KEY fk_user;\n\n-- Drop CHECK\nALTER TABLE products\nDROP CONSTRAINT chk_positive_price;\n\n-- Drop UNIQUE\nALTER TABLE users\nDROP INDEX uq_email;\n\n-- Temporarily disable constraints\n-- MySQL\nSET FOREIGN_KEY_CHECKS = 0;\n-- Perform operations\nSET FOREIGN_KEY_CHECKS = 1;\n\n-- SQL Server\nALTER TABLE orders NOCHECK CONSTRAINT ALL;\n-- Perform operations\nALTER TABLE orders CHECK CONSTRAINT ALL;\n\n-- PostgreSQL\nALTER TABLE orders DISABLE TRIGGER ALL;\n-- Perform operations\nALTER TABLE orders ENABLE TRIGGER ALL;\n\n-- Constraint violation handling\n-- Application level\ntry {\n    await db.query(\n        'INSERT INTO users (id, email) VALUES (?, ?)',\n        [1, 'duplicate@example.com']\n    );\n} catch (error) {\n    if (error.code === 'ER_DUP_ENTRY') {\n        console.log('Email already exists');\n    } else if (error.code === 'ER_NO_REFERENCED_ROW') {\n        console.log('Foreign key violation');\n    } else if (error.code === 'ER_CHECK_CONSTRAINT_VIOLATED') {\n        console.log('Check constraint failed');\n    }\n}\n\n-- View constraint information\n-- MySQL\nSELECT \n    TABLE_NAME,\n    CONSTRAINT_NAME,\n    CONSTRAINT_TYPE\nFROM information_schema.TABLE_CONSTRAINTS\nWHERE TABLE_SCHEMA = 'mydatabase'\n  AND TABLE_NAME = 'users';\n\n-- View foreign key details\nSELECT \n    CONSTRAINT_NAME,\n    TABLE_NAME,\n    COLUMN_NAME,\n    REFERENCED_TABLE_NAME,\n    REFERENCED_COLUMN_NAME\nFROM information_schema.KEY_COLUMN_USAGE\nWHERE TABLE_SCHEMA = 'mydatabase'\n  AND REFERENCED_TABLE_NAME IS NOT NULL;\n\n-- View CHECK constraints\nSELECT \n    TABLE_NAME,\n    CONSTRAINT_NAME,\n    CHECK_CLAUSE\nFROM information_schema.CHECK_CONSTRAINTS\nWHERE CONSTRAINT_SCHEMA = 'mydatabase';\n\n-- Deferred constraints (PostgreSQL)\nCREATE TABLE parent (\n    id INT PRIMARY KEY\n);\n\nCREATE TABLE child (\n    id INT PRIMARY KEY,\n    parent_id INT,\n    FOREIGN KEY (parent_id) REFERENCES parent(id)\n        DEFERRABLE INITIALLY DEFERRED\n);\n\n-- Within transaction, constraint checked at commit\nBEGIN;\nINSERT INTO child (id, parent_id) VALUES (1, 99);  -- parent 99 doesn't exist yet\nINSERT INTO parent (id) VALUES (99);  -- Now it does\nCOMMIT;  -- Constraint checked here, succeeds"
    },
    {
      "id": 35,
      "question": "What are database sequences and auto-increment?",
      "answer": "Sequences: Database objects generating unique numbers\n\nAUTO_INCREMENT (MySQL/MariaDB):\n• Automatic sequential numbers\n• Per-table counter\n• Starts at 1 by default\n• Increments by 1 (configurable)\n• Gaps possible (rollback, delete)\n• Only one per table\n\nSERIAL/SEQUENCE (PostgreSQL):\n• Standalone database object\n• Can be shared across tables\n• More flexible than AUTO_INCREMENT\n• Explicit control over sequence\n• Multiple sequences per table possible\n\nIDENTITY (SQL Server, Oracle):\n• Similar to AUTO_INCREMENT\n• Column property\n• Seed and increment configurable\n\nKey concepts:\n• Guarantees uniqueness\n• Not guaranteed consecutive (gaps)\n• Performance: Cached values\n• Replication: Can cause conflicts\n• Cannot reuse deleted values\n\nCommon uses:\n• Primary keys\n• Order numbers\n• Invoice numbers\n• Unique identifiers\n\nAlternatives:\n• UUID/GUID: Globally unique\n• Composite keys\n• Natural keys\n\nBest practices:\n• Don't assume no gaps\n• Don't expose to users directly\n• Use UUID for distributed systems\n• Consider caching for performance",
      "explanation": "Sequences and auto-increment provide automatic unique number generation, primarily for primary keys, with database-specific implementations.",
      "difficulty": "Medium",
      "code": "-- MySQL AUTO_INCREMENT\nCREATE TABLE users (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(255)\n);\n\n-- Insert without specifying ID\nINSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com');\n-- id = 1 (automatically generated)\n\nINSERT INTO users (name, email) VALUES ('Bob', 'bob@example.com');\n-- id = 2\n\n-- Explicit ID value\nINSERT INTO users (id, name, email) VALUES (100, 'Charlie', 'charlie@example.com');\n-- Next auto-generated ID will be 101\n\n-- Get last inserted ID\n-- MySQL\nSELECT LAST_INSERT_ID();\n\n-- Application level (Node.js)\nconst [result] = await db.query(\n    'INSERT INTO users (name, email) VALUES (?, ?)',\n    ['David', 'david@example.com']\n);\nconsole.log('Inserted ID:', result.insertId);\n\n-- Modify AUTO_INCREMENT start value\nALTER TABLE users AUTO_INCREMENT = 1000;\n\n-- Reset AUTO_INCREMENT\nALTER TABLE users AUTO_INCREMENT = 1;\n\n-- View current AUTO_INCREMENT value\nSHOW TABLE STATUS LIKE 'users';\n-- Check Auto_increment column\n\n-- Custom increment step\nSET @@auto_increment_increment = 10;\n-- Now IDs: 1, 11, 21, 31...\n\n-- PostgreSQL SERIAL\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,  -- Shorthand for sequence\n    name VARCHAR(100),\n    email VARCHAR(255)\n);\n-- Equivalent to:\nCREATE SEQUENCE users_id_seq;\nCREATE TABLE users (\n    id INT NOT NULL DEFAULT nextval('users_id_seq') PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(255)\n);\n\n-- Insert\nINSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com')\nRETURNING id;\n-- Returns generated ID\n\n-- PostgreSQL explicit SEQUENCE\nCREATE SEQUENCE order_number_seq\n    START WITH 1000\n    INCREMENT BY 1\n    MINVALUE 1000\n    MAXVALUE 999999\n    CACHE 20;  -- Cache 20 values for performance\n\n-- Use sequence\nINSERT INTO orders (order_number, user_id, total)\nVALUES (nextval('order_number_seq'), 1, 100.00);\n\n-- Get current value (without incrementing)\nSELECT currval('order_number_seq');\n\n-- Get next value\nSELECT nextval('order_number_seq');\n\n-- Set sequence value\nSELECT setval('order_number_seq', 5000);\n\n-- Reset sequence\nALTER SEQUENCE order_number_seq RESTART WITH 1;\n\n-- Drop sequence\nDROP SEQUENCE order_number_seq;\n\n-- Share sequence across tables\nCREATE SEQUENCE global_id_seq;\n\nCREATE TABLE users (\n    id INT DEFAULT nextval('global_id_seq') PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nCREATE TABLE products (\n    id INT DEFAULT nextval('global_id_seq') PRIMARY KEY,\n    name VARCHAR(100)\n);\n-- Users and products share same sequence (different use case)\n\n-- SQL Server IDENTITY\nCREATE TABLE users (\n    id INT IDENTITY(1,1) PRIMARY KEY,  -- Start 1, Increment 1\n    name VARCHAR(100),\n    email VARCHAR(255)\n);\n\n-- Custom start and increment\nCREATE TABLE invoices (\n    invoice_number INT IDENTITY(1000, 5) PRIMARY KEY,  -- Start 1000, Increment 5\n    amount DECIMAL(10,2)\n);\n-- Values: 1000, 1005, 1010, 1015...\n\n-- Get last identity value\nSELECT SCOPE_IDENTITY();  -- Current scope only (recommended)\nSELECT @@IDENTITY;        -- Any scope (less safe)\nSELECT IDENT_CURRENT('users');  -- Specific table\n\n-- Insert explicit value\nSET IDENTITY_INSERT users ON;\nINSERT INTO users (id, name, email) VALUES (500, 'Alice', 'alice@example.com');\nSET IDENTITY_INSERT users OFF;\n\n-- Reseed identity\nDBCC CHECKIDENT ('users', RESEED, 1);\n\n-- Oracle SEQUENCE\nCREATE SEQUENCE user_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NOCACHE;  -- Don't cache (GARANTEEs no gaps, but slower)\n\n-- Use in INSERT\nINSERT INTO users (id, name, email)\nVALUES (user_id_seq.NEXTVAL, 'Alice', 'alice@example.com');\n\n-- Use in SELECT\nSELECT user_id_seq.NEXTVAL FROM DUAL;\n\n-- Current value\nSELECT user_id_seq.CURRVAL FROM DUAL;\n\n-- Oracle 12c+ IDENTITY column\nCREATE TABLE users (\n    id NUMBER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n    name VARCHAR2(100)\n);\n\n-- Or with options\nCREATE TABLE users (\n    id NUMBER GENERATED BY DEFAULT AS IDENTITY (\n        START WITH 1\n        INCREMENT BY 1\n        CACHE 20\n    ) PRIMARY KEY,\n    name VARCHAR2(100)\n);\n\n-- Handling gaps in sequence\n-- Gaps occur from:\n-- 1. Rolled back transactions\nBEGIN TRANSACTION;\nINSERT INTO users (name) VALUES ('Alice');  -- Gets ID 1\nROLLBACK;  -- ID 1 is lost\n\nINSERT INTO users (name) VALUES ('Bob');  -- Gets ID 2\n-- Gap: No ID 1 in table\n\n-- 2. Deleted rows\nINSERT INTO users (name) VALUES ('Charlie');  -- ID 3\nDELETE FROM users WHERE id = 3;\nINSERT INTO users (name) VALUES ('David');  -- ID 4\n-- Gap: No ID 3\n\n-- 3. Cached sequences\n-- PostgreSQL cache example\nCREATE SEQUENCE cached_seq CACHE 100;\n-- If database crashes, up to 100 values lost\n\n-- UUID alternative (no gaps, globally unique)\nCREATE TABLE users (\n    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,  -- PostgreSQL\n    name VARCHAR(100)\n);\n\n-- MySQL (8.0+)\nCREATE TABLE users (\n    id BINARY(16) DEFAULT (UUID_TO_BIN(UUID())) PRIMARY KEY,\n    name VARCHAR(100)\n);\n\n-- Multi-master replication strategy\n-- Server 1: IDs 1, 3, 5, 7... (odd)\nSET @@auto_increment_increment = 2;\nSET @@auto_increment_offset = 1;\n\n-- Server 2: IDs 2, 4, 6, 8... (even)\nSET @@auto_increment_increment = 2;\nSET @@auto_increment_offset = 2;\n-- Prevents ID conflicts in replication\n\n-- Custom sequence implementation\nCREATE TABLE sequences (\n    name VARCHAR(50) PRIMARY KEY,\n    current_value BIGINT NOT NULL,\n    increment_by INT DEFAULT 1\n);\n\nINSERT INTO sequences (name, current_value) VALUES ('order_number', 1000);\n\n-- Atomic increment function\nDELIMITER //\nCREATE FUNCTION get_next_sequence(seq_name VARCHAR(50))\nRETURNS BIGINT\nDETERMINISTIC\nBEGIN\n    DECLARE next_val BIGINT;\n    \n    UPDATE sequences\n    SET current_value = LAST_INSERT_ID(current_value + increment_by)\n    WHERE name = seq_name;\n    \n    SET next_val = LAST_INSERT_ID();\n    RETURN next_val;\nEND//\nDELIMITER ;\n\n-- Usage\nINSERT INTO orders (order_number, total)\nVALUEs (get_next_sequence('order_number'), 100.00);"
    },
    {
      "id": 36,
      "question": "What are CTEs (Common Table Expressions) vs Temp Tables vs Table Variables?",
      "answer": "CTE (Common Table Expression):\n• Temporary result set for single query\n• Query-scoped only\n• Cannot be indexed\n• Good readability\n• Supports recursion\n• Computed once per reference (non-materialized)\n\nTemporary Tables:\n• Persists for session/connection\n• Can be indexed\n• Statistics available\n• Better for large datasets\n• Supports all table operations\n• Stored in tempdb\n\nTable Variables (SQL Server/MySQL):\n• Memory-based (small data)\n• Function/procedure scoped\n• Cannot be indexed (limited)\n• Minimal logging\n• No statistics\n• Faster for small datasets\n\nWhen to use:\n• CTE: Readability, recursion, simple queries\n• Temp Table: Large data, reuse, complex operations\n• Table Variable: Small data, stored procedures\n\nPerformance:\n• Small data: Table variables\n• Large data: Temp tables\n• One-time use: CTE\n• Multiple references: Temp table",
      "explanation": "CTEs, temp tables, and table variables each serve different use cases for temporary data storage with distinct scoping and performance characteristics.",
      "difficulty": "Medium",
      "code": "-- CTE (Common Table Expression)\nWITH high_value_customers AS (\n    SELECT \n        user_id,\n        SUM(total) AS total_spent\n    FROM orders\n    GROUP BY user_id\n    HAVING SUM(total) > 10000\n)\nSELECT \n    u.name,\n    hvc.total_spent\nFROM users u\nJOIN high_value_customers hvc ON u.id = hvc.user_id;\n-- CTE exists only for this query\n\n-- Multiple CTEs\nWITH \nmonthly_sales AS (\n    SELECT \n        DATE_TRUNC('month', order_date) AS month,\n        SUM(total) total\n    FROM orders\n    GROUP BY DATE_TRUNC('month', order_date)\n),\ntop_months AS (\n    SELECT * \n    FROM monthly_sales\n    WHERE total > 50000\n)\nSELECT * FROM top_months ORDER BY month;\n\n-- Recursive CTE\nWITH RECURSIVE employee_hierarchy AS (\n    SELECT id, name, manager_id, 1 AS level\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    SELECT e.id, e.name, e.manager_id, eh.level + 1\n    FROM employees e\n    JOIN employee_hierarchy eh ON e.manager_id = eh.id\n)\nSELECT * FROM employee_hierarchy;\n\n-- Temporary Table (MySQL)\nCREATE TEMPORARY TABLE temp_sales (\n    product_id INT,\n    total_sales DECIMAL(15,2),\n    PRIMARY KEY (product_id),\n    INDEX idx_sales (total_sales)\n);\n\n-- Populate temp table\nINSERT INTO temp_sales\nSELECT \n    product_id,\n    SUM(quantity * price) AS total_sales\nFROM order_items\nGROUP BY product_id;\n\n-- Use temp table multiple times\nSELECT AVG(total_sales) FROM temp_sales;\nSELECT * FROM temp_sales WHERE total_sales > 1000;\nSELECT * FROM temp_sales ORDER BY total_sales DESC LIMIT 10;\n\n-- Temp table persists for session\n-- Automatically dropped at session end\nDROP TEMPORARY TABLE IF EXISTS temp_sales;\n\n-- SQL Server Temporary Table\n-- Local temp table (# prefix)\nCREATE TABLE #temp_orders (\n    order_id INT PRIMARY KEY,\n    user_id INT,\n    total DECIMAL(10,2)\n);\n\n-- Global temp table (## prefix, visible to all sessions)\nCREATE TABLE ##global_temp (\n    id INT,\n    data VARCHAR(100)\n);\n\n-- Table Variable (SQL Server)\nDECLARE @order_summary TABLE (\n    user_id INT,\n    order_count INT,\n    total_spent DECIMAL(15,2),\n    INDEX idx_user (user_id)  -- SQL Server 2014+\n);\n\nINSERT INTO @order_summary\nSELECT \n    user_id,\n    COUNT(*),\n    SUM(total)\nFROM orders\nGROUP BY user_id;\n\nSELECT * FROM @order_summary WHERE order_count > 5;\n-- Table variable only exists in this batch/procedure\n\n-- MySQL 8.0+ Table Variables\nSET @var = (\n    SELECT JSON_ARRAYAGG(\n        JSON_OBJECT('id', id, 'name', name)\n    )\n    FROM users\n    WHERE age > 25\n);\n\nSELECT JSON_EXTRACT(@var, '$[0].name');\n\n-- Performance comparison\n\n-- CTE (computed each time referenced)\nWITH expensive_query AS (\n    SELECT * FROM large_table WHERE complex_condition\n)\nSELECT * FROM expensive_query  -- Computed\nUNION ALL\nSELECT * FROM expensive_query;  -- Computed again!\n\n-- Temp table (computed once)\nCREATE TEMPORARY TABLE expensive_result AS\nSELECT * FROM large_table WHERE complex_condition;\n\nSELECT * FROM expensive_result  -- Read from temp\nUNION ALL\nSELECT * FROM expensive_result;  -- Read from temp (fast)\n\nDROP TEMPORARY TABLE expensive_result;\n\n-- Table variable (memory, fast for small data)\nDECLARE @small_result TABLE (\n    id INT,\n    value VARCHAR(50)\n);\n\nINSERT INTO @small_result\nSELECT id, value FROM source_table WHERE id < 100;\n\nSELECT * FROM @small_result;  -- Fast for small datasets\n\n-- Complex example: Sales analysis\n\n-- Using CTE (readable)\nWITH \nproduct_sales AS (\n    SELECT \n        product_id,\n        SUM(quantity * price) AS total_sales\n    FROM order_items\n    GROUP BY product_id\n),\nproduct_ranks AS (\n    SELECT \n        product_id,\n        total_sales,\n        RANK() OVER (ORDER BY total_sales DESC) AS sales_rank\n    FROM product_sales\n),\ntop_products AS (\n    SELECT \n        pr.*,\n        p.name\n    FROM product_ranks pr\n    JOIN products p ON pr.product_id = p.id\n    WHERE sales_rank <= 10\n)\nSELECT * FROM top_products;\n\n-- Using temp tables (better for large data)\nCREATE TEMPORARY TABLE product_sales AS\nSELECT \n    product_id,\n    SUM(quantity * price) AS total_sales\nFROM order_items\nGROUP BY product_id;\n\nCREATE INDEX idx_sales ON product_sales(total_sales);\n\nCREATE TEMPORARY TABLE product_ranks AS\nSELECT \n    product_id,\n    total_sales,\n    RANK() OVER (ORDER BY total_sales DESC) AS sales_rank\nFROM product_sales;\n\nSELECT \n    pr.*,\n    p.name\nFROM product_ranks pr\nJOIN products p ON pr.product_id = p.id\nWHERE sales_rank <= 10;\n\nDROP TEMPORARY TABLE product_sales;\nDROP TEMPORARY TABLE product_ranks;\n\n-- Stored procedure with table variable\nDELIMITER //\nCREATE PROCEDURE get_user_stats(IN p_user_id INT)\nBEGIN\n    DECLARE v_stats JSON;\n    \n    -- Use CTE for readability\n    WITH user_orders AS (\n        SELECT \n            COUNT(*) AS order_count,\n            SUM(total) AS total_spent,\n            AVG(total) AS avg_order\n        FROM orders\n        WHERE user_id = p_user_id\n    )\n    SELECT JSON_OBJECT(\n        'order_count', order_count,\n        'total_spent', total_spent,\n        'avg_order', avg_order\n    ) INTO v_stats\n    FROM user_orders;\n    \n    SELECT v_stats AS user_statistics;\nEND//\nDELIMITER ;\n\n-- Materialized CTE (PostgreSQL)\nCREATE MATERIALIZED VIEW product_sales_mv AS\nSELECT \n    product_id,\n    SUM(quantity * price) AS total_sales\nFROM order_items\nGROUP BY product_id;\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW product_sales_mv;\n\n-- Use materialized view\nSELECT * FROM product_sales_mv WHERE total_sales > 10000;\n\n-- Drop materialized view\nDROP MATERIALIZED VIEW product_sales_mv;\n\n-- Best practices\n\n-- Use CTE for:\n-- - Improving readability\n-- - Recursive queries\n-- - One-time use\nWITH readable_query AS (\n    SELECT id, name, price\n    FROM products\n    WHERE category = 'Electronics'\n)\nSELECT * FROM readable_query;\n\n-- Use Temp Table for:\n-- - Large result sets\n-- - Multiple references\n-- - Need for indexes\nCREATE TEMPORARY TABLE large_result AS\nSELECT * FROM huge_table WHERE complex_conditions;\n\nCREATE INDEX idx ON large_result(key_column);\n-- Use multiple times with good performance\n\n-- Use Table Variable for:\n-- - Small datasets (<1000 rows)\n-- - Stored procedures\n-- - Function return values\nDECLARE @small_lookup TABLE (id INT, value VARCHAR(50));\nINSERT INTO @small_lookup VALUES (1, 'A'), (2, 'B'), (3, 'C');\nSELECT * FROM orders o JOIN @small_lookup l ON o.type_id = l.id;"
    },
    {
      "id": 37,
      "question": "What are database stored procedures and functions?",
      "answer": "Stored Procedures:\n• Precompiled SQL statements\n• Stored in database\n• Can perform actions (INSERT, UPDATE, DELETE)\n• Can return multiple result sets\n• Can have OUT parameters\n• Cannot be used in SELECT\n• Transaction control (COMMIT, ROLLBACK)\n\nFunctions:\n• Return single value or table\n• Must return value\n• Cannot modify database state (mostly)\n• Can be used in SELECT statements\n• No OUT parameters\n• Deterministic or non-deterministic\n• Limited transaction control\n\nBenefits:\n• Performance: Precompiled, cached\n• Security: Encapsulation, permission control\n• Reusability: Called from multiple places\n• Maintainability: Centralized logic\n• Network traffic: Less data transfer\n• Business logic: Enforced at database level\n\nDrawbacks:\n• Harder to version control\n• Difficult to debug\n• Database vendor lock-in\n• Testing complexity\n• Not ideal for complex business logic\n\nWhen to use:\n• Complex database operations\n• Batch processing\n• Data validation\n• Calculations\n• Recurring tasks",
      "explanation": "Stored procedures and functions encapsulate SQL logic in the database for reuse, performance, and centralized control, with distinct capabilities and limitations.",
      "difficulty": "Medium",
      "code": "-- MySQL Stored Procedure\nDELIMITER //\n\nCREATE PROCEDURE create_order(\n    IN p_user_id INT,\n    IN p_total DECIMAL(10,2),\n    OUT p_order_id INT\n)\nBEGIN\n    -- Declare variables\n    DECLARE v_balance DECIMAL(10,2);\n    \n    -- Check user balance\n    SELECT balance INTO v_balance\n    FROM accounts\n    WHERE user_id = p_user_id;\n    \n    -- Validate\n    IF v_balance < p_total THEN\n        SIGNAL SQLSTATE '45000'\n        SET MESSAGE_TEXT = 'Insufficient funds';\n    END IF;\n    \n    -- Create order\n    INSERT INTO orders (user_id, total, status, created_at)\n    VALUES (p_user_id, p_total, 'PENDING', NOW());\n    \n    -- Get generated ID\n    SET p_order_id = LAST_INSERT_ID();\n    \n    -- Update balance\n    UPDATE accounts\n    SET balance = balance - p_total\n    WHERE user_id = p_user_id;\n    \n    -- Commit (if not already in transaction)\n    COMMIT;\nEND//\n\nDELIMITER ;\n\n-- Call stored procedure\nCALL create_order(123, 99.99, @new_order_id);\nSELECT @new_order_id AS order_id;\n\n-- Stored procedure with result set\nDELIMITER //\n\nCREATE PROCEDURE get_user_orders(IN p_user_id INT)\nBEGIN\n    SELECT  \n        id,\n        total,\n        status,\n        order_date\n    FROM orders\n    WHERE user_id = p_user_id\n    ORDER BY order_date DESC;\nEND//\n\nDELIMITER ;\n\nCALL get_user_orders(123);\n\n-- Stored procedure with multiple result sets\nDELIMITER //\n\nCREATE PROCEDURE get_user_summary(IN p_user_id INT)\nBEGIN\n    -- First result set: User info\n    SELECT name, email, created_at\n    FROM users\n    WHERE id = p_user_id;\n    \n    -- Second result set: Order stats\n    SELECT \n        COUNT(*) AS order_count,\n        SUM(total) AS total_spent,\n        AVG(total) AS avg_order\n    FROM orders\n    WHERE user_id = p_user_id;\n    \n    -- Third result set: Recent orders\n    SELECT id, total, order_date\n    FROM orders\n    WHERE user_id = p_user_id\n    ORDER BY order_date DESC\n    LIMIT 5;\nEND//\n\nDELIMITER ;\n\n-- Stored procedure with error handling\nDELIMITER //\n\nCREATE PROCEDURE transfer_money(\n    IN p_from_account INT,\n    IN p_to_account INT,\n    IN p_amount DECIMAL(10,2)\n)\nBEGIN\n    DECLARE EXIT HANDLER FOR SQLEXCEPTION\n    BEGIN\n        -- Error occurred, rollback\n        ROLLBACK;\n        SELECT 'Transfer failed' AS message;\n    END;\n    \n    START TRANSACTION;\n    \n    -- Debit\n    UPDATE accounts\n    SET balance = balance - p_amount\n    WHERE id = p_from_account\n      AND balance >= p_amount;\n    \n    IF ROW_COUNT() = 0 THEN\n        SIGNAL SQLSTATE '45000'\n        SET MESSAGE_TEXT = 'Insufficient funds';\n    END IF;\n    \n    -- Credit\n    UPDATE accounts\n    SET balance = balance + p_amount\n    WHERE id = p_to_account;\n    \n    COMMIT;\n    SELECT 'Transfer successful' AS message;\nEND//\n\nDELIMITER ;\n\n-- Stored Function\nDELIMITER //\n\nCREATE FUNCTION calculate_discount(\n    p_total DECIMAL(10,2)\n) RETURNS DECIMAL(5,2)\nDETERMINISTIC\nBEGIN\n    DECLARE v_discount DECIMAL(5,2);\n    \n    IF p_total > 1000 THEN\n        SET v_discount = 0.15;  -- 15%\n    ELSEIF p_total > 500 THEN\n        SET v_discount = 0.10;  -- 10%\n    ELSEIF p_total > 100 THEN\n        SET v_discount = 0.05;  -- 5%\n    ELSE\n        SET v_discount = 0.00;\n    END IF;\n    \n    RETURN v_discount;\nEND//\n\nDELIMITER ;\n\n-- Use function in SELECT\nSELECT \n    id,\n    total,\n    calculate_discount(total) AS discount_pct,\n    total * (1 - calculate_discount(total)) AS final_total\nFROM orders;\n\n-- Use function in WHERE\nSELECT * FROM orders\nWHERE calculate_discount(total) > 0.10;\n\n-- Table-valued function (SQL Server)\nCREATE FUNCTION GetUserOrders(@user_id INT)\nRETURNS TABLE\nAS\nRETURN\n(\n    SELECT \n        id,\n        total,\n        order_date,\n        status\n    FROM orders\n    WHERE user_id = @user_id\n);\n\n-- Use table function\nSELECT * FROM GetUserOrders(123);\n\n-- SQL Server Stored Procedure\nCREATE PROCEDURE sp_CreateUser\n    @username NVARCHAR(50),\n    @email NVARCHAR(255),\n    @user_id INT OUTPUT\nAS\nBEGIN\n    SET NOCOUNT ON;\n    \n    BEGIN TRY\n        BEGIN TRANSACTION;\n        \n        -- Check if email exists\n        IF EXISTS (SELECT 1 FROM users WHERE email = @email)\n        BEGIN\n            RAISERROR('Email already exists', 16, 1);\n            RETURN;\n        END\n        \n        -- Insert user\n        INSERT INTO users (username, email, created_at)\n        VALUES (@username, @email, GETDATE());\n        \n        SET @user_id = SCOPE_IDENTITY();\n        \n        -- Create default account\n        INSERT INTO accounts (user_id, balance)\n        VALUES (@user_id, 0.00);\n        \n        COMMIT TRANSACTION;\n    END TRY\n    BEGIN CATCH\n        ROLLBACK TRANSACTION;\n        THROW;\n    END CATCH\nEND;\n\n-- Call SQL Server procedure\nDECLARE @new_user_id INT;\nEXEC sp_CreateUser \n    @username = 'john_doe',\n    @email = 'john@example.com',\n    @user_id = @new_user_id OUTPUT;\nSELECT @new_user_id AS user_id;\n\n-- PostgreSQL Function\nCREATE OR REPLACE FUNCTION get_user_tier(p_user_id INT)\nRETURNS VARCHAR(20)\nAS $$\nDECLARE\n    v_total_spent DECIMAL(15,2);\n    v_tier VARCHAR(20);\nBEGIN\n    SELECT COALESCE(SUM(total), 0)\n    INTO v_total_spent\n    FROM orders\n    WHERE user_id = p_user_id;\n    \n    IF v_total_spent > 10000 THEN\n        v_tier := 'PLATINUM';\n    ELSIF v_total_spent > 5000 THEN\n        v_tier := 'GOLD';\n    ELSIF v_total_spent > 1000 THEN\n        v_tier := 'SILVER';\n    ELSE\n        v_tier := 'BRONZE';\n    END IF;\n    \n    RETURN v_tier;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Use PostgreSQL function\nSELECT \n    name,\n    email,\n    get_user_tier(id) AS tier\nFROM users;\n\n-- PostgreSQL Procedure (11+)\nCREATE OR REPLACE PROCEDURE process_orders()\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    v_order RECORD;\nBEGIN\n    FOR v_order IN\n        SELECT id, user_id, total\n        FROM orders\n        WHERE status = 'PENDING'\n    LOOP\n        -- Process each order\n        UPDATE orders\n        SET \n            status = 'PROCESSED',\n            processed_at = NOW()\n        WHERE id = v_order.id;\n        \n        -- Update user stats\n        UPDATE user_stats\n        SET \n            total_orders = total_orders + 1,\n            total_spent = total_spent + v_order.total\n        WHERE user_id = v_order.user_id;\n    END LOOP;\n    \n    COMMIT;\nEND;\n$$;\n\n-- Call PostgreSQL procedure\nCALL process_orders();\n\n-- View stored procedures\n-- MySQL\nSHOW PROCEDURE STATUS WHERE Db = 'mydatabase';\nSHOW CREATE PROCEDURE create_order;\n\n-- SQL Server\nSELECT name, create_date, modify_date\nFROM sys.procedures;\n\n-- PostgreSQL\nSELECT * FROM pg_proc WHERE prokind = 'p';  -- Procedures\nSELECT * FROM pg_proc WHERE prokind = 'f';  -- Functions\n\n-- Drop stored procedure/function\nDROP PROCEDURE IF EXISTS create_order;\nDROP FUNCTION IF EXISTS calculate_discount;\n\n-- Alter stored procedure\nDELIMITER //\n\nCREATE OR REPLACE PROCEDURE get_user_orders(IN p_user_id INT)\nBEGIN\n    SELECT * FROM orders\n    WHERE user_id = p_user_id\n    ORDER BY order_date DESC\n    LIMIT 100;  -- Added limit\nEND//\n\nDELIMITER ;\n\n-- Application usage (Node.js)\nconst userId = 123;\nconst total = 99.99;\n\n// Call procedure\nconst [rows] = await db.query(\n    'CALL create_order(?, ?, @order_id)',\n    [userId, total]\n);\n\nconst [[{order_id}]] = await db.query('SELECT @order_id as order_id');\nconsole.log('Created order:', order_id);"
    },
    {
      "id": 38,
      "question": "What is database monitoring and key metrics?",
      "answer": "Database Monitoring: Track performance and health\n\nKey Metrics:\n\nPerformance:\n• Query response time\n• Queries per second (QPS)\n• Transactions per second (TPS)\n• Slow query count\n• Connection pool usage\n• Cache hit ratio\n\nResource Usage:\n• CPU utilization\n• Memory usage\n• Disk I/O (read/write)\n• Network throughput\n• Storage capacity\n\nAvailability:\n• Uptime percentage\n• Failed connections\n• Replication lag\n• Deadlock count\n• Transaction errors\n\nDatabase-specific:\n• Buffer pool hit rate\n• Table/index sizes\n• Lock wait time\n• Thread/connection count\n• Temp table usage\n\nMonitoring Tools:\n• Built-in: SHOW STATUS, pg_stat_*, DMVs\n• External: Prometheus, Datadog, New Relic\n• Logs: Slow query log, error log\n• Profiling: Query analyzer\n\nAlerts:\n• High CPU (>80%)\n• Slow queries (>1s)\n• Low disk space (<10%)\n• Connection limit reached\n• Replication lag (>10s)\n\nBest practices:\n• Set baseline metrics\n• Monitor trends\n• Automated alerts\n• Regular reporting\n• Capacity planning",
      "explanation": "Database monitoring tracks critical metrics to ensure performance, availability, and reliability, enabling proactive problem resolution.",
      "difficulty": "Medium",
      "code": "-- MySQL Monitoring Queries\n\n-- Global status\nSHOW GLOBAL STATUS;\nSHOW GLOBAL VARIABLES;\n\n-- Key performance metrics\nSHOW GLOBAL STATUS LIKE 'Threads_connected';  -- Current connections\nSHOW GLOBAL STATUS LIKE 'Questions';  -- Total queries\nSHOW GLOBAL STATUS LIKE 'Slow_queries';  -- Slow queries count\nSHOW GLOBAL STATUS LIKE 'Uptime';  -- Server uptime\n\n-- Connection statistics\nSHOW GLOBAL STATUS LIKE '%connect%';\nSHOW PROCESSLIST;  -- Active connections\n\n-- Full processlist with details\nSELECT \n    id,\n    user,\n    host,\n    db,\n    command,\n    time,\n    state,\n    LEFT(info, 100) AS query\nFROM information_schema.processlist\nWHERE command != 'Sleep'\nORDER BY time DESC;\n\n-- Long-running queries\nSELECT \n    id,\n    user,\n    host,\n    db,\n    time,\n    state,\n    info AS query\nFROM information_schema.processlist\nWHERE time > 60  -- Running for more than 60 seconds\n  AND command != 'Sleep'\nORDER BY time DESC;\n\n-- Kill long-running query\nKILL 12345;  -- Process ID\n\n-- InnoDB buffer pool statistics\nSHOW ENGINE INNODB STATUS\\G\n\nSELECT \n    @@innodb_buffer_pool_size / 1024 / 1024 / 1024 AS buffer_pool_gb,\n    @@innodb_buffer_pool_instances AS pool_instances;\n\n-- Buffer pool hit ratio\nSHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_read%';\n-- Calculate: (Innodb_buffer_pool_read_requests - Innodb_buffer_pool_reads) \n--           / Innodb_buffer_pool_read_requests * 100\n\n-- Table sizes\nSELECT \n    table_schema AS database_name,\n    table_name,\n    ROUND(((data_length + index_length) / 1024 / 1024), 2) AS size_mb,\n    table_rows\nFROM information_schema.tables\nWHERE table_schema NOT IN ('information_schema', 'mysql', 'performance_schema')\nORDER BY (data_length + index_length) DESC\nLIMIT 20;\n\n-- Index usage statistics\nSELECT \n    t.table_schema,\n    t.table_name,\n    t.index_name,\n    t.seq_in_index,\n    t.column_name,\n    s.cardinality\nFROM information_schema.statistics t\nLEFT JOIN (SELECT table_schema, table_name, index_name, cardinality \n           FROM information_schema.statistics) s\n    ON t.table_schema = s.table_schema\n    AND t.table_name = s.table_name\n    AND t.index_name = s.index_name\nWHERE t.table_schema = 'mydatabase'\nORDER BY t.table_name, t.index_name, t.seq_in_index;\n\n-- Slow query log\n-- Enable slow query log\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 1;  -- Log queries > 1 second\nSET GLOBAL slow_query_log_file = '/var/log/mysql/slow-query.log';\n\n-- Analyze slow query log\n-- Use pt-query-digest (Percona Toolkit)\n-- pt-query-digest /var/log/mysql/slow-query.log\n\n-- PostgreSQL Monitoring\n\n-- Current activity\nSELECT \n    pid,\n    usename,\n    application_name,\n    client_addr,\n    state,\n    query_start,\n    state_change,\n    wait_event,\n    LEFT(query, 100) AS query\nFROM pg_stat_activity\nWHERE state != 'idle'\nORDER BY query_start;\n\n-- Long-running queries\nSELECT \n    pid,\n    NOW() - query_start AS duration,\n    usename,\n    query\nFROM pg_stat_activity\nWHERE state = 'active'\n  AND NOW() - query_start > INTERVAL '5 minutes'\nORDER BY duration DESC;\n\n-- Terminate query\nSELECT pg_terminate_backend(12345);  -- PID\n\n-- Database statistics\nSELECT \n    datname,\n    numbackends,  -- Active connections\n    xact_commit,  -- Committed transactions\n    xact_rollback,  -- Rolled back transactions\n    blks_read,  -- Disk blocks read\n    blks_hit,   -- Buffer hits\n    blks_hit::float / (blks_hit + blks_read) AS cache_hit_ratio,\n    tup_returned,\n    tup_fetched,\n    tup_inserted,\n    tup_updated,\n    tup_deleted,\n    conflicts,\n    deadlocks\nFROM pg_stat_database\nWHERE datname = 'mydatabase';\n\n-- Table statistics\nSELECT \n    schemaname,\n    tablename,\n    n_live_tup AS row_count,\n    n_dead_tup AS dead_rows,\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze,\n    seq_scan,  -- Sequential scans\n    idx_scan   -- Index scans\nFROM pg_stat_user_tables\nORDER BY n_live_tup DESC;\n\n-- Index usage\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,  -- Times index used\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0  -- Unused indexes\nORDER BY pg_relation_size(indexrelid) DESC;\n\n-- Cache hit ratio\nSELECT \n    SUM(heap_blks_read) AS heap_read,\n    SUM(heap_blks_hit) AS heap_hit,\n    SUM(heap_blks_hit) / (SUM(heap_blks_hit) + SUM(heap_blks_read)) AS cache_ratio\nFROM pg_statio_user_tables;\n\n-- Bloat estimation\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,\n    n_dead_tup,\n    n_live_tup,\n    n_dead_tup::float / NULLIF(n_live_tup, 0) AS dead_ratio\nFROM pg_stat_user_tables\nWHERE n_dead_tup > 1000\nORDER BY n_dead_tup DESC;\n\n-- SQL Server DMVs\n\n-- Current sessions\nSELECT \n    session_id,\n    login_name,\n    host_name,\n    program_name,\n    status,\n    cpu_time,\n    memory_usage,\n    total_elapsed_time,\n    reads,\n    writes\nFROM sys.dm_exec_sessions\nWHERE is_user_process = 1;\n\n-- Currently executing queries\nSELECT \n    r.session_id,\n    r.status,\n    r.command,\n    r.cpu_time,\n    r.total_elapsed_time / 1000 AS elapsed_seconds,\n    t.text AS query_text\nFROM sys.dm_exec_requests r\nCROSS APPLY sys.dm_exec_sql_text(r.sql_handle) t;\n\n-- Expensive queries (by CPU)\nSELECT TOP 10\n    total_worker_time / execution_count AS avg_cpu_time,\n    execution_count,\n    total_elapsed_time / execution_count AS avg_elapsed_time,\n    SUBSTRING(st.text, (qs.statement_start_offset/2) + 1,\n        ((CASE qs.statement_end_offset\n            WHEN -1 THEN DATALENGTH(st.text)\n            ELSE qs.statement_end_offset\n        END - qs.statement_start_offset)/2) + 1) AS query_text\nFROM sys.dm_exec_query_stats qs\nCROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) st\nORDER BY avg_cpu_time DESC;\n\n-- Wait statistics\nSELECT \n    wait_type,\n    waiting_tasks_count,\n    wait_time_ms,\n    max_wait_time_ms,\n    signal_wait_time_ms\nFROM sys.dm_os_wait_stats\nWHERE waiting_tasks_count > 0\nORDER BY wait_time_ms DESC;\n\n-- Index usage statistics\nSELECT \n    OBJECT_NAME(s.object_id) AS table_name,\n    i.name AS index_name,\n    s.user_seeks,\n    s.user_scans,\n    s.user_lookups,\n    s.user_updates\nFROM sys.dm_db_index_usage_stats s\nJOIN sys.indexes i ON s.object_id = i.object_id AND s.index_id = i.index_id\nWHERE OBJECTPROPERTY(s.object_id, 'IsUserTable') = 1\nORDER BY s.user_seeks + s.user_scans + s.user_lookups DESC;\n\n-- Database size\nEXEC sp_spaceused;\n\n-- Monitoring script (Python)\nimport pymysql\nimport time\n\ndef monitor_mysql():\n    conn = pymysql.connect(host='localhost', user='monitor', password='pwd', database='sys')\n    cursor = conn.cursor()\n    \n    while True:\n        # Connections\n        cursor.execute(\"SHOW GLOBAL STATUS LIKE 'Threads_connected'\")\n        connections = cursor.fetchone()[1]\n        \n        # QPS\n        cursor.execute(\"SHOW GLOBAL STATUS LIKE 'Questions'\")\n        questions = int(cursor.fetchone()[1])\n        time.sleep(1)\n        cursor.execute(\"SHOW GLOBAL STATUS LIKE 'Questions'\")\n        questions_new = int(cursor.fetchone()[1])\n        qps = questions_new - questions\n        \n        # Slow queries\n        cursor.execute(\"SHOW GLOBAL STATUS LIKE 'Slow_queries'\")\n        slow = cursor.fetchone()[1]\n        \n        print(f\"Connections: {connections}, QPS: {qps}, Slow: {slow}\")\n        \n        # Alert if needed\n        if int(connections) > 100:\n            print(\"ALERT: High connection count!\")\n        \n        time.sleep(60)\n    \n    conn.close()\n\n# monitor_mysql()"
    },
    {
      "id": 39,
      "question": "What is database pooling and connection management?",
      "answer": "Connection Pooling: Reuse database connections\n\nWithout Pooling:\n• Create connection per request\n• Expensive: TCP handshake, authentication\n• Close after use\n• High overhead, slow\n\nWith Pooling:\n• Maintain pool of connections\n• Reuse existing connections\n• Return to pool after use\n• Fast, efficient\n\nPool Configuration:\n• minPoolSize: Minimum connections kept alive\n• maxPoolSize: Maximum connections\n• connectionTimeout: Wait time for connection\n• idleTimeout: Close idle connections after\n• maxLifetime: Max age of connection\n• leakDetectionThreshold: Detect connection leaks\n\nConnection States:\n• Active: In use by application\n• Idle: Available in pool\n• Exhausted: All connections busy\n• Stale: Connection broken/invalid\n\nBest Practices:\n• Set appropriate pool size\n• Always release connections\n• Handle connection failures\n• Monitor pool metrics\n• Use timeouts\n• Close pool on shutdown\n\nCommon Issues:\n• Connection leaks: Not released\n• Pool exhaustion: Too many connections\n• Stale connections: Network issues\n• Timeout errors: Pool too small",
      "explanation": "Connection pooling improves performance and resource usage by reusing database connections instead of creating new ones for each request.",
      "difficulty": "Medium",
      "code": "-- This was already created in earlier part, so continuing with additional concepts\n\n-- Advanced pooling configurations\n\n// HikariCP (Java) - High performance\nimport com.zaxxer.hikari.HikariConfig;\nimport com.zaxxer.hikari.HikariDataSource;\n\npublic class AdvancedPoolConfig {\n    public static HikariDataSource createPool() {\n        HikariConfig config = new HikariConfig();\n        \n        // Connection settings\n        config.setJdbcUrl(\"jdbc:mysql://localhost:3306/mydb\");\n        config.setUsername(\"dbuser\");\n        config.setPassword(\"password\");\n        config.setDriverClassName(\"com.mysql.cj.jdbc.Driver\");\n        \n        // Pool sizing\n        config.setMinimumIdle(5);              // Min connections kept ready\n        config.setMaximumPoolSize(20);         // Max total connections\n        config.setConnectionTimeout(30000);    // 30s wait for connection\n        config.setIdleTimeout(600000);         // 10min idle time\n        config.setMaxLifetime(1800000);        // 30min max age\n        config.setLeakDetectionThreshold(60000); // 1min leak detection\n        \n        // Performance tuning\n        config.addDataSourceProperty(\"cachePrepStmts\", \"true\");\n        config.addDataSourceProperty(\"prepStmtCacheSize\", \"250\");\n        config.addDataSourceProperty(\"prepStmtCacheSqlLimit\", \"2048\");\n        config.addDataSourceProperty(\"useServerPrepStmts\", \"true\");\n        config.addDataSourceProperty(\"useLocalSessionState\", \"true\");\n        config.addDataSourceProperty(\"rewriteBatchedStatements\", \"true\");\n        config.addDataSourceProperty(\"cacheResultSetMetadata\", \"true\");\n        config.addDataSourceProperty(\"cacheServerConfiguration\", \"true\");\n        config.addDataSourceProperty(\"elideSetAutoCommits\", \"true\");\n        config.addDataSourceProperty(\"maintainTimeStats\", \"false\");\n        \n        // Health check\n        config.setConnectionTestQuery(\"SELECT 1\");\n        config.setValidationTimeout(5000);\n        \n        return new HikariDataSource(config);\n    }\n}\n\n// Connection leak detection\npublic class DatabaseService {\n    private final HikariDataSource dataSource;\n    \n    public void problematicMethod() {\n        Connection conn = null;\n        try {\n            conn = dataSource.getConnection();\n            // Use connection\n            PreparedStatement stmt = conn.prepareStatement(\"SELECT * FROM users\");\n            ResultSet rs = stmt.executeQuery();\n            // Process results\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n        // BUG: Connection not closed! Will be detected by leak detection\n    }\n    \n    public void correctMethod() {\n        // Use try-with-resources (auto-close)\n        try (Connection conn = dataSource.getConnection();\n             PreparedStatement stmt = conn.prepareStatement(\"SELECT * FROM users\");\n             ResultSet rs = stmt.executeQuery()) {\n            \n            while (rs.next()) {\n                // Process results\n            }\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n        // Connection automatically returned to pool\n    }\n}\n\n// Node.js - mysql2 pool with promises\nconst mysql = require('mysql2/promise');\n\nconst pool = mysql.createPool({\n    host: 'localhost',\n    user: 'dbuser',\n    password: 'password',\n    database: 'mydb',\n    waitForConnections: true,\n    connectionLimit: 10,\n    maxIdle: 10,\n    idleTimeout: 60000,\n    queueLimit: 0,\n    enableKeepAlive: true,\n    keepAliveInitialDelay: 0\n});\n\n// Proper usage\nasync function getUserOrders(userId) {\n    const connection = await pool.getConnection();\n    try {\n        await connection.beginTransaction();\n        \n        const [users] = await connection.query(\n            'SELECT * FROM users WHERE id = ?',\n            [userId]\n        );\n        \n        const [orders] = await connection.query(\n            'SELECT * FROM orders WHERE user_id = ?',\n            [userId]\n        );\n        \n        await connection.commit();\n        return { user: users[0], orders };\n    } catch (error) {\n        await connection.rollback();\n        throw error;\n    } finally {\n        connection.release();  // MUST release\n    }\n}\n\n// Or use pool directly (auto-manages connection)\nasync function getUsers() {\n    const [rows] = await pool.query('SELECT * FROM users');\n    return rows;\n    // Connection automatically released\n}\n\n// Pool health monitoring\nsetInterval(async () => {\n    console.log('Pool Status:', {\n        total: pool.pool._allConnections.length,\n        active: pool.pool._allConnections.length - pool.pool._freeConnections.length,\n        idle: pool.pool._freeConnections.length,\n        queue: pool.pool._connectionQueue.length\n    });\n}, 30000);\n\n// Graceful shutdown\nprocess.on('SIGTERM', async () => {\n    console.log('Closing database pool...');\n    await pool.end();\n    process.exit(0);\n});\n\n// Python - psycopg2 connection pool\nimport psycopg2\nfrom psycopg2 import pool\nimport contextlib\n\nclass DatabasePool:\n    def __init__(self):\n        self.connection_pool = psycopg2.pool.ThreadedConnectionPool(\n            minconn=1,\n            maxconn=10,\n            host='localhost',\n            database='mydb',\n            user='dbuser',\n            password='password'\n        )\n    \n    @contextlib.contextmanager\n    def get_connection(self):\n        conn = self.connection_pool.getconn()\n        try:\n            yield conn\n        finally:\n            self.connection_pool.putconn(conn)\n    \n    def query(self, sql, params=None):\n        with self.get_connection() as conn:\n            with conn.cursor() as cursor:\n                cursor.execute(sql, params)\n                return cursor.fetchall()\n    \n    def execute(self, sql, params=None):\n        with self.get_connection() as conn:\n            with conn.cursor() as cursor:\n                cursor.execute(sql, params)\n                conn.commit()\n    \n    def close_all(self):\n        self.connection_pool.closeall()\n\n# Usage\ndb = DatabasePool()\n\ntry:\n    users = db.query('SELECT * FROM users WHERE age > %s', (25,))\n    for user in users:\n        print(user)\nfinally:\n    db.close_all()\n\n// C# - SQL Server connection pooling\nusing System.Data.SqlClient;\n\npublic class DatabaseManager\n{\n    // Connection string with pooling enabled\n    private static string connectionString = \n        \"Server=localhost;\" +\n        \"Database=mydb;\" +\n        \"User Id=dbuser;\" +\n        \"Password=password;\" +\n        \"Min Pool Size=5;\" +\n        \"Max Pool Size=100;\" +\n        \"Pooling=true;\" +\n        \"Connection Timeout=30;\" +\n        \"Connection Lifetime=0;\" +  // 0 = no limit\n        \"Enlist=true;\";\n    \n    public static List<User> GetUsers()\n    {\n        var users = new List<User>();\n        \n        // Connection automatically pooled\n        using (var connection = new SqlConnection(connectionString))\n        {\n            connection.Open();\n            \n            using (var command = new SqlCommand(\"SELECT * FROM users\", connection))\n            using (var reader = command.ExecuteReader())\n            {\n                while (reader.Read())\n                {\n                    users.Add(new User\n                    {\n                        Id = reader.GetInt32(0),\n                        Name = reader.GetString(1)\n                    });\n                }\n            }\n        }  // Connection returned to pool here\n        \n        return users;\n    }\n    \n    // Clear all connection pools\n    public static void ClearPools()\n    {\n        SqlConnection.ClearAllPools();\n    }\n}\n\n// Connection pool troubleshooting\n\n// Problem: Pool exhaustion\n// Symptom: Timeout waiting for connection\n// Solutions:\n// 1. Increase poolsize (if server can handle)\nconfig.setMaximumPoolSize(50);  // Increase limit\n\n// 2. Find and fix connection leaks\nconfig.setLeakDetectionThreshold(10000);  // Detect leaks > 10s\n\n// 3. reduce connection hold time\n// Execute queries faster, release sooner\n\n// Problem: Too many connections to database\n// Solution: Reduce pool size per application instance\n// Calculate: (num_app_instances * maxPoolSize) < db_max_connections\n\n// Problem: Stale connections\n// Solution: Test connections before use\nconfig.setConnectionTestQuery(\"SELECT 1\");\nconfig.setValidationTimeout(3000);\n\n// Problem: Connection timeout on burst traffic\n// Solution: Increase connection timeout\nconfig.setConnectionTimeout(60000);  // 60 seconds\n\n// Or implement queue with backpressure\nconst limiter = new Bottleneck({\n    maxConcurrent: 10,\n    minTime: 100\n});\n\nconst wrappedQuery = limiter.wrap(async (sql, params) => {\n    return await pool.query(sql, params);\n});"
    },
    {
      "id": 40,
      "question": "What are materialized views and when to use them?",
      "answer": "Materialized View: Precomputed query results stored as table\n\nRegular View:\n• Virtual table\n• Query executed each time accessed\n• Always up-to-date\n• No storage overhead\n• Slower for complex queries\n\nMaterialized View:\n• Physical table with data\n• Query executed once, results stored\n• Must be refreshed\n• Storage overhead\n• Fast reads, slower writes\n\nRefresh Strategies:\n\nManual:\n• Explicit REFRESH command\n• Full control over timing\n\nScheduled:\n• Cron/scheduled job\n• Regular intervals\n\nOn-Demand:\n• Refresh when needed\n• Before important queries\n\nIncremental:\n• Only update changes (if supported)\n• Faster than full refresh\n\nWhen to use:\n• Expensive aggregations\n• Complex JOINs\n• Reporting/analytics\n• Dashboard queries\n• Read-heavy workloads\n• Data doesn't change frequently\n\nWhen NOT to use:\n• Real-time data required\n• Frequently changing data\n• Simple queries\n• Limited storage\n• High write workload",
      "explanation": "Materialized views precompute and store query results for fast repeated access, trading freshness for performance.",
      "difficulty": "Hard",
      "code": "-- PostgreSQL Materialized View\nCREATE MATERIALIZED VIEW user_order_summary AS\nSELECT \n    u.id AS user_id,\n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.total) AS total_spent,\n    AVG(o.total) AS avg_order_value,\n    MAX(o.order_date) AS last_order_date\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name, u.email;\n\n-- Query materialized view (fast!)\nSELECT * FROM user_order_summary\nWHERE total_spent > 1000\nORDER BY total_spent DESC;\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW user_order_summary;\n\n-- Refresh without locking (allows concurrent reads)\nREFRESH MATERIALIZED VIEW CONCURRENTLY user_order_summary;\n-- Requires unique index for CONCURRENTLY\nCREATE UNIQUE INDEX ON user_order_summary(user_id);\n\n-- Drop materialized view\nDROP MATERIALIZED VIEW user_order_summary;\n\n-- Oracle Materialized View\nCREATE MATERIALIZED VIEW product_sales_mv\nBUILD IMMEDIATE              -- Create and populate now\nREFRESH FAST                 -- Incremental refresh if possible\nON DEMAND                    -- Manual refresh\nENABLE QUERY REWRITE         -- Optimizer can use automatically\nAS\nSELECT \n    p.id AS product_id,\n    p.name AS product_name,\n    p.category,\n    SUM(oi.quantity) AS units_sold,\n    SUM(oi.quantity * oi.price) AS total_revenue,\n    COUNT(DISTINCT oi.order_id) AS order_count\nFROM products p\nJOIN order_items oi ON p.id = oi.product_id\nGROUP BY p.id, p.name, p.category;\n\n-- Refresh Oracle materialized view\nEXEC DBMS_MVIEW.REFRESH('product_sales_mv');\n\n-- Scheduled refresh (Oracle)\nBEGIN\n    DBMS_SCHEDULER.CREATE_JOB (\n        job_name        => 'refresh_product_sales',\n        job_type        => 'PLSQL_BLOCK',\n        job_action      => 'BEGIN DBMS_MVIEW.REFRESH(''product_sales_mv''); END;',\n        start_date      => SYSTIMESTAMP,\n        repeat_interval => 'FREQ=HOURLY; INTERVAL=1',\n        enabled         => TRUE\n    );\nEND;\n/\n\n-- SQL Server Indexed View (similar to materialized view)\nCREATE VIEW dbo.OrderSummary\nWITH SCHEMABINDING  -- Required for indexed view\nAS\nSELECT \n    o.user_id,\n    COUNT_BIG(*) AS order_count,  -- COUNT_BIG required\n    SUM(o.total) AS total_spent\nFROM dbo.orders o\nGROUP BY o.user_id;\nGO\n\n-- Create unique clustered index (materializes the view)\nCREATE UNIQUE CLUSTERED INDEX idx_OrderSummary\nON dbo.OrderSummary(user_id);\nGO\n\n-- Automatically maintained by SQL Server\n-- Updates propagate to indexed view\n\n-- MySQL: No native materialized views, but can simulate\n\n-- Create table for materialized view\nCREATE TABLE mv_daily_sales (\n    sale_date DATE PRIMARY KEY,\n    total_orders INT,\n    total_revenue DECIMAL(15,2),\n    unique_customers INT,\n    avg_order_value DECIMAL(10,2),\n    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);\n\n-- Populate/refresh with stored procedure\nDELIMITER //\n\nCREATE PROCEDURE refresh_daily_sales()\nBEGIN\n    TRUNCATE TABLE mv_daily_sales;\n    \n    INSERT INTO mv_daily_sales (sale_date, total_orders, total_revenue, unique_customers, avg_order_value)\n    SELECT \n        DATE(order_date) AS sale_date,\n        COUNT(*) AS total_orders,\n        SUM(total) AS total_revenue,\n        COUNT(DISTINCT user_id) AS unique_customers,\n        AVG(total) AS avg_order_value\n    FROM orders\n    GROUP BY DATE(order_date);\nEND//\n\nDELIMITER ;\n\n-- Refresh\nCALL refresh_daily_sales();\n\n-- Schedule with MySQL Event Scheduler\nSET GLOBAL event_scheduler = ON;\n\nCREATE EVENT refresh_daily_sales_event\nON SCHEDULE EVERY 1 HOUR\nDO CALL refresh_daily_sales();\n\n-- Incremental refresh pattern (MySQL)\nCREATE TABLE mv_product_stats (\n    product_id INT PRIMARY KEY,\n    total_sold INT,\n    total_revenue DECIMAL(15,2),\n    last_sale_date DATETIME,\n    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);\n\n-- Initial populate\nINSERT INTO mv_product_stats\nSELECT \n    product_id,\n    SUM(quantity) AS total_sold,\n    SUM(quantity * price) AS total_revenue,\n    MAX(created_at) AS last_sale_date\nFROM order_items\nGROUP BY product_id;\n\n-- Incremental refresh (only recent changes)\nDELIMITER //\n\nCREATE PROCEDURE refresh_product_stats_incremental()\nBEGIN\n    DECLARE v_last_update TIMESTAMP;\n    \n    -- Get last refresh time\n    SELECT MAX(last_updated) INTO v_last_update FROM mv_product_stats;\n    \n    -- Update affected products\n    INSERT INTO mv_product_stats (product_id, total_sold, total_revenue, last_sale_date)\n    SELECT \n        oi.product_id,\n        SUM(oi.quantity),\n        SUM(oi.quantity * oi.price),\n        MAX(oi.created_at)\n    FROM order_items oi\n    WHERE oi.created_at > v_last_update\n    GROUP BY oi.product_id\n    ON DUPLICATE KEY UPDATE\n        total_sold = total_sold + VALUES(total_sold),\n        total_revenue = total_revenue + VALUES(total_revenue),\n        last_sale_date = GREATEST(last_sale_date, VALUES(last_sale_date)),\n        last_updated = CURRENT_TIMESTAMP;\nEND//\n\nDELIMITER ;\n\n-- Complex reporting materialized view\nCREATE MATERIALIZED VIEW monthly_revenue_report AS\nSELECT \n    DATE_TRUNC('month', o.order_date) AS month,\n    p.category,\n    u.country,\n    COUNT(DISTINCT o.id) AS order_count,\n    COUNT(DISTINCT o.user_id) AS unique_customers,\n    SUM(oi.quantity) AS units_sold,\n    SUM(oi.quantity * oi.price) AS revenue,\n    AVG(o.total) AS avg_order_value\nFROM orders o\nJOIN order_items oi ON o.id = oi.order_id\nJOIN products p ON oi.product_id = p.id\nJOIN users u ON o.user_id = u.id\nGROUP BY DATE_TRUNC('month', o.order_date), p.category, u.country;\n\n-- Create indexes on materialized view\nCREATE INDEX idx_month ON monthly_revenue_report(month);\nCREATE INDEX idx_category ON monthly_revenue_report(category);\nCREATE INDEX idx_country ON monthly_revenue_report(country);\n\n-- Fast queries on materialized view\nSELECT \n    month,\n    category,\n    SUM(revenue) AS total_revenue\nFROM monthly_revenue_report\nWHERE month >= '2024-01-01'\nGROUP BY month, category\nORDER BY month, total_revenue DESC;\n\n-- Application layer cache (alternative to MV)\nconst redis = require('redis');\nconst client = redis.createClient();\n\nasync function getUserOrderSummary(userId) {\n    const cacheKey = `user:${userId}:summary`;\n    \n    // Check cache\n    let summary = await client.get(cacheKey);\n    \n    if (!summary) {\n        // Compute from database\n        const [rows] = await db.query(`\n            SELECT \n                COUNT(*) AS order_count,\n                SUM(total) AS total_spent,\n                AVG(total) AS avg_order\n            FROM orders\n            WHERE user_id = ?\n        `, [userId]);\n        \n        summary = JSON.stringify(rows[0]);\n        \n        // Cache for 1 hour\n        await client.setex(cacheKey, 3600, summary);\n    }\n    \n    return JSON.parse(summary);\n}\n\n// Invalidate cache on write\nasync function createOrder(userId, total) {\n    await db.query('INSERT INTO orders (user_id, total) VALUES (?, ?)', [userId, total]);\n    \n    // Invalidate cached summary\n    await client.del(`user:${userId}:summary`);\n}\n\n-- Monitor materialized view freshness\nSELECT \n    schemaname,\n    matviewname,\n    last_refresh,\n    NOW() - last_refresh AS staleness\nFROM pg_matviews\nORDER BY staleness DESC;"
    },
    {
      "id": 41,
      "question": "What are database triggers and how do they work?",
      "answer": "Triggers: Automatic procedures that execute on table events\n\nTypes by Timing:\n• BEFORE: Execute before operation\n• AFTER: Execute after operation\n• INSTEAD OF: Replace operation (views)\n\nTypes by Event:\n• INSERT triggers\n• UPDATE triggers\n• DELETE triggers\n• Can combine (INSERT OR UPDATE)\n\nCommon Use Cases:\n• Audit logging\n• Data validation\n• Enforce business rules\n• Maintain denormalized data\n• Prevent invalid operations\n• Cascade operations\n• Timestamp updates\n\nRow vs Statement Level:\n• Row-level: Fires per affected row\n• Statement-level: Fires once per statement\n\nBest Practices:\n• Keep triggers simple and fast\n• Avoid complex logic\n• Document trigger behavior\n• Be careful with cascading triggers\n• Use for data integrity, not business logic\n• Consider alternatives (constraints, app logic)\n\nPitfalls:\n• Performance impact on writes\n• Hidden side effects\n• Debugging difficulty\n• Recursive triggers\n• Transaction complications",
      "explanation": "Triggers automatically execute SQL code in response to INSERT, UPDATE, or DELETE events, useful for auditing, validation, and maintaining data integrity.",
      "difficulty": "Medium",
      "code": "-- MySQL BEFORE trigger (validation)\nDELIMITER //\n\nCREATE TRIGGER validate_order_before_insert\nBEFORE INSERT ON orders\nFOR EACH ROW\nBEGIN\n    -- Validate total is positive\n    IF NEW.total < 0 THEN\n        SIGNAL SQLSTATE '45000'\n        SET MESSAGE_TEXT = 'Order total cannot be negative';\n    END IF;\n    \n    -- Auto-set created date\n    IF NEW.created_at IS NULL THEN\n        SET NEW.created_at = NOW();\n    END IF;\nEND//\n\nDELIMITER ;\n\n-- AFTER trigger (audit logging)\nCREATE TABLE audit_log (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    table_name VARCHAR(50),\n    operation VARCHAR(10),\n    record_id INT,\n    old_value TEXT,\n    new_value TEXT,\n    changed_by VARCHAR(100),\n    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nDELIMITER //\n\nCREATE TRIGGER audit_user_changes\nAFTER UPDATE ON users\nFOR EACH ROW\nBEGIN\n    INSERT INTO audit_log (table_name, operation, record_id, old_value, new_value, changed_by)\n    VALUES (\n        'users',\n        'UPDATE',\n        NEW.id,\n        JSON_OBJECT('email', OLD.email, 'name', OLD.name),\n        JSON_OBJECT('email', NEW.email, 'name', NEW.name),\n        USER()\n    );\nEND//\n\nDELIMITER ;\n\n-- Denormalization trigger\nDELIMITER //\n\nCREATE TRIGGER update_order_count_on_insert\nAFTER INSERT ON orders\nFOR EACH ROW\nBEGIN\n    UPDATE users\n    SET order_count = order_count + 1,\n        total_spent = total_spent + NEW.total\n    WHERE id = NEW.user_id;\nEND//\n\nCREATE TRIGGER update_order_count_on_delete\nAFTER DELETE ON orders\nFOR EACH ROW\nBEGIN\n    UPDATE users\n    SET order_count = order_count - 1,\n        total_spent = total_spent - OLD.total\n    WHERE id = OLD.user_id;\nEND//\n\nDELIMITER ;\n\n-- PostgreSQL trigger function\nCREATE OR REPLACE FUNCTION update_modified_timestamp()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.updated_at = NOW();\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER set_updated_timestamp\nBEFORE UPDATE ON users\nFOR EACH ROW\nEXECUTE FUNCTION update_modified_timestamp();\n\n-- Prevent delete trigger\nDELIMITER //\n\nCREATE TRIGGER prevent_admin_delete\nBEFORE DELETE ON users\nFOR EACH ROW\nBEGIN\n    IF OLD.role = 'ADMIN' THEN\n        SIGNAL SQLSTATE '45000'\n        SET MESSAGE_TEXT = 'Cannot delete admin users';\n    END IF;\nEND//\n\nDELIMITER ;\n\n-- INSTEAD OF trigger (SQL Server view)\nCREATE VIEW active_users AS\nSELECT id, name, email FROM users WHERE status = 'ACTIVE';\nGO\n\nCREATE TRIGGER update_active_users\nINSTEAD OF UPDATE ON active_users\nAS\nBEGIN\n    UPDATE users\n    SET name = i.name,\n        email = i.email,\n        updated_at = GETDATE()\n    FROM users u\n    INNER JOIN inserted i ON u.id = i.id\n    WHERE u.status = 'ACTIVE';\nEND;\nGO\n\n-- Check if column changed\nDELIMITER //\n\nCREATE TRIGGER log_price_changes\nAFTER UPDATE ON products\nFOR EACH ROW\nBEGIN\n    -- Only log if price changed\n    IF OLD.price != NEW.price THEN\n        INSERT INTO price_history (product_id, old_price, new_price, changed_at)\n        VALUES (NEW.id, OLD.price, NEW.price, NOW());\n    END IF;\nEND//\n\nDELIMITER ;\n\n-- List triggers\nSHOW TRIGGERS;\nSHOW TRIGGERS FROM mydatabase;\nSHOW TRIGGERS LIKE 'users';\n\n-- Drop trigger\nDROP TRIGGER IF EXISTS validate_order_before_insert;\n\n-- Disable/Enable triggers (SQL Server)\nDISABLE TRIGGER audit_user_changes ON users;\nENABLE TRIGGER audit_user_changes ON users;"
    },
    {
      "id": 42,
      "question": "What are foreign keys and referential integrity?",
      "answer": "Foreign Key: Column(s) referencing primary key in another table\n\nPurpose:\n• Enforce referential integrity\n• Maintain relationships\n• Prevent orphaned records\n• Document table relationships\n• Enable cascade operations\n\nReferential Actions:\n\nON DELETE:\n• CASCADE: Delete related rows\n• SET NULL: Set foreign key to NULL\n• SET DEFAULT: Set to default value\n• RESTRICT/NO ACTION: Prevent delete\n\nON UPDATE:\n• CASCADE: Update related rows\n• SET NULL: Set foreign key to NULL\n• RESTRICT/NO ACTION: Prevent update\n\nBenefits:\n• Data integrity guaranteed\n• Automatic cascade operations\n• Clear relationships\n• Query optimization hints\n• Documentation\n\nPerformance Impact:\n• Slower INSERT/UPDATE/DELETE\n• Additional checks\n• Lock considerations\n• Index maintenance\n\nWhen to Use:\n• Enforce data integrity\n• Critical relationships\n• Multi-user systems\n• Complex data models\n\nWhen to Skip:\n• High write volume\n• Sharded databases\n• Application enforces integrity\n• Performance critical",
      "explanation": "Foreign keys enforce referential integrity by ensuring values in one table match values in another, preventing orphaned records and maintaining data consistency.",
      "difficulty": "Medium",
      "code": "-- Create tables with foreign key\nCREATE TABLE users (\n    id INT PRIMARY KEY AUTO_INCREMENT,\n    name VARCHAR(100),\n    email VARCHAR(255) UNIQUE\n);\n\nCREATE TABLE orders (\n    id INT PRIMARY KEY AUTO_INCREMENT,\n    user_id INT NOT NULL,\n    total DECIMAL(10,2),\n    order_date DATE,\n    FOREIGN KEY (user_id) REFERENCES users(id)\n);\n\n-- Foreign key with CASCADE\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT NOT NULL,\n    total DECIMAL(10,2),\n    FOREIGN KEY (user_id) REFERENCES users(id)\n        ON DELETE CASCADE\n        ON UPDATE CASCADE\n);\n-- Deleting user automatically deletes their orders\n\n-- SET NULL action\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT,  -- Nullable\n    total DECIMAL(10,2),\n    FOREIGN KEY (user_id) REFERENCES users(id)\n        ON DELETE SET NULL\n);\n-- Deleting user sets user_id to NULL in orders\n\n-- RESTRICT (default)\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT NOT NULL,\n    total DECIMAL(10,2),\n    FOREIGN KEY (user_id) REFERENCES users(id)\n        ON DELETE RESTRICT\n);\n-- Cannot delete user if they have orders\n\n-- Named foreign key\nCREATE TABLE order_items (\n    id INT PRIMARY KEY,\n    order_id INT NOT NULL,\n    product_id INT NOT NULL,\n    quantity INT,\n    CONSTRAINT fk_order FOREIGN KEY (order_id) REFERENCES orders(id),\n    CONSTRAINT fk_product FOREIGN KEY (product_id) REFERENCES products(id)\n);\n\n-- Add foreign key to existing table\nALTER TABLE orders\nADD CONSTRAINT fk_orders_user\nFORREIGN KEY (user_id) REFERENCES users(id)\nON DELETE CASCADE;\n\n-- Composite foreign key\nCREATE TABLE order_shipments (\n    id INT PRIMARY KEY,\n    order_id INT,\n    order_number VARCHAR(20),\n    shipment_date DATE,\n    FOREIGN KEY (order_id, order_number) \n        REFERENCES orders(id, order_number)\n);\n\n-- Self-referencing foreign key (hierarchy)\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    manager_id INT,\n    FOREIGN KEY (manager_id) REFERENCES employees(id)\n        ON DELETE SET NULL\n);\n\n-- Multiple foreign keys\nCREATE TABLE order_reviews (\n    id INT PRIMARY KEY,\n    order_id INT NOT NULL,\n    user_id INT NOT NULL,\n    product_id INT NOT NULL,\n    rating INT,\n    comment TEXT,\n    FOREIGN KEY (order_id) REFERENCES orders(id),\n    FOREIGN KEY (user_id) REFERENCES users(id),\n    FOREIGN KEY (product_id) REFERENCES products(id)\n);\n\n-- Check foreign key constraints\nSELECT \n    TABLE_NAME,\n    COLUMN_NAME,\n    CONSTRAINT_NAME,\n    REFERENCED_TABLE_NAME,\n    REFERENCED_COLUMN_NAME\nFROM information_schema.KEY_COLUMN_USAGE\nWHERE REFERENCED_TABLE_SCHEMA = 'mydatabase'\n  AND REFERENCED_TABLE_NAME IS NOT NULL;\n\n-- Drop foreign key\nALTER TABLE orders DROP FOREIGN KEY fk_orders_user;\n\n-- Disable foreign key checks (temporarily)\nSET FOREIGN_KEY_CHECKS = 0;\n-- Do bulk operations\nSET FOREIGN_KEY_CHECKS = 1;\n\n-- Test referential integrity\n-- This will fail:\nINSERT INTO orders (user_id, total) VALUES (999, 100.00);\n-- Error: Cannot add or update a child row: foreign key constraint fails\n\n-- This will also fail:\nDELETE FROM users WHERE id = 1;\n-- Error: Cannot delete or update a parent row: foreign key constraint fails\n\n-- PostgreSQL deferrable constraints\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT NOT NULL,\n    FOREIGN KEY (user_id) REFERENCES users(id)\n        DEFERRABLE INITIALLY DEFERRED\n);\n-- Constraint checked at commit, not during statement\n\n-- Circular relationships (requires deferrable)\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    department_id INT\n);\n\nCREATE TABLE departments (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    manager_id INT\n);\n\nALTER TABLE employees\nADD CONSTRAINT fk_dept\nFOREIGN KEY (department_id) REFERENCES departments(id)\nDEFERRABLE INITIALLY DEFERRED;\n\nALTER TABLE departments\nADD CONSTRAINT fk_manager\nFOREIGN KEY (manager_id) REFERENCES employees(id)\nDEFERRABLE INITIALLY DEFERRED;\n\n-- View foreign key errors\nSHOW ENGINE INNODB STATUS;\n-- Look for LATEST FOREIGN KEY ERROR section"
    },
    {
      "id": 43,
      "question": "What are database transactions and savepoints?",
      "answer": "Transaction: Unit of work (all succeed or all fail)\n\nACID Properties:\n• Atomicity: All or nothing\n• Consistency: Valid state to valid state\n• Isolation: Concurrent transactions isolated\n• Durability: Committed changes persist\n\nTransaction Commands:\n• BEGIN/START TRANSACTION: Start\n• COMMIT: Make changes permanent\n• ROLLBACK: Undo all changes\n• SAVEPOINT: Create checkpoint\n• ROLLBACK TO: Undo to checkpoint\n\nSavepoints:\n• Named checkpoints within transaction\n• Partial rollback\n• Nested transaction simulation\n• Error recovery\n• Complex multi-step operations\n\nUse Cases:\n• Financial transfers\n• Multi-table updates\n• Batch operations\n• Complex business logic\n• Data migration\n• Import/export\n\nBest Practices:\n• Keep transactions short\n• Avoid user interaction\n• Don't nest transactions\n• Always commit or rollback\n• Use appropriate isolation level\n• Handle deadlocks\n• Monitor long transactions\n\nCommon Mistakes:\n• Forgetting to commit\n• Long-running transactions\n• Locks held too long\n• Mixing transaction modes",
      "explanation": "Transactions group multiple operations into atomic units with savepoints allowing partial rollbacks, essential for maintaining data consistency in complex operations.",
      "difficulty": "Medium",
      "code": "-- Basic transaction\nBEGIN;  -- or START TRANSACTION;\n\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n\nCOMMIT;\n-- Both updates succeed or both rollback\n\n-- Transaction with error handling\nBEGIN;\n\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n\n-- Check if update succeeded\nIF ROW_COUNT() = 0 THEN\n    ROLLBACK;\n    SELECT 'Transaction failed' AS result;\nELSE\n    UPDATE accounts SET balance = balance + 100 WHERE id = 2;\n    COMMIT;\n    SELECT 'Transaction successful' AS result;\nEND IF;\n\n-- Savepoints for partial rollback\nBEGIN;\n\n-- Step 1: Create order\nINSERT INTO orders (user_id, total) VALUES (1, 250.00);\nSET @order_id = LAST_INSERT_ID();\nSAVEPOINT after_order;\n\n-- Step 2: Add items\nINSERT INTO order_items (order_id, product_id, quantity)\nVALUES (@order_id, 101, 2);\nSAVEPOINT after_first_item;\n\nINSERT INTO order_items (order_id, product_id, quantity)\nVALUES (@order_id, 102, 1);\nSAVEPOINT after_second_item;\n\n-- Step 3: Try to apply discount\nINSERT INTO order_discounts (order_id, code, amount)\nVALUES (@order_id, 'INVALID', 10.00);\n-- Error: Invalid discount code\n\n-- Rollback only the discount\nROLLBACK TO after_second_item;\n\n-- Continue with transaction\nUPDATE inventory SET stock = stock - 2 WHERE product_id = 101;\nUPDATE inventory SET stock = stock - 1 WHERE product_id = 102;\n\nCOMMIT;\n-- Order and items saved, discount ignored\n\n-- Complex import with savepoints\nBELIMITER //\n\nCREATE PROCEDURE import_orders_batch()\nBEGIN\n    DECLARE done INT DEFAULT FALSE;\n    DECLARE v_order_id INT;\n    DECLARE v_user_id INT;\n    DECLARE v_total DECIMAL(10,2);\n    DECLARE v_error_count INT DEFAULT 0;\n    \n    DECLARE order_cursor CURSOR FOR\n        SELECT order_id, user_id, total FROM temp_import_orders;\n    \n    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;\n    DECLARE CONTINUE HANDLER FOR SQLEXCEPTION\n    BEGIN\n        -- Log error and continue\n        SET v_error_count = v_error_count + 1;\n        ROLLBACK TO order_savepoint;\n    END;\n    \n    START TRANSACTION;\n    \n    OPEN order_cursor;\n    \n    read_loop: LOOP\n        FETCH order_cursor INTO v_order_id, v_user_id, v_total;\n        \n        IF done THEN\n            LEAVE read_loop;\n        END IF;\n        \n        SAVEPOINT order_savepoint;\n        \n        -- Try to insert order\n        INSERT INTO orders (id, user_id, total)\n        VALUES (v_order_id, v_user_id, v_total);\n        \n        -- If successful, release savepoint\n        RELEASE SAVEPOINT order_savepoint;\n    END LOOP;\n    \n    CLOSE order_cursor;\n    \n    COMMIT;\n    \n    SELECT CONCAT('Imported ', FOUND_ROWS() - v_error_count, ' orders, ', \n                  v_error_count, ' errors') AS result;\nEND//\n\nDELIMITER ;\n\n-- PostgreSQL transaction with savepoints\nBEGIN;\n\nINSERT INTO users (name, email) VALUES ('John Doe', 'john@example.com');\nSAVEPOINT user_created;\n\nINSERT INTO accounts (user_id, balance) \nVALUES (currval('users_id_seq'), 0.00);\nSAVEPOINT account_created;\n\n-- Try to send welcome email (might fail)\nINSERT INTO email_queue (user_id, template, scheduled_at)\nVALUES (currval('users_id_seq'), 'welcome', NOW());\n-- If email service is down, rollback just this part\nROLLBACK TO account_created;\n\nCOMMIT;\n-- User and account created, email skipped\n\n-- Nested transaction simulation with savepoints\nBEGIN;\n\nUPDATE inventory SET stock = stock - 1 WHERE product_id = 1;\nSAVEPOINT sp1;\n\n    UPDATE inventory SET stock = stock - 1 WHERE product_id = 2;\n    SAVEPOINT sp2;\n    \n        UPDATE inventory SET stock = stock - 1 WHERE product_id = 3;\n        -- Error occurred\n        ROLLBACK TO sp2;  -- Only undo product 3\n    \n    UPDATE inventory SET stock = stock - 1 WHERE product_id = 4;\n    -- Success\n\nCOMMIT;\n-- Products 1, 2, 4 updated; product 3 rolled back\n\n-- Isolation levels with transactions\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN;\n\nSELECT * FROM accounts WHERE id = 1 FOR UPDATE;\n-- Locks the row for update\n\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n\nCOMMIT;\n\n-- Transaction timeout (PostgreSQL)\nSET statement_timeout = '5s';\nBEGIN;\n-- Long-running operations\nCOMMIT;\n\n-- Check current transaction\nSELECT * FROM information_schema.innodb_trx;\n\n-- Kill long transaction\nKILL <trx_mysql_thread_id>;\n\n-- Node.js transaction with savepoints\nconst connection = await pool.getConnection();\n\ntry {\n    await connection.beginTransaction();\n    \n    // Create order\n    const [orderResult] = await connection.execute(\n        'INSERT INTO orders (user_id, total) VALUES (?, ?)',\n        [userId, total]\n    );\n    const orderId = orderResult.insertId;\n    \n    await connection.query('SAVEPOINT after_order');\n    \n    // Add items\n    for (const item of items) {\n        try {\n            await connection.execute(\n                'INSERT INTO order_items (order_id, product_id, quantity) VALUES (?, ?, ?)',\n                [orderId, item.productId, item.quantity]\n            );\n        } catch (error) {\n            // Skip invalid items\n            await connection.query('ROLLBACK TO after_order');\n            console.error('Invalid item:', error);\n        }\n    }\n    \n    await connection.commit();\n    console.log('Order created:', orderId);\n    \n} catch (error) {\n    await connection.rollback();\n    console.error('Transaction failed:', error);\n    throw error;\n} finally {\n    connection.release();\n}\n\n-- Release savepoint (no longer needed)\nBEGIN;\nSAVEPOINT sp1;\n-- operations\nRELEASE SAVEPOINT sp1;  -- Cannot rollback to sp1 anymore\nCOMMIT;"
    },
    {
      "id": 44,
      "question": "How do you use EXPLAIN to optimize queries?",
      "answer": "EXPLAIN: Shows query execution plan\n\nKey Information:\n• Table access method\n• Join algorithm\n• Index usage\n• Rows examined estimate\n• Filtering conditions\n• Cost estimates\n\nAccess Types (MySQL - Best to Worst):\n• system: Single row\n• const: Primary key/unique lookup\n• eq_ref: Unique index join\n• ref: Non-unique index\n• range: Index range scan\n• index: Full index scan\n• ALL: Full table scan (worst)\n\nAnalysis Steps:\n1. Run EXPLAIN on slow query\n2. Identify full table scans (ALL)\n3. Check for missing indexes\n4. Look at rows examined\n5. Verify join methods\n6. Check for filesort/temporary\n7. Add appropriate indexes\n8. Re-run EXPLAIN and verify\n\nRed Flags:\n• type: ALL (full table scan)\n• rows: Large numbers\n• Extra: Using filesort\n• Extra: Using temporary\n• Extra: Using where (inefficient)\n• NULL key usage\n\nOptimization Actions:\n• Add indexes on WHERE columns\n• Add indexes on JOIN columns\n• Use covering indexes\n• Rewrite query structure\n• Update table statistics\n• Consider query hints",
      "explanation": "EXPLAIN analyzes query execution plans showing how databases will execute queries, helping identify performance issues like missing indexes and full table scans.",
      "difficulty": "Hard",
      "code": "-- Basic EXPLAIN (MySQL)\nEXPLAIN SELECT * FROM orders WHERE user_id = 123;\n\n-- Output columns:\n-- id: Query identifier\n-- select_type: Type of SELECT\n-- table: Table name\n-- type: Access method (system, const, ref, range, index, ALL)\n-- possible_keys: Indexes that could be used\n-- key: Index actually used\n-- key_len: Length of key used\n-- ref: Columns/constants compared to index\n-- rows: Estimated rows to examine\n-- Extra: Additional information\n\n-- Example output:\n-- +----+-------------+--------+------+---------------+-------------+---------+-------+------+-------+\n-- | id | select_type | table  | type | possible_keys | key         | key_len | ref   | rows | Extra |\n-- +----+-------------+--------+------+---------------+-------------+---------+-------+------+-------+\n-- |  1 | SIMPLE      | orders | ref  | idx_user_id   | idx_user_id | 4       | const |   10 | NULL  |\n-- +----+-------------+--------+------+---------------+-------------+---------+-------+------+-------+\n\n-- Bad query (full table scan)\nEXPLAIN SELECT * FROM orders WHERE YEAR(order_date) = 2024;\n-- type: ALL (scanning all rows)\n-- rows: 1000000\n-- Extra: Using where\n\n-- Optimized query\nCREATE INDEX idx_order_date ON orders(order_date);\n\nEXPLAIN SELECT * FROM orders \nWHERE order_date >= '2024-01-01' AND order_date < '2025-01-01';\n-- type: range (using index)\n-- key: idx_order_date\n-- rows: 50000 (much better!)\n\n-- EXPLAIN with JOIN\nEXPLAIN SELECT u.name, o.total\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.email = 'john@example.com';\n\n-- Add indexes\nCREATE INDEX idx_email ON users(email);\nCREATE INDEX idx_user_id ON orders(user_id);\n\n-- After indexes:\n-- users: type=ref, key=idx_email, rows=1\n-- orders: type=ref, key=idx_user_id, rows=10\n\n-- EXPLAIN ANALYZE (PostgreSQL) - actual execution\nEXPLAIN ANALYZE SELECT * FROM orders WHERE user_id = 123;\n\n-- Output includes:\n-- Planning Time: 0.125 ms\n-- Execution Time: 1.234 ms\n-- Actual rows vs estimated rows\n-- Actual time per operation\n\n-- EXPLAIN with format options\nEXPLAIN FORMAT=JSON SELECT * FROM orders WHERE user_id = 123;\nEXPLAIN FORMAT=TREE SELECT * FROM orders WHERE user_id = 123;\n\n-- Identify missing indexes\nEXPLAIN SELECT * FROM orders WHERE status = 'PENDING';\n-- type: ALL\n-- possible_keys: NULL\n-- Solution: CREATE INDEX idx_status ON orders(status);\n\n-- Filesort problem\nEXPLAIN SELECT * FROM orders ORDER BY created_at DESC LIMIT 10;\n-- Extra: Using filesort (slow!)\n\n-- Solution: Add index\nCREATE INDEX idx_created_at ON orders(created_at DESC);\nEXPLAIN SELECT * FROM orders ORDER BY created_at DESC LIMIT 10;\n-- Extra: Using index (fast!)\n\n-- Covering index optimization\nEXPLAIN SELECT id, user_id, total FROM orders WHERE user_id = 123;\n-- key: idx_user_id\n-- Extra: NULL (good, but can be better)\n\n-- Covering index includes all needed columns\nCREATE INDEX idx_covering ON orders(user_id, id, total);\nEXPLAIN SELECT id, user_id, total FROM orders WHERE user_id = 123;\n-- Extra: Using index (best! no table lookup needed)\n\n-- Subquery optimization\nEXPLAIN SELECT * FROM users\nWHERE id IN (SELECT user_id FROM orders WHERE total > 1000);\n-- Check for dependent subquery (slow)\n\n-- Better: Use JOIN\nEXPLAIN SELECT DISTINCT u.* FROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.total > 1000;\n-- Faster execution plan\n\n-- Analyze multiple query variations\n-- Option 1: WHERE IN\nEXPLAIN SELECT * FROM products \nWHERE category IN ('Electronics', 'Computers', 'Phones');\n\n-- Option 2: OR conditions\nEXPLAIN SELECT * FROM products \nWHERE category = 'Electronics' \n   OR category = 'Computers' \n   OR category = 'Phones';\n\n-- Option 3: UNION\nEXPLAIN\nSELECT * FROM products WHERE category = 'Electronics'\nUNION\nSELECT * FROM products WHERE category = 'Computers'\nUNION\nSELECT * FROM products WHERE category = 'Phones';\n-- Compare costs and choose best\n\n-- EXPLAIN for UPDATE/DELETE\nEXPLAIN UPDATE orders SET status = 'SHIPPED' WHERE user_id = 123;\nEXPLAIN DELETE FROM logs WHERE created_at < '2020-01-01';\n-- Ensure indexes used, avoid full scans\n\n-- Partition pruning\nCREATE TABLE orders_partitioned (\n    id INT,\n    order_date DATE\n) PARTITION BY RANGE (YEAR(order_date)) (\n    PARTITION p2023 VALUES LESS THAN (2024),\n    PARTITION p2024 VALUES LESS THAN (2025)\n);\n\nEXPLAIN SELECT * FROM orders_partitioned \nWHERE order_date >= '2024-01-01';\n-- partitions: p2024 (only scans relevant partition)\n\n-- Complex JOIN analysis\nEXPLAIN SELECT \n    u.name,\n    o.total,\n    p.name AS product_name\nFROM users u\nJOIN orders o ON u.id = o.user_id\nJOIN order_items oi ON o.id = oi.order_id\nJOIN products p ON oi.product_id = p.id\nWHERE u.email = 'john@example.com';\n-- Check join order and types\n-- Ensure appropriate indexes on all join columns\n\n-- SQL Server execution plan\nSET SHOWPLAN_ALL ON;\nSELECT * FROM orders WHERE user_id = 123;\nSET SHOWPLAN_ALL OFF;\n\n-- Or graphical plan\nSET STATISTICS PROFILE ON;\nSELECT * FROM orders WHERE user_id = 123;\nSET STATISTICS PROFILE OFF;\n\n-- PostgreSQL verbose EXPLAIN\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE, FORMAT JSON)\nSELECT * FROM orders WHERE user_id = 123;\n-- Shows buffer hits/misses, I/O stats\n\n-- Real-world optimization example\n-- Before: Slow query (2.5 seconds)\nEXPLAIN SELECT \n    DATE(o.order_date) AS day,\n    COUNT(*) AS order_count,\n    SUM(o.total) AS revenue\nFROM orders o\nWHERE o.order_date >= '2024-01-01'\nGROUP BY DATE(o.order_date);\n-- type: ALL, rows: 1000000, Extra: Using filesort\n\n-- After: Add index\nCREATE INDEX idx_order_date ON orders(order_date);\n\nEXPLAIN SELECT \n    DATE(o.order_date) AS day,\n    COUNT(*) AS order_count,\n    SUM(o.total) AS revenue\nFROM orders o\nWHERE o.order_date >= '2024-01-01'\nGROUP BY DATE(o.order_date);\n-- type: range, rows: 50000, key: idx_order_date (0.2 seconds!)\n\n-- Monitor query cost\nSHOW STATUS LIKE 'Last_query_cost';\n-- Lower is better"
    },
    {
      "id": 45,
      "question": "What are database high availability and disaster recovery strategies?",
      "answer": "HA & DR: Minimize downtime and data loss\n\nHigh Availability (HA):\n• Minimize planned/unplanned downtime\n• Automatic failover\n• Load balancing\n• Redundancy at all levels\n• Target: 99.9%+ uptime\n\nDisaster Recovery (DR):\n• Recover from catastrophic failures\n• Restore operations after disaster\n• Data backup and restore\n• Geographic redundancy\n• Business continuity\n\nKey Metrics:\n• RTO: Recovery Time Objective (max downtime)\n• RPO: Recovery Point Objective (max data loss)\n• MTTR: Mean Time To Repair\n• MTBF: Mean Time Between Failures\n\nHA Strategies:\n\n1. Replication:\n• Master-slave (read scaling)\n• Master-master (write scaling)\n• Synchronous (consistent, slower)\n• Asynchronous (faster, lag)\n\n2. Clustering:\n• Shared storage\n• Multiple active nodes\n• Automatic failover\n• No data loss\n\n3. Load Balancing:\n• Distribute traffic\n• Health checks\n• Connection pooling\n• Geographic distribution\n\nDR Strategies:\n\n1. Backup & Restore:\n• Full, incremental, differential\n• Offsite storage\n• Regular testing\n• Slowest recovery\n\n2. Pilot Light:\n• Minimal DR environment\n• Scale up on disaster\n• Medium RTO\n\n3. Warm Standby:\n• Scaled-down replica\n• Always running\n• Quick scale-up\n• Better RTO\n\n4. Hot Standby:\n• Full replica active\n• Immediate failover\n• Best RTO/RPO\n• Most expensive\n\nBest Practices:\n• Test failover regularly\n• Automate everything\n• Monitor continuously\n• Document procedures\n• Multi-region deployment\n• Regular backups\n• Encrypt backups\n• Version control schema",
      "explanation": "High availability ensures minimal downtime through replication and clustering, while disaster recovery provides recovery strategies with defined RTO/RPO targets using backups and standby systems.",
      "difficulty": "Hard",
      "code": "-- MySQL Master-Slave Replication Setup\n-- On Master Server:\nCREATE USER 'repl'@'%' IDENTIFIED BY 'password';\nGRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';\nFLUSH PRIVILEGES;\n\n-- Edit my.cnf on master:\n[mysqld]\nserver-id=1\nlog-bin=mysql-bin\nbinlog-format=ROW\nbinlog-do-db=myapp\n\n-- Restart MySQL and get master status:\nSHOW MASTER STATUS;\n-- Note: File and Position\n\n-- On Slave Server:\n-- Edit my.cnf:\n[mysqld]\nserver-id=2\nrelay-log=relay-bin\nread-only=1\n\n-- Configure slave:\nCHANGE MASTER TO\n    MASTER_HOST='master-ip',\n    MASTER_USER='repl',\n    MASTER_PASSWORD='password',\n    MASTER_LOG_FILE='mysql-bin.000001',\n    MASTER_LOG_POS=154;\n\nSTART SLAVE;\nSHOW SLAVE STATUS\\G\n-- Check: Slave_IO_Running: Yes, Slave_SQL_Running: Yes\n\n-- Monitor replication lag\nSHOW SLAVE STATUS\\G\n-- Seconds_Behind_Master: 0 (good), >10 (lagging)\n\n-- PostgreSQL Streaming Replication\n-- On Primary:\nCREATE ROLE replicator WITH REPLICATION PASSWORD 'password' LOGIN;\n\n-- Edit postgresql.conf:\nwal_level = replica\nmax_wal_senders = 3\nwal_keep_size = 1GB\n\n-- pg_hba.conf:\nhost replication replicator standby-ip/32 md5\n\n-- On Standby:\ntouch /var/lib/postgresql/data/standby.signal\n\n-- postgresql.conf:\nprimary_conninfo = 'host=primary-ip port=5432 user=replicator password=password'\n\npg_ctl start\n\n-- Check replication status:\nSELECT * FROM pg_stat_replication;\n\n-- Failover procedure (manual)\n-- 1. Stop primary (or confirm it's down)\n-- 2. Promote standby:\npg_ctl promote -D /var/lib/postgresql/data\n\n-- 3. Update application connection string\n-- 4. Set up new standby from old primary\n\n-- Automated failover with ProxySQL\n-- Install ProxySQL\nsudo apt install proxysql\n\n-- Configure (proxysql-admin)\nmysql -u admin -padmin -h 127.0.0.1 -P6032\n\n-- Add servers\nINSERT INTO mysql_servers (hostgroup_id, hostname, port, weight) VALUES\n    (1, 'master', 3306, 1000),\n    (2, 'slave1', 3306, 1000),\n    (2, 'slave2', 3306, 1000);\n\nLOAD MYSQL SERVERS TO RUNTIME;\n\n-- Configure health checks\nUPDATE global_variables \nSET variable_value='repl'\nWHERE variable_name='mysql-monitor_username';\n\nUPDATE global_variables \nSET variable_value='password'\nWHERE variable_name='mysql-monitor_password';\n\nLOAD MYSQL VARIABLES TO RUNTIME;\n\n-- ProxySQL automatically detects failed master and promotes slave\n\n-- Backup strategy (comprehensive)\n#!/bin/bash\n# full_backup.sh\n\nBACKUP_DIR=\"/backup/mysql\"\nDATE=$(date +%Y%m%d_%H%M%S)\nRETENTION_DAYS=7\n\n# Full backup with compression\nmysqldump -u backup -p'password' \\\n    --single-transaction \\\n    --routines \\\n    --triggers \\\n    --events \\\n    --all-databases \\\n    --master-data=2 \\\n    | gzip > \"$BACKUP_DIR/full_$DATE.sql.gz\"\n\n# Encrypt backup\nopenssl enc -aes-256-cbc -salt \\\n    -in \"$BACKUP_DIR/full_$DATE.sql.gz\" \\\n    -out \"$BACKUP_DIR/full_$DATE.sql.gz.enc\" \\\n    -k \"encryption_key\"\n\n# Upload to S3\naws s3 cp \"$BACKUP_DIR/full_$DATE.sql.gz.enc\" \\\n    s3://backups/mysql/ \\\n    --storage-class GLACIER\n\n# Delete old local backups\nfind \"$BACKUP_DIR\" -name \"full_*.sql.gz.enc\" \\\n    -mtime +$RETENTION_DAYS -delete\n\necho \"Backup completed: full_$DATE.sql.gz.enc\"\n\n-- Point-in-time recovery setup\n-- Enable binary logging:\n[mysqld]\nlog-bin=mysql-bin\nserver-id=1\nbinlog-format=ROW\nexpire_logs_days=7\n\n-- Full backup with position\nmysqldump --single-transaction \\\n    --master-data=2 \\\n    --all-databases > full_backup.sql\n\n-- Backup binary logs\nmysqlbinlog mysql-bin.000001 > binlog_001.sql\n\n-- Restore to specific time\n# 1. Restore full backup\nmysql < full_backup.sql\n\n# 2. Apply binary logs up to specific time\nmysqlbinlog --stop-datetime=\"2024-01-15 14:30:00\" \\\n    mysql-bin.000001 mysql-bin.000002 | mysql\n\n-- DR Testing Procedure\n-- 1. Schedule DR drill\n-- 2. Simulate failure\n-- 3. Execute failover\n-- 4. Verify application connectivity\n-- 5. Test data integrity\n-- 6. Measure RTO/RPO\n-- 7. Document results\n-- 8. Update runbooks\n\n-- Multi-region setup (AWS)\n-- Primary: us-east-1\naws rds create-db-instance \\\n    --db-instance-identifier myapp-primary \\\n    --db-instance-class db.m5.large \\\n    --engine mysql \\\n    --master-username admin \\\n    --master-user-password password \\\n    --allocated-storage 100 \\\n    --backup-retention-period 7 \\\n    --multi-az\n\n-- Read replica in different region\naws rds create-db-instance-read-replica \\\n    --db-instance-identifier myapp-replica-west \\\n    --source-db-instance-identifier myapp-primary \\\n    --db-instance-class db.m5.large \\\n    --region us-west-2\n\n-- Promote replica to standalone (DR failover)\naws rds promote-read-replica \\\n    --db-instance-identifier myapp-replica-west\n\n-- Health check script\n#!/bin/bash\n# check_db_health.sh\n\nMASTER_HOST=\"master-db\"\nSLAVE_HOST=\"slave-db\"\n\n# Check master\nif ! mysql -h $MASTER_HOST -u monitor -p'pwd' -e \"SELECT 1\" &>/dev/null; then\n    echo \"CRITICAL: Master database down!\"\n    # Trigger failover\n    ./failover.sh\nfi\n\n# Check replication lag\nLAG=$(mysql -h $SLAVE_HOST -u monitor -p'pwd' -e \"SHOW SLAVE STATUS\\G\" | \\\n    grep \"Seconds_Behind_Master\" | awk '{print $2}')\n\nif [ \"$LAG\" -gt 30 ]; then\n    echo \"WARNING: Replication lag is $LAG seconds\"\n    # Alert operations team\nfi\n\n-- Application-level failover (Node.js)\nconst pools = {\n    primary: mysql.createPool({ host: 'master-db', ...config }),\n    replica: mysql.createPool({ host: 'slave-db', ...config })\n};\n\nlet currentPool = pools.primary;\n\nasync function executeQuery(sql, params) {\n    try {\n        return await currentPool.execute(sql, params);\n    } catch (error) {\n        if (error.code === 'ECONNREFUSED') {\n            console.error('Primary down, failing over to replica');\n            currentPool = pools.replica;\n            return await currentPool.execute(sql, params);\n        }\n        throw error;\n    }\n}\n\n// Health check endpoint\napp.get('/health/db', async (req, res) => {\n    try {\n        await currentPool.execute('SELECT 1');\n        res.json({ status: 'healthy', database: 'connected' });\n    } catch (error) {\n        res.status(503).json({ status: 'unhealthy', error: error.message });\n    }\n});"
    },
    {
      "id": 46,
      "question": "What are database slow query optimization techniques?",
      "answer": "Slow Query Optimization: Identify and fix performance bottlenecks\n\nIdentification:\n• Enable slow query log\n• Set time threshold\n• Monitor query patterns\n• Use APM tools\n• Profile in production\n\nAnalysis Tools:\n• EXPLAIN/EXPLAIN ANALYZE\n• Query execution plans\n• Index usage statistics\n• Table statistics\n• Lock analysis\n\nCommon Issues:\n• Missing indexes\n• Full table scans\n• Inefficient JOINs\n• Large result sets\n• Suboptimal query structure\n• Outdated statistics\n\nSolutions:\n• Add appropriate indexes\n• Rewrite queries\n• Use covering indexes\n• Limit result sets\n• Partition large tables\n• Update statistics\n• Query caching",
      "explanation": "Slow query optimization involves identifying bottlenecks through logging and analysis, then applying targeted fixes like indexing and query rewriting.",
      "difficulty": "Medium",
      "code": "-- Enable slow query logging (MySQL)\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 2;"
    },
    {
      "id": 47,
      "question": "What is database replication and its types?",
      "answer": "Database Replication: Copy data across multiple servers\n\nPurpose:\n• High availability\n• Load distribution\n• Disaster recovery\n• Geographic distribution\n• Read scaling\n\nTypes:\n\nMaster-Slave (Primary-Replica):\n• One write master\n• Multiple read replicas\n• Asynchronous/synchronous\n• Simple to implement\n• Read scalability\n\nMaster-Master (Multi-Master):\n• Multiple write nodes\n• Bidirectional replication\n• Higher complexity\n• Conflict resolution needed\n• Geographic distribution",
      "explanation": "Database replication copies data across servers for availability and scalability, using master-slave or master-master topologies.",
      "difficulty": "Hard",
      "code": "-- MySQL Master-Slave Replication\nCREATE USER 'repl'@'%' IDENTIFIED BY 'password';\nGRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';"
    },
    {
      "id": 48,
      "question": "What are database backup and recovery strategies?",
      "answer": "Backup & Recovery: Protect against data loss\n\nBackup Types:\n\nFull Backup:\n• Complete database copy\n• Longest time, most space\n• Simplest recovery\n• Base for other types\n\nIncremental Backup:\n• Only changes since last backup\n• Faster, less space\n• Complex recovery\n\nDifferential Backup:\n• Changes since last full backup\n• Medium time/space\n• Easier recovery than incremental\n\nBest Practices:\n• Regular schedule\n• Test restores\n• Off-site storage\n• Encryption\n• Monitoring",
      "explanation": "Database backup strategies include full, incremental, and differential backups with recovery options like PITR.",
      "difficulty": "Hard",
      "code": "-- MySQL Backup\nmysqldump -u root -p --single-transaction myapp > backup.sql"
    },
    {
      "id": 49,
      "question": "What is database sharding and partitioning?",
      "answer": "Sharding: Split data across multiple databases\n\nSharding Strategies:\n\nHash-based:\n• Hash function determines shard\n• Even distribution\n• Hard to rebalance\n\nRange-based:\n• Data range per shard\n• Easy to understand\n• Risk of hotspots\n\nGeographic:\n• By geographic location\n• Low latency for users\n• Complexity in cross-region\n\nPartitioning:\n• Split table within database\n• Range, list, hash, composite\n• Query performance improvement\n• Easier than sharding",
      "explanation": "Sharding distributes data across multiple databases for horizontal scaling, while partitioning splits tables within a single database.",
      "difficulty": "Hard",
      "code": "-- Range Partitioning (MySQL)\nCREATE TABLE orders (\n    id INT,\n    order_date DATE\n) PARTITION BY RANGE (YEAR(order_date)) (\n    PARTITION p2023 VALUES LESS THAN (2024),\n    PARTITION p2024 VALUES LESS THAN (2025)\n);"
    },
    {
      "id": 50,
      "question": "What are database connection pooling best practices?",
      "answer": "Connection Pooling: Reuse database connections efficiently\n\nBenefits:\n• Reduced connection overhead\n• Better performance\n• Resource management\n• Connection reuse\n• Limit concurrent connections\n\nConfiguration:\n\nPool Size:\n• Min connections (keep warm)\n• Max connections (limit ceiling)\n• Calculate based on workload\n• Consider CPU cores, disk I/O\n\nTimeouts:\n• Connection timeout\n• Idle timeout\n• Max lifetime\n• Leak detection\n\nBest Practices:\n• Always return connections\n• Use try-with-resources\n• Monitor pool metrics\n• Set appropriate sizes\n• Handle failures gracefully",
      "explanation": "Connection pooling reuses database connections to improve performance through proper sizing, timeout configuration, and lifecycle management.",
      "difficulty": "Medium",
      "code": "// HikariCP (Java)\nHikariConfig config = new HikariConfig();\nconfig.setJdbcUrl(\"jdbc:mysql://localhost:3306/myapp\");\nconfig.setMinimumIdle(5);\nconfig.setMaximumPoolSize(20);\nHikariDataSource ds = new HikariDataSource(config);"
    },
    {
      "id": 51,
      "question": "What are ACID properties in transactions?",
      "answer": "ACID: Transaction guarantees\n\nAtomicity:\n• All or nothing\n• Transaction completes fully or rolls back\n• No partial updates\n• Use BEGIN/COMMIT/ROLLBACK\n\nConsistency:\n• Database remains in valid state\n• Constraints enforced\n• Integrity maintained\n• Business rules preserved\n\nIsolation:\n• Concurrent transactions don't interfere\n• Isolation levels control visibility\n• Prevents dirty reads, lost updates\n• Lock mechanisms\n\nDurability:\n• Committed changes persist\n• Survives crashes/power loss\n• Transaction logs\n• Write-ahead logging",
      "explanation": "ACID properties ensure reliable transaction processing through Atomicity (all-or-nothing), Consistency (valid states), Isolation (concurrency control), and Durability (persistence).",
      "difficulty": "Medium",
      "code": "-- Transaction example\nBEGIN TRANSACTION;\n\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n\nCOMMIT;  -- Both succeed or both fail"
    },
    {
      "id": 52,
      "question": "What are database isolation levels?",
      "answer": "Isolation Levels: Control transaction visibility\n\nRead Uncommitted:\n• Dirty reads allowed\n• Lowest isolation\n• Best performance\n• Not recommended\n\nRead Committed:\n• No dirty reads\n• Default in many databases\n• Non-repeatable reads possible\n• Good balance\n\nRepeatable Read:\n• No dirty or non-repeatable reads\n• Phantom reads possible\n• MySQL/InnoDB default\n• Good for most cases\n\nSerializable:\n• Strongest isolation\n• No anomalies\n• Worst performance\n• Acts like serial execution\n\nTrade-off:\n• Higher isolation = More consistency\n• Lower isolation = Better performance",
      "explanation": "Isolation levels balance concurrency and consistency, from Read Uncommitted (fastest, least safe) to Serializable (slowest, most safe).",
      "difficulty": "Hard",
      "code": "-- Set isolation level (MySQL)\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\n\n-- PostgreSQL\nBEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nSELECT * FROM accounts WHERE id = 1;\nCOMMIT;"
    },
    {
      "id": 53,
      "question": "What are database deadlocks and how to prevent them?",
      "answer": "Deadlock: Two transactions waiting for each other\n\nExample:\n• Transaction A locks Row 1, waits for Row 2\n• Transaction B locks Row 2, waits for Row 1\n• Neither can proceed (deadlock!)\n\nDetection:\n• Database monitors wait-for graph\n• Chooses victim transaction\n• Rolls back victim\n• Other transaction proceeds\n\nPrevention:\n• Lock resources in same order\n• Use shorter transactions\n• Access fewer resources\n• Use appropriate isolation level\n• Set lock timeouts\n• Retry on deadlock\n\nSymptoms:\n• Transaction rollbacks\n• Application errors\n• Performance degradation\n• Lock wait timeouts",
      "explanation": "Deadlocks occur when transactions wait circularly for locks. Prevention strategies include consistent lock ordering and short transactions.",
      "difficulty": "Hard",
      "code": "-- Deadlock example\n-- Transaction 1\nBEGIN;\nUPDATE accounts SET balance = balance - 10 WHERE id = 1;  -- Lock row 1\n-- Wait...\nUPDATE accounts SET balance = balance + 10 WHERE id = 2;  -- Wait for row 2\n\n-- Transaction 2 (simultaneously)\nBEGIN;\nUPDATE accounts SET balance = balance - 10 WHERE id = 2;  -- Lock row 2\n-- Wait...\nUPDATE accounts SET balance = balance + 10 WHERE id = 1;  -- Wait for row 1 (DEADLOCK!)\n\n-- Prevention: Lock in same order\n-- Both transactions:\nBEGIN;\nUPDATE accounts SET balance = balance - 10 WHERE id = 1;  -- Always lock lower ID first\nUPDATE accounts SET balance = balance + 10 WHERE id = 2;  -- Then higher ID\nCOMMIT;"
    },
    {
      "id": 54,
      "question": "What is database denormalization and when to use it?",
      "answer": "Denormalization: Add redundancy for performance\n\nPurpose:\n• Reduce JOINs\n• Faster reads\n• Simpler queries\n• Better query performance\n\nTechniques:\n\nDuplicate Data:\n• Copy frequently accessed data\n• Avoid JOINs\n• Update in multiple places\n\nPrecompute Aggregates:\n• Store COUNT, SUM, AVG\n• Update on data  change\n• Fast reporting\n\nAdd Summary Tables:\n• Materialized views\n• Periodic refresh\n• Analytics performance\n\nWhen to Use:\n• Read-heavy workload\n• Complex JOINs hurt performance\n• Reporting/analytics\n• Caching layer unavailable\n\nTrade-offs:\n• More storage space\n• Update complexity\n• Data inconsistency risk\n• Maintenance overhead",
      "explanation": "Denormalization adds controlled redundancy to improve read performance by reducing JOINs, trading storage and update complexity for query speed.",
      "difficulty": "Medium",
      "code": "-- Normalized (requires JOIN)\nSELECT u.name, COUNT(o.id)\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id;\n\n-- Denormalized (no JOIN needed)\nALTER TABLE users ADD COLUMN order_count INT DEFAULT 0;\n\n-- Update trigger to maintain denormalized data\nCREATE TRIGGER update_order_count\nAFTER INSERT ON orders\nFOR EACH ROW\nUPDATE users SET order_count = order_count + 1 WHERE id = NEW.user_id;\n\n-- Fast query\nSELECT name, order_count FROM users;"
    },
    {
      "id": 55,
      "question": "What are database stored procedures vs functions?",
      "answer": "Stored Procedures vs Functions\n\nStored Procedures:\n• Can perform DML (INSERT/UPDATE/DELETE)\n• Can have transactions\n• No return value (OUT parameters)\n• Called with CALL\n• Can call functions\n• More flexible\n\nFunctions:\n• Must return a value\n• Cannot perform DML (in most databases)\n• No transaction control\n• Used in SELECT statements\n• Cannot call procedures\n• More restrictive\n\nWhen to Use:\n\nProcedures:\n• Complex business logic\n• Multiple operations\n• Transaction management\n• Data modifications\n\nFunctions:\n• Calculations\n• Data transformations\n• Reusable logic in queries\n• Read-only operations\n\nBenefits:\n• Code reusability\n• Network efficiency\n• Security (controlled access)\n• Performance (precompiled)\n• Centralized logic",
      "explanation": "Stored procedures perform actions and can modify data, while functions return values and are used in expressions. Procedures offer more flexibility.",
      "difficulty": "Medium",
      "code": "-- Stored Procedure (MySQL)\nDELIMITER //\n\nCREATE PROCEDURE transfer_funds(\n    IN from_account INT,\n    IN to_account INT,\n    IN amount DECIMAL(10,2),\n    OUT result VARCHAR(50)\n)\nBEGIN\n    DECLARE balance DECIMAL(10,2);\n    \n    START TRANSACTION;\n    \n    SELECT account_balance INTO balance\n    FROM accounts\n    WHERE id = from_account\n    FOR UPDATE;\n    \n    IF balance >= amount THEN\n        UPDATE accounts SET account_balance = account_balance - amount\n        WHERE id = from_account;\n        \n        UPDATE accounts SET account_balance = account_balance + amount\n        WHERE id = to_account;\n        \n        COMMIT;\n        SET result = 'SUCCESS';\n    ELSE\n        ROLLBACK;\n        SET result = 'INSUFFICIENT_FUNDS';\n    END IF;\nEND//\n\nDELIMITER ;\n\n-- Call procedure\nCALL transfer_funds(1, 2, 100.00, @result);\nSELECT @result;\n\n-- Function (MySQL)\nDELIMITER //\n\nCREATE FUNCTION calculate_tax(\n    amount DECIMAL(10,2)\n)\nRETURNS DECIMAL(10,2)\nDETERMINISTIC\nBEGIN\n    RETURN amount * 0.10;\nEND//\n\nDELIMITER ;\n\n-- Use in query\nSELECT \n    product_name,\n    price,\n    calculate_tax(price) AS tax,\n    price + calculate_tax(price) AS total\nFROM products;"
    },
    {
      "id": 56,
      "question": "What are database cursors and their use cases?",
      "answer": "Cursors: Iterate through result sets row-by-row\n\nTypes:\n\nForward-only:\n• Scroll forward only\n• Most efficient\n• Default type\n• One-time traversal\n\nScrollable:\n• Move forward/backward\n• Random access\n• More overhead\n• Flexible navigation\n\nRead-only vs Updatable:\n• Read-only: Cannot modify data\n• Updatable: Can UPDATE/DELETE current row\n\nWhen to Use:\n• Row-by-row processing needed\n• Complex calculations per row\n• Legacy systems\n• Procedural logic required\n\nAlternatives (Preferred):\n• Set-based operations (faster)\n• JOINs and subqueries\n• Aggregate functions\n• Window functions\n\nPerformance Impact:\n• Slower than set-based\n• More memory usage\n• Lock contention\n• Avoid when possible",
      "explanation": "Cursors allow row-by-row processing but are slower than set-based operations. Use set-based SQL when possible for better performance.",
      "difficulty": "Medium",
      "code": "-- Cursor example (MySQL)\nDELIMITER //\n\nCREATE PROCEDURE process_orders()\nBEGIN\n    DECLARE done INT DEFAULT FALSE;\n    DECLARE order_id INT;\n    DECLARE total DECIMAL(10,2);\n    \n    DECLARE order_cursor CURSOR FOR\n        SELECT id, order_total FROM orders WHERE status = 'PENDING';\n    \n    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;\n    \n    OPEN order_cursor;\n    \n    read_loop: LOOP\n        FETCH order_cursor INTO order_id, total;\n        \n        IF done THEN\n            LEAVE read_loop;\n        END IF;\n        \n        -- Process each order\n        IF total > 1000 THEN\n            UPDATE orders SET priority = 'HIGH' WHERE id = order_id;\n        END IF;\n    END LOOP;\n    \n    CLOSE order_cursor;\nEND//\n\nDELIMITER ;\n\n-- Better alternative: Set-based\nUPDATE orders \nSET priority = 'HIGH' \nWHERE status = 'PENDING' AND order_total > 1000;\n-- Much faster!"
    },
    {
      "id": 57,
      "question": "What are database triggers and their best practices?",
      "answer": "Triggers: Automatic actions on database events\n\nTypes:\n\nBEFORE Triggers:\n• Execute before operation\n• Can modify new values\n• Validation, defaults\n• Cannot change other tables\n\nAFTER Triggers:\n• Execute after operation\n• Cannot modify new values\n• Audit logging, notifications\n• Can modify other tables\n\nINSTEAD OF Triggers:\n• Replace the operation\n• Views (not tables)\n• Custom behavior\n• Complex logic\n\nEvents:\n• INSERT\n• UPDATE\n• DELETE\n\nUse Cases:\n• Audit logging\n• Data validation\n• Maintain denormalized data\n• Enforce complex constraints\n• Automatic calculations\n\nBest Practices:\n• Keep logic simple\n• Avoid complex operations\n• No user interaction\n• Document thoroughly\n• Test extensively\n• Consider alternatives\n• Avoid recursive triggers",
      "explanation": "Triggers automatically execute code on INSERT/UPDATE/DELETE events. Use them for audit logging and data integrity, but keep logic simple.",
      "difficulty": "Medium",
      "code": "-- BEFORE INSERT trigger (MySQL)\nDELIMITER //\n\nCREATE TRIGGER before_user_insert\nBEFORE INSERT ON users\nFOR EACH ROW\nBEGIN\n    -- Validate email\n    IF NEW.email NOT LIKE '%@%.%' THEN\n        SIGNAL SQLSTATE '45000'\n        SET MESSAGE_TEXT = 'Invalid email format';\n    END IF;\n    \n    -- Set defaults\n    IF NEW.status IS NULL THEN\n        SET NEW.status = 'ACTIVE';\n    END IF;\n    \n    -- Lowercase email\n    SET NEW.email = LOWER(NEW.email);\nEND//\n\n-- AFTER UPDATE trigger (audit log)\nCREATE TRIGGER after_user_update\nAFTER UPDATE ON users\nFOR EACH ROW\nBEGIN\n    INSERT INTO audit_log (table_name, record_id, action, old_values, new_values)\n    VALUES (\n        'users',\n        NEW.id,\n        'UPDATE',\n        JSON_OBJECT('name', OLD.name, 'email', OLD.email),\n        JSON_OBJECT('name', NEW.name, 'email', NEW.email)\n    );\nEND//\n\nDELIMITER ;\n\n-- Drop trigger\nDROP TRIGGER IF EXISTS before_user_insert;"
    },
    {
      "id": 58,
      "question": "What are database views and materialized views?",
      "answer": "Views: Virtual tables based on queries\n\nStandard Views:\n• Virtual query result\n• No data storage\n• Always up-to-date\n• Query executed each time\n• Security/abstraction layer\n\nMaterialized Views:\n• Physical storage\n• Precomputed results\n• Periodic refresh\n• Fast queries\n• Stale data between refreshes\n\nBenefits:\n• Simplify complex queries\n• Security (hide columns)\n• Abstraction layer\n• Reusable logic\n• Backward compatibility\n\nWhen to Use:\n\nStandard Views:\n• Always need current data\n• Simple queries\n• Security requirements\n• Logical organization\n\nMaterialized Views:\n• Complex aggregations\n• Reporting/analytics\n• Heavy queries\n• Acceptable staleness\n• Read-heavy workloads",
      "explanation": "Views are virtual tables from queries, while materialized views store precomputed results. Standard views are always current; materialized views are faster but potentially stale.",
      "difficulty": "Medium",
      "code": "-- Create view (MySQL)\nCREATE VIEW active_users AS\nSELECT id, name, email, created_at\nFROM users\nWHERE status = 'ACTIVE';\n\n-- Query view\nSELECT * FROM active_users WHERE name LIKE 'John%';\n\n-- View with JOIN\nCREATE VIEW user_orders AS\nSELECT \n    u.id,\n    u.name,\n    COUNT(o.id) AS order_count,\n    SUM(o.total) AS total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n\n-- Materialized view (PostgreSQL)\nCREATE MATERIALIZED VIEW order_stats AS\nSELECT \n    DATE(order_date) AS day,\n    COUNT(*) AS order_count,\n    SUM(total) AS revenue\nFROM orders\nGROUP BY DATE(order_date);\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW order_stats;\n\n-- Concurrent refresh (non-blocking)\nREFRESH MATERIALIZED VIEW CONCURRENTLY order_stats;\n\n-- Drop view\nDROP VIEW IF EXISTS active_users;\nDROP MATERIALIZED VIEW IF EXISTS order_stats;"
    },
    {
      "id": 59,
      "question": "What are database transactions and savepoints?",
      "answer": "Transactions: Group operations as single unit\n\nCommands:\n• BEGIN/START TRANSACTION: Start\n• COMMIT: Permanent save\n• ROLLBACK: Undo all changes\n• SAVEPOINT: Partial rollback point\n\nSavepoints:\n• Named checkpoint\n• Rollback to savepoint\n• Multiple savepoints allowed\n• Nested transactions\n• Complex error handling\n\nUse Cases:\n\nTransactions:\n• All-or-nothing operations\n• Data integrity\n• Consistency\n• Bank transfers\n• Order processing\n\nSavepoints:\n• Batch processing\n• Partial rollback\n• Error recovery\n• Complex workflows\n• Long transactions\n\nBest Practices:\n• Keep transactions short\n• Avoid user interaction\n• Handle exceptions\n• Release locks quickly\n• Use appropriate isolation\n• Monitor long transactions",
      "explanation": "Transactions ensure ACID properties for grouped operations. Savepoints allow partial rollback within transactions for complex error handling.",
      "difficulty": "Medium",
      "code": "-- Basic transaction\nSTART TRANSACTION;\n\nINSERT INTO orders (user_id, total) VALUES (1, 299.99);\nINSERT INTO order_items (order_id, product_id, quantity) VALUES (LAST_INSERT_ID(), 5, 2);\nUPDATE inventory SET quantity = quantity - 2 WHERE product_id = 5;\n\nCOMMIT;\n\n-- Transaction with savepoint (PostgreSQL)\nBEGIN;\n\nINSERT INTO users (name, email) VALUES ('John', 'john@example.com');\nSAVEPOINT user_inserted;\n\nINSERT INTO addresses (user_id, street) VALUES (CURRVAL('users_id_seq'), '123 Main St');\nSAVEPOINT address_inserted;\n\n-- Error in payment processing\nINSERT INTO payments (user_id, amount) VALUES (CURRVAL('users_id_seq'), -100);\n-- Error: amount cannot be negative\n\n-- Rollback only payment, keep user and address\nROLLBACK TO SAVEPOINT address_inserted;\n\n-- Fix and retry\nINSERT INTO payments (user_id, amount) VALUES (CURRVAL('users_id_seq'), 100);\n\nCOMMIT;\n\n-- Application code (Node.js)\nconst conn = await pool.getConnection();\n\ntry {\n    await conn.beginTransaction();\n    \n    const [orderResult] = await conn.execute(\n        'INSERT INTO orders (user_id, total) VALUES (?, ?)',\n        [userId, total]\n    );\n    const orderId = orderResult.insertId;\n    \n    await conn.execute('SAVEPOINT items_start');\n    \n    for (const item of items) {\n        try {\n            await conn.execute(\n                'INSERT INTO order_items (order_id, product_id, quantity) VALUES (?, ?, ?)',\n                [orderId, item.productId, item.quantity]\n            );\n        } catch (itemError) {\n            // Rollback to savepoint, continue with other items\n            await conn.execute('ROLLBACK TO SAVEPOINT items_start');\n            console.error(`Failed to add item ${item.productId}`);\n        }\n    }\n    \n    await conn.commit();\n    return orderId;\n    \n} catch (error) {\n    await conn.rollback();\n    throw error;\n} finally {\n    conn.release();\n}"
    },
    {
      "id": 60,
      "question": "What are database constraints and their enforcement?",
      "answer": "Constraints: Rules to enforce data integrity\n\nTypes:\n\nNOT NULL:\n• Prevents NULL values\n• Column-level\n• Most basic constraint\n\nUNIQUE:\n• No duplicate values\n• Allows NULL (usually)\n• Composite unique constraints\n\nPRIMARY KEY:\n• Unique + NOT NULL\n• One per table\n• Identifies row uniquely\n\nFOREIGN KEY:\n• References another table\n• Enforces referential integrity\n• CASCADE/SET NULL/RESTRICT\n\nCHECK:\n• Custom validation rule\n• Column or table level\n• Boolean expression\n\nDEFAULT:\n• Automatic value\n• When no value provided\n• Expressions allowed\n\nEnforcement:\n• At INSERT/UPDATE\n• Can be deferred\n• Performance impact\n• Error handling needed",
      "explanation": "Database constraints enforce data integrity rules like NOT NULL, UNIQUE, PRIMARY KEY, FOREIGN KEY, and CHECK, preventing invalid data from entering the database.",
      "difficulty": "Medium",
      "code": "-- Create table with constraints\nCREATE TABLE users (\n    id INT AUTO_INCREMENT,\n    email VARCHAR(255) NOT NULL,\n    age INT,\n    status VARCHAR(20) DEFAULT 'ACTIVE',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    \n    -- Primary key\n    PRIMARY KEY (id),\n    \n    -- Unique constraint\n    UNIQUE KEY uk_email (email),\n    \n    -- Check constraint\n    CONSTRAINT chk_age CHECK (age >= 18 AND age <= 120),\n    CONSTRAINT chk_status CHECK (status IN ('ACTIVE', 'INACTIVE', 'SUSPENDED'))\n);\n\n-- Foreign key constraint\nCREATE TABLE orders (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    user_id INT NOT NULL,\n    order_date DATE NOT NULL,\n    total DECIMAL(10,2) NOT NULL,\n    \n    CONSTRAINT fk_user\n        FOREIGN KEY (user_id)\n        REFERENCES users(id)\n        ON DELETE CASCADE\n        ON UPDATE CASCADE,\n    \n    CONSTRAINT chk_total CHECK (total > 0)\n);\n\n-- Add constraint to existing table\nALTER TABLE products\nADD CONSTRAINT chk_price CHECK (price >= 0);\n\n-- Drop constraint\nALTER TABLE products\nDROP CONSTRAINT chk_price;\n\n-- Disable/Enable constraints (SQL Server)\nALTER TABLE orders NOCHECK CONSTRAINT fk_user;\nALTER TABLE orders CHECK CONSTRAINT fk_user;"
    },
    {
      "id": 61,
      "question": "What are database window functions?",
      "answer": "Window Functions: Calculations across rows without grouping\n\nCategories:\n\nRanking:\n• ROW_NUMBER(): Sequential numbering\n• RANK(): With gaps for ties\n• DENSE_RANK(): No gaps\n• NTILE(n): Divide into n groups\n\nAggregate:\n• SUM(), AVG(), COUNT()\n• MIN(), MAX()\n• Over window of rows\n\nValue:\n• LAG(): Previous row value\n• LEAD(): Next row value\n• FIRST_VALUE(), LAST_VALUE()\n\nWindow Clause:\n• PARTITION BY: Groups\n• ORDER BY: Ordering\n• ROWS/RANGE: Frame specification\n\nBenefits:\n• No grouping needed\n• Access to individual rows\n• Running totals\n• Moving averages\n• Row comparisons",
      "explanation": "Window functions perform calculations across row sets without collapsing results, enabling rankings, running totals, and row comparisons while preserving individual rows.",
      "difficulty": "Hard",
      "code": "-- ROW_NUMBER: Sequential numbering\nSELECT \n    name,\n    department,\n    salary,\n    ROW_NUMBER() OVER (ORDER BY salary DESC) AS row_num\nFROM employees;\n\n-- RANK per partition\nSELECT \n    name,\n    department,\n    salary,\n    RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_rank\nFROM employees;\n\n-- Running total\nSELECT \n    order_date,\n    amount,\n    SUM(amount) OVER (ORDER BY order_date) AS running_total\nFROM orders;\n\n-- LAG and LEAD\nSELECT \n    month,\n    revenue,\n    LAG(revenue) OVER (ORDER BY month) AS prev_month,\n    LEAD(revenue) OVER (ORDER BY month) AS next_month,\n    revenue - LAG(revenue) OVER (ORDER BY month) AS growth\nFROM monthly_revenue;\n\n-- Moving average (3-month)\nSELECT \n    month,\n    sales,\n    AVG(sales) OVER (\n        ORDER BY month\n        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n    ) AS moving_avg_3mo\nFROM monthly_sales;"
    },
    {
      "id": 62,
      "question": "What are common SQL injection prevention techniques?",
      "answer": "SQL Injection: Malicious SQL code insertion\n\nAttack Example:\n• Input: `admin'--`\n• Query: `SELECT * FROM users WHERE username = 'admin'--' AND password = 'x'`\n• Comments out password check!\n\nPrevention Techniques:\n\n1. Prepared Statements (Best):\n• Parameterized queries\n• Separate data from code\n• Database handles escaping\n• Most effective\n\n2. Stored Procedures:\n• Precompiled SQL\n• Parameters separate\n• Limited attack surface\n\n3. Input Validation:\n• Whitelist allowed characters\n• Type checking\n• Length limits\n• Not sufficient alone\n\n4. Escape User Input:\n• Last resort\n• Database-specific escaping\n• Error-prone\n• Use libraries\n\n5. Least Privilege:\n• Limited database permissions\n• Separate accounts per service\n• No DROP/ALTER in app accounts\n\nNever:\n• String concatenation with user input\n• Trust user input\n• Use only client-side validation",
      "explanation": "SQL injection allows attackers to execute malicious SQL. Prevent it with prepared statements, input validation, stored procedures, and least privilege database accounts.",
      "difficulty": "Medium",
      "code": "-- UNSAFE (vulnerable)\nconst username = req.body.username;  // 'admin'--'\nconst password = req.body.password;\nconst query = `SELECT * FROM users WHERE username = '${username}' AND password = '${password}'`;\ndb.query(query);  // SQL INJECTION!\n\n-- SAFE: Prepared statements (Node.js)\nconst [rows] = await connection.execute(\n    'SELECT * FROM users WHERE username = ? AND password = ?',\n    [username, password]\n);  // Parameters safely escaped\n\n-- SAFE: Java PreparedStatement\nString sql = \"SELECT * FROM users WHERE username = ? AND password = ?\";\nPreparedStatement pstmt = conn.prepareStatement(sql);\npstmt.setString(1, username);\npstmt.setString(2, password);\nResultSet rs = pstmt.executeQuery();\n\n-- SAFE: Python with parameters\ncursor.execute(\n    \"SELECT * FROM users WHERE username = %s AND password = %s\",\n    (username, password)\n)\n\n-- SAFE: C# with parameters\nstring sql = \"SELECT * FROM users WHERE username = @username AND password = @password\";\nusing (SqlCommand cmd = new SqlCommand(sql, connection))\n{\n    cmd.Parameters.AddWithValue(\"@username\", username);\n    cmd.Parameters.AddWithValue(\"@password\", password);\n    SqlDataReader reader = cmd.ExecuteReader();\n}\n\n-- Input validation\nfunction validateUsername(username) {\n    // Only alphanumeric and underscore\n    if (!/^[a-zA-Z0-9_]+$/.test(username)) {\n        throw new Error('Invalid username format');\n    }\n    if (username.length > 50) {\n        throw new Error('Username too long');\n    }\n    return username;\n}"
    },
    {
      "id": 63,
      "question": "What are database locks and locking strategies?",
      "answer": "Database Locks: Control concurrent access\n\nLock Types:\n\nShared Lock (S):\n• Read lock\n• Multiple readers allowed\n• Blocks writers\n• Released after read\n\nExclusive Lock (X):\n• Write lock\n• Blocks readers and writers\n• Only one at a time\n• Held during write\n\nGranularity:\n• Row-level: Lock individual rows\n• Page-level: Lock data pages\n• Table-level: Lock entire table\n• Database-level: Lock database\n\nLocking Strategies:\n\nOptimistic:\n• Assume no conflicts\n• Check at commit time\n• Version numbers\n• Good for low contention\n\nPessimistic:\n• Lock immediately\n• Hold until commit\n• Prevents conflicts\n• Good for high contention\n\nDeadlock Prevention:\n• Timeout settings\n• Lock ordering\n• Deadlock detection",
      "explanation": "Database locks control concurrent access using shared (read) and exclusive (write) locks at various granularities, with optimistic or pessimistic strategies.",
      "difficulty": "Hard",
      "code": "-- Shared lock (SELECT FOR SHARE)\nBEGIN;\nSELECT * FROM accounts WHERE id = 1 FOR SHARE;\n-- Other transactions can read, cannot write\nCOMMIT;\n\n-- Exclusive lock (SELECT FOR UPDATE)\nBEGIN;\nSELECT * FROM accounts WHERE id = 1 FOR UPDATE;\n-- Blocks all other access\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nCOMMIT;\n\n-- Table-level lock\nLOCK TABLES accounts WRITE;\n-- Exclusive access to table\nUPDATE accounts SET balance = balance * 1.05;\nUNLOCK TABLES;\n\n-- Optimistic locking (version column)\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    price DECIMAL(10,2),\n    version INT DEFAULT 1\n);\n\n-- Update with version check\nUPDATE products\nSET price = 29.99, version = version + 1\nWHERE id = 1 AND version = 5;\n-- If affected rows = 0, version conflict\n\n-- Check lock status (MySQL)\nSHOW ENGINE INNODB STATUS;\n\nSELECT \n    r.trx_id waiting_trx,\n    r.trx_mysql_thread_id waiting_thread,\n    b.trx_id blocking_trx,\n    b.trx_mysql_thread_id blocking_thread\nFROM information_schema.innodb_lock_waits w\nJOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id\nJOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id;"
    },
    {
      "id": 64,
      "question": "What is database query execution plan analysis?",
      "answer": "Execution Plan: How database executes query\n\nComponents:\n\nAccess Methods:\n• Full table scan\n• Index scan/seek\n• Index-only scan\n• Range scan\n\nJoin Methods:\n• Nested loop join\n• Hash join\n• Merge join\n• Join order\n\nOperations:\n• Sorting\n• Aggregation\n• Filtering\n• Subquery execution\n\nMetrics:\n• Estimated vs actual rows\n• Cost estimates\n• Execution time\n• Memory usage\n\nAnalysis Tools:\n• EXPLAIN (MySQL/PostgreSQL)\n• EXPLAIN ANALYZE (actual execution)\n• Query Store (SQL Server)\n• Execution plan viewers\n\nRed Flags:\n• Full table scans on large tables\n• No index usage\n• High row estimates\n• Type = ALL\n• Nested loops on large sets\n• Filesort/temporary tables",
      "explanation": "Execution plans show how databases execute queries, revealing access methods, join strategies, and performance bottlenecks for optimization.",
      "difficulty": "Hard",
      "code": "-- EXPLAIN (MySQL)\nEXPLAIN SELECT \n    u.name,\n    COUNT(o.id) AS order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01'\nGROUP BY u.id, u.name;\n\n/*\nKey columns:\n- id: Select order\n- select_type: SIMPLE, SUBQUERY, UNION\n- table: Table name\n- type: Access method (const, eq_ref, ref, range, index, ALL)\n- possible_keys: Indexes considered\n- key: Index actually used (NULL = no index)\n- key_len: Index bytes used\n- ref: Columns compared to index\n- rows: Estimated rows examined\n- Extra: Additional info (Using where, Using filesort, Using temporary)\n*/\n\n-- EXPLAIN ANALYZE (PostgreSQL) - shows actual execution\nEXPLAIN ANALYZE\nSELECT * FROM orders\nWHERE user_id = 123\n  AND status = 'COMPLETED';\n\n/*\nOutput:\nIndex Scan using idx_user_status on orders  (cost=0.42..8.44 rows=1 width=48) \n                                             (actual time=0.015..0.016 rows=1 loops=1)\n  Index Cond: ((user_id = 123) AND (status = 'COMPLETED'))\nPlanning Time: 0.123 ms\nExecution Time: 0.045 ms\n\nKey info:\n- cost: Estimated cost (startup..total)\n- rows: Estimated rows\n- actual time: Real execution time\n- rows: Actual rows returned\n- loops: Times operation executed\n*/\n\n-- Analyze access methods\nEXPLAIN SELECT * FROM orders WHERE id = 123;\n-- type: const (single row, fastest)\n\nEXPLAIN SELECT * FROM orders WHERE user_id = 123;\n-- type: ref (index lookup, good)\n\nEXPLAIN SELECT * FROM orders WHERE user_id > 100;\n-- type: range (index range scan, acceptable)\n\nEXPLAIN SELECT * FROM orders WHERE YEAR(created_at) = 2024;\n-- type: ALL (full table scan, BAD - function on column prevents index)\n\n-- Better query\nEXPLAIN SELECT * FROM orders \nWHERE created_at >= '2024-01-01' AND created_at < '2025-01-01';\n-- type: range (uses index)\n\n-- Visual execution plan (SQL Server)\nSET SHOWPLAN_TEXT ON;\nGO\nSELECT * FROM users WHERE email = 'test@example.com';\nGO\nSET SHOWPLAN_TEXT OFF;\nGO"
    },
    {
      "id": 65,
      "question": "What are database statistics and why are they important?",
      "answer": "Database Statistics: Metadata about data distribution\n\nWhat They Track:\n• Row counts per table\n• Column value distribution\n• Index cardinality\n• Data density\n• Null percentages\n• Histogram data\n\nPurpose:\n• Query optimizer decisions\n• Index selection\n• Join order\n• Access method choice\n• Cost estimation\n\nWhen to Update:\n• After bulk data changes\n• After major updates\n• Scheduled maintenance\n• When plans degrade\n• New indexes created\n\nImpact of Stale Statistics:\n• Poor query plans\n• Wrong index selection\n• Inefficient joins\n• Slow queries\n• Performance degradation\n\nBest Practices:\n• Auto-update enabled\n• Manual update after bulk ops\n• Regular maintenance schedule\n• Monitor statistics age\n• Update after index creation\n• Sampling for large tables",
      "explanation": "Database statistics contain metadata used by the query optimizer to choose efficient execution plans. Outdated statistics lead to poor performance.",
      "difficulty": "Medium",
      "code": "-- Update statistics (MySQL)\nANALYZE TABLE users;\nANALYZE TABLE orders, products;\n\n-- View table statistics (MySQL)\nSHOW TABLE STATUS LIKE 'users';\nSHOW INDEX FROM users;\n\n-- Detailed statistics\nSELECT \n    table_name,\n    table_rows,\n    avg_row_length,\n    data_length,\n    index_length,\n    data_free\nFROM information_schema.tables\nWHERE table_schema = 'mydb';\n\n-- PostgreSQL: Update statistics\nANALYZE users;\nANALYZE;  -- All tables\n\n-- Verbose output\nANALYZE VERBOSE users;\n\n-- View statistics\nSELECT \n    schemaname,\n    tablename,\n    n_live_tup AS row_count,\n    n_dead_tup AS dead_rows,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables;\n\n-- SQL Server: Update statistics\nUPDATE STATISTICS users;\nUPDATE STATISTICS users idx_email;  -- Specific index\n\n-- With full scan\nUPDATE STATISTICS users WITH FULLSCAN;\n\n-- Auto-update statistics (SQL Server)\nALTER DATABASE mydb SET AUTO_UPDATE_STATISTICS ON;\nALTER DATABASE mydb SET AUTO_CREATE_STATISTICS ON;\n\n-- View statistics info\nDBCC SHOW_STATISTICS ('users', 'idx_email');\n\n-- Statistics maintenance script\n-- Run after bulk operations\nDO $$\nDECLARE\n    tbl record;\nBEGIN\n    FOR tbl IN \n        SELECT tablename \n        FROM pg_tables \n        WHERE schemaname = 'public'\n    LOOP\n        EXECUTE 'ANALYZE ' || quote_ident(tbl.tablename);\n        RAISE NOTICE 'Analyzed table: %', tbl.tablename;\n    END LOOP;\nEND $$;\n\n-- Check statistics age (PostgreSQL)\nSELECT \n    schemaname || '.' || tablename AS table,\n    n_live_tup AS rows,\n    last_analyze,\n    NOW() - last_analyze AS time_since_analyze\nFROM pg_stat_user_tables\nWHERE last_analyze IS NOT NULL\nORDER BY last_analyze ASC\nLIMIT 20;"
    },
    {
      "id": 66,
      "question": "What are database indexes and their types?",
      "answer": "Indexes: Data structures for fast retrieval\n\nB-Tree Index (Default):\n• Balanced tree structure\n• Sorted data\n• Range queries efficient\n• Most common type\n\nHash Index:\n• Hash table based\n• Exact match only\n• Very fast lookups\n• No range queries\n\nBitmap Index:\n• For low cardinality\n• Gender, status, boolean\n• Multiple conditions\n• Data warehouses\n\nFull-Text Index:\n• Text search\n• Word matching\n• Relevance ranking\n• Language-aware\n\nSpatial Index:\n• Geographic data\n• Coordinate searches\n• Distance queries\n\nCovering Index:\n• Includes all query columns\n• No table lookup\n• Fast queries",
      "explanation": "Database indexes speed up queries using various structures: B-Tree (general), Hash (exact match), Bitmap (low cardinality), Full-Text (text search), and Spatial (geographic).",
      "difficulty": "Medium",
      "code": "-- B-Tree index (default)\nCREATE INDEX idx_email ON users(email);\nCREATE INDEX idx_created ON orders(created_at);\n\n-- Composite index\nCREATE INDEX idx_user_status ON orders(user_id, status);\n\n-- Unique index\nCREATE UNIQUE INDEX idx_username ON users(username);\n\n-- Hash index (PostgreSQL)\nCREATE INDEX idx_hash_email ON users USING HASH (email);\n\n-- Full-text index (MySQL)\nCREATE FULLTEXT INDEX idx_ft_content ON articles(title, body);\n\n-- Query with full-text\nSELECT * FROM articles\nWHERE MATCH(title, body) AGAINST ('database optimization' IN NATURAL LANGUAGE MODE);\n\n-- Spatial index (MySQL)\nCREATE SPATIAL INDEX idx_location ON stores(location);\n\n-- Covering index\nCREATE INDEX idx_covering ON orders(user_id, status, created_at, total);\n\n-- Query uses only index (no table access)\nSELECT user_id, status, total\nFROM orders\nWHERE user_id = 123;\n\n-- Partial index (PostgreSQL)\nCREATE INDEX idx_active_users ON users(email) WHERE status = 'ACTIVE';\n\n-- Expression index\nCREATE INDEX idx_lower_email ON users(LOWER(email));\n\nSELECT * FROM users WHERE LOWER(email) = 'test@example.com';"
    },
    {
      "id": 67,
      "question": "What is database caching and strategies?",
      "answer": "Database Caching: Store frequently accessed data in memory\n\nCache Layers:\n\nQuery Result Cache:\n• Store query results\n• Exact query match\n• Invalidate on data change\n• Built-in or external\n\nObject Cache:\n• Store application objects\n• ORM entities\n• Reduce serialization\n• Redis, Memcached\n\nPage Cache:\n• Cache entire pages\n• Web content\n• CDN or reverse proxy\n\nStrategies:\n\nCache-Aside:\n• App checks cache first\n• On miss, load from DB\n• App updates cache\n• Most common\n\nWrite-Through:\n• Write to cache and DB\n• Synchronous\n• Always consistent\n• Slower writes\n\nWrite-Behind:\n• Write to cache only\n• Async DB write\n• Fast writes\n• Risk of data loss\n\nInvalidation:\n• Time-based (TTL)\n• Event-based\n• Manual\n• Cache stampede prevention",
      "explanation": "Database caching stores data in memory for fast access using strategies like cache-aside, write-through, and write-behind with various invalidation approaches.",
      "difficulty": "Medium",
      "code": "// Cache-aside pattern (Node.js + Redis)\nconst redis = require('redis');\nconst client = redis.createClient();\n\nasync function getUser(userId) {\n    const cacheKey = `user:${userId}`;\n    \n    // 1. Try cache\n    let user = await client.get(cacheKey);\n    \n    if (user) {\n        // Cache hit\n        console.log('Cache hit');\n        return JSON.parse(user);\n    }\n    \n    // 2. Cache miss - load from DB\n    console.log('Cache miss');\n    const [rows] = await db.execute(\n        'SELECT * FROM users WHERE id = ?',\n        [userId]\n    );\n    user = rows[0];\n    \n    // 3. Store in cache (1 hour TTL)\n    if (user) {\n        await client.setex(cacheKey, 3600, JSON.stringify(user));\n    }\n    \n    return user;\n}\n\n// Write-through pattern\nasync function updateUser(userId, data) {\n    // 1. Update database\n    await db.execute(\n        'UPDATE users SET name = ?, email = ? WHERE id = ?',\n        [data.name, data.email, userId]\n    );\n    \n    // 2. Update cache immediately\n    const cacheKey = `user:${userId}`;\n    await client.setex(cacheKey, 3600, JSON.stringify({\n        id: userId,\n        ...data\n    }));\n}\n\n// Cache invalidation\nasync function deleteUser(userId) {\n    // Delete from DB\n    await db.execute('DELETE FROM users WHERE id = ?', [userId]);\n    \n    // Invalidate cache\n    await client.del(`user:${userId}`);\n}\n\n// Pattern: Cache all user orders\nasync function getUserOrders(userId) {\n    const cacheKey = `user:${userId}:orders`;\n    \n    let orders = await client.get(cacheKey);\n    if (orders) {\n        return JSON.parse(orders);\n    }\n    \n    const [rows] = await db.execute(\n        'SELECT * FROM orders WHERE user_id = ? ORDER BY created_at DESC',\n        [userId]\n    );\n    \n    await client.setex(cacheKey, 300, JSON.stringify(rows));  // 5 min TTL\n    return rows;\n}\n\n// Query result caching (MySQL)\nSET GLOBAL query_cache_size = 67108864;  -- 64MB\nSET GLOBAL query_cache_type = 1;\n\n-- Cache this query\nSELECT SQL_CACHE * FROM products WHERE category = 'Electronics';\n\n-- Don't cache this query\nSELECT SQL_NO_CACHE * FROM orders WHERE user_id = 123;\n\n-- Cache stampede prevention\nconst locks = new Map();\n\nasync function getUserWithLock(userId) {\n    const cacheKey = `user:${userId}`;\n    const lockKey = `lock:${cacheKey}`;\n    \n    // Check cache\n    let user = await client.get(cacheKey);\n    if (user) return JSON.parse(user);\n    \n    // Try to acquire lock\n    const lockAcquired = await client.setnx(lockKey, '1');\n    \n    if (lockAcquired) {\n        // We got the lock - load from DB\n        try {\n            await client.expire(lockKey, 10);  // Lock timeout\n            \n            const [rows] = await db.execute(\n                'SELECT * FROM users WHERE id = ?',\n                [userId]\n            );\n            user = rows[0];\n            \n            if (user) {\n                await client.setex(cacheKey, 3600, JSON.stringify(user));\n            }\n            \n            return user;\n        } finally {\n            await client.del(lockKey);\n        }\n    } else {\n        // Someone else is loading - wait and retry\n        await new Promise(resolve => setTimeout(resolve, 100));\n        return getUserWithLock(userId);  // Retry\n    }\n}"
    },
    {
      "id": 68,
      "question": "What are Common Table Expressions (CTEs)?",
      "answer": "CTEs: Temporary named result sets\n\nSyntax:\n```sql\nWITH cte_name AS (\n    SELECT ...\n)\nSELECT * FROM cte_name;\n```\n\nBenefits:\n• Improved readability\n• Reusable in query\n• Simplified complex queries\n• Better than subqueries\n• Recursive queries support\n\nTypes:\n\nSimple CTE:\n• Single query result\n• Used once or multiple times\n• Clean syntax\n\nMultiple CTEs:\n• Chain CTEs with commas\n• Each builds on previous\n• Step-by-step logic\n\nRecursive CTE:\n• Self-referencing\n• Hierarchical data\n• Tree structures\n• Graph traversal\n\nWhen to Use:\n• Complex queries\n• Hierarchical data\n• Multiple subqueries\n• Improve maintainability\n• Recursive operations",
      "explanation": "CTEs create temporary named result sets that improve query readability and enable recursive queries for hierarchical data like organizational charts or category trees.",
      "difficulty": "Hard",
      "code": "-- Simple CTE\nWITH active_users AS (\n    SELECT id, name, email\n    FROM users\n    WHERE status = 'ACTIVE'\n)\nSELECT * FROM active_users\nWHERE email LIKE '%@gmail.com';\n\n-- Multiple CTEs\nWITH \n    active_users AS (\n        SELECT id, name FROM users WHERE status = 'ACTIVE'\n    ),\n    user_orders AS (\n        SELECT user_id, COUNT(*) AS order_count\n        FROM orders\n        GROUP BY user_id\n    )\nSELECT \n    u.name,\n    COALESCE(o.order_count, 0) AS orders\nFROM active_users u\nLEFT JOIN user_orders o ON u.id = o.user_id;\n\n-- Recursive CTE: Employee hierarchy\nWITH RECURSIVE employee_hierarchy AS (\n    -- Base case: Top-level employees\n    SELECT id, name, manager_id, 1 AS level\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive case: Employees under managers\n    SELECT e.id, e.name, e.manager_id, eh.level + 1\n    FROM employees e\n    JOIN employee_hierarchy eh ON e.manager_id = eh.id\n)\nSELECT * FROM employee_hierarchy\nORDER BY level, name;\n\n-- Recursive CTE: Category tree\nWITH RECURSIVE category_tree AS (\n    -- Root categories\n    SELECT \n        id,\n        name,\n        parent_id,\n        name AS path,\n        1 AS depth\n    FROM categories\n    WHERE parent_id IS NULL\n    \n    UNION ALL\n    \n    -- Child categories\n    SELECT \n        c.id,\n        c.name,\n        c.parent_id,\n        CONCAT(ct.path, ' > ', c.name) AS path,\n        ct.depth + 1\n    FROM categories c\n    JOIN category_tree ct ON c.parent_id = ct.id\n    WHERE ct.depth < 10  -- Prevent infinite recursion\n)\nSELECT * FROM category_tree\nORDER BY path;\n\n-- CTE vs Subquery comparison\n-- Subquery (harder to read)\nSELECT u.name, o.orders\nFROM users u\nLEFT JOIN (\n    SELECT user_id, COUNT(*) AS orders\n    FROM orders\n    WHERE status = 'COMPLETED'\n    GROUP BY user_id\n) o ON u.id = o.user_id\nWHERE u.status = 'ACTIVE';\n\n-- CTE (cleaner)\nWITH completed_orders AS (\n    SELECT user_id, COUNT(*) AS orders\n    FROM orders\n    WHERE status = 'COMPLETED'\n    GROUP BY user_id\n)\nSELECT u.name, COALESCE(o.orders, 0) AS orders\nFROM users u\nLEFT JOIN completed_orders o ON u.id = o.user_id\nWHERE u.status = 'ACTIVE';"
    },
    {
      "id": 69,
      "question": "What is database connection management in applications?",
      "answer": "Connection Management: Efficiently handle DB connections\n\nKey Concepts:\n\nConnection Lifecycle:\n• Open: Establish connection\n• Use: Execute queries\n• Close: Release resources\n• Reuse: Pool connections\n\nBest Practices:\n\n1. Use Connection Pooling:\n• Reuse connections\n• Limit max connections\n• Set timeouts\n• Monitor metrics\n\n2. Always Close Connections:\n• Use try-finally\n• Try-with-resources (Java)\n• Context managers (Python)\n• Defer (Go)\n\n3. Handle Errors:\n• Retry logic\n• Circuit breakers\n• Graceful degradation\n• Log failures\n\n4. Transactions:\n• Explicit begin/commit\n• Rollback on error\n• Keep short\n• Release locks quickly\n\n5. Security:\n• Use credentials securely\n• Least privilege\n• SSL/TLS encryption\n• Connection limits",
      "explanation": "Proper connection management uses pooling to reuse connections, always closes resources, handles errors gracefully, and keeps transactions short for optimal performance.",
      "difficulty": "Medium",
      "code": "// Node.js - Connection pooling\nconst mysql = require('mysql2/promise');\n\nconst pool = mysql.createPool({\n    host: 'localhost',\n    user: 'app',\n    password: process.env.DB_PASSWORD,\n    database: 'myapp',\n    connectionLimit: 10,\n    waitForConnections: true,\n    queueLimit: 0\n});\n\n// Good: Pool handles connection lifecycle\nasync function getUsers() {\n    const [rows] = await pool.execute('SELECT * FROM users');\n    return rows;\n}  // Connection automatically returned to pool\n\n// Transaction handling\nasync function transferFunds(fromId, toId, amount) {\n    const connection = await pool.getConnection();\n    \n    try {\n        await connection.beginTransaction();\n        \n        await connection.execute(\n            'UPDATE accounts SET balance = balance - ? WHERE id = ?',\n            [amount, fromId]\n        );\n        \n        await connection.execute(\n            'UPDATE accounts SET balance = balance + ? WHERE id = ?',\n            [amount, toId]\n        );\n        \n        await connection.commit();\n        \n    } catch (error) {\n        await connection.rollback();\n        throw error;\n    } finally {\n        connection.release();  // CRITICAL: Always release\n    }\n}\n\n// Java - Try-with-resources\npublic List<User> getUsers() throws SQLException {\n    List<User> users = new ArrayList<>();\n    \n    String sql = \"SELECT id, name, email FROM users\";\n    \n    try (Connection conn = dataSource.getConnection();\n         PreparedStatement pstmt = conn.prepareStatement(sql);\n         ResultSet rs = pstmt.executeQuery()) {\n        \n        while (rs.next()) {\n            users.add(new User(\n                rs.getInt(\"id\"),\n                rs.getString(\"name\"),\n                rs.getString(\"email\")\n            ));\n        }\n    }  // Automatic cleanup\n    \n    return users;\n}\n\n// Python - Context manager\nimport psycopg2\nfrom contextlib import contextmanager\n\n@contextmanager\ndef get_db_connection():\n    conn = psycopg2.connect(\n        host=\"localhost\",\n        database=\"myapp\",\n        user=\"app\",\n        password=os.getenv(\"DB_PASSWORD\")\n    )\n    try:\n        yield conn\n        conn.commit()\n    except Exception:\n        conn.rollback()\n        raise\n    finally:\n        conn.close()\n\n# Usage\nwith get_db_connection() as conn:\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT * FROM users\")\n    users = cursor.fetchall()\n\n// Error handling and retry\nconst retry = require('async-retry');\n\nasync function queryWithRetry(sql, params) {\n    return await retry(\n        async (bail) => {\n            try {\n                const [rows] = await pool.execute(sql, params);\n                return rows;\n            } catch (error) {\n                if (error.code === 'PROTOCOL_CONNECTION_LOST') {\n                    throw error;  // Retry\n                } else {\n                    bail(error);  // Don't retry\n                }\n            }\n        },\n        {\n            retries: 3,\n            minTimeout: 1000,\n            maxTimeout: 5000\n        }\n    );\n}\n\n// Monitor connection pool\nsetInterval(() => {\n    console.log('Pool status:', {\n        total: pool.pool._allConnections.length,\n        free: pool.pool._freeConnections.length,\n        used: pool.pool._allConnections.length - pool.pool._freeConnections.length\n    });\n}, 60000);  // Every minute"
    },
    {
      "id": 70,
      "question": "What are database best practices for production?",
      "answer": "Production Database Best Practices\n\nPerformance:\n• Proper indexing\n• Query optimization\n• Connection pooling\n• Caching strategies\n• Regular ANALYZE/OPTIMIZE\n\nSecurity:\n• Least privilege access\n• Encrypted connections (SSL/TLS)\n• Secure credentials\n• Regular security updates\n• Audit logging\n• No default passwords\n\nReliability:\n• Regular backups\n• Test restore procedures\n• Replication/HA setup\n• Monitoring and alerting\n• Disaster recovery plan\n• Failover testing\n\nMaintenance:\n• Scheduled maintenance windows\n• Index maintenance\n• Statistics updates\n• Log rotation\n• Cleanup old data\n• Vacuum (PostgreSQL)\n\nMonitoring:\n• Query performance\n• Resource usage (CPU/RAM/disk)\n• Connection counts\n• Slow query log\n• Replication lag\n• Error rates\n\nDevelopment:\n• Use migrations\n• Test changes in staging\n• Rollback procedures\n• Documentation\n• Code reviews",
      "explanation": "Production databases require proper performance tuning, security measures, reliable backups, regular maintenance, comprehensive monitoring, and disciplined development practices.",
      "difficulty": "Medium",
      "code": "-- Security: Create application user with limited privileges\nCREATE USER 'app_user'@'%' IDENTIFIED BY 'strong_password';\nGRANT SELECT, INSERT, UPDATE, DELETE ON myapp.* TO 'app_user'@'%';\n-- NO DROP, ALTER, or admin privileges\n\n-- Read-only user for reporting\nCREATE USER 'reporting'@'%' IDENTIFIED BY 'strong_password';\nGRANT SELECT ON myapp.* TO 'reporting'@'%';\n\n-- Enable SSL (my.cnf)\n[mysqld]\nssl-ca=/etc/mysql/ssl/ca.pem\nssl-cert=/etc/mysql/ssl/server-cert.pem\nssl-key=/etc/mysql/ssl/server-key.pem\nrequire_secure_transport=ON\n\n-- Backup script (daily)\n#!/bin/bash\nBACKUP_DIR=\"/backups/mysql\"\nDATE=$(date +%Y%m%d_%H%M%S)\nRETENTION_DAYS=7\n\nmysqldump \\\n    -u backup_user \\\n    -p\"$BACKUP_PASSWORD\" \\\n    --single-transaction \\\n    --routines \\\n    --triggers \\\n    --all-databases \\\n    | gzip > \"$BACKUP_DIR/backup_$DATE.sql.gz\"\n\n# Upload to S3\naws s3 cp \"$BACKUP_DIR/backup_$DATE.sql.gz\" s3://my-backups/mysql/\n\n# Cleanup old backups\nfind $BACKUP_DIR -name \"backup_*.sql.gz\" -mtime +$RETENTION_DAYS -delete\n\n-- Monitoring query (slow queries)\nSELECT \n    query_time,\n    lock_time,\n    rows_examined,\n    LEFT(sql_text, 200) AS query\nFROM mysql.slow_log\nWHERE query_time > 2\nORDER BY query_time DESC\nLIMIT 20;\n\n-- Connection monitoring\nSHOW PROCESSLIST;\n\nSELECT \n    user,\n    COUNT(*) AS connections\nFROM information_schema.processlist\nGROUP BY user;\n\n-- Resource usage (PostgreSQL)\nSELECT \n    pid,\n    usename,\n    application_name,\n    state,\n    query_start,\n    NOW() - query_start AS duration,\n    query\nFROM pg_stat_activity\nWHERE state = 'active'\n  AND query NOT LIKE '%pg_stat_activity%'\nORDER BY duration DESC;\n\n-- Replication lag monitoring\nSHOW SLAVE STATUS\\G\n\nSELECT \n    CASE \n        WHEN Seconds_Behind_Master IS NULL THEN 'Not Replicating'\n        WHEN Seconds_Behind_Master > 60 THEN 'CRITICAL'\n        WHEN Seconds_Behind_Master > 10 THEN 'WARNING'\n        ELSE 'OK'\n    END AS status,\n    Seconds_Behind_Master AS lag_seconds\nFROM information_schema.slave_status;\n\n-- Maintenance script\n#!/bin/bash\n# Run during maintenance window\n\n# Optimize tables\nmysql -u root -p -e \"OPTIMIZE TABLE myapp.users, myapp.orders, myapp.products;\"\n\n# Update statistics\nmysql -u root -p -e \"ANALYZE TABLE myapp.users, myapp.orders;\"\n\n# Purge old audit logs (keep 90 days)\nmysql -u root -p myapp -e \"\n    DELETE FROM audit_log \n    WHERE created_at < DATE_SUB(NOW(), INTERVAL 90 DAY)\n    LIMIT 10000;\n\"\n\n# Check for unused indexes\nmysql -u root -p -e \"\n    SELECT \n        table_name,\n        index_name,\n        index_type\n    FROM information_schema.statistics\n    WHERE table_schema = 'myapp'\n      AND index_name NOT IN (\n          SELECT index_name \n          FROM performance_schema.table_io_waits_summary_by_index_usage\n          WHERE count_star > 0\n      );\n\"\n\n-- Application monitoring\nconst monitor = {\n    queryCount: 0,\n    slowQueries: 0,\n    errors: 0,\n    startTime: Date.now()\n};\n\n// Track metrics\napp.use((req, res, next) => {\n    const startTime = Date.now();\n    \n    res.on('finish', () => {\n        const duration = Date.now() - startTime;\n        \n        monitor.queryCount++;\n        \n        if (duration > 1000) {\n            monitor.slowQueries++;\n            console.warn(`Slow query: ${req.path} took ${duration}ms`);\n        }\n    });\n    \n    next();\n});\n\n// Report metrics\nsetInterval(() => {\n    const uptime = Date.now() - monitor.startTime;\n    const qps = monitor.queryCount / (uptime / 1000);\n    \n    console.log('Metrics:', {\n        queries: monitor.queryCount,\n        qps: qps.toFixed(2),\n        slowQueries: monitor.slowQueries,\n        errors: monitor.errors\n    });\n}, 60000);  // Every minute"
    },
    {
      "id": 71,
      "question": "What are aggregate functions in SQL?",
      "answer": "Aggregate Functions: Calculate across multiple rows\n\nCommon Functions:\n• COUNT(): Count rows\n• SUM(): Total values\n• AVG(): Average value\n• MIN(): Minimum value\n• MAX(): Maximum value\n• GROUP_CONCAT(): Concatenate strings\n\nUsage:\n• GROUP BY clause\n• HAVING for filtering\n• Window functions\n• NULL handling\n\nBest Practices:\n• Use with GROUP BY\n• HAVING for aggregate filters\n• Consider NULL values\n• Performance on large datasets",
      "explanation": "Aggregate functions perform calculations across rows like COUNT, SUM, AVG, MIN, and MAX, typically used with GROUP BY to summarize data.",
      "difficulty": "Easy",
      "code": "-- Basic aggregates\nSELECT \n    COUNT(*) AS total_users,\n    COUNT(DISTINCT email) AS unique_emails,\n    AVG(age) AS avg_age,\n    MIN(created_at) AS first_user,\n    MAX(created_at) AS latest_user\nFROM users;\n\n-- GROUP BY\nSELECT \n    status,\n    COUNT(*) AS count,\n    AVG(order_total) AS avg_total\nFROM orders\nGROUP BY status;\n\n-- HAVING clause\nSELECT \n    user_id,\n    COUNT(*) AS order_count\nFROM orders\nGROUP BY user_id\nHAVING COUNT(*) > 5;"
    },
    {
      "id": 72,
      "question": "What is the difference between WHERE and HAVING?",
      "answer": "WHERE vs HAVING: Different filtering stages\n\nWHERE Clause:\n• Filters individual rows\n• Before aggregation\n• Cannot use aggregate functions\n• Processed first\n• Filters FROM/JOIN results\n\nHAVING Clause:\n• Filters grouped results\n• After aggregation\n• Can use aggregate functions\n• Processed after GROUP BY\n• Filters aggregate results\n\nWhen to Use:\n• WHERE: Row-level filtering\n• HAVING: Group-level filtering\n• Can use both together\n\nPerformance:\n• WHERE is faster (fewer rows)\n• Filter early with WHERE\n• Use HAVING only for aggregates",
      "explanation": "WHERE filters rows before grouping; HAVING filters grouped results after aggregation. Use WHERE for row filtering and HAVING for aggregate filtering.",
      "difficulty": "Easy",
      "code": "-- WHERE: Filter rows before aggregation\nSELECT \n    department,\n    COUNT(*) AS employee_count\nFROM employees\nWHERE salary > 50000  -- Row-level filter\nGROUP BY department;\n\n-- HAVING: Filter groups after aggregation\nSELECT \n    department,\n    COUNT(*) AS employee_count\nFROM employees\nGROUP BY department\nHAVING COUNT(*) > 10;  -- Group-level filter\n\n-- Both together\nSELECT \n    department,\n    AVG(salary) AS avg_salary\nFROM employees\nWHERE status = 'ACTIVE'  -- First: Filter active employees\nGROUP BY department\nHAVING AVG(salary) > 60000;  -- Then: Filter departments"
    },
    {
      "id": 73,
      "question": "What are subqueries and their types?",
      "answer": "Subqueries: Queries within queries\n\nTypes:\n\nScalar Subquery:\n• Returns single value\n• Used in SELECT, WHERE\n• Can be in expressions\n\nRow Subquery:\n• Returns single row\n• Multiple columns\n• Used with IN, =\n\nTable Subquery:\n• Returns multiple rows/columns\n• Used in FROM clause\n• Derived tables\n\nCorrelated Subquery:\n• References outer query\n• Executes per outer row\n• Slower performance\n\nNon-Correlated:\n• Independent execution\n• Executes once\n• Better performance\n\nAlternatives:\n• JOINs (usually faster)\n• CTEs (better readability)\n• Window functions",
      "explanation": "Subqueries are queries nested within others, including scalar (single value), row (single row), table (multiple rows), correlated (references outer query), and non-correlated types.",
      "difficulty": "Medium",
      "code": "-- Scalar subquery\nSELECT \n    name,\n    salary,\n    salary - (SELECT AVG(salary) FROM employees) AS diff_from_avg\nFROM employees;\n\n-- Subquery in WHERE\nSELECT * FROM employees\nWHERE salary > (SELECT AVG(salary) FROM employees);\n\n-- IN subquery\nSELECT * FROM users\nWHERE id IN (SELECT user_id FROM orders WHERE total > 1000);\n\n-- Correlated subquery\nSELECT \n    e.name,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = e.id) AS order_count\nFROM employees e;\n\n-- EXISTS\nSELECT * FROM users u\nWHERE EXISTS (\n    SELECT 1 FROM orders o WHERE o.user_id = u.id\n);"
    },
    {
      "id": 74,
      "question": "What is database normalization?",
      "answer": "Normalization: Organize data to reduce redundancy\n\nGoals:\n• Eliminate redundancy\n• Ensure data integrity\n• Reduce anomalies\n• Optimize structure\n\nNormal Forms:\n\n1NF: Atomic values, no repeating groups\n2NF: 1NF + no partial dependencies\n3NF: 2NF + no transitive dependencies\nBCNF: Stricter than 3NF\n4NF: No multi-valued dependencies\n5NF: No join dependencies\n\nBenefits:\n• Data consistency\n• Easier updates\n• Less storage (sometimes)\n• Better integrity\n\nTrade-offs:\n• More tables\n• More JOINs\n• Slower queries\n• Complex queries\n\nWhen to Denormalize:\n• Read-heavy workload\n• Performance critical\n• Reporting/analytics",
      "explanation": "Normalization reduces data redundancy through normal forms (1NF-5NF), improving integrity but potentially requiring more JOINs. Denormalize strategically for performance.",
      "difficulty": "Medium",
      "code": "-- Unnormalized (1NF violation)\nCREATE TABLE orders_bad (\n    id INT,\n    customer_name VARCHAR(100),\n    items VARCHAR(500)  -- Multiple items in one field!\n);\n\n-- 1NF: Atomic values\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    customer_id INT\n);\n\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT\n);\n\n-- 2NF: Remove partial dependencies\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    customer_id INT,\n    order_date DATE\n);\n\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    PRIMARY KEY (order_id, product_id)\n);\n\n-- 3NF: Remove transitive dependencies\nCREATE TABLE customers (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    city_id INT\n);\n\nCREATE TABLE cities (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    state VARCHAR(50)\n);"
    },
    {
      "id": 75,
      "question": "What are UNION, INTERSECT, and EXCEPT operations?",
      "answer": "Set Operations: Combine query results\n\nUNION:\n• Combines results from multiple queries\n• Removes duplicates (UNION ALL keeps)\n• Same number of columns\n• Compatible data types\n\nINTERSECT:\n• Returns common rows\n• Only matching records\n• Both queries must return\n\nEXCEPT (MINUS):\n• Returns rows from first query\n• Not in second query\n• Set difference\n\nRequirements:\n• Same column count\n• Compatible types\n• Column order matters\n\nPerformance:\n• UNION ALL faster (no dedup)\n• Use UNION ALL when possible\n• Indexes help\n• Consider alternatives (JOIN)",
      "explanation": "Set operations combine query results: UNION merges all rows, INTERSECT finds common rows, and EXCEPT returns rows in first query but not second.",
      "difficulty": "Medium",
      "code": "-- UNION: Combine results, remove duplicates\nSELECT name, email FROM customers\nUNION\nSELECT name, email FROM suppliers;\n\n-- UNION ALL: Keep duplicates (faster)\nSELECT name FROM customers WHERE city = 'NYC'\nUNION ALL\nSELECT name FROM suppliers WHERE city = 'NYC';\n\n-- INTERSECT: Common rows\nSELECT email FROM customers\nINTERSECT\nSELECT email FROM employees;\n\n-- EXCEPT: In first, not in second\nSELECT email FROM customers\nEXCEPT\nSELECT email FROM unsubscribed;\n\n-- Multiple UNIONs\nSELECT 'Customer' AS type, name FROM customers\nUNION ALL\nSELECT 'Supplier', name FROM suppliers\nUNION ALL\nSELECT 'Employee', name FROM employees\nORDER BY name;"
    },
    {
      "id": 76,
      "question": "What are database sequences and auto-increment?",
      "answer": "Sequences: Generate unique numbers\n\nAUTO_INCREMENT (MySQL):\n• Column attribute\n• Per-table counter\n• Automatic assignment\n• Starts at 1 (default)\n• Integer types only\n\nSEQUENCE (PostgreSQL/Oracle):\n• Database object\n• Shared across tables\n• More flexible\n• Custom increment\n• Can be cached\n\nIDENTITY (SQL Server):\n• Column property\n• SEED and INCREMENT\n• Per-table\n• Reseedable\n\nBest Practices:\n• Use for primary keys\n• Don't reuse values\n• Consider UUIDs for distributed\n• Plan for scale\n• Monitor gaps",
      "explanation": "Sequences generate unique numbers for primary keys using AUTO_INCREMENT (MySQL), SEQUENCE (PostgreSQL/Oracle), or IDENTITY (SQL Server) with database-specific features.",
      "difficulty": "Easy",
      "code": "-- MySQL AUTO_INCREMENT\nCREATE TABLE users (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nINSERT INTO users (name) VALUES ('John');\nSELECT LAST_INSERT_ID();  -- Get last ID\n\n-- PostgreSQL SERIAL (sequence shorthand)\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100)\n);\n\n-- PostgreSQL explicit SEQUENCE\nCREATE SEQUENCE user_id_seq START 1001 INCREMENT 1;\n\nCREATE TABLE users (\n    id INT DEFAULT nextval('user_id_seq') PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nSELECT nextval('user_id_seq');  -- Get next value\nSELECT currval('user_id_seq');  -- Get current value\n\n-- SQL Server IDENTITY\nCREATE TABLE users (\n    id INT IDENTITY(1,1) PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nINSERT INTO users (name) VALUES ('John');\nSELECT SCOPE_IDENTITY();  -- Get last ID"
    },
    {
      "id": 77,
      "question": "What are database data types and their selection?",
      "answer": "Data Types: Define column storage format\n\nNumeric:\n• INT: Integers (-2B to 2B)\n• BIGINT: Large integers\n• DECIMAL(p,s): Exact decimals\n• FLOAT/DOUBLE: Approximate decimals\n\nString:\n• CHAR(n): Fixed length\n• VARCHAR(n): Variable length\n• TEXT: Large text\n• ENUM: Predefined values\n\nDate/Time:\n• DATE: Date only\n• TIME: Time only\n• DATETIME: Date + time\n• TIMESTAMP: Unix timestamp\n\nBinary:\n• BLOB: Binary data\n• VARBINARY: Variable binary\n\nJSON:\n• JSON: JSON documents\n• JSONB (PostgreSQL): Binary JSON\n\nSelection Criteria:\n• Storage size\n• Query performance\n• Data range\n• Precision needs\n• Future growth",
      "explanation": "Choose appropriate data types based on data characteristics, storage efficiency, and query performance: numeric, string, date/time, binary, and JSON types.",
      "difficulty": "Easy",
      "code": "-- Numeric types\nCREATE TABLE products (\n    id INT,\n    price DECIMAL(10,2),  -- Exact: $99999999.99\n    weight FLOAT,         -- Approximate\n    stock SMALLINT,       -- -32768 to 32767\n    views BIGINT          -- Large numbers\n);\n\n-- String types\nCREATE TABLE users (\n    id INT,\n    country_code CHAR(2),        -- Fixed: 'US'\n    name VARCHAR(100),            -- Variable length\n    bio TEXT,                     -- Large text\n    status ENUM('ACTIVE', 'INACTIVE')  -- Predefined\n);\n\n-- Date/Time types\nCREATE TABLE events (\n    id INT,\n    event_date DATE,              -- 2024-01-15\n    event_time TIME,              -- 14:30:00\n    created_at DATETIME,          -- 2024-01-15 14:30:00\n    updated_at TIMESTAMP          -- Auto-updates\n      DEFAULT CURRENT_TIMESTAMP\n      ON UPDATE CURRENT_TIMESTAMP\n);\n\n-- JSON type\nCREATE TABLE settings (\n    user_id INT,\n    preferences JSON\n);\n\nINSERT INTO settings VALUES (\n    1,\n    '{\"theme\": \"dark\", \"language\": \"en\"}'\n);\n\nSELECT JSON_EXTRACT(preferences, '$.theme') FROM settings;"
    },
    {
      "id": 78,
      "question": "What are database NULL values and handling?",
      "answer": "NULL: Absence of value (not zero/empty string)\n\nCharacteristics:\n• Represents unknown\n• Different from zero\n• Different from empty string\n• NULL = NULL is NULL (not TRUE!)\n• Use IS NULL / IS NOT NULL\n\nNULL in Operations:\n• Arithmetic: NULL + 5 = NULL\n• Comparison: NULL = NULL → NULL\n• Aggregates: Ignores NULLs\n• COUNT(*) includes NULLs\n• COUNT(column) excludes NULLs\n\nHandling:\n• COALESCE(): First non-NULL\n• IFNULL()/ISNULL(): Replace NULL\n• NULLIF(): Return NULL if equal\n• IS NULL / IS NOT NULL\n\nBest Practices:\n• Use NOT NULL when possible\n• Default values instead\n• Explicit IS NULL checks\n• Consider tri-state logic\n• Document NULL meaning",
      "explanation": "NULL represents unknown/missing values with special comparison rules. Handle with IS NULL/IS NOT NULL operators and functions like COALESCE, IFNULL.",
      "difficulty": "Medium",
      "code": "-- NULL comparisons (tricky!)\nSELECT * FROM users WHERE email = NULL;     -- Wrong! Returns nothing\nSELECT * FROM users WHERE email IS NULL;    -- Correct\n\nSELECT * FROM users WHERE email != NULL;    -- Wrong!\nSELECT * FROM users WHERE email IS NOT NULL; -- Correct\n\n-- NULL in arithmetic\nSELECT 5 + NULL;        -- Result: NULL\nSELECT 5 * NULL;        -- Result: NULL\n\n-- NULL in logic\nSELECT NULL = NULL;     -- Result: NULL (not TRUE!)\nSELECT NULL AND TRUE;   -- Result: NULL\nSELECT NULL OR TRUE;    -- Result: TRUE\n\n-- COALESCE: First non-NULL value\nSELECT \n    name,\n    COALESCE(phone, email, 'No contact') AS contact\nFROM users;\n\n-- IFNULL: Replace NULL with value\nSELECT \n    name,\n    IFNULL(middle_name, '') AS middle_name\nFROM users;\n\n-- NULLIF: Return NULL if equal\nSELECT \n    name,\n    NULLIF(status, 'UNKNOWN') AS status  -- NULL if status is 'UNKNOWN'\nFROM users;\n\n-- Aggregates and NULL\nSELECT \n    COUNT(*) AS total_rows,          -- Includes NULLs\n    COUNT(email) AS emails_count,    -- Excludes NULLs\n    AVG(age) AS avg_age              -- Ignores NULLs\nFROM users;\n\n-- Default value vs NULL\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    description VARCHAR(500) DEFAULT '',  -- Empty string, not NULL\n    stock INT DEFAULT 0,                  -- Zero, not NULL\n    updated_at TIMESTAMP NULL             -- Allow NULL\n);"
    },
    {
      "id": 79,
      "question": "What are database collations and character sets?",
      "answer": "Character Set: Encoding for characters\nCollation: Sorting and comparison rules\n\nCharacter Sets:\n• UTF8: Unicode (up to 3 bytes)\n• UTF8MB4: Full Unicode (4 bytes, emojis)\n• Latin1: Western European\n• ASCII: English only\n\nCollations:\n• Case sensitivity\n• Accent sensitivity\n• Sorting rules\n• Language-specific\n\nCommon Collations:\n• utf8mb4_unicode_ci: Case-insensitive, Unicode\n• utf8mb4_bin: Binary (case-sensitive)\n• utf8mb4_general_ci: Fast, less accurate\n\nImpact:\n• String comparisons\n• Sorting behavior\n• Index performance\n• Storage size\n• Query results\n\nBest Practices:\n• Use UTF8MB4 for new apps\n• Match collation in comparisons\n• Consider case sensitivity needs\n• Set at database/table/column level",
      "explanation": "Character sets define character encoding (use UTF8MB4); collations define comparison/sorting rules (case-sensitivity, language-specific). Both affect query behavior.",
      "difficulty": "Medium",
      "code": "-- Set character set (MySQL)\nCREATE DATABASE myapp\nCHARACTER SET utf8mb4\nCOLLATE utf8mb4_unicode_ci;\n\n-- Table-level\nCREATE TABLE users (\n    id INT,\n    name VARCHAR(100)\n) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n\n-- Column-level\nCREATE TABLE users (\n    id INT,\n    email VARCHAR(255) COLLATE utf8mb4_bin,  -- Case-sensitive\n    name VARCHAR(100) COLLATE utf8mb4_unicode_ci  -- Case-insensitive\n);\n\n-- Collation in queries\nSELECT * FROM users\nWHERE email = 'test@example.com' COLLATE utf8mb4_bin;\n-- Exact match (case-sensitive)\n\nSELECT * FROM users\nWHERE name = 'john' COLLATE utf8mb4_unicode_ci;\n-- Matches 'John', 'JOHN', 'john'\n\n-- View character sets\nSHOW CHARACTER SET;\nSHOW COLLATION;\n\n-- Check table collation\nSHOW TABLE STATUS LIKE 'users';\n\n-- Convert existing table\nALTER TABLE users\nCONVERT TO CHARACTER SET utf8mb4\nCOLLATE utf8mb4_unicode_ci;\n\n-- PostgreSQL (uses locale)\nCREATE DATABASE myapp\nENCODING 'UTF8'\nLC_COLLATE = 'en_US.UTF-8'\nLC_CTYPE = 'en_US.UTF-8';"
    },
    {
      "id": 80,
      "question": "What are database transactions logs and WAL?",
      "answer": "Transaction Log: Record of all database changes\nWAL: Write-Ahead Logging\n\nPurpose:\n• Crash recovery\n• Point-in-time recovery\n• Replication\n• Audit trail\n• Durability (ACID)\n\nWrite-Ahead Logging:\n• Log changes before data\n• Sequential writes (fast)\n• Ensures durability\n• Recovery possible\n• All major databases use it\n\nComponents:\n• Log sequence number (LSN)\n• Transaction ID\n• Before/after values\n• Operation type\n• Timestamp\n\nOperations:\n• Checkpoint: Flush to disk\n• Log rotation: Archive old logs\n• Truncate: Remove old logs\n• Replay: Recovery process\n\nBest Practices:\n• Monitor log size\n• Regular backups\n• Proper retention\n• Fast storage for logs\n• Archive to separate disk",
      "explanation": "Transaction logs record all changes for crash recovery, replication, and PITR. WAL ensures durability by writing logs before data modifications.",
      "difficulty": "Hard",
      "code": "-- MySQL Binary Log (transaction log)\n-- Enable in my.cnf\n[mysqld]\nlog_bin = /var/log/mysql/mysql-bin.log\nbinlog_format = ROW\nexpire_logs_days = 7\nmax_binlog_size = 100M\n\n-- View binary logs\nSHOW BINARY LOGS;\nSHOW MASTER STATUS;\n\n-- Show log contents\nSHOW BINLOG EVENTS IN 'mysql-bin.000003';\n\n-- Purge old logs\nPURGE BINARY LOGS BEFORE '2024-01-01 00:00:00';\nPURGE BINARY LOGS TO 'mysql-bin.000010';\n\n-- PostgreSQL WAL\n-- Configuration in postgresql.conf\nwal_level = replica\nmax_wal_size = 1GB\nmin_wal_size = 80MB\narchive_mode = on\narchive_command = 'cp %p /archive/%f'\n\n-- View WAL status\nSELECT * FROM pg_stat_wal;\n\n-- Current WAL location\nSELECT pg_current_wal_lsn();\n\n-- WAL file list\nSELECT * FROM pg_ls_waldir();\n\n-- Checkpoint\nCHECKPOINT;\n\n-- View checkpoint info\nSELECT \n    checkpoint_lsn,\n    redo_lsn,\n    checkpoint_time\nFROM pg_control_checkpoint();\n\n-- SQL Server Transaction Log\n-- View log info\nDBCC SQLPERF(LOGSPACE);\n\n-- View log details\nSELECT \n    name,\n    type_desc,\n    size * 8/1024 AS size_mb,\n    max_size\nFROM sys.master_files\nWHERE type_desc = 'LOG';\n\n-- Shrink log (after backup)\nUSE myapp;\nBACKUP LOG myapp TO DISK = 'C:\\backup\\myapp_log.bak';\nDBCC SHRINKFILE (myapp_log, 1024);  -- Shrink to 1GB\n\n-- Log monitoring script\n#!/bin/bash\n# Check MySQL binary log size\nLOG_DIR=\"/var/log/mysql\"\nTOTAL_SIZE=$(du -sh $LOG_DIR | cut -f1)\n\necho \"Binary log size: $TOTAL_SIZE\"\n\nif [ $(du -sb $LOG_DIR | cut -f1) -gt 10737418240 ]; then  # 10GB\n    echo \"WARNING: Binary logs exceed 10GB\"\n    # Alert or auto-purge\nfi\n\n-- Log-based replication\n-- Master configuration\nCREATE USER 'repl'@'%' IDENTIFIED BY 'password';\nGRANT REPLICATION SLAVE ON *.* TO 'repl'@'%';\n\nSHOW MASTER STATUS;\n-- Record File and Position\n\n-- Slave reads binary log from master\nCHANGE MASTER TO\n    MASTER_HOST='master-server',\n    MASTER_USER='repl',\n    MASTER_PASSWORD='password',\n    MASTER_LOG_FILE='mysql-bin.000003',\n    MASTER_LOG_POS=154;\n\nSTART SLAVE;"
    },
    {
      "id": 81,
      "question": "What are database query hints and optimizer directives?",
      "answer": "Query Hints: Guide query optimizer\n\nTypes:\n\nIndex Hints:\n• Force index usage\n• Ignore index\n• Suggest index\n\nJoin Hints:\n• Join method (LOOP/HASH/MERGE)\n• Join order\n• Force specific algorithm\n\nExecution Hints:\n• Parallel execution\n• Result set size\n• Timeout\n• Memory grants\n\nWhen to Use:\n• Optimizer chooses poorly\n• Testing performance\n• Temporary workaround\n• Known better plan\n\nCautions:\n• Override optimizer intelligence\n• May break with data changes\n• Hard to maintain\n• Update statistics instead\n• Last resort\n\nBest Practice:\n• Document why used\n• Review periodically\n• Remove when unnecessary\n• Prefer optimizer fixes",
      "explanation": "Query hints override optimizer decisions for index selection, join methods, and execution strategies. Use sparingly as last resort when optimizer fails.",
      "difficulty": "Hard",
      "code": "-- MySQL Index hints\nSELECT * FROM users\nUSE INDEX (idx_email)\nWHERE email = 'test@example.com';\n\nSELECT * FROM users\nFORCE INDEX (idx_created)\nWHERE created_at > '2024-01-01';\n\nSELECT * FROM users\nIGNORE INDEX (idx_name)\nWHERE name LIKE 'John%';\n\n-- Join order hint\nSELECT STRAIGHT_JOIN u.name, o.total\nFROM users u\nJOIN orders o ON u.id = o.user_id;\n-- Forces join in this order\n\n-- Timeout hint\nSELECT /*+ MAX_EXECUTION_TIME(5000) */ *\nFROM large_table\nWHERE complex_condition;\n-- Kill if takes > 5 seconds\n\n-- PostgreSQL hints (via pg_hint_plan extension)\n/*+ SeqScan(users) */\nSELECT * FROM users WHERE name = 'John';\n-- Force sequential scan\n\n/*+ IndexScan(users idx_email) */\nSELECT * FROM users WHERE email = 'test@example.com';\n-- Force index scan\n\n/*+ HashJoin(users orders) */\nSELECT * FROM users u\nJOIN orders o ON u.id = o.user_id;\n-- Force hash join\n\n-- SQL Server query hints\nSELECT * FROM users WITH (INDEX(idx_email))\nWHERE email = 'test@example.com';\n\nSELECT * FROM users WITH (NOLOCK)\nWHERE status = 'ACTIVE';\n-- Read uncommitted (dirty reads)\n\nSELECT * FROM users\nWHERE name = 'John'\nOPTION (RECOMPILE);\n-- Recompile every time\n\n-- Parallel execution hint\nSELECT /*+ PARALLEL(users, 4) */ *\nFROM users\nWHERE created_at > '2024-01-01';\n-- Use 4 parallel workers"
    },
    {
      "id": 82,
      "question": "What are database table partitioning strategies?",
      "answer": "Partitioning: Split table into smaller pieces\n\nTypes:\n\nRange Partitioning:\n• By value range\n• Date ranges (common)\n• Numeric ranges\n• Easy to manage\n\nList Partitioning:\n• By discrete values\n• Geographic regions\n• Categories\n• Specific assignments\n\nHash Partitioning:\n• By hash function\n• Even distribution\n• Random assignment\n• Load balancing\n\nKey Partitioning:\n• MySQL-specific\n• By primary key\n• Automatic distribution\n\nComposite:\n• Combine multiple types\n• Range + hash\n• More complex\n\nBenefits:\n• Query performance\n• Partition pruning\n• Easier maintenance\n• Parallel operations\n• Archive old data\n\nConsiderations:\n• Plan partitioning key\n• Partition count\n• Maintenance overhead\n• Query patterns",
      "explanation": "Partitioning splits tables by range, list, hash, or key to improve query performance through partition pruning and enable efficient data management.",
      "difficulty": "Hard",
      "code": "-- Range partitioning by date (MySQL)\nCREATE TABLE orders (\n    id INT,\n    user_id INT,\n    total DECIMAL(10,2),\n    order_date DATE\n) PARTITION BY RANGE (YEAR(order_date)) (\n    PARTITION p2022 VALUES LESS THAN (2023),\n    PARTITION p2023 VALUES LESS THAN (2024),\n    PARTITION p2024 VALUES LESS THAN (2025),\n    PARTITION pfuture VALUES LESS THAN MAXVALUE\n);\n\n-- Hash partitioning\nCREATE TABLE users (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(255)\n) PARTITION BY HASH(id) PARTITIONS 4;\n\n-- List partitioning\nCREATE TABLE sales (\n    id INT,\n    region VARCHAR(50),\n    amount DECIMAL(10,2)\n) PARTITION BY LIST COLUMNS(region) (\n    PARTITION pNorth VALUES IN ('NY', 'MA', 'CT'),\n    PARTITION pSouth VALUES IN ('FL', 'GA', 'TX'),\n    PARTITION pWest VALUES IN ('CA', 'OR', 'WA')\n);\n\n-- PostgreSQL range partitioning\nCREATE TABLE orders (\n    id SERIAL,\n    order_date DATE,\n    total DECIMAL(10,2)\n) PARTITION BY RANGE (order_date);\n\nCREATE TABLE orders_2024_q1 PARTITION OF orders\n    FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');\n\nCREATE TABLE orders_2024_q2 PARTITION OF orders\n    FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');\n\n-- View partitions\nSELECT \n    table_name,\n    partition_name,\n    partition_expression,\n    partition_description\nFROM information_schema.partitions\nWHERE table_name = 'orders';\n\n-- Add new partition\nALTER TABLE orders\nADD PARTITION (\n    PARTITION p2025 VALUES LESS THAN (2026)\n);\n\n-- Drop old partition\nALTER TABLE orders\nDROP PARTITION p2022;\n\n-- Query with partition pruning\nEXPLAIN SELECT * FROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-12-31';\n-- Only scans p2024 partition\n\n-- Composite partitioning\nCREATE TABLE logs (\n    id BIGINT,\n    created_at TIMESTAMP,\n    message TEXT\n) PARTITION BY RANGE (YEAR(created_at))\nSUBPARTITION BY HASH(id) SUBPARTITIONS 4 (\n    PARTITION p2024 VALUES LESS THAN (2025),\n    PARTITION p2025 VALUES LESS THAN (2026)\n);"
    },
    {
      "id": 83,
      "question": "What are database user-defined functions (UDFs)?",
      "answer": "UDFs: Custom functions in database\n\nTypes:\n\nScalar Functions:\n• Return single value\n• Used in expressions\n• Per-row calculation\n• Like built-in functions\n\nTable-Valued Functions:\n• Return table\n• Used in FROM clause\n• Like views\n• Parameterized\n\nAggregate Functions:\n• Operate on groups\n• Custom aggregations\n• Like SUM, COUNT\n\nBenefits:\n• Reusable logic\n• Encapsulation\n• Performance (precompiled)\n• Complex calculations\n• Domain logic in DB\n\nConsiderations:\n• Database-specific syntax\n• Performance impact\n• Maintenance complexity\n• Portability issues\n• Testing challenges\n\nBest Practices:\n• Keep simple\n• Document well\n• Test thoroughly\n• Consider application layer\n• Version control",
      "explanation": "UDFs create custom functions in the database for calculations, transformations, or complex logic, available as scalar (single value) or table-valued (result set) functions.",
      "difficulty": "Medium",
      "code": "-- MySQL scalar function\nDELIMITER //\n\nCREATE FUNCTION calculate_tax(amount DECIMAL(10,2))\nRETURNS DECIMAL(10,2)\nDETERMINISTIC\nBEGIN\n    DECLARE tax DECIMAL(10,2);\n    SET tax = amount * 0.10;\n    RETURN tax;\nEND//\n\nDELIMITER ;\n\n-- Usage\nSELECT \n    product_name,\n    price,\n    calculate_tax(price) AS tax,\n    price + calculate_tax(price) AS total\nFROM products;\n\n-- Drop function\nDROP FUNCTION IF EXISTS calculate_tax;\n\n-- PostgreSQL function\nCREATE OR REPLACE FUNCTION get_full_name(\n    first VARCHAR,\n    last VARCHAR\n)\nRETURNS VARCHAR AS $$\nBEGIN\n    RETURN first || ' ' || last;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage\nSELECT get_full_name(first_name, last_name) FROM users;\n\n-- Table-valued function (SQL Server)\nCREATE FUNCTION GetUserOrders(@userId INT)\nRETURNS TABLE\nAS\nRETURN (\n    SELECT \n        id,\n        order_date,\n        total\n    FROM orders\n    WHERE user_id = @userId\n);\n\n-- Usage\nSELECT * FROM GetUserOrders(123);\n\n-- Complex function\nDELIMITER //\n\nCREATE FUNCTION calculate_discount(\n    amount DECIMAL(10,2),\n    customer_type VARCHAR(20)\n)\nRETURNS DECIMAL(10,2)\nDETERMINISTIC\nBEGIN\n    DECLARE discount DECIMAL(10,2);\n    \n    IF customer_type = 'VIP' THEN\n        SET discount = amount * 0.20;\n    ELSEIF customer_type = 'REGULAR' THEN\n        SET discount = amount * 0.10;\n    ELSE\n        SET discount = 0;\n    END IF;\n    \n    RETURN discount;\nEND//\n\nDELIMITER ;\n\n-- Function with error handling\nCREATE OR REPLACE FUNCTION safe_divide(\n    numerator NUMERIC,\n    denominator NUMERIC\n)\nRETURNS NUMERIC AS $$\nBEGIN\n    IF denominator = 0 THEN\n        RETURN NULL;\n    END IF;\n    RETURN numerator / denominator;\nEXCEPTION\n    WHEN OTHERS THEN\n        RETURN NULL;\nEND;\n$$ LANGUAGE plpgsql;"
    },
    {
      "id": 84,
      "question": "What are database constraints validation and deferred constraints?",
      "answer": "Constraint Validation: When and how constraints are checked\n\nValidation Timing:\n\nImmediate:\n• Check on each statement\n• Default behavior\n• Fail fast\n• Most common\n\nDeferred:\n• Check at  COMMIT\n• Allow temporary violations\n• Complex operations\n• Circular references\n\nConstraint Types:\n• NOT NULL: Per statement\n• UNIQUE: Immediate/deferred\n• PRIMARY KEY: Immediate/deferred\n• FOREIGN KEY: Immediate/deferred\n• CHECK: Immediate/deferred\n\nUse Cases for Deferred:\n• Circular FKs\n• Bulk operations\n• Data migrations\n• Complex transactions\n• Temporary violations\n\nBest Practices:\n• Use immediate by default\n• Defer only when needed\n• Document deferred constraints\n• Test thoroughly\n• Consider performance impact",
      "explanation": "Constraints are validated immediately (per statement) or deferred (at commit), allowing temporary violations for complex operations like circular foreign keys.",
      "difficulty": "Hard",
      "code": "-- PostgreSQL deferred constraints\n\n-- Create table with deferrable FK\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    manager_id INT,\n    FOREIGN KEY (manager_id) \n        REFERENCES employees(id)\n        DEFERRABLE INITIALLY DEFERRED\n);\n\n-- Insert with circular reference\nBEGIN;\n\nSET CONSTRAINTS ALL DEFERRED;\n\nINSERT INTO employees VALUES (1, 'Alice', 2);\nINSERT INTO employees VALUES (2, 'Bob', 1);\n-- Would fail with immediate constraint\n\nCOMMIT;\n-- Validates at commit\n\n-- Defer specific constraint\nBEGIN;\n\nSET CONSTRAINTS fk_manager DEFERRED;\n\n-- ... operations ...\n\nCOMMIT;\n\n-- Check immediate (default)\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    user_id INT NOT NULL,\n    FOREIGN KEY (user_id) \n        REFERENCES users(id)\n        -- Immediate by default\n);\n\n-- This fails immediately\nINSERT INTO orders VALUES (1, 999);  -- User 999 doesn't exist\n\n-- Bulk operation with deferred\nBEGIN;\n\nSET CONSTRAINTS ALL DEFERRED;\n\n-- Swap primary keys\nUPDATE orders SET id = id + 10000 WHERE id IN (1, 2);\nUPDATE orders SET id = 1 WHERE id = 10002;\nUPDATE orders SET id = 2 WHERE id = 10001;\n-- Temporary PK violations allowed\n\nCOMMIT;\n\n-- Check constraint (deferrable)\nCREATE TABLE accounts (\n    id INT PRIMARY KEY,\n    balance DECIMAL(10,2),\n    CONSTRAINT chk_balance \n        CHECK (balance >= 0)\n        DEFERRABLE INITIALLY IMMEDIATE\n);\n\nBEGIN;\n\nSET CONSTRAINTS chk_balance DEFERRED;\n\nUPDATE accounts SET balance = -100 WHERE id = 1;  -- Temporary negative\nUPDATE accounts SET balance = 100 WHERE id = 2;\nUPDATE accounts SET balance = 0 WHERE id = 1;     -- Fix before commit\n\nCOMMIT;\n\n-- View constraint info\nSELECT \n    conname AS constraint_name,\n    contype AS constraint_type,\n    condeferrable AS deferrable,\n    condeferred AS initially_deferred\nFROM pg_constraint\nWHERE conrelid = 'employees'::regclass;"
    },
    {
      "id": 85,
      "question": "What are database query performance profiling tools?",
      "answer": "Profiling Tools: Analyze query performance\n\nBuilt-in Tools:\n\nEXPLAIN:\n• Show execution plan\n• Estimated costs\n• Index usage\n• Join methods\n\nEXPLAIN ANALYZE:\n• Actual execution\n• Real timing\n• Actual rows\n• Performance breakdown\n\nSlow Query Log:\n• Log slow queries\n• Threshold-based\n• Automatic tracking\n• Production monitoring\n\nPerformance Schema (MySQL):\n• Detailed metrics\n• Statement statistics\n• Table I/O\n• Lock waits\n\npg_stat_statements (PostgreSQL):\n• Query statistics\n• Total/mean time\n• Call counts\n• Buffer usage\n\nExternal Tools:\n• New Relic, DataDog\n• pgBadger, pt-query-digest\n• Visual explain tools\n• APM solutions\n\nMetrics to Track:\n• Execution time\n• Rows examined vs returned\n• Index usage\n• Temp table usage\n• Sort operations\n• Lock waits",
      "explanation": "Profiling tools like EXPLAIN, slow query logs, Performance Schema, and pg_stat_statements help analyze query performance and identify bottlenecks.",
      "difficulty": "Hard",
      "code": "-- EXPLAIN (MySQL)\nEXPLAIN SELECT u.name, COUNT(o.id)\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01'\nGROUP BY u.id;\n\n-- EXPLAIN with FORMAT=JSON\nEXPLAIN FORMAT=JSON\nSELECT * FROM orders WHERE user_id = 123;\n\n-- EXPLAIN ANALYZE (PostgreSQL)\nEXPLAIN (ANALYZE, BUFFERS)\nSELECT * FROM orders\nWHERE user_id = 123 AND status = 'COMPLETED';\n\n-- Enable slow query log (MySQL)\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 2;  -- Queries > 2 seconds\nSET GLOBAL log_queries_not_using_indexes = 'ON';\n\n-- Analyze slow query log with pt-query-digest\n-- pt-query-digest /var/log/mysql/slow.log\n\n-- Performance Schema (MySQL)\n-- Enable\nUPDATE performance_schema.setup_instruments\nSET ENABLED = 'YES', TIMED = 'YES'\nWHERE NAME LIKE '%statement/%';\n\nUPDATE performance_schema.setup_consumers\nSET ENABLED = 'YES'\nWHERE NAME LIKE '%events_statements_%';\n\n-- Query statistics\nSELECT \n    DIGEST_TEXT AS query,\n    COUNT_STAR AS exec_count,\n    AVG_TIMER_WAIT/1000000000 AS avg_time_ms,\n    SUM_ROWS_EXAMINED AS rows_examined,\n    SUM_ROWS_SENT AS rows_sent\nFROM performance_schema.events_statements_summary_by_digest\nORDER BY AVG_TIMER_WAIT DESC\nLIMIT 10;\n\n-- Table I/O statistics\nSELECT \n    object_name AS table_name,\n    count_read,\n    count_write,\n    count_fetch,\n    sum_timer_wait/1000000000 AS total_time_ms\nFROM performance_schema.table_io_waits_summary_by_table\nWHERE object_schema = 'myapp'\nORDER BY sum_timer_wait DESC\nLIMIT 10;\n\n-- pg_stat_statements (PostgreSQL)\n-- Install extension\nCREATE EXTENSION pg_stat_statements;\n\n-- Configuration (postgresql.conf)\nshared_preload_libraries = 'pg_stat_statements'\npg_stat_statements.track = all\n\n-- Query statistics\nSELECT \n    query,\n    calls,\n    total_exec_time,\n    mean_exec_time,\n    max_exec_time,\n    rows\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n-- Reset statistics\nSELECT pg_stat_statements_reset();\n\n-- Lock monitoring (MySQL)\nSELECT \n    r.trx_id AS waiting_trx,\n    r.trx_query AS waiting_query,\n    b.trx_id AS blocking_trx,\n    b.trx_query AS blocking_query,\n    TIMESTAMPDIFF(SECOND, r.trx_wait_started, NOW()) AS wait_seconds\nFROM information_schema.innodb_lock_waits w\nJOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id\nJOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id;\n\n-- Query profiling (MySQL)\nSET profiling = 1;\n\nSELECT * FROM orders WHERE user_id = 123;\n\nSHOW PROFILES;\nSHOW PROFILE FOR QUERY 1;\n\nSET profiling = 0;\n\n-- Auto-EXPLAIN (PostgreSQL)\n-- postgresql.conf\nshared_preload_libraries = 'auto_explain'\nauto_explain.log_min_duration = 1000  -- Log queries > 1s\nauto_explain.log_analyze = true\nauto_explain.log_buffers = true\n\n-- Plans automatically logged to server log"
    },
    {
      "id": 86,
      "question": "What are database prepared statements?",
      "answer": "Prepared Statements: Precompiled SQL templates\n\nBenefits:\n• SQL injection prevention\n• Performance (compiled once)\n• Parameter type safety\n• Cleaner code\n• Reusable execution\n\nHow They Work:\n1. Prepare: Parse and compile\n2. Bind: Attach parameters\n3. Execute: Run with values\n4. Reuse: Execute again with new values\n\nvs Regular Statements:\n• Regular: Parse every time\n• Prepared: Parse once\n• Regular: String concatenation (unsafe)\n• Prepared: Parameters separate (safe)\n\nPerformance:\n• First execution: Slightly slower (compile)\n• Repeated: Much faster (cached plan)\n• Best for repeated queries\n• Connection-scoped\n\nBest Practices:\n• Always use for user input\n• Reuse when possible\n• Close when done\n• Use connection pooling\n• Parameterize all values",
      "explanation": "Prepared statements precompile SQL with parameters, preventing SQL injection and improving performance for repeated queries through plan caching.",
      "difficulty": "Medium",
      "code": "// Node.js (mysql2)\nconst [rows] = await connection.execute(\n    'SELECT * FROM users WHERE email = ? AND status = ?',\n    ['test@example.com', 'ACTIVE']\n);  // Safe from SQL injection\n\n// Java JDBC\nString sql = \"SELECT * FROM users WHERE email = ? AND status = ?\";\nPreparedStatement pstmt = connection.prepareStatement(sql);\npstmt.setString(1, \"test@example.com\");\npstmt.setString(2, \"ACTIVE\");\nResultSet rs = pstmt.executeQuery();\n\nwhile (rs.next()) {\n    System.out.println(rs.getString(\"name\"));\n}\n\npstmt.close();\n\n// Reusable prepared statement\nPreparedStatement pstmt = connection.prepareStatement(\n    \"INSERT INTO logs (user_id, action, created_at) VALUES (?, ?, NOW())\"\n);\n\n// Execute multiple times\nfor (int userId : userIds) {\n    pstmt.setInt(1, userId);\n    pstmt.setString(2, \"LOGIN\");\n    pstmt.executeUpdate();\n}\n\npstmt.close();\n\n// Python (psycopg2)\ncursor.execute(\n    \"SELECT * FROM users WHERE email = %s AND status = %s\",\n    ('test@example.com', 'ACTIVE')\n)\n\n// C# ADO.NET\nstring sql = \"SELECT * FROM users WHERE email = @email AND status = @status\";\nusing (SqlCommand cmd = new SqlCommand(sql, connection))\n{\n    cmd.Parameters.AddWithValue(\"@email\", \"test@example.com\");\n    cmd.Parameters.AddWithValue(\"@status\", \"ACTIVE\");\n    \n    using (SqlDataReader reader = cmd.ExecuteReader())\n    {\n        while (reader.Read())\n        {\n            Console.WriteLine(reader[\"name\"]);\n        }\n    }\n}\n\n-- MySQL server-side prepared statements\nPREPARE stmt FROM 'SELECT * FROM users WHERE email = ? AND status = ?';\nSET @email = 'test@example.com';\nSET @status = 'ACTIVE';\nEXECUTE stmt USING @email, @status;\nDEALLOCATE PREPARE stmt;\n\n-- PostgreSQL prepared statements\nPREPARE get_user (TEXT, TEXT) AS\n    SELECT * FROM users WHERE email = $1 AND status = $2;\n\nEXECUTE get_user('test@example.com', 'ACTIVE');\n\nDEALLOCATE get_user;\n\n// Named parameters (better readability)\n// Node.js with named-placeholders\nconst [rows] = await connection.execute(\n    'SELECT * FROM users WHERE email = :email AND status = :status',\n    { email: 'test@example.com', status: 'ACTIVE' }\n);\n\n// Batch execution\nconst users = [\n    ['john@example.com', 'John', 'Doe'],\n    ['jane@example.com', 'Jane', 'Smith']\n];\n\nconst sql = 'INSERT INTO users (email, first_name, last_name) VALUES (?, ?, ?)';\nconst pstmt = await connection.prepare(sql);\n\nfor (const user of users) {\n    await pstmt.execute(user);\n}\n\nawait pstmt.close();"
    },
    {
      "id": 87,
      "question": "What are database temporary tables and table variables?",
      "answer": "Temporary Storage: Short-lived tables for intermediate results\n\nTemporary Tables:\n• Session or connection-scoped\n• Stored in temp database\n• Can have indexes\n• Statistics tracked\n• Auto-deleted on close\n• Multiple queries can use\n\nTable Variables:\n• Statement or batch-scoped\n• Memory-optimized\n• No statistics\n• Limited indexes\n• Faster for small datasets\n• SQL Server, PostgreSQL\n\nMemory Tables:\n• In-memory storage\n• Very fast access\n• Limited by RAM\n• No durability\n\nWhen to Use:\n\nTemp Tables:\n• Large intermediate results\n• Multiple query steps\n• Need indexes\n• Complex processing\n\nTable Variables:\n• Small datasets (< 100 rows)\n• Single batch\n• Simple operations\n• Less overhead\n\nBest Practices:\n• Clean up explicitly\n• Monitor disk usage\n• Use appropriate type\n• Consider CTEs first\n• Index strategically",
      "explanation": "Temporary tables store intermediate results within a session, while table variables are batch-scoped and memory-optimized. Choose based on data size and usage pattern.",
      "difficulty": "Medium",
      "code": "-- MySQL temporary table\nCREATE TEMPORARY TABLE temp_users (\n    id INT,\n    name VARCHAR(100),\n    order_count INT\n);\n\nINSERT INTO temp_users\nSELECT \n    u.id,\n    u.name,\n    COUNT(o.id)\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n\n-- Use temporary table\nSELECT * FROM temp_users WHERE order_count > 5;\n\n-- Auto-dropped at session end, or explicitly:\nDROP TEMPORARY TABLE IF EXISTS temp_users;\n\n-- SQL Server table variable\nDECLARE @TempUsers TABLE (\n    id INT,\n    name VARCHAR(100),\n    order_count INT,\n    PRIMARY KEY (id)\n);\n\nINSERT INTO @TempUsers\nSELECT \n    u.id,\n    u.name,\n    COUNT(o.id)\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n\nSELECT * FROM @TempUsers WHERE order_count > 5;\n-- Automatically dropped at batch end\n\n-- PostgreSQL temporary table\nCREATE TEMP TABLE temp_summary AS\nSELECT \n    DATE(order_date) AS day,\n    COUNT(*) AS order_count,\n    SUM(total) AS revenue\nFROM orders\nWHERE order_date >= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY DATE(order_date);\n\n-- Add index\nCREATE INDEX idx_temp_day ON temp_summary(day);\n\n-- Use it\nSELECT * FROM temp_summary WHERE revenue > 1000;\n\nDROP TABLE temp_summary;\n\n-- Global temporary table (SQL Server)\nCREATE TABLE ##GlobalTemp (\n    id INT,\n    data VARCHAR(100)\n);\n-- Visible to all sessions\n-- Dropped when last session disconnects\n\n-- Memory table (MySQL)\nCREATE TEMPORARY TABLE temp_cache (\n    id INT PRIMARY KEY,\n    value VARCHAR(255)\n) ENGINE=MEMORY;\n-- Fast but limited by max_heap_table_size\n\n-- Complex processing example\nCREATE TEMPORARY TABLE top_customers AS\nSELECT \n    user_id,\n    SUM(total) AS lifetime_value\nFROM orders\nGROUP BY user_id\nHAVING SUM(total) > 10000;\n\nCREATE INDEX idx_user ON top_customers(user_id);\n\nSELECT \n    u.name,\n    u.email,\n    tc.lifetime_value\nFROM users u\nJOIN top_customers tc ON u.id = tc.user_id\nORDER BY tc.lifetime_value DESC;\n\nDROP TEMPORARY TABLE top_customers;\n\n-- Alternative: CTE (often cleaner)\nWITH top_customers AS (\n    SELECT \n        user_id,\n        SUM(total) AS lifetime_value\n    FROM orders\n    GROUP BY user_id\n    HAVING SUM(total) > 10000\n)\nSELECT \n    u.name,\n    u.email,\n    tc.lifetime_value\nFROM users u\nJOIN top_customers tc ON u.id = tc.user_id\nORDER BY tc.lifetime_value DESC;\n-- No cleanup needed"
    },
    {
      "id": 88,
      "question": "What are database execution plans and plan caching?",
      "answer": "Execution Plan: Database's query execution strategy\n\nWhat It Contains:\n• Access methods (scan/seek)\n• Join algorithms\n• Sort operations\n• Filter predicates\n• Cost estimates\n• Row estimates\n\nPlan Generation:\n1. Parse SQL\n2. Validate syntax\n3. Optimize (choose best plan)\n4. Compile\n5. Cache (if enabled)\n6. Execute\n\nPlan Caching:\n• Save compiled plans\n• Reuse for same queries\n• Parameter value variations\n• Connection-level or global\n• Memory-limited\n• Eviction policies\n\nCache Benefits:\n• Skip parse/optimize\n• Faster execution\n• Reduced CPU\n• Consistent performance\n\nCache Issues:\n• Stale plans (data changes)\n• Parameter sniffing\n• Memory consumption\n• Plan pollution\n\nPlan Invalidation:\n• Schema changes\n• Statistics update\n• Manual flush\n• Memory pressure\n\nBest Practices:\n• Monitor plan cache\n• Update statistics regularly\n• Use parameterized queries\n• Clear cache selectively\n• Test plan changes",
      "explanation": "Execution plans define query execution strategy and are cached to avoid recompilation. Monitor cache health and update statistics to maintain optimal plans.",
      "difficulty": "Hard",
      "code": "-- View execution plan (MySQL)\nEXPLAIN SELECT * FROM orders\nWHERE user_id = 123 AND status = 'COMPLETED';\n\n-- JSON format (more detail)\nEXPLAIN FORMAT=JSON\nSELECT * FROM orders WHERE user_id = 123;\n\n-- PostgreSQL: See actual execution\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE)\nSELECT * FROM orders\nWHERE user_id = 123 AND status = 'COMPLETED';\n\n-- SQL Server: Show execution plan\nSET SHOWPLAN_ALL ON;\nGO\nSELECT * FROM orders WHERE user_id = 123;\nGO\nSET SHOWPLAN_ALL OFF;\nGO\n\n-- View plan cache (SQL Server)\nSELECT \n    cp.objtype AS plan_type,\n    st.text AS query_text,\n    cp.usecounts AS execution_count,\n    cp.size_in_bytes/1024 AS size_kb,\n    qp.query_plan\nFROM sys.dm_exec_cached_plans cp\nCROSS APPLY sys.dm_exec_sql_text(cp.plan_handle) st\nCROSS APPLY sys.dm_exec_query_plan(cp.plan_handle) qp\nWHERE st.text LIKE '%orders%'\nORDER BY cp.usecounts DESC;\n\n-- Clear plan cache (SQL Server)\n-- Entire cache\nDBCC FREEPROCCACHE;\n\n-- Specific plan\nDBCC FREEPROCCACHE (plan_handle);\n\n-- Specific database\nALTER DATABASE myapp SET QUERY_STORE CLEAR ALL;\n\n-- MySQL query cache (deprecated in 8.0)\nSET GLOBAL query_cache_size = 67108864;  -- 64MB\nSET GLOBAL query_cache_type = 1;\n\n-- View cache stats\nSHOW STATUS LIKE 'Qcache%';\n\n-- Clear query cache\nFLUSH QUERY CACHE;\nRESET QUERY CACHE;\n\n-- PostgreSQL prepared statement cache\nPREPARE get_orders (INT) AS\n    SELECT * FROM orders WHERE user_id = $1;\n\n-- View prepared statements\nSELECT name, statement, prepare_time\nFROM pg_prepared_statements;\n\n-- Deallocate (remove from cache)\nDEALLOCATE get_orders;\nDEALLOCATE ALL;\n\n-- Force recompile (SQL Server)\nSELECT * FROM orders\nWHERE user_id = 123\nOPTION (RECOMPILE);\n\n-- Parameter sniffing example\n-- Problem: Plan optimized for first parameter value\nCREATE PROCEDURE GetUserOrders @UserId INT\nAS\nBEGIN\n    SELECT * FROM orders WHERE user_id = @UserId;\nEND\n\n-- First call: @UserId = 123 (few orders, index seek)\n-- Plan cached\n\n-- Second call: @UserId = 999 (millions of orders, should scan)\n-- Uses wrong cached plan (index seek), slow!\n\n-- Solution 1: RECOMPILE\nCREATE PROCEDURE GetUserOrders @UserId INT\nAS\nBEGIN\n    SELECT * FROM orders WHERE user_id = @UserId\n    OPTION (RECOMPILE);\nEND\n\n-- Solution 2: OPTIMIZE FOR hint\nCREATE PROCEDURE GetUserOrders @UserId INT\nAS\nBEGIN\n    SELECT * FROM orders WHERE user_id = @UserId\n    OPTION (OPTIMIZE FOR (@UserId = 500));  -- Average case\nEND\n\n-- Solution 3: Local variable\nCREATE PROCEDURE GetUserOrders @UserId INT\nAS\nBEGIN\n    DECLARE @LocalUserId INT = @UserId;\n    SELECT * FROM orders WHERE user_id = @LocalUserId;\nEND\n\n-- Monitor plan cache memory (SQL Server)\nSELECT \n    (SUM(single_pages_kb) + SUM(multi_pages_kb))/1024 AS cache_size_mb\nFROM sys.dm_os_memory_clerks\nWHERE type = 'CACHESTORE_SQLCP';\n\n-- Plan cache hit ratio\nSELECT \n    CAST(SUM(hit_count) AS FLOAT) / \n    CAST(SUM(hit_count + miss_count) AS FLOAT) * 100 AS cache_hit_ratio\nFROM sys.dm_exec_query_optimizer_info\nWHERE counter IN ('optimizations', 'cache hit');"
    },
    {
      "id": 89,
      "question": "What are database computed/generated columns?",
      "answer": "Computed Columns: Values derived from other columns\n\nTypes:\n\nVirtual/Non-Persisted:\n• Calculated on read\n• No storage space\n• Always current\n• Slower queries\n\nStored/Persisted:\n• Calculated on write\n• Stored in table\n• Faster queries\n• Uses storage\n• Updated automatically\n\nUse Cases:\n• Derived calculations\n• Full name from parts\n• Age from birthdate\n• JSON field extraction\n• Expressions\n\nBenefits:\n• Eliminate redundancy\n• Consistency guaranteed\n• Simplified queries\n• Can be indexed (stored)\n• Automatic updates\n\nLimitations:\n• Read-only\n• Cannot INSERT/UPDATE\n• Function restrictions\n• Deterministic only\n• Performance considerations\n\nBest Practices:\n• Use for common calculations\n• Index stored columns\n• Keep expressions simple\n• Document formulas\n• Test performance",
      "explanation": "Computed columns automatically derive values from other columns, either calculated on read (virtual) or stored for faster access, eliminating redundancy and ensuring consistency.",
      "difficulty": "Medium",
      "code": "-- MySQL generated column (virtual)\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    full_name VARCHAR(101) AS (CONCAT(first_name, ' ', last_name)),\n    salary DECIMAL(10,2),\n    annual_salary DECIMAL(12,2) AS (salary * 12)\n);\n\n-- Stored generated column\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    full_name VARCHAR(101) AS (CONCAT(first_name, ' ', last_name)) STORED,\n    birth_date DATE,\n    age INT AS (YEAR(CURDATE()) - YEAR(birth_date)) STORED\n);\n\n-- Can index stored columns\nCREATE INDEX idx_full_name ON employees(full_name);\nCREATE INDEX idx_age ON employees(age);\n\n-- Usage\nINSERT INTO employees (first_name, last_name, birth_date)\nVALUES ('John', 'Doe', '1990-05-15');\n-- full_name and age automatically calculated\n\nSELECT full_name, age FROM employees;\n-- John Doe, 34\n\n-- SQL Server computed column\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    full_name AS (first_name + ' ' + last_name),\n    salary DECIMAL(10,2),\n    annual_salary AS (salary * 12) PERSISTED\n);\n\n-- PostgreSQL generated column\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    full_name VARCHAR(101) GENERATED ALWAYS AS (first_name || ' ' || last_name) STORED,\n    birth_date DATE,\n    age INT GENERATED ALWAYS AS (EXTRACT(YEAR FROM AGE(birth_date))) STORED\n);\n\n-- JSON field extraction\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    data JSON,\n    name VARCHAR(100) AS (JSON_UNQUOTE(JSON_EXTRACT(data, '$.name'))) STORED,\n    price DECIMAL(10,2) AS (JSON_EXTRACT(data, '$.price')) STORED\n);\n\nCREATE INDEX idx_name ON products(name);\nCREATE INDEX idx_price ON products(price);\n\n-- Query is fast (uses index)\nSELECT * FROM products WHERE name LIKE 'Phone%';\n\n-- Complex calculation\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    subtotal DECIMAL(10,2),\n    tax_rate DECIMAL(4,2),\n    tax_amount DECIMAL(10,2) AS (subtotal * tax_rate) STORED,\n    total DECIMAL(10,2) AS (subtotal + (subtotal * tax_rate)) STORED\n);\n\nINSERT INTO orders (subtotal, tax_rate) VALUES (100.00, 0.10);\nSELECT * FROM orders;\n-- subtotal: 100.00, tax_amount: 10.00, total: 110.00\n\n-- Cannot update computed columns\nUPDATE employees SET full_name = 'Jane Smith' WHERE id = 1;\n-- Error: Cannot update generated column\n\n-- Instead, update source columns\nUPDATE employees SET first_name = 'Jane', last_name = 'Smith' WHERE id = 1;\n-- full_name automatically updates to 'Jane Smith'\n\n-- Virtual vs Stored performance\n-- Virtual: No storage, slower queries (calculates each time)\nSELECT COUNT(*) FROM employees WHERE full_name LIKE 'John%';\n-- Calculates full_name for every row\n\n-- Stored: Uses storage, faster queries (pre-calculated)\nSELECT COUNT(*) FROM employees WHERE full_name LIKE 'John%';\n-- Reads pre-calculated values, can use index\n\n-- Deterministic function requirement\nCREATE TABLE logs (\n    id INT,\n    message TEXT,\n    -- ERROR: NOW() is non-deterministic\n    -- created_at TIMESTAMP AS (NOW())\n    \n    -- OK: Use default instead\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);"
    },
    {
      "id": 90,
      "question": "What are database implicit vs explicit transactions?",
      "answer": "Transaction Modes: How transactions are managed\n\nExplicit Transactions:\n• Manual BEGIN/COMMIT\n• Full control\n• Clear boundaries\n• Best practice\n• Multiple statements\n\nImplicit Transactions:\n• Auto-BEGIN per statement\n• Auto-COMMIT after statement\n• Default in most databases\n• Single statement = transaction\n• Less control\n\nAutocommit:\n• ON: Each statement is transaction\n• OFF: Manual commit required\n• Database default varies\n• Connection-level setting\n\nWhen to Use:\n\nExplicit:\n• Multiple related operations\n• All-or-nothing requirement\n• Data integrity critical\n• Rollback needed\n• Best for applications\n\nImplicit:\n• Single operations\n• Simple queries\n• Ad-hoc scripts\n• When atomicity not needed\n\nBest Practices:\n• Use explicit for complex ops\n• Keep transactions short\n• Handle errors properly\n• Always COMMIT or ROLLBACK\n• Monitor long transactions\n• Avoid user interaction",
      "explanation": "Explicit transactions use manual BEGIN/COMMIT for control over multiple statements; implicit transactions auto-commit each statement. Use explicit for complex multi-step operations.",
      "difficulty": "Medium",
      "code": "-- Implicit transaction (autocommit ON)\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n-- Automatically committed\n\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n-- Automatically committed\n\n-- Problem: If second fails, first is already committed!\n\n-- Explicit transaction (correct)\nBEGIN TRANSACTION;  -- or START TRANSACTION;\n\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n\nCOMMIT;\n-- Both succeed or both fail\n\n-- With error handling (Node.js)\nconst connection = await pool.getConnection();\n\ntry {\n    await connection.beginTransaction();\n    \n    await connection.execute(\n        'UPDATE accounts SET balance = balance - ? WHERE id = ?',\n        [amount, fromAccount]\n    );\n    \n    await connection.execute(\n        'UPDATE accounts SET balance = balance + ? WHERE id = ?',\n        [amount, toAccount]\n    );\n    \n    await connection.commit();\n    console.log('Transfer successful');\n    \n} catch (error) {\n    await connection.rollback();\n    console.error('Transfer failed, rolled back');\n    throw error;\n} finally {\n    connection.release();\n}\n\n-- Check autocommit status (MySQL)\nSELECT @@autocommit;\n-- 1 = ON (implicit), 0 = OFF (explicit)\n\n-- Disable autocommit\nSET autocommit = 0;\n\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n-- Not committed yet\n\nCOMMIT;  -- Must commit manually\n\n-- Enable autocommit\nSET autocommit = 1;\n\n-- PostgreSQL (implicit autocommit by default)\nBEGIN;\n    UPDATE accounts SET balance = balance - 100 WHERE id = 1;\n    UPDATE accounts SET balance = balance + 100 WHERE id = 2;\nCOMMIT;\n\n-- SQL Server\nSET IMPLICIT_TRANSACTIONS OFF;  -- Default (autocommit)\n-- Each statement auto-commits\n\nSET IMPLICIT_TRANSACTIONS ON;  -- Explicit mode\n-- Must manually COMMIT/ROLLBACK\n\nBEGIN TRANSACTION;\n    UPDATE accounts SET balance = balance - 100 WHERE id = 1;\n    UPDATE accounts SET balance = balance + 100 WHERE id = 2;\nCOMMIT TRANSACTION;\n\n-- Java JDBC\nconnection.setAutoCommit(false);  // Explicit mode\n\ntry {\n    Statement stmt = connection.createStatement();\n    stmt.executeUpdate(\"UPDATE accounts SET balance = balance - 100 WHERE id = 1\");\n    stmt.executeUpdate(\"UPDATE accounts SET balance = balance + 100 WHERE id = 2\");\n    \n    connection.commit();\n    \n} catch (SQLException e) {\n    connection.rollback();\n    throw e;\n}\n\n// Python\nconn.autocommit = False  # Explicit mode\n\ntry:\n    cursor.execute(\"UPDATE accounts SET balance = balance - %s WHERE id = %s\", (100, 1))\n    cursor.execute(\"UPDATE accounts SET balance = balance + %s WHERE id = %s\", (100, 2))\n    \n    conn.commit()\nexcept Exception as e:\n    conn.rollback()\n    raise e\n\n-- Long transaction monitoring\nSELECT \n    trx_id,\n    trx_state,\n    trx_started,\n    NOW() - trx_started AS duration,\n    trx_query\nFROM information_schema.innodb_trx\nWHERE trx_started < NOW() - INTERVAL 10 SECOND\nORDER BY trx_started;\n\n-- Kill long transaction\nKILL <trx_mysql_thread_id>;"
    },
    {
      "id": 91,
      "question": "What is database connection pooling and how does it work?",
      "answer": "Connection Pool: Reusable connections cache\n\nHow It Works:\n1. Create pool of connections\n2. Application requests connection\n3. Pool provides available connection\n4. Application uses connection\n5. Application releases connection\n6. Pool reuses for next request\n\nBenefits:\n• Avoid connection overhead\n• Faster response time\n• Resource management\n• Limit concurrent connections\n• Connection reuse\n• Better performance\n\nConfiguration:\n• Min connections (always open)\n• Max connections (limit)\n• Connection timeout\n• Idle timeout\n• Max lifetime\n• Validation query\n\nPool Management:\n• Acquire: Get from pool\n• Release: Return to pool\n• Validate: Test before use\n• Evict: Remove stale/broken\n• Grow: Create new if needed\n• Shrink: Remove idle\n\nBest Practices:\n• Size pool appropriately\n• Always release connections\n• Use try-finally blocks\n• Monitor pool metrics\n• Set timeouts\n• Validate connections\n• Handle errors\n\nCommon Libraries:\n• HikariCP (Java)\n• mysql2 pool (Node.js)\n• SQLAlchemy (Python)\n• Npgsql (C#)",
      "explanation": "Connection pooling reuses database connections instead of creating new ones, significantly improving performance by avoiding connection overhead and managing resources efficiently.",
      "difficulty": "Medium",
      "code": "// Node.js (mysql2) connection pool\nconst mysql = require('mysql2/promise');\n\nconst pool = mysql.createPool({\n    host: 'localhost',\n    user: 'root',\n    password: 'password',\n    database: 'myapp',\n    connectionLimit: 10,        // Max connections\n    queueLimit: 0,              // No queue limit\n    waitForConnections: true,   // Wait if no connections\n    maxIdle: 10,                // Max idle connections\n    idleTimeout: 60000,         // 60 seconds\n    enableKeepAlive: true,\n    keepAliveInitialDelay: 0\n});\n\n// Using connection\nasync function getUser(userId) {\n    // Auto-acquire and release\n    const [rows] = await pool.execute(\n        'SELECT * FROM users WHERE id = ?',\n        [userId]\n    );\n    return rows[0];\n}\n\n// Manual connection management\nasync function transferMoney(fromId, toId, amount) {\n    const connection = await pool.getConnection();\n    \n    try {\n        await connection.beginTransaction();\n        \n        await connection.execute(\n            'UPDATE accounts SET balance = balance - ? WHERE id = ?',\n            [amount, fromId]\n        );\n        \n        await connection.execute(\n            'UPDATE accounts SET balance = balance + ? WHERE id = ?',\n            [amount, toId]\n        );\n        \n        await connection.commit();\n    } catch (error) {\n        await connection.rollback();\n        throw error;\n    } finally {\n        connection.release();  // Return to pool\n    }\n}\n\n// Monitor pool\nsetInterval(() => {\n    console.log('Pool connections:', pool.pool._allConnections.length);\n    console.log('Free connections:', pool.pool._freeConnections.length);\n}, 5000);\n\n// Java HikariCP (fastest pool)\nimport com.zaxxer.hikari.HikariConfig;\nimport com.zaxxer.hikari.HikariDataSource;\n\nHikariConfig config = new HikariConfig();\nconfig.setJdbcUrl(\"jdbc:mysql://localhost:3306/myapp\");\nconfig.setUsername(\"root\");\nconfig.setPassword(\"password\");\nconfig.setMaximumPoolSize(10);           // Max connections\nconfig.setMinimumIdle(2);                // Min idle\nconfig.setIdleTimeout(600000);           // 10 minutes\nconfig.setMaxLifetime(1800000);          // 30 minutes\nconfig.setConnectionTimeout(30000);      // 30 seconds\nconfig.setConnectionTestQuery(\"SELECT 1\");\nconfig.setLeakDetectionThreshold(60000); // Detect leaks\n\nHikariDataSource dataSource = new HikariDataSource(config);\n\n// Using connection\ntry (Connection conn = dataSource.getConnection()) {\n    PreparedStatement pstmt = conn.prepareStatement(\n        \"SELECT * FROM users WHERE id = ?\"\n    );\n    pstmt.setInt(1, userId);\n    ResultSet rs = pstmt.executeQuery();\n    // ... process results\n}  // Auto-released\n\n// Python SQLAlchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.pool import QueuePool\n\nengine = create_engine(\n    'mysql+pymysql://root:password@localhost/myapp',\n    poolclass=QueuePool,\n    pool_size=10,           # Connections to keep\n    max_overflow=5,         # Extra connections if needed\n    pool_timeout=30,        # Wait timeout\n    pool_recycle=3600,      # Recycle after 1 hour\n    pool_pre_ping=True      # Validate before use\n)\n\n# Using connection\nwith engine.connect() as conn:\n    result = conn.execute(\n        \"SELECT * FROM users WHERE id = :id\",\n        {\"id\": user_id}\n    )\n    user = result.fetchone()\n\n// C# Npgsql\nusing Npgsql;\n\nvar connString = \"Host=localhost;Username=postgres;Password=password;Database=myapp;\" +\n    \"Minimum Pool Size=2;\" +\n    \"Maximum Pool Size=10;\" +\n    \"Connection Idle Lifetime=300;\" +  // 5 minutes\n    \"Connection Pruning Interval=10;\" +  // Check every 10s\n    \"Timeout=30\";\n\nusing (var conn = new NpgsqlConnection(connString))\n{\n    await conn.OpenAsync();\n    \n    using (var cmd = new NpgsqlCommand(\"SELECT * FROM users WHERE id = @id\", conn))\n    {\n        cmd.Parameters.AddWithValue(\"id\", userId);\n        using (var reader = await cmd.ExecuteReaderAsync())\n        {\n            // ... read data\n        }\n    }\n}  // Auto-released\n\n// Pool sizing calculation\n// Formula: connections = ((core_count * 2) + effective_spindle_count)\n// Example: 4 cores, 2 disks = (4 * 2) + 2 = 10 connections\n\n// Monitor pool metrics\n// Java JMX\nMBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer();\nObjectName poolName = new ObjectName(\"com.zaxxer.hikari:type=Pool (mypool)\");\n\nint totalConnections = (int) mBeanServer.getAttribute(poolName, \"TotalConnections\");\nint activeConnections = (int) mBeanServer.getAttribute(poolName, \"ActiveConnections\");\nint idleConnections = (int) mBeanServer.getAttribute(poolName, \"IdleConnections\");\nint threadsWaiting = (int) mBeanServer.getAttribute(poolName, \"ThreadsAwaitingConnection\");\n\nSystem.out.println(\"Total: \" + totalConnections);\nSystem.out.println(\"Active: \" + activeConnections);\nSystem.out.println(\"Idle: \" + idleConnections);\nSystem.out.println(\"Waiting: \" + threadsWaiting);\n\n// Connection leak detection\nconfig.setLeakDetectionThreshold(60000);  // Warn if held > 60s\n// Logs warning with stack trace of where connection was acquired"
    },
    {
      "id": 92,
      "question": "What are database monitoring and alerting best practices?",
      "answer": "Database Monitoring: Track health and performance\n\nKey Metrics:\n\nPerformance:\n• Query response time\n• Throughput (QPS)\n• Connection count\n• CPU usage\n• Memory usage\n• Disk I/O\n• Cache hit ratio\n\nAvailability:\n• Uptime\n• Replication lag\n• Failed connections\n• Error rate\n• Deadlocks\n\nCapacity:\n• Disk usage\n• Connection limits\n• Table/index size\n• Growth rate\n\nAlert Thresholds:\n• Warning: 70-80%\n• Critical: 90%+\n• Disk full: 85%+\n• Replication lag: > 30s\n• Slow queries: > 1s\n• Connection saturation: 80%+\n\nTools:\n• Prometheus + Grafana\n• DataDog\n• New Relic\n• CloudWatch (AWS)\n• Azure Monitor\n• Percona Monitoring\n• pgBadger (PostgreSQL)\n\nBest Practices:\n• Monitor continuously\n• Set multiple thresholds\n• Alert on trends\n• Log slow queries\n• Track query patterns\n• Regular health checks\n• Automated remediation\n• Document runbooks\n• Test alerts",
      "explanation": "Monitor database health continuously with key metrics (performance, availability, capacity), set appropriate alert thresholds, and use tools like Prometheus/Grafana for visualization and alerting.",
      "difficulty": "Medium",
      "code": "-- MySQL slow query log\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 1;  -- Queries > 1 second\nSET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';\nSET GLOBAL log_queries_not_using_indexes = 'ON';\n\n-- Monitor connections\nSHOW STATUS LIKE 'Threads_connected';\nSHOW STATUS LIKE 'Max_used_connections';\nSHOW VARIABLES LIKE 'max_connections';\n\n-- Connection usage percentage\nSELECT \n    (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS \n     WHERE VARIABLE_NAME='Threads_connected') /\n    (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_VARIABLES \n     WHERE VARIABLE_NAME='max_connections') * 100 AS connection_usage_percent;\n\n-- Cache hit ratio\nSHOW STATUS LIKE 'Innodb_buffer_pool_read_requests';\nSHOW STATUS LIKE 'Innodb_buffer_pool_reads';\n\n-- Calculate hit ratio (should be > 99%)\nSELECT \n    100 - ((\n        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS \n         WHERE VARIABLE_NAME='Innodb_buffer_pool_reads') /\n        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS \n         WHERE VARIABLE_NAME='Innodb_buffer_pool_read_requests')\n    ) * 100) AS buffer_pool_hit_ratio;\n\n-- Monitor replication lag\nSHOW SLAVE STATUS\\G\n-- Check Seconds_Behind_Master\n\n-- PostgreSQL monitoring queries\n-- Active connections\nSELECT count(*) FROM pg_stat_activity WHERE state = 'active';\n\n-- Long running queries\nSELECT \n    pid,\n    now() - query_start AS duration,\n    state,\n    query\nFROM pg_stat_activity\nWHERE (now() - query_start) > interval '5 minutes'\nORDER BY duration DESC;\n\n-- Kill long query\nSELECT pg_terminate_backend(pid);\n\n-- Database size\nSELECT \n    pg_database.datname,\n    pg_size_pretty(pg_database_size(pg_database.datname)) AS size\nFROM pg_database\nORDER BY pg_database_size(pg_database.datname) DESC;\n\n-- Table bloat\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,\n    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,\n    pg_size_pretty(pg_indexes_size(schemaname||'.'||tablename)) AS index_size\nFROM pg_tables\nWHERE schemaname NOT IN ('pg_catalog', 'information_schema')\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 10;\n\n-- Cache hit ratio (should be > 95%)\nSELECT \n    sum(blks_hit)*100/(sum(blks_hit)+sum(blks_read)) AS cache_hit_ratio\nFROM pg_stat_database;\n\n-- Prometheus exporter configuration\n# mysqld_exporter\n./mysqld_exporter \\\n    --config.my-cnf=/etc/mysql/.my.cnf \\\n    --web.listen-address=:9104\n\n# postgres_exporter\nDATA_SOURCE_NAME=\"postgresql://user:password@localhost:5432/myapp?sslmode=disable\" \\\n./postgres_exporter --web.listen-address=:9187\n\n# Prometheus config (prometheus.yml)\nscrape_configs:\n  - job_name: 'mysql'\n    static_configs:\n      - targets: ['localhost:9104']\n  \n  - job_name: 'postgresql'\n    static_configs:\n      - targets: ['localhost:9187']\n\n# Grafana alert rules\n# Alert: High connection usage\n- alert: HighConnectionUsage\n  expr: mysql_global_status_threads_connected / mysql_global_variables_max_connections > 0.8\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"MySQL connection usage > 80%\"\n    description: \"{{ $value | humanizePercentage }} connections used\"\n\n# Alert: Replication lag\n- alert: ReplicationLag\n  expr: mysql_slave_status_seconds_behind_master > 30\n  for: 2m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"Replication lag > 30 seconds\"\n\n# Alert: Slow queries\n- alert: SlowQueries\n  expr: rate(mysql_global_status_slow_queries[5m]) > 10\n  for: 5m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"High rate of slow queries\"\n\n# Alert: Disk usage\n- alert: HighDiskUsage\n  expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes > 0.85\n  for: 5m\n  labels:\n    severity: critical\n  annotations:\n    summary: \"Disk usage > 85%\"\n\n// Node.js health check endpoint\napp.get('/health/db', async (req, res) => {\n    try {\n        const [result] = await pool.execute('SELECT 1');\n        \n        const [[status]] = await pool.execute(`\n            SELECT \n                (SELECT COUNT(*) FROM information_schema.processlist) AS connections,\n                (SELECT @@max_connections) AS max_connections,\n                (SELECT @@slow_query_log) AS slow_log_enabled\n        `);\n        \n        const connectionUsage = (status.connections / status.max_connections) * 100;\n        \n        res.json({\n            status: 'healthy',\n            connections: status.connections,\n            maxConnections: status.max_connections,\n            connectionUsage: `${connectionUsage.toFixed(2)}%`,\n            slowLogEnabled: status.slow_log_enabled === 1\n        });\n    } catch (error) {\n        res.status(503).json({\n            status: 'unhealthy',\n            error: error.message\n        });\n    }\n});\n\n// Automated health check script\n#!/bin/bash\n# check_db_health.sh\n\nCONN_USAGE=$(mysql -u monitor -p'password' -e \"\n    SELECT ROUND(\n        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_STATUS WHERE VARIABLE_NAME='Threads_connected') /\n        (SELECT VARIABLE_VALUE FROM information_schema.GLOBAL_VARIABLES WHERE VARIABLE_NAME='max_connections') * 100\n    ) AS usage\" -sN)\n\nif [ $CONN_USAGE -gt 80 ]; then\n    echo \"CRITICAL: Connection usage at ${CONN_USAGE}%\" | mail -s \"DB Alert\" admin@example.com\nfi"
    },
    {
      "id": 93,
      "question": "What are database high availability (HA) patterns?",
      "answer": "High Availability: Minimize downtime\n\nHA Patterns:\n\n1. Master-Slave Replication:\n• One master (writes)\n• Multiple slaves (reads)\n• Automatic failover\n• Read scaling\n• Geographic distribution\n\n2. Master-Master:\n• Multiple writeable nodes\n• Bidirectional replication\n• Active-active\n• Conflict resolution needed\n• Higher complexity\n\n3. Cluster:\n• Shared storage\n• Multiple nodes\n• Automatic failover\n• Resource pooling\n• Higher availability\n\n4. Database Proxy:\n• Route traffic\n• Health checks\n• Automatic failover\n• Load balancing\n• Connection pooling\n\nComponents:\n• Primary database\n• Standby replicas\n• Load balancer\n• Health monitoring\n• Failover automation\n• Backup systems\n\nFailover Types:\n• Automatic (recommended)\n• Manual (slower)\n• Planned (maintenance)\n\nKey Metrics:\n• Uptime: 99.9%+ (< 8.76h/year)\n• RTO: Recovery Time Objective\n• RPO: Recovery Point Objective\n• Replication lag: < 1s\n\nBest Practices:\n• Monitor continuously\n• Test failover regularly\n• Automate failover\n• Keep replicas in sync\n• Geographic distribution\n• Document procedures",
      "explanation": "High availability patterns like master-slave replication, clustering, and automatic failover ensure minimal downtime. Aim for 99.9%+ uptime with continuous monitoring and tested failover procedures.",
      "difficulty": "Hard",
      "code": "-- MySQL Master-Slave setup\n-- On Master:\nCREATE USER 'replicator'@'%' IDENTIFIED BY 'password';\nGRANT REPLICATION SLAVE ON *.* TO 'replicator'@'%';\nFLUSH PRIVILEGES;\n\n-- Edit my.cnf\n[mysqld]\nserver-id = 1\nlog-bin = mysql-bin\nbinlog-format = ROW\ngtid-mode = ON\nenforce-gtid-consistency = ON\n\n-- Get master status\nSHOW MASTER STATUS;\n-- Note: File and Position\n\n-- On Slave:\n-- Edit my.cnf\n[mysqld]\nserver-id = 2\nrelay-log = relay-bin\nread-only = ON\ngtid-mode = ON\nenforce-gtid-consistency = ON\n\n-- Configure replication\nCHANGE MASTER TO\n    MASTER_HOST='master-host',\n    MASTER_USER='replicator',\n    MASTER_PASSWORD='password',\n    MASTER_AUTO_POSITION = 1;  -- Using GTID\n\nSTART SLAVE;\n\n-- Check replication status\nSHOW SLAVE STATUS\\G\n-- Seconds_Behind_Master should be 0 or low\n\n-- Multi-source replication (multiple masters)\nCHANGE MASTER TO\n    MASTER_HOST='master1',\n    MASTER_USER='replicator',\n    MASTER_PASSWORD='password',\n    MASTER_AUTO_POSITION = 1\n    FOR CHANNEL 'master1';\n\nCHANGE MASTER TO\n    MASTER_HOST='master2',\n    MASTER_USER='replicator',\n    MASTER_PASSWORD='password',\n    MASTER_AUTO_POSITION = 1\n    FOR CHANNEL 'master2';\n\nSTART SLAVE FOR CHANNEL 'master1';\nSTART SLAVE FOR CHANNEL 'master2';\n\n-- PostgreSQL streaming replication\n-- On Primary (postgresql.conf):\nwal_level = replica\nmax_wal_senders = 3\nwal_keep_size = 1GB\n\n-- Create replication user\nCREATE ROLE replicator WITH REPLICATION PASSWORD 'password' LOGIN;\n\n-- pg_hba.conf\nhost replication replicator standby-ip/32 md5\n\n-- On Standby:\n-- Create standby.signal file\ntouch /var/lib/postgresql/data/standby.signal\n\n-- postgresql.conf\nprimary_conninfo = 'host=primary-host port=5432 user=replicator password=password'\n\n-- Start standby\npg_ctl start\n\n-- Check replication status\nSELECT * FROM pg_stat_replication;  -- On primary\n\n-- ProxySQL load balancer config\n-- Install ProxySQL\nsudo apt install proxysql\n\n-- Configure (proxysql-admin)\nmysql -u admin -padmin -h 127.0.0.1 -P6032 --prompt='ProxySQL> '\n\n-- Add MySQL servers\nINSERT INTO mysql_servers (hostgroup_id, hostname, port) VALUES\n    (1, 'master', 3306),  -- Write group\n    (2, 'slave1', 3306),  -- Read group\n    (2, 'slave2', 3306);  -- Read group\n\nLOAD MYSQL SERVERS TO RUNTIME;\nSAVE MYSQL SERVERS TO DISK;\n\n-- Configure users\nINSERT INTO mysql_users (username, password, default_hostgroup) \n    VALUES ('app_user', 'password', 1);\n\nLOAD MYSQL USERS TO RUNTIME;\nSAVE MYSQL USERS TO DISK;\n\n-- Query routing rules\nINSERT INTO mysql_query_rules (rule_id, active, match_pattern, destination_hostgroup, apply)\n    VALUES\n    (1, 1, '^SELECT.*FOR UPDATE', 1, 1),  -- Writes to master\n    (2, 1, '^SELECT', 2, 1);              -- Reads to slaves\n\nLOAD MYSQL QUERY RULES TO RUNTIME;\nSAVE MYSQL QUERY RULES TO DISK;\n\n-- Health check\nUPDATE global_variables SET variable_value='2000' \n    WHERE variable_name='mysql-monitor_connect_interval';\nUPDATE global_variables SET variable_value='1000' \n    WHERE variable_name='mysql-monitor_ping_interval';\n\nLOAD MYSQL VARIABLES TO RUNTIME;\nSAVE MYSQL VARIABLES TO DISK;\n\n-- Application connection\nconst pool = mysql.createPool({\n    host: 'proxysql-host',\n    port: 6033,\n    user: 'app_user',\n    password: 'password',\n    database: 'myapp'\n});\n// Writes go to master, reads go to slaves automatically\n\n-- Automatic failover with MHA (Master High Availability)\n# Install MHA\nsudo apt install mha4mysql-node mha4mysql-manager\n\n# mha.conf\n[server1]\nhostname=master\ncandidate_master=1\n\n[server2]\nhostname=slave1\ncandidate_master=1\n\n[server3]\nhostname=slave2\nno_master=1\n\n# Start MHA manager\nmasterha_manager --conf=/etc/mha/mha.conf\n\n# Check status\nmasterha_check_repl --conf=/etc/mha/mha.conf\n\n-- Galera Cluster (synchronous multi-master)\n# Install on all nodes\nsudo apt install mysql-wsrep galera-3\n\n# Node 1 (bootstrap)\n[mysqld]\nwsrep_provider=/usr/lib/galera/libgalera_smm.so\nwsrep_cluster_address=\"gcomm://node1,node2,node3\"\nwsrep_node_address=\"node1\"\nwsrep_cluster_name=\"my_cluster\"\nbinlog_format=ROW\n\nsudo galera_new_cluster\n\n# Node 2, 3 (join)\n[mysqld]\nwsrep_cluster_address=\"gcomm://node1,node2,node3\"\nwsrep_node_address=\"node2\"  # or node3\n\nsudo systemctl start mysql\n\n-- Check cluster status\nSHOW STATUS LIKE 'wsrep_cluster_size';  -- Should be 3\nSHOW STATUS LIKE 'wsrep_local_state_comment';  -- Should be 'Synced'"
    },
    {
      "id": 94,
      "question": "What are database backup and disaster recovery strategies?",
      "answer": "Backup & DR: Protect against data loss\n\nBackup Types:\n\n1. Full Backup:\n• Complete database copy\n• Slowest, largest\n• Easiest restore\n• Weekly/monthly\n\n2. Incremental:\n• Changes since last backup\n• Faster, smaller\n• Requires full + all incrementals\n• Daily\n\n3. Differential:\n• Changes since last full\n• Faster than full\n• Requires full + latest differential\n• Daily\n\nBackup Methods:\n• Logical (SQL dump)\n• Physical (file copy)\n• Snapshot (storage-level)\n• Continuous (binlog/WAL)\n\nKey Metrics:\n• RPO: Recovery Point Objective (data loss tolerance)\n• RTO: Recovery Time Objective (downtime tolerance)\n• Backup frequency\n• Retention period\n• Test frequency\n\nDR Strategies:\n• Backup + restore\n• Standby replica\n• Multi-region replication\n• Cloud backup\n• Point-in-time recovery\n\nBest Practices:\n• 3-2-1 Rule: 3 copies, 2 media, 1 offsite\n• Test restores regularly\n• Automate backups\n• Encrypt backups\n• Monitor backup jobs\n• Document procedures\n• Verify backup integrity\n• Offsite storage\n• Version backups",
      "explanation": "Use full, incremental, and differential backups based on RPO/RTO requirements. Follow 3-2-1 rule and test restores regularly to ensure disaster recovery readiness.",
      "difficulty": "Hard",
      "code": "-- MySQL logical backup (mysqldump)\n# Full backup\nmysqldump -u root -p \\\n    --single-transaction \\\n    --routines \\\n    --triggers \\\n    --all-databases \\\n    --master-data=2 \\\n    > /backup/full_$(date +%Y%m%d).sql\n\n# Single database\nmysqldump -u root -p myapp > myapp_backup.sql\n\n# Restore\nmysql -u root -p < full_20240115.sql\n# OR\nmysql -u root -p myapp < myapp_backup.sql\n\n# Compressed backup\nmysqldump -u root -p --all-databases | gzip > backup.sql.gz\n\n# Restore compressed\ngunzip < backup.sql.gz | mysql -u root -p\n\n-- Point-in-time recovery (PITR)\n# Enable binary logging (my.cnf)\n[mysqld]\nlog-bin=mysql-bin\nserver-id=1\nbinlog-format=ROW\n\n# Full backup with position\nmysqldump -u root -p \\\n    --single-transaction \\\n    --flush-logs \\\n    --master-data=2 \\\n    --all-databases > full_backup.sql\n\n# Binary log backups (hourly)\nmysqlbinlog mysql-bin.000001 > binlog_backup_001.sql\n\n# Restore to point in time\n# 1. Restore full backup\nmysql -u root -p < full_backup.sql\n\n# 2. Apply binary logs up to specific time\nmysqlbinlog --stop-datetime=\"2024-01-15 14:30:00\" \\\n    mysql-bin.000001 mysql-bin.000002 | mysql -u root -p\n\n-- PostgreSQL backup\n# Logical backup\npg_dump -U postgres myapp > myapp_backup.sql\n\n# All databases\npg_dumpall -U postgres > full_backup.sql\n\n# Compressed\npg_dump -U postgres -Fc myapp > myapp_backup.dump\n\n# Restore\npsql -U postgres myapp < myapp_backup.sql\n# OR for custom format\npg_restore -U postgres -d myapp myapp_backup.dump\n\n# Continuous archiving (PITR)\n# postgresql.conf\nwal_level = replica\narchive_mode = on\narchive_command = 'cp %p /backup/wal/%f'\n\n# Base backup\npg_basebackup -U postgres -D /backup/base -Fp -Xs -P\n\n# Recovery\n# 1. Restore base backup\ncp -r /backup/base/* /var/lib/postgresql/data/\n\n# 2. Create recovery.conf\nrestore_command = 'cp /backup/wal/%f %p'\nrecovery_target_time = '2024-01-15 14:30:00'\n\n# 3. Start PostgreSQL\npg_ctl start\n\n-- Automated backup script\n#!/bin/bash\n# mysql_backup.sh\n\nBACKUP_DIR=\"/backup/mysql\"\nDATE=$(date +%Y%m%d_%H%M%S)\nDAYS_TO_KEEP=7\n\n# Full backup on Sunday, incremental rest of week\nif [ $(date +%u) -eq 7 ]; then\n    # Full backup\n    mysqldump -u backup -p'password' \\\n        --single-transaction \\\n        --routines \\\n        --triggers \\\n        --all-databases \\\n        --master-data=2 \\\n        | gzip > \"$BACKUP_DIR/full_$DATE.sql.gz\"\n    \n    echo \"Full backup completed: full_$DATE.sql.gz\"\nelse\n    # Incremental (binary logs)\n    mysql -u backup -p'password' -e \"FLUSH LOGS\"\n    \n    # Copy binary logs\n    cp /var/lib/mysql/mysql-bin.* \"$BACKUP_DIR/binlog/\"\n    \n    echo \"Incremental backup completed\"\nfi\n\n# Delete old backups\nfind \"$BACKUP_DIR\" -name \"*.sql.gz\" -mtime +$DAYS_TO_KEEP -delete\n\n# Upload to cloud (AWS S3)\naws s3 sync \"$BACKUP_DIR\" s3://my-db-backups/mysql/ \\\n    --storage-class STANDARD_IA \\\n    --delete\n\necho \"Backup uploaded to S3\"\n\n# Verify backup\nif [ -f \"$BACKUP_DIR/full_$DATE.sql.gz\" ]; then\n    gunzip -t \"$BACKUP_DIR/full_$DATE.sql.gz\"\n    if [ $? -eq 0 ]; then\n        echo \"Backup verification successful\"\n    else\n        echo \"ERROR: Backup verification failed\" | mail -s \"Backup Alert\" admin@example.com\n    fi\nfi\n\n-- Backup monitoring\n-- Check last backup time\nSELECT \n    table_schema,\n    MAX(update_time) AS last_update\nFROM information_schema.tables\nGROUP BY table_schema;\n\n-- Node.js backup script\nconst { exec } = require('child_process');\nconst fs = require('fs');\nconst AWS = require('aws-sdk');\n\nconst s3 = new AWS.S3();\n\nasync function backupDatabase() {\n    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');\n    const filename = `backup_${timestamp}.sql`;\n    const gzFilename = `${filename}.gz`;\n    \n    // Dump database\n    await execPromise(\n        `mysqldump -u root -p'password' myapp > /tmp/${filename}`\n    );\n    \n    // Compress\n    await execPromise(`gzip /tmp/${filename}`);\n    \n    // Upload to S3\n    const fileContent = fs.readFileSync(`/tmp/${gzFilename}`);\n    \n    await s3.upload({\n        Bucket: 'my-db-backups',\n        Key: `mysql/${gzFilename}`,\n        Body: fileContent,\n        StorageClass: 'STANDARD_IA',\n        ServerSideEncryption: 'AES256'\n    }).promise();\n    \n    console.log(`Backup completed: ${gzFilename}`);\n    \n    // Cleanup\n    fs.unlinkSync(`/tmp/${gzFilename}`);\n    \n    // Notify\n    await sendNotification('Backup completed successfully');\n}\n\nfunction execPromise(command) {\n    return new Promise((resolve, reject) => {\n        exec(command, (error, stdout, stderr) => {\n            if (error) reject(error);\n            else resolve(stdout);\n        });\n    });\n}\n\n// Schedule (cron: 2 AM daily)\nconst cron = require('node-cron');\ncron.schedule('0 2 * * *', backupDatabase);\n\n-- DR drill script\n#!/bin/bash\n# test_restore.sh - Test backup restoration\n\nTEST_DB=\"myapp_test\"\nBACKUP_FILE=\"/backup/latest.sql.gz\"\n\n# Drop test database\nmysql -u root -p'password' -e \"DROP DATABASE IF EXISTS $TEST_DB\"\n\n# Create test database\nmysql -u root -p'password' -e \"CREATE DATABASE $TEST_DB\"\n\n# Restore backup\ngunzip < \"$BACKUP_FILE\" | mysql -u root -p'password' \"$TEST_DB\"\n\n# Verify data\nROW_COUNT=$(mysql -u root -p'password' -sN -e \"SELECT COUNT(*) FROM $TEST_DB.users\")\n\nif [ $ROW_COUNT -gt 0 ]; then\n    echo \"✓ DR test successful: $ROW_COUNT rows restored\"\nelse\n    echo \"✗ DR test failed: No data restored\" | mail -s \"DR Test Failed\" admin@example.com\nfi\n\n# Cleanup\nmysql -u root -p'password' -e \"DROP DATABASE $TEST_DB\""
    },
    {
      "id": 95,
      "question": "What are database JSON/NoSQL features in relational databases?",
      "answer": "JSON Support: Semi-structured data in RDBMS\n\nJSON Features:\n• Store JSON documents\n• Query JSON fields\n• Index JSON paths\n• Validate JSON schema\n• Extract values\n• Array operations\n• Hybrid queries (SQL + JSON)\n\nBenefits:\n• Flexible schema\n• Reduce tables\n• Store complex objects\n• Fast development\n• Combine relational + document\n\nMySQL JSON:\n• Native JSON type\n• Binary storage (JSONB-like)\n• Path expressions\n• JSON functions\n• Generated columns\n• Indexes on paths\n\nPostgreSQL JSONB:\n• Binary JSON (faster)\n• GIN indexes\n• Rich operators\n• Powerful queries\n• Full-text search\n\nUse Cases:\n• API responses\n• Configuration data\n• Dynamic attributes\n• Event logging\n• Audit trails\n• User preferences\n• Flexible metadata\n\nBest Practices:\n• Use for semi-structured data\n• Index frequently queried paths\n• Validate schema\n• Don't overuse\n• Normalize when appropriate\n• Monitor size\n• Use generated columns\n\nWhen to Use:\n• Variable schema\n• Rapid prototyping\n• Third-party data\n• Complex nested data\n\nWhen NOT to Use:\n• Well-defined schema\n• Frequent joins\n• Complex aggregations\n• High write volume",
      "explanation": "Modern RDBMS support JSON for semi-structured data, enabling hybrid relational-document models. Use for flexible schema needs while maintaining ACID guarantees and SQL querying power.",
      "difficulty": "Medium",
      "code": "-- MySQL JSON column\nCREATE TABLE users (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    profile JSON,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Insert JSON data\nINSERT INTO users (id, name, profile) VALUES\n    (1, 'John Doe', '{\"age\": 30, \"email\": \"john@example.com\", \"tags\": [\"admin\", \"vip\"]}'),\n    (2, 'Jane Smith', '{\"age\": 25, \"email\": \"jane@example.com\", \"address\": {\"city\": \"NYC\", \"zip\": \"10001\"}}');\n\n-- Query JSON fields\nSELECT \n    id,\n    name,\n    JSON_EXTRACT(profile, '$.email') AS email,\n    JSON_EXTRACT(profile, '$.age') AS age\nFROM users;\n\n-- Shorthand (MySQL 5.7+)\nSELECT \n    id,\n    name,\n    profile->>'$.email' AS email,\n    profile->>'$.age' AS age\nFROM users;\n\n-- Filter by JSON field\nSELECT * FROM users\nWHERE profile->>'$.age' > 25;\n\n-- Array operations\nSELECT * FROM users\nWHERE JSON_CONTAINS(profile->'$.tags', '\"admin\"');\n\n-- Update JSON field\nUPDATE users\nSET profile = JSON_SET(profile, '$.age', 31, '$.status', 'active')\nWHERE id = 1;\n\n-- Add to array\nUPDATE users\nSET profile = JSON_ARRAY_APPEND(profile, '$.tags', 'premium')\nWHERE id = 1;\n\n-- Remove from JSON\nUPDATE users\nSET profile = JSON_REMOVE(profile, '$.status')\nWHERE id = 1;\n\n-- Generated column (improved queries)\nCREATE TABLE users (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    profile JSON,\n    email VARCHAR(100) AS (profile->>'$.email') STORED,\n    age INT AS (profile->>'$.age') STORED\n);\n\n-- Index generated column\nCREATE INDEX idx_email ON users(email);\nCREATE INDEX idx_age ON users(age);\n\n-- Fast query (uses index)\nSELECT * FROM users WHERE email = 'john@example.com';\n\n-- Multi-value index (MySQL 8.0.17+)\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    data JSON,\n    INDEX idx_tags ((CAST(data->'$.tags' AS CHAR(50) ARRAY)))\n);\n\n-- Query uses index\nSELECT * FROM products\nWHERE 'electronics' MEMBER OF (data->'$.tags');\n\n-- PostgreSQL JSONB\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100),\n    profile JSONB\n);\n\n-- Insert\nINSERT INTO users (name, profile) VALUES\n    ('John Doe', '{\"age\": 30, \"email\": \"john@example.com\", \"tags\": [\"admin\"]}');\n\n-- Query with operators\nSELECT \n    name,\n    profile->>'email' AS email,\n    (profile->>'age')::int AS age\nFROM users\nWHERE profile->>'email' = 'john@example.com';\n\n-- Nested field access\nSELECT profile->'address'->>'city' AS city FROM users;\n\n-- Array contains\nSELECT * FROM users\nWHERE profile->'tags' ? 'admin';\n\n-- Any array element\nSELECT * FROM users\nWHERE profile->'tags' ?| ARRAY['admin', 'vip'];\n\n-- All array elements\nSELECT * FROM users\nWHERE profile->'tags' ?& ARRAY['admin', 'vip'];\n\n-- JSONB operators\nSELECT * FROM users\nWHERE profile @> '{\"age\": 30}';  -- Contains\n\nSELECT * FROM users\nWHERE profile <@ '{\"age\": 30}';  -- Contained by\n\n-- Update JSONB\nUPDATE users\nSET profile = profile || '{\"status\": \"active\"}'::jsonb\nWHERE id = 1;\n\n-- Remove key\nUPDATE users\nSET profile = profile - 'status'\nWHERE id = 1;\n\n-- GIN index (fast JSONB queries)\nCREATE INDEX idx_profile ON users USING GIN (profile);\n\n-- Specific path index\nCREATE INDEX idx_email ON users ((profile->>'email'));\n\n-- JSON schema validation (PostgreSQL)\nALTER TABLE users ADD CONSTRAINT profile_schema\nCHECK (\n    jsonb_typeof(profile->'age') = 'number' AND\n    (profile->>'age')::int > 0 AND\n    (profile->>'age')::int < 150\n);\n\n-- Application usage (Node.js)\nconst [rows] = await connection.execute(\n    'SELECT * FROM users WHERE profile->\"$.email\" = ?',\n    ['john@example.com']\n);\n\n// Insert with JSON\nawait connection.execute(\n    'INSERT INTO users (name, profile) VALUES (?, ?)',\n    ['John', JSON.stringify({ age: 30, email: 'john@example.com' })]\n);\n\n// Java with Jackson\nObjectMapper mapper = new ObjectMapper();\nJsonNode profile = mapper.createObjectNode()\n    .put(\"age\", 30)\n    .put(\"email\", \"john@example.com\");\n\nPreparedStatement pstmt = conn.prepareStatement(\n    \"INSERT INTO users (name, profile) VALUES (?, ?)\"\n);\npstmt.setString(1, \"John\");\npstmt.setString(2, profile.toString());\npstmt.executeUpdate();\n\n-- Complex query: Join relational + JSON\nSELECT \n    u.name,\n    u.profile->>'$.email' AS email,\n    COUNT(o.id) AS order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.profile->>'$.age' > 25\nGROUP BY u.id, u.name, email\nHAVING COUNT(o.id) > 10;"
    },
    {
      "id": 96,
      "question": "What are database read and write splitting patterns?",
      "answer": "Read/Write Splitting: Scale reads separately from writes\n\nPattern:\n• Master: Handles writes\n• Slaves/Replicas: Handle reads\n• Application routes queries\n• Or use proxy/load balancer\n\nBenefits:\n• Scale reads horizontally\n• Reduce master load\n• Improve performance\n• Geographic distribution\n• Cost optimization\n\nImplementation:\n\n1. Application-Level:\n• Code decides routing\n• Full control\n• More complex\n• Framework support\n\n2. Proxy-Level:\n• ProxySQL, MaxScale\n• Automatic routing\n• Connection pooling\n• Transparent to app\n\n3. DNS-Level:\n• Route53, load balancer\n• Simple setup\n• Less flexible\n\nChallenges:\n• Replication lag\n• Consistency issues\n• Session affinity\n• Transaction routing\n• Failover complexity\n\nSolutions:\n• Read from master when needed\n• Sticky sessions\n• Async operations\n• Cache recent writes\n• Monitor lag\n\nBest Practices:\n• Route writes to master\n• Distribute reads to replicas\n• Handle lag gracefully\n• Use read-after-write consistency when needed\n• Monitor replication health\n• Test failover\n• Document routing logic",
      "explanation": "Read/write splitting routes writes to master and reads to replicas, scaling read capacity horizontally. Handle replication lag and implement read-after-write consistency for critical operations.",
      "difficulty": "Hard",
      "code": "// Node.js application-level splitting\nconst mysql = require('mysql2/promise');\n\nconst masterPool = mysql.createPool({\n    host: 'master-host',\n    user: 'app',\n    password: 'password',\n    database: 'myapp',\n    connectionLimit: 10\n});\n\nconst slavePool = mysql.createPool({\n    host: 'slave-host',\n    user: 'app_readonly',\n    password: 'password',\n    database: 'myapp',\n    connectionLimit: 50  // More read connections\n});\n\nclass Database {\n    // Write operations\n    static async write(sql, params) {\n        return await masterPool.execute(sql, params);\n    }\n    \n    // Read operations\n    static async read(sql, params) {\n        return await slavePool.execute(sql, params);\n    }\n    \n    // Read from master (consistency required)\n    static async readFromMaster(sql, params) {\n        return await masterPool.execute(sql, params);\n    }\n}\n\n// Usage\nasync function createUser(userData) {\n    const [result] = await Database.write(\n        'INSERT INTO users (name, email) VALUES (?, ?)',\n        [userData.name, userData.email]\n    );\n    return result.insertId;\n}\n\nasync function getUser(userId) {\n    const [rows] = await Database.read(\n        'SELECT * FROM users WHERE id = ?',\n        [userId]\n    );\n    return rows[0];\n}\n\n// Read-after-write: Use master for recent writes\nconst recentWrites = new Set();\n\nasync function getUserSafe(userId) {\n    // If recently written, read from master\n    if (recentWrites.has(userId)) {\n        const [rows] = await Database.readFromMaster(\n            'SELECT * FROM users WHERE id = ?',\n            [userId]\n        );\n        return rows[0];\n    }\n    \n    // Otherwise read from slave\n    return await getUser(userId);\n}\n\nasync function updateUser(userId, data) {\n    await Database.write(\n        'UPDATE users SET name = ? WHERE id = ?',\n        [data.name, userId]\n    );\n    \n    // Mark as recently written\n    recentWrites.add(userId);\n    \n    // Remove after replication lag window (e.g., 1 second)\n    setTimeout(() => recentWrites.delete(userId), 1000);\n}\n\n// Multiple read replicas with load balancing\nconst slaves = [\n    mysql.createPool({ host: 'slave1', user: 'app', database: 'myapp' }),\n    mysql.createPool({ host: 'slave2', user: 'app', database: 'myapp' }),\n    mysql.createPool({ host: 'slave3', user: 'app', database: 'myapp' })\n];\n\nlet slaveIndex = 0;\n\nfunction getSlavePool() {\n    // Round-robin\n    const pool = slaves[slaveIndex];\n    slaveIndex = (slaveIndex + 1) % slaves.length;\n    return pool;\n}\n\nasync function readWithLoadBalancing(sql, params) {\n    const pool = getSlavePool();\n    return await pool.execute(sql, params);\n}\n\n// Java with routing data source\nimport org.springframework.jdbc.datasource.lookup.AbstractRoutingDataSource;\n\npublic class RoutingDataSource extends AbstractRoutingDataSource {\n    \n    private static final ThreadLocal<String> contextHolder = new ThreadLocal<>();\n    \n    public static void setReadOnly() {\n        contextHolder.set(\"slave\");\n    }\n    \n    public static void setReadWrite() {\n        contextHolder.set(\"master\");\n    }\n    \n    public static void clear() {\n        contextHolder.remove();\n    }\n    \n    @Override\n    protected Object determineCurrentLookupKey() {\n        return contextHolder.get();\n    }\n}\n\n// Spring configuration\n@Configuration\npublic class DataSourceConfig {\n    \n    @Bean\n    public DataSource routingDataSource() {\n        RoutingDataSource routing = new RoutingDataSource();\n        \n        Map<Object, Object> dataSources = new HashMap<>();\n        dataSources.put(\"master\", masterDataSource());\n        dataSources.put(\"slave\", slaveDataSource());\n        \n        routing.setTargetDataSources(dataSources);\n        routing.setDefaultTargetDataSource(masterDataSource());\n        \n        return routing;\n    }\n    \n    private DataSource masterDataSource() {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:mysql://master-host/myapp\");\n        config.setMaximumPoolSize(10);\n        return new HikariDataSource(config);\n    }\n    \n    private DataSource slaveDataSource() {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:mysql://slave-host/myapp\");\n        config.setMaximumPoolSize(50);\n        config.setReadOnly(true);\n        return new HikariDataSource(config);\n    }\n}\n\n// Aspect for automatic routing\n@Aspect\n@Component\npublic class DataSourceAspect {\n    \n    @Around(\"@annotation(org.springframework.transaction.annotation.Transactional)\")\n    public Object route(ProceedingJoinPoint pjp) throws Throwable {\n        try {\n            // Check if read-only\n            Transactional tx = getTransactionalAnnotation(pjp);\n            if (tx != null && tx.readOnly()) {\n                RoutingDataSource.setReadOnly();\n            } else {\n                RoutingDataSource.setReadWrite();\n            }\n            \n            return pjp.proceed();\n        } finally {\n            RoutingDataSource.clear();\n        }\n    }\n}\n\n// Service usage\n@Service\npublic class UserService {\n    \n    @Transactional(readOnly = true)  // Routes to slave\n    public User getUser(Long id) {\n        return userRepository.findById(id).orElse(null);\n    }\n    \n    @Transactional  // Routes to master\n    public void createUser(User user) {\n        userRepository.save(user);\n    }\n}\n\n-- ProxySQL configuration for automatic routing\n-- Install ProxySQL\nsudo apt install proxysql\n\n-- Configure servers\nmysql -u admin -padmin -h 127.0.0.1 -P6032\n\nINSERT INTO mysql_servers (hostgroup_id, hostname, port, weight) VALUES\n    (1, 'master-host', 3306, 1000),\n    (2, 'slave1-host', 3306, 1000),\n    (2, 'slave2-host', 3306, 1000),\n    (2, 'slave3-host', 3306, 1000);\n\nLOAD MYSQL SERVERS TO RUNTIME;\nSAVE MYSQL SERVERS TO DISK;\n\n-- Query routing rules\nINSERT INTO mysql_query_rules (\n    rule_id, active, match_pattern, destination_hostgroup, apply\n) VALUES\n    -- Writes to master\n    (1, 1, '^INSERT', 1, 1),\n    (2, 1, '^UPDATE', 1, 1),\n    (3, 1, '^DELETE', 1, 1),\n    (4, 1, '^REPLACE', 1, 1),\n    (5, 1, '^CREATE', 1, 1),\n    (6, 1, '^ALTER', 1, 1),\n    \n    -- SELECT FOR UPDATE to master (locks)\n    (10, 1, '^SELECT.*FOR UPDATE', 1, 1),\n    \n    -- Reads to slaves\n    (100, 1, '^SELECT', 2, 1);\n\nLOAD MYSQL QUERY RULES TO RUNTIME;\nSAVE MYSQL QUERY RULES TO DISK;\n\n-- Application connects to ProxySQL (transparent)\nconst pool = mysql.createPool({\n    host: 'proxysql-host',\n    port: 6033,\n    user: 'app',\n    password: 'password',\n    database: 'myapp'\n});\n// Writes and reads routed automatically\n\n-- Monitor routing stats\nSELECT \n    hostgroup,\n    srv_host,\n    queries,\n    bytes_sent,\n    bytes_recv\nFROM stats_mysql_connection_pool;\n\n-- Python with separate connections\nimport pymysql\nfrom contextlib import contextmanager\n\nmaster_config = {'host': 'master', 'user': 'app', 'db': 'myapp'}\nslave_config = {'host': 'slave', 'user': 'app', 'db': 'myapp'}\n\n@contextmanager\ndef get_master():\n    conn = pymysql.connect(master_config)\n    try:\n        yield conn\n    finally:\n        conn.close()\n\n@contextmanager\ndef get_slave():\n    conn = pymysql.connect(slave_config)\n    try:\n        yield conn\n    finally:\n        conn.close()\n\n# Usage\ndef get_user(user_id):\n    with get_slave() as conn:\n        cursor = conn.cursor()\n        cursor.execute('SELECT * FROM users WHERE id = %s', (user_id,))\n        return cursor.fetchone()\n\ndef create_user(name, email):\n    with get_master() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\n            'INSERT INTO users (name, email) VALUES (%s, %s)',\n            (name, email)\n        )\n        conn.commit()\n        return cursor.lastrowid"
    },
    {
      "id": 97,
      "question": "What are database migration tools and version control?",
      "answer": "Database Migrations: Version control for schema\n\nWhat Are Migrations:\n• SQL scripts for schema changes\n• Version controlled\n• Up (apply) and down (revert)\n• Tracked execution\n• Reproducible\n• Team collaboration\n\nBenefits:\n• Version history\n• Reproducible deployments\n• Rollback capability\n• Team coordination\n• Environment consistency\n• Automated deployment\n• Audit trail\n\nPopular Tools:\n• Flyway (Java)\n• Liquibase (XML/JSON/SQL)\n• Alembic (Python)\n• Knex (Node.js)\n• Entity Framework Migrations (.NET)\n• Rails ActiveRecord Migrations\n• Prisma Migrate\n\nMigration Naming:\n• Timestamp prefix\n• Version number\n• Descriptive name\n• Example: V1__create_users_table.sql\n• Example: 20240115120000_add_email_index.sql\n\nBest Practices:\n• One change per migration\n• Test before production\n• Backup before migrate\n• Make reversible\n• Avoid data loss\n• Use transactions\n• Document complex changes\n• Review migrations\n• Automate in CI/CD\n\nSafe Patterns:\n• Add columns (nullable)\n• Add indexes (online)\n• Add tables\n• Expand constraints\n\nDangerous Patterns:\n• Drop columns (data loss)\n• Rename (breaks code)\n• Change types (data loss)\n• Drop tables\n\nZero-Downtime:\n• Multi-phase migrations\n• Backward compatible\n• Feature flags",
      "explanation": "Database migration tools like Flyway and Liquibase version control schema changes, enabling reproducible deployments, rollbacks, and team collaboration. Follow safe patterns and test thoroughly.",
      "difficulty": "Medium",
      "code": "-- Flyway migration files\n-- V1__create_users_table.sql\nCREATE TABLE users (\n    id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX idx_email ON users(email);\n\n-- V2__add_users_status.sql\nALTER TABLE users ADD COLUMN status VARCHAR(20) DEFAULT 'ACTIVE';\n\n-- V3__create_orders_table.sql\nCREATE TABLE orders (\n    id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    user_id BIGINT NOT NULL,\n    total DECIMAL(10,2) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (user_id) REFERENCES users(id)\n);\n\n-- Flyway configuration (flyway.conf)\nflyway.url=jdbc:mysql://localhost:3306/myapp\nflyway.user=root\nflyway.password=password\nflyway.locations=filesystem:db/migrations\nflyway.baselineOnMigrate=true\n\n# Run migrations\nflyway migrate\n\n# Check status\nflyway info\n\n# Rollback (Flyway Teams)\nflyway undo\n\n-- Liquibase changelog (db.changelog-master.xml)\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<databaseChangeLog\n    xmlns=\"http://www.liquibase.org/xml/ns/dbchangelog\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://www.liquibase.org/xml/ns/dbchangelog\n        http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-4.0.xsd\">\n    \n    <changeSet id=\"1\" author=\"john\">\n        <createTable tableName=\"users\">\n            <column name=\"id\" type=\"BIGINT\" autoIncrement=\"true\">\n                <constraints primaryKey=\"true\"/>\n            </column>\n            <column name=\"name\" type=\"VARCHAR(100)\">\n                <constraints nullable=\"false\"/>\n            </column>\n            <column name=\"email\" type=\"VARCHAR(255)\">\n                <constraints nullable=\"false\"/>\n            </column>\n        </createTable>\n        \n        <createIndex indexName=\"idx_email\" tableName=\"users\">\n            <column name=\"email\"/>\n        </createIndex>\n    </changeSet>\n    \n    <changeSet id=\"2\" author=\"jane\">\n        <addColumn tableName=\"users\">\n            <column name=\"status\" type=\"VARCHAR(20)\" defaultValue=\"ACTIVE\"/>\n        </addColumn>\n    </changeSet>\n    \n    <changeSet id=\"3\" author=\"john\">\n        <createTable tableName=\"orders\">\n            <column name=\"id\" type=\"BIGINT\" autoIncrement=\"true\">\n                <constraints primaryKey=\"true\"/>\n            </column>\n            <column name=\"user_id\" type=\"BIGINT\">\n                <constraints nullable=\"false\" foreignKeyName=\"fk_orders_user\" \n                    references=\"users(id)\"/>\n            </column>\n            <column name=\"total\" type=\"DECIMAL(10,2)\">\n                <constraints nullable=\"false\"/>\n            </column>\n        </createTable>\n    </changeSet>\n    \n</databaseChangeLog>\n\n# Run Liquibase\nliquibase --changelog-file=db.changelog-master.xml update\n\n# Rollback\nliquibase rollback-count 1\nliquibase rollback-to-tag v1.0\n\n# Node.js Knex migrations\nconst Knex = require('knex');\n\nconst knex = Knex({\n    client: 'mysql2',\n    connection: {\n        host: 'localhost',\n        user: 'root',\n        password: 'password',\n        database: 'myapp'\n    }\n});\n\n// Create migration\n// npx knex migrate:make create_users_table\n\n// migrations/20240115120000_create_users_table.js\nexports.up = function(knex) {\n    return knex.schema.createTable('users', table => {\n        table.bigIncrements('id').primary();\n        table.string('name', 100).notNullable();\n        table.string('email', 255).notNullable();\n        table.timestamp('created_at').defaultTo(knex.fn.now());\n        \n        table.index('email');\n    });\n};\n\nexports.down = function(knex) {\n    return knex.schema.dropTable('users');\n};\n\n// migrations/20240115130000_add_users_status.js\nexports.up = function(knex) {\n    return knex.schema.table('users', table => {\n        table.string('status', 20).defaultTo('ACTIVE');\n    });\n};\n\nexports.down = function(knex) {\n    return knex.schema.table('users', table => {\n        table.dropColumn('status');\n    });\n};\n\n// Run migrations\nknex.migrate.latest()\n    .then(() => console.log('Migrations complete'))\n    .catch(err => console.error(err));\n\n// Rollback\nknex.migrate.rollback()\n    .then(() => console.log('Rollback complete'));\n\n# Python Alembic\n# Initialize\nalembic init migrations\n\n# Create migration\nalembic revision -m \"create users table\"\n\n# migrations/versions/001_create_users_table.py\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade():\n    op.create_table(\n        'users',\n        sa.Column('id', sa.BigInteger, primary_key=True),\n        sa.Column('name', sa.String(100), nullable=False),\n        sa.Column('email', sa.String(255), nullable=False),\n        sa.Column('created_at', sa.DateTime, server_default=sa.func.now())\n    )\n    op.create_index('idx_email', 'users', ['email'])\n\ndef downgrade():\n    op.drop_index('idx_email', 'users')\n    op.drop_table('users')\n\n# Run migrations\nalembic upgrade head\n\n# Rollback\nalembic downgrade -1\n\n# Zero-downtime migration pattern\n# Phase 1: Add new column (nullable)\n-- V1__add_full_name_column.sql\nALTER TABLE users ADD COLUMN full_name VARCHAR(200) NULL;\n\n# Phase 2: Backfill data\n-- V2__backfill_full_name.sql\nUPDATE users \nSET full_name = CONCAT(first_name, ' ', last_name)\nWHERE full_name IS NULL;\n\n# Phase 3: Make not null\n-- V3__full_name_not_null.sql\nALTER TABLE users MODIFY COLUMN full_name VARCHAR(200) NOT NULL;\n\n# Phase 4: Drop old columns (after code deployed)\n-- V4__drop_name_columns.sql\nALTER TABLE users DROP COLUMN first_name;\nALTER TABLE users DROP COLUMN last_name;\n\n-- Track migrations manually\nCREATE TABLE schema_migrations (\n    version VARCHAR(255) PRIMARY KEY,\n    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nINSERT INTO schema_migrations (version) VALUES ('V1__create_users');\n\n-- CI/CD integration (GitHub Actions)\nname: Database Migration\n\non:\n  push:\n    branches: [main]\n\njobs:\n  migrate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      \n      - name: Run Flyway migrations\n        run: |\n          flyway -url=${{ secrets.DB_URL }} \\\n                 -user=${{ secrets.DB_USER }} \\\n                 -password=${{ secrets.DB_PASSWORD }} \\\n                 migrate"
    },
    {
      "id": 98,
      "question": "What are database performance anti-patterns?",
      "answer": "Anti-Patterns: Common mistakes that hurt performance\n\nN+1 Query Problem:\n• Loop executes query per item\n• Should use JOIN or IN clause\n• Massive performance killer\n• Use eager loading\n\nSELECT * Usage:\n• Fetches unnecessary columns\n• Wastes bandwidth\n• Prevents covering indexes\n• Select only needed columns\n\nMissing Indexes:\n• Full table scans\n• Slow WHERE clauses\n• Slow JOINs\n• Slow ORDER BY\n\nOver-Indexing:\n• Slows writes\n• Wastes space\n• Index maintenance overhead\n• Balance read vs write\n\nLIKE with Leading Wildcard:\n• LIKE '%value' can't use index\n• Full table scan\n• Use full-text search instead\n\nOR in WHERE:\n• Often prevents index use\n• Use UNION or IN instead\n\nFunctions in WHERE:\n• WHERE YEAR(date) = 2024\n• Prevents index use\n• Use range instead\n\nLarge OFFSET:\n• LIMIT 1000000, 10\n• Scans all skipped rows\n• Use cursor-based pagination\n\nToo Many Joins:\n• > 5 joins = complexity\n• Consider denormalization\n• Use materialized views\n\nNo Connection Pooling:\n• Connection overhead\n• Resource waste\n• Poor scalability\n\nLong Transactions:\n• Locks held too long\n• Blocking queries\n• Keep transactions short\n\nPrevention:\n• Review query plans\n• Monitor slow queries\n• Load testing\n• Code reviews\n• Use ORMs wisely",
      "explanation": "Avoid common anti-patterns like N+1 queries, SELECT *, missing indexes, functions in WHERE clauses, and large OFFSETs. Review query plans and monitor slow queries to catch issues early.",
      "difficulty": "Medium",
      "code": "-- N+1 Query Problem\n-- BAD: Execute query for each user\nSELECT * FROM users;\n-- For each user:\n--   SELECT * FROM orders WHERE user_id = ?;\n-- Result: 1 + N queries\n\n-- GOOD: Single query with JOIN\nSELECT \n    u.*,\n    o.id AS order_id,\n    o.total AS order_total\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id;\n-- Result: 1 query\n\n// Node.js N+1 example\n// BAD\nconst users = await db.query('SELECT * FROM users');\nfor (const user of users) {\n    // N+1 problem!\n    const orders = await db.query(\n        'SELECT * FROM orders WHERE user_id = ?',\n        [user.id]\n    );\n    user.orders = orders;\n}\n\n// GOOD: Use IN clause\nconst users = await db.query('SELECT * FROM users');\nconst userIds = users.map(u => u.id);\n\nconst orders = await db.query(\n    'SELECT * FROM orders WHERE user_id IN (?)',\n    [userIds]\n);\n\n// Group orders by user_id\nconst ordersByUser = {};\nfor (const order of orders) {\n    if (!ordersByUser[order.user_id]) {\n        ordersByUser[order.user_id] = [];\n    }\n    ordersByUser[order.user_id].push(order);\n}\n\nfor (const user of users) {\n    user.orders = ordersByUser[user.id] || [];\n}\n\n-- SELECT * Problem\n-- BAD: Fetches all columns (wasteful)\nSELECT * FROM users WHERE id = 123;\n\n-- GOOD: Select only needed columns\nSELECT id, name, email FROM users WHERE id = 123;\n\n-- BAD: Prevents covering index\nSELECT * FROM orders WHERE user_id = 123 ORDER BY created_at DESC;\n\n-- GOOD: Covering index can satisfy query\nSELECT id, user_id, total, created_at \nFROM orders \nWHERE user_id = 123 \nORDER BY created_at DESC;\n\n-- Create covering index\nCREATE INDEX idx_orders_covering \nON orders(user_id, created_at, id, total);\n\n-- Function in WHERE (bad)\n-- BAD: Can't use index on created_at\nSELECT * FROM orders\nWHERE YEAR(created_at) = 2024\nAND MONTH(created_at) = 1;\n\n-- GOOD: Use range (can use index)\nSELECT * FROM orders\nWHERE created_at >= '2024-01-01'\nAND created_at < '2024-02-01';\n\n-- BAD: Function prevents index use\nSELECT * FROM users WHERE LOWER(email) = 'john@example.com';\n\n-- GOOD: Store lowercase, or use collation\nSELECT * FROM users WHERE email = 'john@example.com';\n-- With case-insensitive collation\n\n-- LIKE with leading wildcard\n-- BAD: Can't use index\nSELECT * FROM products WHERE name LIKE '%phone%';\n\n-- GOOD: No leading wildcard\nSELECT * FROM products WHERE name LIKE 'phone%';\n\n-- BEST: Full-text search\nCREATE FULLTEXT INDEX idx_name ON products(name);\n\nSELECT * FROM products\nWHERE MATCH(name) AGAINST('phone' IN NATURAL LANGUAGE MODE);\n\n-- OR in WHERE\n-- BAD: Often can't use index effectively\nSELECT * FROM users\nWHERE status = 'ACTIVE' OR status = 'PENDING';\n\n-- GOOD: Use IN\nSELECT * FROM users\nWHERE status IN ('ACTIVE', 'PENDING');\n\n-- BAD: Multiple columns with OR\nSELECT * FROM users\nWHERE first_name = 'John' OR last_name = 'Smith';\n\n-- GOOD: Use UNION\nSELECT * FROM users WHERE first_name = 'John'\nUNION\nSELECT * FROM users WHERE last_name = 'Smith';\n\n-- Large OFFSET problem\n-- BAD: Scans and discards 1M rows\nSELECT * FROM orders\nORDER BY id\nLIMIT 1000000, 10;\n\n-- GOOD: Cursor-based pagination\nSELECT * FROM orders\nWHERE id > 1000000\nORDER BY id\nLIMIT 10;\n\n-- BETTER: Remember last ID from previous page\nSELECT * FROM orders\nWHERE id > :lastSeenId\nORDER BY id\nLIMIT 10;\n\n-- Missing index\n-- Check slow query log\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 1;\n\n-- Analyze query\nEXPLAIN SELECT * FROM orders WHERE user_id = 123;\n-- type: ALL (bad - full table scan)\n\n-- Add index\nCREATE INDEX idx_user_id ON orders(user_id);\n\n-- Verify\nEXPLAIN SELECT * FROM orders WHERE user_id = 123;\n-- type: ref (good - using index)\n\n-- Over-indexing problem\n-- BAD: Too many indexes\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    category VARCHAR(50),\n    price DECIMAL(10,2),\n    stock INT,\n    created_at TIMESTAMP,\n    updated_at TIMESTAMP\n);\n\n-- Don't create every possible index!\nCREATE INDEX idx_name ON products(name);\nCREATE INDEX idx_category ON products(category);\nCREATE INDEX idx_price ON products(price);\nCREATE INDEX idx_stock ON products(stock);\nCREATE INDEX idx_created ON products(created_at);\nCREATE INDEX idx_updated ON products(updated_at);\n-- Each index slows INSERT/UPDATE/DELETE\n\n-- GOOD: Only indexes actually used\nCREATE INDEX idx_category_price ON products(category, price);\nCREATE INDEX idx_name ON products(name);\n-- Composite index can handle:\n-- WHERE category = X\n-- WHERE category = X AND price > Y\n\n-- Analyze index usage\nSELECT \n    index_name,\n    table_name,\n    cardinality,\n    stat_value AS rows_read\nFROM information_schema.statistics s\nLEFT JOIN mysql.index_stats i ON s.table_name = i.table_name \n    AND s.index_name = i.index_name\nWHERE s.table_schema = 'myapp'\nORDER BY stat_value DESC;\n\n-- Remove unused indexes\nDROP INDEX idx_unused ON products;\n\n-- Long transaction problem\n-- BAD: Long transaction holding locks\nBEGIN;\n\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n\n-- ... lots of business logic\n-- ... external API calls\n-- ... user interaction\n\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n\nCOMMIT;\n\n-- GOOD: Short transaction\n-- Do business logic first\nconst amount = calculateAmount();\nconst isValid = await validateTransaction();\n\nif (!isValid) {\n    throw new Error('Invalid transaction');\n}\n\n// Then short transaction\nawait connection.beginTransaction();\ntry {\n    await connection.execute(\n        'UPDATE accounts SET balance = balance - ? WHERE id = ?',\n        [amount, fromAccount]\n    );\n    await connection.execute(\n        'UPDATE accounts SET balance = balance + ? WHERE id = ?',\n        [amount, toAccount]\n    );\n    await connection.commit();\n} catch (error) {\n    await connection.rollback();\n    throw error;\n}\n\n-- Correlated subquery (often slow)\n-- BAD: Executes subquery for each row\nSELECT \n    u.name,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) AS order_count\nFROM users u;\n\n-- GOOD: Use JOIN\nSELECT \n    u.name,\n    COUNT(o.id) AS order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;"
    },
    {
      "id": 99,
      "question": "What are database security best practices?",
      "answer": "Database Security: Protect sensitive data\n\nAuthentication:\n• Strong passwords\n• Principle of least privilege\n• Role-based access (RBAC)\n• Application-specific users\n• Disable root remote access\n• MFA when possible\n\nAuthorization:\n• Grant minimum permissions\n• Separate read/write users\n• Schema-level permissions\n• Row-level security\n• Column-level permissions\n• Audit privileged access\n\nEncryption:\n• At rest (disk encryption)\n• In transit (TLS/SSL)\n• Backup encryption\n• Column encryption (sensitive data)\n• Key management (KMS)\n\nSQL Injection Prevention:\n• Prepared statements\n• Parameterized queries\n• Input validation\n• ORM usage\n• Stored procedures\n• Least privilege\n\nNetwork Security:\n• Firewall rules\n• VPC/private network\n• Bastion hosts\n• No public access\n• IP whitelisting\n\nAudit Logging:\n• Log all access\n• Monitor suspicious activity\n• Alert on anomalies\n• Retain logs\n• Compliance (GDPR, HIPAA)\n\nBackup Security:\n• Encrypt backups\n• Secure storage\n• Test restores\n• Access controls\n\nRegular Maintenance:\n• Apply security patches\n• Update database version\n• Review permissions\n• Remove unused accounts\n• Rotate passwords\n• Scan for vulnerabilities",
      "explanation": "Secure databases with strong authentication, encryption at rest and in transit, prepared statements to prevent SQL injection, network isolation, audit logging, and regular security maintenance.",
      "difficulty": "Hard",
      "code": "-- Create application user with limited privileges\nCREATE USER 'app_user'@'app-server-ip' IDENTIFIED BY 'strong_password_here!';\n\n-- Grant only necessary permissions\nGRANT SELECT, INSERT, UPDATE ON myapp.users TO 'app_user'@'app-server-ip';\nGRANT SELECT, INSERT, UPDATE ON myapp.orders TO 'app_user'@'app-server-ip';\n-- No DELETE or admin privileges\n\nFLUSH PRIVILEGES;\n\n-- Read-only user\nCREATE USER 'readonly'@'reporting-server' IDENTIFIED BY 'password';\nGRANT SELECT ON myapp.* TO 'readonly'@'reporting-server';\n\n-- Disable root remote access\n-- Edit /etc/mysql/mysql.conf.d/mysqld.cnf\n[mysqld]\nbind-address = 127.0.0.1\n-- Root can only connect from localhost\n\n-- Or restrict by IP\nCREATE USER 'admin'@'admin-ip' IDENTIFIED BY 'strong_password';\nGRANT ALL PRIVILEGES ON *.* TO 'admin'@'admin-ip' WITH GRANT OPTION;\n\n-- Row-level security (PostgreSQL)\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INT NOT NULL,\n    total DECIMAL(10,2),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Enable RLS\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\n\n-- Policy: Users see only their orders\nCREATE POLICY user_orders ON orders\n    FOR SELECT\n    USING (user_id = current_setting('app.current_user_id')::int);\n\n-- Application sets user context\nSET app.current_user_id = 123;\nSELECT * FROM orders;  -- Only returns orders for user 123\n\n-- MySQL encryption at rest\n-- Edit my.cnf\n[mysqld]\nearly-plugin-load=keyring_file.so\nkeyring_file_data=/var/lib/mysql-keyring/keyring\n\n-- Enable encryption for new tables\nCREATE TABLE sensitive_data (\n    id INT PRIMARY KEY,\n    ssn VARCHAR(11) ENCRYPTED,\n    credit_card VARCHAR(19) ENCRYPTED\n) ENCRYPTION='Y';\n\n-- Encrypt existing table\nALTER TABLE users ENCRYPTION='Y';\n\n-- Encrypt backups\nmysqldump -u root -p \\\n    --single-transaction \\\n    --all-databases \\\n    | openssl enc -aes-256-cbc -salt -k 'encryption_password' \\\n    > encrypted_backup.sql.enc\n\n-- Decrypt and restore\nopenssl enc -aes-256-cbc -d -k 'encryption_password' \\\n    -in encrypted_backup.sql.enc \\\n    | mysql -u root -p\n\n-- SSL/TLS connection (MySQL)\n-- Server configuration (my.cnf)\n[mysqld]\nssl-ca=/etc/mysql/ssl/ca.pem\nssl-cert=/etc/mysql/ssl/server-cert.pem\nssl-key=/etc/mysql/ssl/server-key.pem\nrequire_secure_transport=ON\n\n-- Require SSL for user\nCREATE USER 'app_user'@'%' IDENTIFIED BY 'password' REQUIRE SSL;\n\n-- Client connection\nmysql -u app_user -p \\\n    --ssl-ca=/path/to/ca.pem \\\n    --ssl-mode=REQUIRED\n\n// Node.js with SSL\nconst pool = mysql.createPool({\n    host: 'db-host',\n    user: 'app_user',\n    password: 'password',\n    database: 'myapp',\n    ssl: {\n        ca: fs.readFileSync('/path/to/ca.pem')\n    }\n});\n\n-- Audit logging (MySQL)\n-- Install audit plugin\nINSTALL PLUGIN audit_log SONAME 'audit_log.so';\n\n-- Configure (my.cnf)\n[mysqld]\naudit_log_format=JSON\naudit_log_file=/var/log/mysql/audit.log\naudit_log_policy=ALL\naudit_log_rotate_on_size=100M\naudit_log_rotations=10\n\n-- PostgreSQL audit with pgAudit\nCREATE EXTENSION pgaudit;\n\n-- Configure (postgresql.conf)\nshared_preload_libraries = 'pgaudit'\npgaudit.log = 'all'\npgaudit.log_catalog = off\npgaudit.log_parameter = on\n\n-- Monitor audit logs\nSELECT \n    timestamp,\n    user_name,\n    command_class,\n    object_name,\n    sql_text\nFROM audit_log\nWHERE timestamp > NOW() - INTERVAL 1 HOUR\nAND command_class IN ('WRITE', 'DDL')\nORDER BY timestamp DESC;\n\n-- SQL injection prevention\n// BAD: Vulnerable to SQL injection\nconst userId = req.params.id;\nconst query = `SELECT * FROM users WHERE id = ${userId}`;\nconst [rows] = await connection.query(query);\n// Attack: /users/1 OR 1=1\n\n// GOOD: Prepared statement\nconst [rows] = await connection.execute(\n    'SELECT * FROM users WHERE id = ?',\n    [userId]\n);\n\n// Input validation\nfunction validateUserId(id) {\n    const userId = parseInt(id, 10);\n    if (isNaN(userId) || userId <= 0) {\n        throw new Error('Invalid user ID');\n    }\n    return userId;\n}\n\nconst userId = validateUserId(req.params.id);\n\n-- Column-level encryption\n-- MySQL AES encryption\nCREATE TABLE users (\n    id INT PRIMARY KEY,\n    email VARCHAR(255),\n    ssn VARBINARY(255)  -- Encrypted\n);\n\n-- Encrypt on insert\nINSERT INTO users (id, email, ssn)\nVALUES (1, 'john@example.com', AES_ENCRYPT('123-45-6789', 'encryption_key'));\n\n-- Decrypt on select\nSELECT \n    id,\n    email,\n    CAST(AES_DECRYPT(ssn, 'encryption_key') AS CHAR) AS ssn\nFROM users;\n\n-- Better: Use application-level encryption with proper key management\n// Node.js with crypto\nconst crypto = require('crypto');\nconst algorithm = 'aes-256-gcm';\nconst key = Buffer.from(process.env.ENCRYPTION_KEY, 'hex');  // 32 bytes\n\nfunction encrypt(text) {\n    const iv = crypto.randomBytes(16);\n    const cipher = crypto.createCipheriv(algorithm, key, iv);\n    \n    let encrypted = cipher.update(text, 'utf8', 'hex');\n    encrypted += cipher.final('hex');\n    \n    const authTag = cipher.getAuthTag();\n    \n    return {\n        iv: iv.toString('hex'),\n        authTag: authTag.toString('hex'),\n        encrypted: encrypted\n    };\n}\n\nfunction decrypt(encrypted, iv, authTag) {\n    const decipher = crypto.createDecipheriv(\n        algorithm,\n        key,\n        Buffer.from(iv, 'hex')\n    );\n    \n    decipher.setAuthTag(Buffer.from(authTag, 'hex'));\n    \n    let decrypted = decipher.update(encrypted, 'hex', 'utf8');\n    decrypted += decipher.final('utf8');\n    \n    return decrypted;\n}\n\n// Store encrypted data\nconst ssn = '123-45-6789';\nconst { iv, authTag, encrypted } = encrypt(ssn);\n\nawait connection.execute(\n    'INSERT INTO users (email, ssn, ssn_iv, ssn_auth) VALUES (?, ?, ?, ?)',\n    ['john@example.com', encrypted, iv, authTag]\n);\n\n-- Security audit checklist\n-- 1. Check user privileges\nSELECT user, host, authentication_string FROM mysql.user;\nSHOW GRANTS FOR 'app_user'@'host';\n\n-- 2. Find users with excessive privileges\nSELECT DISTINCT user, host\nFROM mysql.user\nWHERE (Select_priv = 'Y' OR Insert_priv = 'Y' OR Update_priv = 'Y' \n       OR Delete_priv = 'Y' OR Create_priv = 'Y' OR Drop_priv = 'Y')\nAND user NOT IN ('root', 'mysql.sys')\nORDER BY user;\n\n-- 3. Check for blank passwords\nSELECT user, host FROM mysql.user WHERE authentication_string = '';\n\n-- 4. Review failed login attempts\nSELECT * FROM mysql.general_log \nWHERE command_type = 'Connect' \nAND argument LIKE '%Access denied%'\nORDER BY event_time DESC LIMIT 100;"
    },
    {
      "id": 100,
      "question": "What are database scalability patterns and when to use them?",
      "answer": "Database Scalability: Handle growing data and traffic\n\nVertical Scaling (Scale Up):\n• More CPU/RAM/Storage\n• Simpler implementation\n• Single server limits\n• Expensive\n• Downtime for upgrades\n\nHorizontal Scaling (Scale Out):\n• Add more servers\n• Distributed system\n• Complex implementation\n• Better cost/performance\n• High availability\n\nScalability Patterns:\n\n1. Read Replicas:\n• Master for writes\n• Replicas for reads\n• Scales read capacity\n• Geographic distribution\n\n2. Sharding:\n• Split data across servers\n• Horizontal partitioning\n• Each shard independent\n• Complex queries harder\n• Shard key critical\n\n3. Caching:\n• Redis/Memcached\n• Reduce database load\n• Fast access\n• Cache invalidation needed\n\n4. CQRS:\n• Separate read/write models\n• Optimize independently\n• Event sourcing\n• Eventually consistent\n\n5. Partitioning:\n• Range, hash, list\n• Within single database\n• Partition pruning\n• Better performance\n\nWhen to Use:\n• < 1M records: Single server\n• 1M-10M: Indexes + caching\n• 10M-100M: Read replicas\n• 100M+: Sharding/partitioning\n• High read: Replicas + cache\n• High write: Sharding\n• Global: Multi-region\n\nBest Practices:\n• Plan for scale early\n• Monitor metrics\n• Test at scale\n• Gradual migration\n• Document architecture\n• Automate operations",
      "explanation": "Scale databases vertically (bigger server) for simplicity or horizontally (more servers) for capacity. Use read replicas for read-heavy, sharding for write-heavy, and caching to reduce load.",
      "difficulty": "Hard",
      "code": "-- Sharding by user ID\n-- Shard 0: user_id % 4 = 0\n-- Shard 1: user_id % 4 = 1\n-- Shard 2: user_id % 4 = 2\n-- Shard 3: user_id % 4 = 3\n\n// Node.js sharding logic\nclass ShardManager {\n    constructor() {\n        this.shards = [\n            mysql.createPool({ host: 'shard0', database: 'myapp_0' }),\n            mysql.createPool({ host: 'shard1', database: 'myapp_1' }),\n            mysql.createPool({ host: 'shard2', database: 'myapp_2' }),\n            mysql.createPool({ host: 'shard3', database: 'myapp_3' })\n        ];\n    }\n    \n    getShard(userId) {\n        const shardId = userId % this.shards.length;\n        return this.shards[shardId];\n    }\n    \n    async getUser(userId) {\n        const shard = this.getShard(userId);\n        const [rows] = await shard.execute(\n            'SELECT * FROM users WHERE id = ?',\n            [userId]\n        );\n        return rows[0];\n    }\n    \n    async createUser(userId, data) {\n        const shard = this.getShard(userId);\n        await shard.execute(\n            'INSERT INTO users (id, name, email) VALUES (?, ?, ?)',\n            [userId, data.name, data.email]\n        );\n    }\n    \n    // Cross-shard query (scatter-gather)\n    async getAllActiveUsers() {\n        const queries = this.shards.map(shard =>\n            shard.execute('SELECT * FROM users WHERE status = ?', ['ACTIVE'])\n        );\n        \n        const results = await Promise.all(queries);\n        \n        // Merge results from all shards\n        return results.flatMap(([rows]) => rows);\n    }\n}\n\nconst shardManager = new ShardManager();\n\n-- Range-based sharding (by date)\n-- Shard 2023: orders from 2023\n-- Shard 2024: orders from 2024\n\nfunction getShardByDate(orderDate) {\n    const year = orderDate.getFullYear();\n    return pools[`shard_${year}`];\n}\n\n-- Table partitioning (MySQL)\nCREATE TABLE orders (\n    id BIGINT,\n    user_id BIGINT,\n    total DECIMAL(10,2),\n    created_at DATE,\n    PRIMARY KEY (id, created_at)\n)\nPARTITION BY RANGE (YEAR(created_at)) (\n    PARTITION p2022 VALUES LESS THAN (2023),\n    PARTITION p2023 VALUES LESS THAN (2024),\n    PARTITION p2024 VALUES LESS THAN (2025),\n    PARTITION pmax VALUES LESS THAN MAXVALUE\n);\n\n-- Query uses partition pruning\nEXPLAIN SELECT * FROM orders\nWHERE created_at BETWEEN '2024-01-01' AND '2024-12-31';\n-- Only scans p2024 partition\n\n-- Hash partitioning\nCREATE TABLE users (\n    id BIGINT PRIMARY KEY,\n    name VARCHAR(100)\n)\nPARTITION BY HASH(id)\nPARTITIONS 4;\n\n-- Caching layer (Redis)\nconst redis = require('redis');\nconst client = redis.createClient();\n\nclass UserService {\n    async getUser(userId) {\n        // Check cache first\n        const cached = await client.get(`user:${userId}`);\n        if (cached) {\n            return JSON.parse(cached);\n        }\n        \n        // Cache miss: Query database\n        const [rows] = await pool.execute(\n            'SELECT * FROM users WHERE id = ?',\n            [userId]\n        );\n        \n        const user = rows[0];\n        \n        // Store in cache (5 minutes TTL)\n        await client.setEx(\n            `user:${userId}`,\n            300,\n            JSON.stringify(user)\n        );\n        \n        return user;\n    }\n    \n    async updateUser(userId, data) {\n        // Update database\n        await pool.execute(\n            'UPDATE users SET name = ? WHERE id = ?',\n            [data.name, userId]\n        );\n        \n        // Invalidate cache\n        await client.del(`user:${userId}`);\n    }\n}\n\n-- CQRS pattern\n// Write model (command)\nasync function createOrder(orderData) {\n    // Write to transactional database\n    const [result] = await masterDb.execute(\n        'INSERT INTO orders (user_id, total) VALUES (?, ?)',\n        [orderData.userId, orderData.total]\n    );\n    \n    // Publish event\n    await eventBus.publish('OrderCreated', {\n        orderId: result.insertId,\n        userId: orderData.userId,\n        total: orderData.total,\n        timestamp: new Date()\n    });\n    \n    return result.insertId;\n}\n\n// Read model (query)\n// Updated by event handlers\nclass OrderReadModel {\n    async onOrderCreated(event) {\n        // Update denormalized read model\n        await readDb.execute(`\n            INSERT INTO order_summary (order_id, user_id, user_name, total)\n            SELECT ?, u.id, u.name, ?\n            FROM users u\n            WHERE u.id = ?\n        `, [event.orderId, event.total, event.userId]);\n        \n        // Update cache\n        await redis.del(`user:${event.userId}:orders`);\n    }\n}\n\n// Query read model (optimized for reads)\nasync function getUserOrders(userId) {\n    return await readDb.execute(\n        'SELECT * FROM order_summary WHERE user_id = ? ORDER BY created_at DESC',\n        [userId]\n    );\n}\n\n-- Database proxy for automatic routing\n// ProxySQL handles read/write split automatically\nconst pool = mysql.createPool({\n    host: 'proxysql-host',\n    port: 6033,\n    user: 'app',\n    database: 'myapp'\n});\n// Writes -> master, reads -> replicas automatically\n\n-- Multi-region setup (AWS Aurora Global Database)\n-- Primary region: us-east-1\n-- Secondary region: eu-west-1 (read replica)\n\n// Route by region\nfunction getDatabasePool(region) {\n    if (region === 'us-east-1') {\n        return primaryPool;  // Read/write\n    } else {\n        return replicaPool;  // Read-only\n    }\n}\n\n-- Scalability monitoring\n-- Connection count\nSHOW STATUS LIKE 'Threads_connected';\nSHOW STATUS LIKE 'Max_used_connections';\n\n-- Query throughput\nSHOW GLOBAL STATUS LIKE 'Questions';\nSHOW GLOBAL STATUS LIKE 'Queries';\n\n-- Slow queries\nSHOW GLOBAL STATUS LIKE 'Slow_queries';\n\n-- Buffer pool usage\nSHOW STATUS LIKE 'Innodb_buffer_pool%';\n\n-- Replication lag\nSHOW SLAVE STATUS\\G\n-- Check: Seconds_Behind_Master\n\n-- Capacity planning\n-- Current: 10K QPS, 100GB data\n-- Growth: 50% per year\n-- Year 1: 15K QPS, 150GB -> Add read replica\n-- Year 2: 22K QPS, 225GB -> Add more replicas + caching\n-- Year 3: 33K QPS, 338GB -> Consider sharding\n\n// Load testing with k6\nimport http from 'k6/http';\nimport { check } from 'k6';\n\nexport let options = {\n    stages: [\n        { duration: '2m', target: 100 },   // Ramp up to 100 users\n        { duration: '5m', target: 100 },   // Stay at 100 users\n        { duration: '2m', target: 200 },   // Ramp to 200 users\n        { duration: '5m', target: 200 },   // Stay at 200 users\n        { duration: '2m', target: 0 },     // Ramp down\n    ],\n};\n\nexport default function() {\n    let res = http.get('http://api.example.com/users/123');\n    check(res, {\n        'status is 200': (r) => r.status === 200,\n        'response time < 200ms': (r) => r.timings.duration < 200,\n    });\n}"
    }
  ]
}
