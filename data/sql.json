{
  "topic": "SQL",
  "questions": [
    {
      "id": 1,
      "question": "What is SQL and what is it used for?",
      "answer": "SQL (Structured Query Language) is a standardized programming language used for managing and manipulating relational databases.\n\nPrimary Uses:\nâ€¢ Querying data from databases\nâ€¢ Inserting, updating, and deleting records\nâ€¢ Creating and modifying database schemas\nâ€¢ Controlling access to data\nâ€¢ Managing database transactions\n\nKey Characteristics:\nâ€¢ Declarative language - you specify what you want, not how to get it\nâ€¢ Set-based operations - works with sets of data rather than individual records\nâ€¢ ANSI/ISO standard with vendor-specific extensions",
      "explanation": "SQL is a standardized language for interacting with relational databases, allowing users to query, manipulate, and manage data through declarative commands.",
      "difficulty": "Easy",
      "code": "-- Basic SQL query example\nSELECT first_name, last_name, email\nFROM customers\nWHERE country = 'USA'\nORDER BY last_name;"
    },
    {
      "id": 2,
      "question": "What are the different types of SQL commands?",
      "answer": "SQL commands are categorized into five main types based on their functionality.\n\nDDL (Data Definition Language):\nâ€¢ CREATE - creates database objects\nâ€¢ ALTER - modifies database objects\nâ€¢ DROP - deletes database objects\nâ€¢ TRUNCATE - removes all records from table\n\nDML (Data Manipulation Language):\nâ€¢ SELECT - retrieves data\nâ€¢ INSERT - adds new records\nâ€¢ UPDATE - modifies existing records\nâ€¢ DELETE - removes records\n\nDCL (Data Control Language):\nâ€¢ GRANT - gives user access privileges\nâ€¢ REVOKE - removes user access privileges\n\nTCL (Transaction Control Language):\nâ€¢ COMMIT - saves transaction changes\nâ€¢ ROLLBACK - undoes transaction changes\nâ€¢ SAVEPOINT - creates points within transactions\n\nDQL (Data Query Language):\nâ€¢ SELECT - sometimes categorized separately for querying",
      "explanation": "SQL commands are divided into DDL for structure, DML for data manipulation, DCL for permissions, TCL for transactions, and DQL for querying.",
      "difficulty": "Easy",
      "code": "-- DDL example\nCREATE TABLE employees (\n    id INT PRIMARY KEY,\n    name VARCHAR(100)\n);\n\n-- DML example\nINSERT INTO employees VALUES (1, 'John Doe');\n\n-- DCL example\nGRANT SELECT ON employees TO user1;\n\n-- TCL example\nBEGIN TRANSACTION;\nUPDATE employees SET name = 'Jane Doe' WHERE id = 1;\nCOMMIT;"
    },
    {
      "id": 3,
      "question": "What is the difference between WHERE and HAVING clauses?",
      "answer": "WHERE and HAVING are both filtering clauses but operate at different stages of query execution.\n\nWHERE Clause:\nâ€¢ Filters rows before grouping\nâ€¢ Used with individual rows\nâ€¢ Cannot use aggregate functions\nâ€¢ Applied before GROUP BY\nâ€¢ Works with SELECT, UPDATE, DELETE\n\nHAVING Clause:\nâ€¢ Filters groups after grouping\nâ€¢ Used with grouped data\nâ€¢ Can use aggregate functions\nâ€¢ Applied after GROUP BY\nâ€¢ Only works with SELECT statements\n\nExecution Order:\n1. WHERE filters rows\n2. GROUP BY groups remaining rows\n3. Aggregate functions are calculated\n4. HAVING filters groups",
      "explanation": "WHERE filters individual rows before grouping, while HAVING filters grouped results after aggregation.",
      "difficulty": "Easy",
      "code": "-- WHERE: Filter before grouping\nSELECT department, COUNT(*) as emp_count\nFROM employees\nWHERE salary > 50000\nGROUP BY department;\n\n-- HAVING: Filter after grouping\nSELECT department, AVG(salary) as avg_salary\nFROM employees\nGROUP BY department\nHAVING AVG(salary) > 60000;\n\n-- Both together\nSELECT department, COUNT(*) as emp_count\nFROM employees\nWHERE hire_date > '2020-01-01'\nGROUP BY department\nHAVING COUNT(*) > 5;"
    },
    {
      "id": 4,
      "question": "What are Primary Keys and Foreign Keys?",
      "answer": "Primary and Foreign Keys are constraints that establish relationships and ensure data integrity in relational databases.\n\nPrimary Key:\nâ€¢ Uniquely identifies each record in a table\nâ€¢ Cannot contain NULL values\nâ€¢ Only one primary key per table\nâ€¢ Can be single column or composite (multiple columns)\nâ€¢ Automatically creates a unique index\n\nForeign Key:\nâ€¢ Creates relationship between two tables\nâ€¢ References primary key of another table\nâ€¢ Can contain NULL values (unless specified NOT NULL)\nâ€¢ Multiple foreign keys allowed per table\nâ€¢ Enforces referential integrity\nâ€¢ Prevents orphaned records\n\nBenefits:\nâ€¢ Data integrity and consistency\nâ€¢ Enforces relationships between tables\nâ€¢ Prevents invalid data entry",
      "explanation": "Primary keys uniquely identify records within a table, while foreign keys create relationships by referencing primary keys in other tables.",
      "difficulty": "Easy",
      "code": "-- Primary Key\nCREATE TABLE customers (\n    customer_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(100) UNIQUE\n);\n\n-- Foreign Key\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    order_date DATE,\n    customer_id INT,\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n        ON DELETE CASCADE\n        ON UPDATE CASCADE\n);\n\n-- Composite Primary Key\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    PRIMARY KEY (order_id, product_id)\n);"
    },
    {
      "id": 5,
      "question": "What is the difference between INNER JOIN and OUTER JOIN?",
      "answer": "JOINs combine rows from two or more tables based on related columns, with different types returning different result sets.\n\nINNER JOIN:\nâ€¢ Returns only matching records from both tables\nâ€¢ Most common type of join\nâ€¢ Excludes non-matching rows\nâ€¢ Default JOIN type\n\nOUTER JOINs (return matching + non-matching):\nâ€¢ LEFT JOIN (LEFT OUTER JOIN) - all records from left table, matching from right\nâ€¢ RIGHT JOIN (RIGHT OUTER JOIN) - all records from right table, matching from left\nâ€¢ FULL JOIN (FULL OUTER JOIN) - all records from both tables\n\nNULL Handling:\nâ€¢ INNER JOIN excludes NULLs\nâ€¢ OUTER JOINs include NULLs for non-matching rows\n\nUse Cases:\nâ€¢ INNER JOIN - when you need only related data\nâ€¢ LEFT JOIN - when you need all records from main table\nâ€¢ FULL JOIN - when you need all records from both tables",
      "explanation": "INNER JOIN returns only matching records from both tables, while OUTER JOINs return matching records plus non-matching records from one or both tables.",
      "difficulty": "Easy",
      "code": "-- INNER JOIN: Only customers with orders\nSELECT c.name, o.order_date, o.total\nFROM customers c\nINNER JOIN orders o ON c.customer_id = o.customer_id;\n\n-- LEFT JOIN: All customers, including those without orders\nSELECT c.name, o.order_date, o.total\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id;\n\n-- RIGHT JOIN: All orders, including orphaned ones\nSELECT c.name, o.order_date, o.total\nFROM customers c\nRIGHT JOIN orders o ON c.customer_id = o.customer_id;\n\n-- FULL OUTER JOIN: All customers and all orders\nSELECT c.name, o.order_date, o.total\nFROM customers c\nFULL OUTER JOIN orders o ON c.customer_id = o.customer_id;"
    },
    {
      "id": 6,
      "question": "What are SQL aggregate functions?",
      "answer": "Aggregate functions perform calculations on multiple rows and return a single result value.\n\nCommon Aggregate Functions:\nâ€¢ COUNT() - counts number of rows\nâ€¢ SUM() - calculates total of numeric column\nâ€¢ AVG() - calculates average value\nâ€¢ MIN() - finds minimum value\nâ€¢ MAX() - finds maximum value\n\nKey Characteristics:\nâ€¢ Ignore NULL values (except COUNT(*))\nâ€¢ Used with GROUP BY for grouped calculations\nâ€¢ Can be used in SELECT, HAVING, and ORDER BY\nâ€¢ Cannot be used in WHERE clause\n\nSpecial Notes:\nâ€¢ COUNT(*) counts all rows including NULLs\nâ€¢ COUNT(column) counts non-NULL values\nâ€¢ Can combine with DISTINCT for unique values",
      "explanation": "Aggregate functions perform calculations across multiple rows to return summary values like counts, sums, averages, minimums, and maximums.",
      "difficulty": "Easy",
      "code": "-- Basic aggregate functions\nSELECT \n    COUNT(*) as total_employees,\n    COUNT(commission) as employees_with_commission,\n    SUM(salary) as total_salary,\n    AVG(salary) as average_salary,\n    MIN(salary) as lowest_salary,\n    MAX(salary) as highest_salary\nFROM employees;\n\n-- With GROUP BY\nSELECT \n    department,\n    COUNT(*) as emp_count,\n    AVG(salary) as avg_salary\nFROM employees\nGROUP BY department;\n\n-- With DISTINCT\nSELECT COUNT(DISTINCT department) as dept_count\nFROM employees;"
    },
    {
      "id": 7,
      "question": "What is a subquery and what are its types?",
      "answer": "A subquery is a query nested inside another SQL query, used to perform operations that require multiple steps.\n\nTypes of Subqueries:\nâ€¢ Scalar subquery - returns single value\nâ€¢ Row subquery - returns single row with multiple columns\nâ€¢ Column subquery - returns single column with multiple rows\nâ€¢ Table subquery - returns multiple rows and columns\n\nBased on Execution:\nâ€¢ Correlated subquery - references outer query, executes for each row\nâ€¢ Non-correlated subquery - independent, executes once\n\nUsage Locations:\nâ€¢ SELECT clause\nâ€¢ FROM clause (derived table)\nâ€¢ WHERE clause\nâ€¢ HAVING clause\n\nPerformance Considerations:\nâ€¢ Non-correlated usually faster\nâ€¢ Can often be rewritten as JOINs\nâ€¢ Use EXISTS for better performance with correlated subqueries",
      "explanation": "Subqueries are nested queries within another query, classified by their return type and execution method, useful for complex multi-step operations.",
      "difficulty": "Easy",
      "code": "-- Scalar subquery in WHERE\nSELECT name, salary\nFROM employees\nWHERE salary > (SELECT AVG(salary) FROM employees);\n\n-- Column subquery with IN\nSELECT name, department\nFROM employees\nWHERE department IN (SELECT dept_name FROM departments WHERE location = 'NY');\n\n-- Correlated subquery\nSELECT e1.name, e1.salary, e1.department\nFROM employees e1\nWHERE salary > (SELECT AVG(salary) \n                FROM employees e2 \n                WHERE e2.department = e1.department);\n\n-- Subquery in FROM (derived table)\nSELECT dept, avg_sal\nFROM (SELECT department as dept, AVG(salary) as avg_sal\n      FROM employees\n      GROUP BY department) AS dept_avg\nWHERE avg_sal > 50000;"
    },
    {
      "id": 8,
      "question": "What is the difference between DELETE, TRUNCATE, and DROP?",
      "answer": "DELETE, TRUNCATE, and DROP are commands for removing data, but they differ in scope, speed, and reversibility.\n\nDELETE:\nâ€¢ DML command\nâ€¢ Removes specific rows based on WHERE clause\nâ€¢ Can be rolled back (with transactions)\nâ€¢ Slower for large datasets\nâ€¢ Triggers are fired\nâ€¢ Maintains table structure and indexes\nâ€¢ Logs individual row deletions\n\nTRUNCATE:\nâ€¢ DDL command\nâ€¢ Removes all rows from table\nâ€¢ Cannot be rolled back (in most databases)\nâ€¢ Much faster than DELETE\nâ€¢ Triggers are not fired\nâ€¢ Maintains table structure\nâ€¢ Minimal logging\nâ€¢ Resets identity/auto-increment values\n\nDROP:\nâ€¢ DDL command\nâ€¢ Removes entire table including structure\nâ€¢ Cannot be rolled back\nâ€¢ Removes all data, indexes, triggers, constraints\nâ€¢ Cannot use WHERE clause\nâ€¢ Table must be recreated to use again",
      "explanation": "DELETE removes specific rows and can be rolled back, TRUNCATE quickly removes all rows, and DROP removes the entire table structure.",
      "difficulty": "Easy",
      "code": "-- DELETE: Remove specific rows (can rollback)\nDELETE FROM employees WHERE department = 'Sales';\nDELETE FROM employees; -- deletes all rows\n\n-- TRUNCATE: Fast removal of all rows\nTRUNCATE TABLE employees;\n\n-- DROP: Remove entire table\nDROP TABLE employees;\n\n-- Transaction example with DELETE\nBEGIN TRANSACTION;\nDELETE FROM employees WHERE salary < 30000;\nROLLBACK; -- Can undo DELETE\n\n-- Cannot rollback TRUNCATE (in most systems)\nBEGIN TRANSACTION;\nTRUNCATE TABLE employees;\nROLLBACK; -- This won't restore the data"
    },
    {
      "id": 9,
      "question": "What are constraints in SQL?",
      "answer": "Constraints are rules applied to table columns to enforce data integrity and validity.\n\nTypes of Constraints:\nâ€¢ NOT NULL - column cannot have NULL values\nâ€¢ UNIQUE - all values must be unique\nâ€¢ PRIMARY KEY - unique identifier, combines NOT NULL and UNIQUE\nâ€¢ FOREIGN KEY - maintains referential integrity between tables\nâ€¢ CHECK - ensures values meet specific condition\nâ€¢ DEFAULT - sets default value when none provided\n\nBenefits:\nâ€¢ Data integrity and accuracy\nâ€¢ Prevents invalid data entry\nâ€¢ Enforces business rules\nâ€¢ Maintains relationships between tables\n\nConstraint Levels:\nâ€¢ Column-level - applies to single column\nâ€¢ Table-level - applies to multiple columns or entire table",
      "explanation": "Constraints are rules that enforce data integrity by restricting the type of data that can be stored in table columns.",
      "difficulty": "Easy",
      "code": "-- Creating table with constraints\nCREATE TABLE employees (\n    employee_id INT PRIMARY KEY,\n    email VARCHAR(100) NOT NULL UNIQUE,\n    first_name VARCHAR(50) NOT NULL,\n    last_name VARCHAR(50) NOT NULL,\n    age INT CHECK (age >= 18 AND age <= 65),\n    salary DECIMAL(10,2) CHECK (salary > 0),\n    department_id INT,\n    hire_date DATE DEFAULT CURRENT_DATE,\n    status VARCHAR(20) DEFAULT 'Active',\n    FOREIGN KEY (department_id) REFERENCES departments(dept_id)\n);\n\n-- Adding constraint to existing table\nALTER TABLE employees\nADD CONSTRAINT chk_salary CHECK (salary >= 30000);\n\n-- Dropping constraint\nALTER TABLE employees\nDROP CONSTRAINT chk_salary;"
    },
    {
      "id": 10,
      "question": "What is normalization and what are its normal forms?",
      "answer": "Normalization is the process of organizing database tables to reduce redundancy and improve data integrity.\n\nFirst Normal Form (1NF):\nâ€¢ Eliminate repeating groups\nâ€¢ Each column contains atomic (indivisible) values\nâ€¢ Each column contains values of single type\nâ€¢ Each row is unique\n\nSecond Normal Form (2NF):\nâ€¢ Must be in 1NF\nâ€¢ Remove partial dependencies\nâ€¢ Non-key columns depend on entire primary key\n\nThird Normal Form (3NF):\nâ€¢ Must be in 2NF\nâ€¢ Remove transitive dependencies\nâ€¢ Non-key columns depend only on primary key\n\nBoyce-Codd Normal Form (BCNF):\nâ€¢ Stricter version of 3NF\nâ€¢ Every determinant must be a candidate key\n\nBenefits:\nâ€¢ Reduces data redundancy\nâ€¢ Improves data consistency\nâ€¢ Easier to maintain and update\nâ€¢ Better query performance for updates\n\nTrade-offs:\nâ€¢ May require more JOINs\nâ€¢ Can impact read performance",
      "explanation": "Normalization organizes database tables through progressive normal forms to eliminate redundancy and ensure data integrity.",
      "difficulty": "Medium",
      "code": "-- Unnormalized (0NF)\nCREATE TABLE orders (\n    order_id INT,\n    customer_name VARCHAR(100),\n    customer_address VARCHAR(200),\n    products VARCHAR(500) -- 'ProductA, ProductB, ProductC'\n);\n\n-- 1NF: Atomic values\nCREATE TABLE orders (\n    order_id INT,\n    customer_name VARCHAR(100),\n    customer_address VARCHAR(200),\n    product_name VARCHAR(100)\n);\n\n-- 2NF: Remove partial dependencies\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date DATE\n);\n\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    PRIMARY KEY (order_id, product_id)\n);\n\n-- 3NF: Remove transitive dependencies\nCREATE TABLE customers (\n    customer_id INT PRIMARY KEY,\n    customer_name VARCHAR(100),\n    address_id INT\n);\n\nCREATE TABLE addresses (\n    address_id INT PRIMARY KEY,\n    street VARCHAR(100),\n    city VARCHAR(50),\n    state VARCHAR(50),\n    zip_code VARCHAR(10)\n);"
    },
    {
      "id": 11,
      "question": "What is an index and why is it important?",
      "answer": "An index is a database object that improves the speed of data retrieval operations on a table at the cost of additional storage and slower writes.\n\nTypes of Indexes:\nâ€¢ Clustered Index - determines physical order of data, one per table\nâ€¢ Non-clustered Index - separate structure with pointers, multiple allowed\nâ€¢ Unique Index - ensures uniqueness of values\nâ€¢ Composite Index - index on multiple columns\nâ€¢ Full-text Index - for text searching\n\nBenefits:\nâ€¢ Faster SELECT queries\nâ€¢ Speeds up WHERE, JOIN, ORDER BY operations\nâ€¢ Improves query performance significantly\nâ€¢ Enforces uniqueness (unique indexes)\n\nDrawbacks:\nâ€¢ Slows down INSERT, UPDATE, DELETE operations\nâ€¢ Requires additional disk space\nâ€¢ Needs maintenance (rebuilding/reorganizing)\n\nWhen to Use:\nâ€¢ Columns frequently used in WHERE clauses\nâ€¢ Columns used in JOIN conditions\nâ€¢ Columns used in ORDER BY\nâ€¢ Large tables with many read operations",
      "explanation": "Indexes are database structures that speed up data retrieval by creating efficient lookup mechanisms, similar to book indexes.",
      "difficulty": "Easy",
      "code": "-- Creating indexes\nCREATE INDEX idx_lastname ON employees(last_name);\n\n-- Unique index\nCREATE UNIQUE INDEX idx_email ON employees(email);\n\n-- Composite index\nCREATE INDEX idx_name ON employees(last_name, first_name);\n\n-- Clustered index (typically on primary key)\nCREATE CLUSTERED INDEX idx_employee_id ON employees(employee_id);\n\n-- Viewing index usage\nSELECT name, salary\nFROM employees\nWHERE last_name = 'Smith'; -- Uses idx_lastname\n\n-- Dropping index\nDROP INDEX idx_lastname ON employees;\n\n-- Index with INCLUDE (covering index)\nCREATE INDEX idx_dept_salary \nON employees(department_id) \nINCLUDE (salary, hire_date);"
    },
    {
      "id": 12,
      "question": "What is the difference between UNION and UNION ALL?",
      "answer": "UNION and UNION ALL combine result sets from multiple SELECT statements, but differ in duplicate handling.\n\nUNION:\nâ€¢ Combines results and removes duplicates\nâ€¢ Performs implicit DISTINCT operation\nâ€¢ Slower due to duplicate removal\nâ€¢ Requires sorting/comparison\nâ€¢ Returns unique rows only\n\nUNION ALL:\nâ€¢ Combines all results including duplicates\nâ€¢ No duplicate removal\nâ€¢ Faster performance\nâ€¢ No sorting required\nâ€¢ Returns all rows from all queries\n\nCommon Requirements:\nâ€¢ Same number of columns in each SELECT\nâ€¢ Compatible data types in corresponding columns\nâ€¢ Column names from first SELECT used in result\n\nWhen to Use:\nâ€¢ UNION - when you need distinct results\nâ€¢ UNION ALL - when duplicates are acceptable or impossible",
      "explanation": "UNION removes duplicate rows from combined results while UNION ALL includes all rows including duplicates, making it faster.",
      "difficulty": "Easy",
      "code": "-- UNION: Removes duplicates\nSELECT city FROM customers\nUNION\nSELECT city FROM suppliers;\n-- Result: Unique cities from both tables\n\n-- UNION ALL: Keeps duplicates\nSELECT city FROM customers\nUNION ALL\nSELECT city FROM suppliers;\n-- Result: All cities including duplicates\n\n-- Multiple UNION operations\nSELECT product_id, 'Order' as source FROM orders\nUNION\nSELECT product_id, 'Wishlist' as source FROM wishlist\nUNION\nSELECT product_id, 'Cart' as source FROM shopping_cart\nORDER BY product_id;\n\n-- With WHERE and ORDER BY\nSELECT name, 'Customer' as type FROM customers WHERE country = 'USA'\nUNION ALL\nSELECT name, 'Employee' as type FROM employees WHERE country = 'USA'\nORDER BY name;"
    },
    {
      "id": 13,
      "question": "What are transactions and ACID properties?",
      "answer": "A transaction is a logical unit of work that must be completed in its entirety or not at all, ensuring database consistency.\n\nACID Properties:\n\nAtomicity:\nâ€¢ All operations complete successfully or none do\nâ€¢ No partial transactions\nâ€¢ All-or-nothing principle\n\nConsistency:\nâ€¢ Database moves from one valid state to another\nâ€¢ All constraints and rules are satisfied\nâ€¢ Data integrity is maintained\n\nIsolation:\nâ€¢ Concurrent transactions don't interfere with each other\nâ€¢ Each transaction appears to execute alone\nâ€¢ Prevents dirty reads, lost updates\n\nDurability:\nâ€¢ Committed changes are permanent\nâ€¢ Survive system failures\nâ€¢ Written to non-volatile storage\n\nTransaction Commands:\nâ€¢ BEGIN/START TRANSACTION - starts transaction\nâ€¢ COMMIT - saves changes permanently\nâ€¢ ROLLBACK - undoes changes\nâ€¢ SAVEPOINT - creates restore point within transaction",
      "explanation": "Transactions are atomic units of work that follow ACID properties to ensure data integrity and consistency in databases.",
      "difficulty": "Medium",
      "code": "-- Basic transaction\nBEGIN TRANSACTION;\n\nUPDATE accounts SET balance = balance - 1000 WHERE account_id = 1;\nUPDATE accounts SET balance = balance + 1000 WHERE account_id = 2;\n\nCOMMIT; -- Save changes\n\n-- Transaction with error handling\nBEGIN TRANSACTION;\n\nDECLARE @Error INT;\n\nUPDATE accounts SET balance = balance - 1000 WHERE account_id = 1;\nSET @Error = @@ERROR;\n\nIF @Error <> 0\n    ROLLBACK; -- Undo changes\nELSE\n    COMMIT; -- Save changes\n\n-- Using SAVEPOINT\nBEGIN TRANSACTION;\n\nINSERT INTO orders (customer_id, total) VALUES (1, 500);\nSAVEPOINT order_created;\n\nINSERT INTO order_items (order_id, product_id) VALUES (1, 101);\n\n-- Rollback to savepoint if needed\nROLLBACK TO order_created;\n\nCOMMIT;"
    },
    {
      "id": 14,
      "question": "What is a view and what are its advantages?",
      "answer": "A view is a virtual table based on a SQL query that presents data from one or more tables without storing the data itself.\n\nTypes of Views:\nâ€¢ Simple view - based on single table\nâ€¢ Complex view - based on multiple tables with joins\nâ€¢ Materialized view - stores result set physically (not all databases)\n\nAdvantages:\nâ€¢ Security - restrict access to specific columns/rows\nâ€¢ Simplification - hide complex queries\nâ€¢ Data abstraction - present data differently\nâ€¢ Logical data independence - change underlying tables without affecting queries\nâ€¢ Reusability - encapsulate common queries\nâ€¢ Consistency - ensure same calculations across applications\n\nLimitations:\nâ€¢ Performance overhead (non-materialized)\nâ€¢ Cannot always be updated\nâ€¢ No indexes (except materialized views)\nâ€¢ Dependencies on base tables\n\nUpdatable Views:\nâ€¢ Simple views on single table usually updatable\nâ€¢ No aggregates, DISTINCT, GROUP BY, or UNION",
      "explanation": "Views are virtual tables that simplify complex queries, enhance security, and provide a consistent interface to underlying data without storing it.",
      "difficulty": "Easy",
      "code": "-- Creating a simple view\nCREATE VIEW active_employees AS\nSELECT employee_id, first_name, last_name, department, salary\nFROM employees\nWHERE status = 'Active';\n\n-- Using the view\nSELECT * FROM active_employees WHERE department = 'IT';\n\n-- Complex view with joins\nCREATE VIEW employee_details AS\nSELECT \n    e.employee_id,\n    e.first_name,\n    e.last_name,\n    d.department_name,\n    d.location,\n    e.salary\nFROM employees e\nJOIN departments d ON e.department_id = d.dept_id\nWHERE e.status = 'Active';\n\n-- View with aggregation\nCREATE VIEW department_summary AS\nSELECT \n    department,\n    COUNT(*) as employee_count,\n    AVG(salary) as avg_salary,\n    MAX(salary) as max_salary\nFROM employees\nGROUP BY department;\n\n-- Updating a view (if updatable)\nUPDATE active_employees\nSET salary = salary * 1.1\nWHERE department = 'IT';\n\n-- Dropping a view\nDROP VIEW active_employees;"
    },
    {
      "id": 15,
      "question": "What are stored procedures and their benefits?",
      "answer": "A stored procedure is a precompiled collection of SQL statements stored in the database that can be executed as a single unit.\n\nKey Features:\nâ€¢ Accept input parameters\nâ€¢ Return output parameters\nâ€¢ Contain control-flow logic (IF, WHILE, loops)\nâ€¢ Execute multiple SQL statements\nâ€¢ Can call other stored procedures\nâ€¢ Support error handling\n\nBenefits:\nâ€¢ Performance - precompiled and cached execution plans\nâ€¢ Reusability - write once, use many times\nâ€¢ Security - control access, hide logic, prevent SQL injection\nâ€¢ Reduced network traffic - execute multiple statements in one call\nâ€¢ Centralized business logic\nâ€¢ Transaction management\nâ€¢ Maintainability - change logic without changing application code\n\nDrawbacks:\nâ€¢ Database vendor specific syntax\nâ€¢ Harder to debug than application code\nâ€¢ Version control challenges\nâ€¢ Can be overused leading to logic in database",
      "explanation": "Stored procedures are precompiled SQL programs stored in the database that improve performance, security, and code reusability.",
      "difficulty": "Medium",
      "code": "-- Creating a stored procedure\nCREATE PROCEDURE GetEmployeesByDepartment\n    @DepartmentName VARCHAR(50)\nAS\nBEGIN\n    SELECT employee_id, first_name, last_name, salary\n    FROM employees\n    WHERE department = @DepartmentName\n    ORDER BY last_name;\nEND;\n\n-- Executing the procedure\nEXEC GetEmployeesByDepartment @DepartmentName = 'IT';\n\n-- Procedure with output parameter\nCREATE PROCEDURE GetEmployeeCount\n    @DepartmentName VARCHAR(50),\n    @EmployeeCount INT OUTPUT\nAS\nBEGIN\n    SELECT @EmployeeCount = COUNT(*)\n    FROM employees\n    WHERE department = @DepartmentName;\nEND;\n\n-- Using output parameter\nDECLARE @Count INT;\nEXEC GetEmployeeCount @DepartmentName = 'IT', @EmployeeCount = @Count OUTPUT;\nSELECT @Count as TotalEmployees;\n\n-- Procedure with transaction\nCREATE PROCEDURE TransferFunds\n    @FromAccount INT,\n    @ToAccount INT,\n    @Amount DECIMAL(10,2)\nAS\nBEGIN\n    BEGIN TRANSACTION;\n    \n    UPDATE accounts SET balance = balance - @Amount WHERE account_id = @FromAccount;\n    UPDATE accounts SET balance = balance + @Amount WHERE account_id = @ToAccount;\n    \n    IF @@ERROR <> 0\n        ROLLBACK;\n    ELSE\n        COMMIT;\nEND;"
    },
    {
      "id": 16,
      "question": "What are triggers and when should they be used?",
      "answer": "A trigger is a stored procedure that automatically executes in response to specific events on a table or view.\n\nTypes of Triggers:\nâ€¢ BEFORE trigger - executes before the event\nâ€¢ AFTER trigger - executes after the event\nâ€¢ INSTEAD OF trigger - replaces the event\n\nTrigger Events:\nâ€¢ INSERT - when new row is added\nâ€¢ UPDATE - when existing row is modified\nâ€¢ DELETE - when row is removed\n\nCommon Use Cases:\nâ€¢ Audit logging and tracking changes\nâ€¢ Enforcing complex business rules\nâ€¢ Maintaining derived data\nâ€¢ Enforcing referential integrity\nâ€¢ Preventing invalid transactions\nâ€¢ Synchronizing tables\n\nBest Practices:\nâ€¢ Keep triggers simple and fast\nâ€¢ Avoid complex logic\nâ€¢ Document trigger behavior\nâ€¢ Be careful with cascading triggers\nâ€¢ Consider alternatives (constraints, stored procedures)\n\nDrawbacks:\nâ€¢ Hidden logic, hard to debug\nâ€¢ Performance overhead\nâ€¢ Can cause unexpected side effects\nâ€¢ Complexity in maintenance",
      "explanation": "Triggers are automatic database procedures that execute in response to specific table events, useful for enforcing rules and maintaining data consistency.",
      "difficulty": "Medium",
      "code": "-- AFTER INSERT trigger for audit logging\nCREATE TRIGGER trg_employee_audit\nAFTER INSERT ON employees\nFOR EACH ROW\nBEGIN\n    INSERT INTO employee_audit (employee_id, action, action_date)\n    VALUES (NEW.employee_id, 'INSERT', NOW());\nEND;\n\n-- BEFORE UPDATE trigger for validation\nCREATE TRIGGER trg_validate_salary\nBEFORE UPDATE ON employees\nFOR EACH ROW\nBEGIN\n    IF NEW.salary < 0 THEN\n        SIGNAL SQLSTATE '45000'\n        SET MESSAGE_TEXT = 'Salary cannot be negative';\n    END IF;\nEND;\n\n-- AFTER DELETE trigger\nCREATE TRIGGER trg_archive_deleted_employee\nAFTER DELETE ON employees\nFOR EACH ROW\nBEGIN\n    INSERT INTO deleted_employees\n    SELECT *, NOW() FROM OLD;\nEND;\n\n-- INSTEAD OF trigger on view\nCREATE TRIGGER trg_update_view\nINSTEAD OF UPDATE ON employee_view\nFOR EACH ROW\nBEGIN\n    UPDATE employees\n    SET salary = NEW.salary\n    WHERE employee_id = NEW.employee_id;\n    \n    INSERT INTO salary_changes (employee_id, old_salary, new_salary, change_date)\n    VALUES (NEW.employee_id, OLD.salary, NEW.salary, NOW());\nEND;"
    },
    {
      "id": 17,
      "question": "What is the difference between CHAR and VARCHAR data types?",
      "answer": "CHAR and VARCHAR are character string data types that differ in storage and performance characteristics.\n\nCHAR (Fixed Length):\nâ€¢ Fixed length storage\nâ€¢ Pads with spaces to fill declared length\nâ€¢ Faster for fixed-size data\nâ€¢ Uses more storage for variable length data\nâ€¢ Better performance in some scenarios\nâ€¢ Length: CHAR(n) where n is 1-255\n\nVARCHAR (Variable Length):\nâ€¢ Variable length storage\nâ€¢ Stores only actual string length\nâ€¢ More efficient for varying lengths\nâ€¢ Uses less storage\nâ€¢ Slight overhead for length tracking\nâ€¢ Length: VARCHAR(n) where n is 1-65535\n\nStorage Comparison:\nâ€¢ CHAR(10) with 'abc' stores 'abc       ' (10 bytes)\nâ€¢ VARCHAR(10) with 'abc' stores 'abc' (3 bytes + length overhead)\n\nWhen to Use:\nâ€¢ CHAR - fixed length data (country codes, status flags)\nâ€¢ VARCHAR - variable length data (names, descriptions, emails)",
      "explanation": "CHAR stores fixed-length strings with space padding, while VARCHAR stores variable-length strings using only needed space.",
      "difficulty": "Easy",
      "code": "-- Table with CHAR and VARCHAR\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    product_code CHAR(10),        -- Fixed: 'PROD000001'\n    sku VARCHAR(50),              -- Variable length\n    status CHAR(1),               -- Fixed: 'A', 'I', 'D'\n    name VARCHAR(200),            -- Variable length\n    description VARCHAR(1000),    -- Variable length\n    country_code CHAR(2)          -- Fixed: 'US', 'UK', 'CA'\n);\n\n-- Inserting data\nINSERT INTO products VALUES\n(1, 'PROD000001', 'ABC-123-XYZ', 'A', 'Laptop', 'High performance laptop', 'US');\n\n-- Comparison query\nSELECT \n    LENGTH(product_code) as char_length,  -- Returns 10 (padded)\n    LENGTH(name) as varchar_length        -- Returns actual length\nFROM products;\n\n-- Storage consideration\nCREATE TABLE user_data (\n    user_id INT,\n    gender CHAR(1),              -- Good: 'M', 'F' (fixed)\n    username VARCHAR(50),        -- Good: variable length\n    bio VARCHAR(500)            -- Good: variable length\n);"
    },
    {
      "id": 18,
      "question": "What is a self-join and when would you use it?",
      "answer": "A self-join is a join operation where a table is joined with itself to compare rows within the same table.\n\nCommon Use Cases:\nâ€¢ Hierarchical data (employees and managers)\nâ€¢ Finding duplicates or similar records\nâ€¢ Comparing rows within same table\nâ€¢ Building tree structures\nâ€¢ Finding relationships between records in same table\n\nKey Requirements:\nâ€¢ Use table aliases to differentiate instances\nâ€¢ Usually involves recursive relationships\nâ€¢ Can use any join type (INNER, LEFT, etc.)\n\nImplementation:\nâ€¢ Same table appears twice with different aliases\nâ€¢ Join condition relates different rows of same table\nâ€¢ Often used with foreign keys referencing same table\n\nPerformance Considerations:\nâ€¢ Can be expensive on large tables\nâ€¢ Proper indexing is crucial\nâ€¢ Consider CTEs or recursive queries for deep hierarchies",
      "explanation": "Self-joins join a table to itself using aliases, commonly used for hierarchical data like employee-manager relationships or comparing rows within the same table.",
      "difficulty": "Medium",
      "code": "-- Employee-Manager relationship\nSELECT \n    e.employee_id,\n    e.name as employee_name,\n    m.name as manager_name\nFROM employees e\nLEFT JOIN employees m ON e.manager_id = m.employee_id;\n\n-- Finding all employees in the same department\nSELECT \n    e1.name as employee1,\n    e2.name as employee2,\n    e1.department\nFROM employees e1\nJOIN employees e2 ON e1.department = e2.department\n    AND e1.employee_id < e2.employee_id; -- Avoid duplicates\n\n-- Finding employees earning more than their manager\nSELECT \n    e.name as employee_name,\n    e.salary as employee_salary,\n    m.name as manager_name,\n    m.salary as manager_salary\nFROM employees e\nJOIN employees m ON e.manager_id = m.employee_id\nWHERE e.salary > m.salary;\n\n-- Hierarchical query - all levels\nWITH RECURSIVE emp_hierarchy AS (\n    SELECT employee_id, name, manager_id, 1 as level\n    FROM employees\n    WHERE manager_id IS NULL\n    UNION ALL\n    SELECT e.employee_id, e.name, e.manager_id, eh.level + 1\n    FROM employees e\n    JOIN emp_hierarchy eh ON e.manager_id = eh.employee_id\n)\nSELECT * FROM emp_hierarchy ORDER BY level, name;"
    },
    {
      "id": 19,
      "question": "What is the difference between a clustered and non-clustered index?",
      "answer": "Clustered and non-clustered indexes are different ways of organizing and accessing table data.\n\nClustered Index:\nâ€¢ Determines physical order of data in table\nâ€¢ Only one per table (data can only be sorted one way)\nâ€¢ Table data is stored in index leaf nodes\nâ€¢ Faster for range queries\nâ€¢ Primary key creates clustered index by default\nâ€¢ Directly contains row data\n\nNon-Clustered Index:\nâ€¢ Separate structure from table data\nâ€¢ Multiple allowed per table\nâ€¢ Contains pointers/row locators to actual data\nâ€¢ Additional lookup required to get full row\nâ€¢ Can include additional columns (covering index)\nâ€¢ Slower than clustered for full row retrieval\n\nPerformance Impact:\nâ€¢ Clustered: Faster for range scans, slower for inserts/updates\nâ€¢ Non-clustered: Faster for specific lookups, minimal impact on inserts\n\nStorage:\nâ€¢ Clustered: Data pages are index leaf pages\nâ€¢ Non-clustered: Separate index pages with pointers",
      "explanation": "Clustered indexes determine physical data order with one per table, while non-clustered indexes are separate structures with pointers, allowing multiple per table.",
      "difficulty": "Medium",
      "code": "-- Clustered index (usually on primary key)\nCREATE TABLE employees (\n    employee_id INT PRIMARY KEY, -- Creates clustered index\n    name VARCHAR(100),\n    department VARCHAR(50),\n    salary DECIMAL(10,2)\n);\n\n-- Explicit clustered index\nCREATE CLUSTERED INDEX idx_emp_id \nON employees(employee_id);\n\n-- Non-clustered indexes (multiple allowed)\nCREATE NONCLUSTERED INDEX idx_name \nON employees(name);\n\nCREATE NONCLUSTERED INDEX idx_dept_salary \nON employees(department, salary);\n\n-- Covering index (includes additional columns)\nCREATE NONCLUSTERED INDEX idx_dept_covering\nON employees(department)\nINCLUDE (name, salary); -- Avoid key lookup\n\n-- Query using clustered index (range scan)\nSELECT * FROM employees\nWHERE employee_id BETWEEN 100 AND 200; -- Fast range scan\n\n-- Query using non-clustered index\nSELECT name, department FROM employees\nWHERE department = 'IT'; -- Uses idx_dept_covering\n\n-- Viewing index information\nSELECT \n    name,\n    type_desc, -- CLUSTERED or NONCLUSTERED\n    is_unique,\n    is_primary_key\nFROM sys.indexes\nWHERE object_id = OBJECT_ID('employees');"
    },
    {
      "id": 20,
      "question": "What are window functions in SQL?",
      "answer": "Window functions (analytic functions) perform calculations across a set of rows related to the current row without grouping the result into a single row.\n\nCommon Window Functions:\nâ€¢ ROW_NUMBER() - assigns unique sequential number\nâ€¢ RANK() - assigns rank with gaps for ties\nâ€¢ DENSE_RANK() - assigns rank without gaps\nâ€¢ NTILE(n) - divides rows into n groups\nâ€¢ LAG() - accesses previous row value\nâ€¢ LEAD() - accesses next row value\nâ€¢ FIRST_VALUE() - returns first value in window\nâ€¢ LAST_VALUE() - returns last value in window\n\nWindow Clauses:\nâ€¢ PARTITION BY - divides rows into partitions\nâ€¢ ORDER BY - defines order within partition\nâ€¢ ROWS/RANGE - defines window frame\n\nKey Differences from GROUP BY:\nâ€¢ Keeps all rows in result\nâ€¢ Can access individual row values\nâ€¢ Performs calculations across row sets\nâ€¢ Does not reduce row count",
      "explanation": "Window functions perform calculations across row sets while maintaining individual rows, unlike aggregate functions that group results into single rows.",
      "difficulty": "Medium",
      "code": "-- ROW_NUMBER: Unique sequential numbers\nSELECT \n    employee_id,\n    name,\n    department,\n    salary,\n    ROW_NUMBER() OVER (ORDER BY salary DESC) as row_num\nFROM employees;\n\n-- RANK and DENSE_RANK: Handle ties differently\nSELECT \n    name,\n    department,\n    salary,\n    RANK() OVER (ORDER BY salary DESC) as rank,\n    DENSE_RANK() OVER (ORDER BY salary DESC) as dense_rank\nFROM employees;\n\n-- PARTITION BY: Calculate within groups\nSELECT \n    name,\n    department,\n    salary,\n    AVG(salary) OVER (PARTITION BY department) as dept_avg,\n    salary - AVG(salary) OVER (PARTITION BY department) as diff_from_avg\nFROM employees;\n\n-- LAG and LEAD: Access adjacent rows\nSELECT \n    order_date,\n    total,\n    LAG(total) OVER (ORDER BY order_date) as previous_total,\n    LEAD(total) OVER (ORDER BY order_date) as next_total,\n    total - LAG(total) OVER (ORDER BY order_date) as difference\nFROM orders;\n\n-- NTILE: Divide into quartiles\nSELECT \n    name,\n    salary,\n    NTILE(4) OVER (ORDER BY salary) as salary_quartile\nFROM employees;\n\n-- Running total with ROWS\nSELECT \n    order_date,\n    total,\n    SUM(total) OVER (ORDER BY order_date \n                     ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_total\nFROM orders;"
    },
    {
      "id": 21,
      "question": "What is the difference between IN and EXISTS?",
      "answer": "IN and EXISTS are both used with subqueries but have different execution strategies and performance characteristics.\n\nIN Operator:\nâ€¢ Compares value to list of values\nâ€¢ Subquery executed first, returns set\nâ€¢ Main query filters against returned set\nâ€¢ Returns NULL if subquery contains NULL\nâ€¢ Better for small subquery results\nâ€¢ Can use with static lists\n\nEXISTS Operator:\nâ€¢ Tests for existence of rows\nâ€¢ Returns TRUE or FALSE\nâ€¢ Stops searching when first match found (short-circuits)\nâ€¢ Better for large subquery results\nâ€¢ Typically faster with correlated subqueries\nâ€¢ Ignores NULL values\nâ€¢ Cannot use with static lists\n\nPerformance:\nâ€¢ EXISTS usually faster for checking existence\nâ€¢ IN faster when subquery returns few distinct values\nâ€¢ EXISTS better with NOT condition\nâ€¢ Modern optimizers often produce same execution plan",
      "explanation": "IN compares against a value list from a subquery, while EXISTS checks for row existence and short-circuits on first match.",
      "difficulty": "Medium",
      "code": "-- Using IN (good for small result sets)\nSELECT name, department\nFROM employees\nWHERE department_id IN (SELECT dept_id FROM departments WHERE location = 'NY');\n\n-- Using EXISTS (good for large result sets)\nSELECT name, department\nFROM employees e\nWHERE EXISTS (SELECT 1 FROM departments d \n              WHERE d.dept_id = e.department_id \n              AND d.location = 'NY');\n\n-- NOT IN vs NOT EXISTS (EXISTS usually better)\n-- NOT IN (can have NULL issues)\nSELECT name FROM employees\nWHERE department_id NOT IN (SELECT dept_id FROM departments WHERE budget < 100000);\n\n-- NOT EXISTS (safer with NULLs)\nSELECT name FROM employees e\nWHERE NOT EXISTS (SELECT 1 FROM departments d \n                  WHERE d.dept_id = e.department_id \n                  AND d.budget < 100000);\n\n-- IN with static list\nSELECT name, status\nFROM orders\nWHERE status IN ('Pending', 'Processing', 'Shipped');\n\n-- Performance comparison\n-- Find employees with orders\nSELECT * FROM employees e\nWHERE e.employee_id IN (SELECT customer_id FROM orders);\n-- vs\nSELECT * FROM employees e\nWHERE EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = e.employee_id);"
    },
    {
      "id": 22,
      "question": "What is a Common Table Expression (CTE)?",
      "answer": "A CTE (Common Table Expression) is a temporary named result set that exists within the execution scope of a single SQL statement.\n\nSyntax:\nâ€¢ Uses WITH clause\nâ€¢ Defined before main query\nâ€¢ Can be referenced multiple times\nâ€¢ More readable than subqueries\n\nTypes of CTEs:\nâ€¢ Simple CTE - single query\nâ€¢ Multiple CTEs - several CTEs in one statement\nâ€¢ Recursive CTE - references itself for hierarchical data\n\nAdvantages:\nâ€¢ Improved readability and maintainability\nâ€¢ Can be referenced multiple times in same query\nâ€¢ Supports recursion for hierarchical queries\nâ€¢ Logical organization of complex queries\nâ€¢ Alternative to derived tables and views\n\nLimitations:\nâ€¢ Exists only for query duration\nâ€¢ Cannot be indexed\nâ€¢ May not optimize as well as derived tables in some cases\n\nUse Cases:\nâ€¢ Breaking down complex queries\nâ€¢ Recursive queries for hierarchies\nâ€¢ When same subquery needed multiple times",
      "explanation": "CTEs are temporary named result sets defined with WITH clause that improve query readability and support recursive operations for hierarchical data.",
      "difficulty": "Medium",
      "code": "-- Simple CTE\nWITH high_earners AS (\n    SELECT employee_id, name, salary, department\n    FROM employees\n    WHERE salary > 100000\n)\nSELECT department, COUNT(*) as high_earner_count\nFROM high_earners\nGROUP BY department;\n\n-- Multiple CTEs\nWITH \ndept_avg AS (\n    SELECT department, AVG(salary) as avg_salary\n    FROM employees\n    GROUP BY department\n),\ncompany_avg AS (\n    SELECT AVG(salary) as avg_salary\n    FROM employees\n)\nSELECT d.department, d.avg_salary, c.avg_salary as company_avg\nFROM dept_avg d\nCROSS JOIN company_avg c\nWHERE d.avg_salary > c.avg_salary;\n\n-- Recursive CTE for organizational hierarchy\nWITH RECURSIVE employee_hierarchy AS (\n    -- Anchor: Top level employees\n    SELECT employee_id, name, manager_id, 1 as level, \n           CAST(name AS VARCHAR(1000)) as path\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive: Employees under managers\n    SELECT e.employee_id, e.name, e.manager_id, eh.level + 1,\n           CAST(eh.path || ' > ' || e.name AS VARCHAR(1000))\n    FROM employees e\n    JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n)\nSELECT level, name, path\nFROM employee_hierarchy\nORDER BY path;\n\n-- CTE referenced multiple times\nWITH sales_summary AS (\n    SELECT product_id, SUM(quantity) as total_quantity, SUM(total) as total_sales\n    FROM orders\n    GROUP BY product_id\n)\nSELECT \n    p.product_name,\n    s1.total_quantity,\n    s1.total_sales,\n    s1.total_sales * 100.0 / (SELECT SUM(total_sales) FROM sales_summary) as pct_of_total\nFROM products p\nJOIN sales_summary s1 ON p.product_id = s1.product_id;"
    },
    {
      "id": 23,
      "question": "What are the different types of SQL joins?",
      "answer": "SQL joins combine rows from two or more tables based on related columns, with different types returning different result sets.\n\nINNER JOIN:\nâ€¢ Returns only matching records from both tables\nâ€¢ Most common join type\nâ€¢ Default if just JOIN is specified\n\nLEFT JOIN (LEFT OUTER JOIN):\nâ€¢ All records from left table\nâ€¢ Matching records from right table\nâ€¢ NULL for non-matching right table columns\n\nRIGHT JOIN (RIGHT OUTER JOIN):\nâ€¢ All records from right table\nâ€¢ Matching records from left table\nâ€¢ NULL for non-matching left table columns\n\nFULL OUTER JOIN:\nâ€¢ All records from both tables\nâ€¢ NULL where no match exists\n\nCROSS JOIN:\nâ€¢ Cartesian product of both tables\nâ€¢ Every row from first table with every row from second\nâ€¢ No ON clause needed\n\nSELF JOIN:\nâ€¢ Table joined with itself\nâ€¢ Uses aliases to differentiate",
      "explanation": "SQL provides multiple join types including INNER for matching records, LEFT/RIGHT for including all records from one table, FULL for all records, and CROSS for cartesian products.",
      "difficulty": "Easy",
      "code": "-- INNER JOIN: Only matching records\nSELECT c.name, o.order_date, o.total\nFROM customers c\nINNER JOIN orders o ON c.customer_id = o.customer_id;\n\n-- LEFT JOIN: All customers, even without orders\nSELECT c.name, o.order_date, o.total\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id;\n\n-- RIGHT JOIN: All orders, even without valid customer\nSELECT c.name, o.order_date, o.total\nFROM customers c\nRIGHT JOIN orders o ON c.customer_id = o.customer_id;\n\n-- FULL OUTER JOIN: All customers and all orders\nSELECT c.name, o.order_date, o.total\nFROM customers c\nFULL OUTER JOIN orders o ON c.customer_id = o.customer_id;\n\n-- CROSS JOIN: Every combination\nSELECT p.product_name, c.color\nFROM products p\nCROSS JOIN colors c;\n\n-- Multiple joins\nSELECT \n    c.name as customer,\n    o.order_date,\n    p.product_name,\n    oi.quantity\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nJOIN products p ON oi.product_id = p.product_id;\n\n-- Finding records in left but not right\nSELECT c.name\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nWHERE o.order_id IS NULL; -- Customers without orders"
    },
    {
      "id": 24,
      "question": "What is the CASE statement and how is it used?",
      "answer": "CASE is a conditional expression that allows you to add if-else logic within SQL queries.\n\nTwo Forms:\n\nSimple CASE:\nâ€¢ Compares expression to set of values\nâ€¢ Uses equality comparison\nâ€¢ More concise for simple comparisons\n\nSearched CASE:\nâ€¢ Evaluates multiple conditions\nâ€¢ Uses boolean expressions\nâ€¢ More flexible and powerful\n\nKey Features:\nâ€¢ Can be used in SELECT, WHERE, ORDER BY, UPDATE\nâ€¢ Returns single value\nâ€¢ Supports multiple conditions\nâ€¢ Requires ELSE for default value (optional but recommended)\nâ€¢ Can be nested\n\nCommon Use Cases:\nâ€¢ Creating derived columns\nâ€¢ Conditional aggregation\nâ€¢ Data transformation\nâ€¢ Custom sorting\nâ€¢ Replacing codes with descriptions\nâ€¢ Implementing business logic",
      "explanation": "CASE provides conditional logic in SQL queries, allowing if-else style branching to return different values based on conditions.",
      "difficulty": "Easy",
      "code": "-- Simple CASE: Value matching\nSELECT \n    name,\n    grade,\n    CASE grade\n        WHEN 'A' THEN 'Excellent'\n        WHEN 'B' THEN 'Good'\n        WHEN 'C' THEN 'Average'\n        WHEN 'D' THEN 'Below Average'\n        ELSE 'Fail'\n    END as performance\nFROM students;\n\n-- Searched CASE: Conditional logic\nSELECT \n    name,\n    salary,\n    CASE \n        WHEN salary < 30000 THEN 'Low'\n        WHEN salary BETWEEN 30000 AND 70000 THEN 'Medium'\n        WHEN salary > 70000 THEN 'High'\n        ELSE 'Unknown'\n    END as salary_category\nFROM employees;\n\n-- CASE in ORDER BY\nSELECT name, department\nFROM employees\nORDER BY \n    CASE department\n        WHEN 'Executive' THEN 1\n        WHEN 'Management' THEN 2\n        WHEN 'Staff' THEN 3\n        ELSE 4\n    END;\n\n-- Conditional aggregation\nSELECT \n    department,\n    COUNT(*) as total_employees,\n    SUM(CASE WHEN salary > 50000 THEN 1 ELSE 0 END) as high_earners,\n    SUM(CASE WHEN hire_date > '2023-01-01' THEN 1 ELSE 0 END) as recent_hires\nFROM employees\nGROUP BY department;\n\n-- Nested CASE\nSELECT \n    name,\n    age,\n    experience,\n    CASE \n        WHEN age < 25 THEN 'Junior'\n        WHEN age BETWEEN 25 AND 40 THEN\n            CASE \n                WHEN experience > 5 THEN 'Senior'\n                ELSE 'Mid-level'\n            END\n        ELSE 'Expert'\n    END as level\nFROM employees;"
    },
    {
      "id": 25,
      "question": "What are the different types of relationships in a database?",
      "answer": "Database relationships define how tables are related to each other through keys.\n\nOne-to-One (1:1):\nâ€¢ One record in Table A relates to one record in Table B\nâ€¢ Uncommon, often indicates tables could be merged\nâ€¢ Implemented with unique foreign key\nâ€¢ Example: User and UserProfile\n\nOne-to-Many (1:M):\nâ€¢ One record in Table A relates to many records in Table B\nâ€¢ Most common relationship type\nâ€¢ Implemented with foreign key in many side\nâ€¢ Example: Customer and Orders\n\nMany-to-One (M:1):\nâ€¢ Reverse of one-to-many\nâ€¢ Many records in Table A relate to one record in Table B\nâ€¢ Example: Orders and Customer\n\nMany-to-Many (M:M):\nâ€¢ Many records in Table A relate to many in Table B\nâ€¢ Requires junction/bridge/linking table\nâ€¢ Junction table has foreign keys to both tables\nâ€¢ Example: Students and Courses\n\nImplementation:\nâ€¢ Primary keys identify unique records\nâ€¢ Foreign keys establish relationships\nâ€¢ Constraints enforce referential integrity",
      "explanation": "Database relationships connect tables through primary and foreign keys in patterns: one-to-one, one-to-many, and many-to-many using junction tables.",
      "difficulty": "Easy",
      "code": "-- One-to-One: User and Profile\nCREATE TABLE users (\n    user_id INT PRIMARY KEY,\n    username VARCHAR(50) UNIQUE\n);\n\nCREATE TABLE user_profiles (\n    profile_id INT PRIMARY KEY,\n    user_id INT UNIQUE, -- Unique ensures 1:1\n    bio TEXT,\n    avatar_url VARCHAR(255),\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\n);\n\n-- One-to-Many: Customer and Orders\nCREATE TABLE customers (\n    customer_id INT PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT, -- Multiple orders per customer\n    order_date DATE,\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n\n-- Many-to-Many: Students and Courses (with junction table)\nCREATE TABLE students (\n    student_id INT PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nCREATE TABLE courses (\n    course_id INT PRIMARY KEY,\n    course_name VARCHAR(100)\n);\n\nCREATE TABLE enrollments ( -- Junction table\n    enrollment_id INT PRIMARY KEY,\n    student_id INT,\n    course_id INT,\n    enrollment_date DATE,\n    grade VARCHAR(2),\n    FOREIGN KEY (student_id) REFERENCES students(student_id),\n    FOREIGN KEY (course_id) REFERENCES courses(course_id),\n    UNIQUE (student_id, course_id) -- Prevent duplicate enrollments\n);\n\n-- Querying many-to-many relationship\nSELECT s.name, c.course_name, e.grade\nFROM students s\nJOIN enrollments e ON s.student_id = e.student_id\nJOIN courses c ON e.course_id = c.course_id\nWHERE s.student_id = 1;"
    },
    {
      "id": 26,
      "question": "What is the difference between a temporary table and a table variable?",
      "answer": "Temporary tables and table variables are both used to store intermediate results but have different characteristics and use cases.\n\nTemporary Tables (#temp):\nâ€¢ Created in tempdb database\nâ€¢ Can be indexed\nâ€¢ Support statistics and query optimization\nâ€¢ Participate in transactions\nâ€¢ Can be created explicitly with DDL\nâ€¢ Local (#) or global (##) scope\nâ€¢ Better for large datasets\nâ€¢ Can have triggers\n\nTable Variables (@table):\nâ€¢ Stored in memory (with tempdb fallback)\nâ€¢ Limited index support (only during declaration)\nâ€¢ No statistics, limited optimization\nâ€¢ Limited transaction scope\nâ€¢ Declared like variables\nâ€¢ Scope limited to batch/procedure\nâ€¢ Better for small datasets (typically < 100 rows)\nâ€¢ Cannot have triggers\nâ€¢ Less locking, less recompilation\n\nWhen to Use:\nâ€¢ Temp tables: Large datasets, complex operations, need indexes\nâ€¢ Table variables: Small datasets, simple operations, minimal scope",
      "explanation": "Temporary tables are physical tempdb tables supporting indexes and statistics for large datasets, while table variables are lighter-weight in-memory structures for small datasets.",
      "difficulty": "Medium",
      "code": "-- Temporary table\nCREATE TABLE #TempEmployees (\n    emp_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    salary DECIMAL(10,2)\n);\n\nINSERT INTO #TempEmployees\nSELECT employee_id, name, salary FROM employees WHERE salary > 50000;\n\nCREATE INDEX idx_salary ON #TempEmployees(salary);\n\nSELECT * FROM #TempEmployees;\n\nDROP TABLE #TempEmployees;\n\n-- Table variable\nDECLARE @EmployeeTable TABLE (\n    emp_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    salary DECIMAL(10,2),\n    INDEX idx_salary (salary) -- Index defined in declaration\n);\n\nINSERT INTO @EmployeeTable\nSELECT employee_id, name, salary FROM employees WHERE salary > 50000;\n\nSELECT * FROM @EmployeeTable;\n-- No need to drop, automatically cleaned up\n\n-- Global temporary table (visible to all sessions)\nCREATE TABLE ##GlobalTemp (\n    id INT,\n    data VARCHAR(100)\n);\n\n-- Comparison in stored procedure\nCREATE PROCEDURE ProcessEmployees\nAS\nBEGIN\n    -- For large result set (> 1000 rows)\n    CREATE TABLE #LargeTemp (emp_id INT, name VARCHAR(100));\n    INSERT INTO #LargeTemp SELECT employee_id, name FROM employees;\n    \n    -- For small result set (< 100 rows)\n    DECLARE @SmallTable TABLE (dept_id INT, dept_name VARCHAR(50));\n    INSERT INTO @SmallTable SELECT dept_id, dept_name FROM departments;\n    \n    -- Use both\n    SELECT * FROM #LargeTemp;\n    SELECT * FROM @SmallTable;\n    \n    DROP TABLE #LargeTemp;\nEND;"
    },
    {
      "id": 27,
      "question": "What is denormalization and when should it be used?",
      "answer": "Denormalization is the intentional introduction of redundancy into a database by merging tables or adding redundant data to improve read performance.\n\nReasons for Denormalization:\nâ€¢ Improve query performance\nâ€¢ Reduce number of joins\nâ€¢ Optimize for read-heavy workloads\nâ€¢ Meet specific performance requirements\nâ€¢ Simplify complex queries\nâ€¢ Pre-calculate aggregates\n\nCommon Denormalization Techniques:\nâ€¢ Adding redundant columns\nâ€¢ Storing calculated/derived values\nâ€¢ Creating summary/aggregate tables\nâ€¢ Flattening normalized structures\nâ€¢ Duplicating data across tables\n\nTrade-offs:\nâ€¢ Faster reads vs slower writes\nâ€¢ Storage space increase\nâ€¢ Data inconsistency risk\nâ€¢ More complex update logic\nâ€¢ Increased maintenance overhead\n\nWhen to Use:\nâ€¢ Read-heavy applications\nâ€¢ Complex joins impacting performance\nâ€¢ Reporting and analytics\nâ€¢ When consistency can be eventually achieved\nâ€¢ After identifying actual bottlenecks",
      "explanation": "Denormalization strategically adds redundancy to improve read performance by reducing joins, trading consistency and storage for query speed.",
      "difficulty": "Medium",
      "code": "-- Normalized design (3NF)\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date DATE,\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n\nCREATE TABLE order_items (\n    item_id INT PRIMARY KEY,\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    price DECIMAL(10,2)\n);\n\n-- Query requires multiple joins and calculations\nSELECT \n    o.order_id,\n    c.customer_name,\n    SUM(oi.quantity * oi.price) as total\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id, c.customer_name;\n\n-- Denormalized design\nCREATE TABLE orders_denorm (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    customer_name VARCHAR(100),    -- Redundant from customers\n    customer_email VARCHAR(100),   -- Redundant from customers\n    order_date DATE,\n    total_amount DECIMAL(10,2),    -- Pre-calculated\n    item_count INT                 -- Pre-calculated\n);\n\n-- Simpler, faster query\nSELECT order_id, customer_name, total_amount\nFROM orders_denorm\nWHERE order_date > '2024-01-01';\n\n-- Materialized view for denormalization\nCREATE MATERIALIZED VIEW order_summary AS\nSELECT \n    o.order_id,\n    o.order_date,\n    c.customer_name,\n    c.customer_email,\n    SUM(oi.quantity * oi.price) as total_amount,\n    COUNT(oi.item_id) as item_count\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id, o.order_date, c.customer_name, c.customer_email;\n\n-- Maintaining consistency with trigger\nCREATE TRIGGER update_order_total\nAFTER INSERT OR UPDATE OR DELETE ON order_items\nFOR EACH ROW\nBEGIN\n    UPDATE orders_denorm\n    SET total_amount = (SELECT SUM(quantity * price) \n                        FROM order_items \n                        WHERE order_id = NEW.order_id),\n        item_count = (SELECT COUNT(*) \n                      FROM order_items \n                      WHERE order_id = NEW.order_id)\n    WHERE order_id = NEW.order_id;\nEND;"
    },
    {
      "id": 28,
      "question": "What is a deadlock and how can it be prevented?",
      "answer": "A deadlock occurs when two or more transactions wait for each other to release locks, causing all involved transactions to be blocked indefinitely.\n\nDeadlock Conditions:\nâ€¢ Mutual exclusion - resources cannot be shared\nâ€¢ Hold and wait - transactions hold locks while requesting more\nâ€¢ No preemption - locks cannot be forcibly removed\nâ€¢ Circular wait - cycle of transactions waiting for each other\n\nDetection and Resolution:\nâ€¢ Database automatically detects deadlocks\nâ€¢ Chooses deadlock victim (usually transaction with least cost)\nâ€¢ Rolls back victim transaction\nâ€¢ Returns deadlock error to application\n\nPrevention Strategies:\nâ€¢ Access objects in consistent order\nâ€¢ Keep transactions short\nâ€¢ Use appropriate isolation levels\nâ€¢ Minimize lock hold time\nâ€¢ Use row-level locks instead of table locks\nâ€¢ Avoid user interaction within transactions\nâ€¢ Use timeout settings\nâ€¢ Consider optimistic locking\n\nBest Practices:\nâ€¢ Handle deadlock errors with retry logic\nâ€¢ Use proper indexing to reduce lock duration\nâ€¢ Monitor and analyze deadlock patterns",
      "explanation": "Deadlocks occur when transactions circularly wait for locks held by each other, resolved by database killing one transaction and preventable through consistent resource ordering.",
      "difficulty": "Hard",
      "code": "-- Scenario causing deadlock\n-- Transaction 1:\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1; -- Locks row 1\n-- Wait...\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 2; -- Waits for row 2\nCOMMIT;\n\n-- Transaction 2 (running simultaneously):\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance - 50 WHERE account_id = 2;  -- Locks row 2\n-- Wait...\nUPDATE accounts SET balance = balance + 50 WHERE account_id = 1;  -- Waits for row 1\nCOMMIT;\n-- DEADLOCK! Each waits for the other\n\n-- Prevention: Access resources in consistent order\n-- Transaction 1:\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1; -- Row 1 first\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 2; -- Row 2 second\nCOMMIT;\n\n-- Transaction 2:\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance + 50 WHERE account_id = 1;  -- Row 1 first (same order)\nUPDATE accounts SET balance = balance - 50 WHERE account_id = 2;  -- Row 2 second\nCOMMIT;\n\n-- Deadlock handling with retry logic\nCREATE PROCEDURE TransferMoney\n    @FromAccount INT,\n    @ToAccount INT,\n    @Amount DECIMAL(10,2)\nAS\nBEGIN\n    DECLARE @Retry INT = 3;\n    DECLARE @Error INT;\n    \n    WHILE @Retry > 0\n    BEGIN\n        BEGIN TRY\n            BEGIN TRANSACTION;\n            \n            -- Access in consistent order (lower ID first)\n            IF @FromAccount < @ToAccount\n            BEGIN\n                UPDATE accounts SET balance = balance - @Amount WHERE account_id = @FromAccount;\n                UPDATE accounts SET balance = balance + @Amount WHERE account_id = @ToAccount;\n            END\n            ELSE\n            BEGIN\n                UPDATE accounts SET balance = balance + @Amount WHERE account_id = @ToAccount;\n                UPDATE accounts SET balance = balance - @Amount WHERE account_id = @FromAccount;\n            END\n            \n            COMMIT;\n            RETURN; -- Success\n        END TRY\n        BEGIN CATCH\n            IF ERROR_NUMBER() = 1205 -- Deadlock error\n            BEGIN\n                ROLLBACK;\n                SET @Retry = @Retry - 1;\n                WAITFOR DELAY '00:00:01'; -- Wait before retry\n            END\n            ELSE\n                THROW; -- Other error\n        END CATCH\n    END\nEND;\n\n-- Using lock timeout\nSET LOCK_TIMEOUT 5000; -- 5 seconds\nBEGIN TRANSACTION;\n-- Operations\nCOMMIT;"
    },
    {
      "id": 29,
      "question": "What are isolation levels in SQL?",
      "answer": "Isolation levels control how transaction changes are visible to other concurrent transactions, balancing consistency with performance.\n\nFour Standard Isolation Levels:\n\nREAD UNCOMMITTED:\nâ€¢ Lowest isolation, highest performance\nâ€¢ Allows dirty reads (uncommitted changes visible)\nâ€¢ No locks on reads\nâ€¢ Risk: dirty reads, non-repeatable reads, phantom reads\n\nREAD COMMITTED:\nâ€¢ Default in most databases\nâ€¢ Prevents dirty reads\nâ€¢ Shared locks released after read\nâ€¢ Risk: non-repeatable reads, phantom reads\n\nREPEATABLE READ:\nâ€¢ Prevents dirty and non-repeatable reads\nâ€¢ Locks held until transaction ends\nâ€¢ Risk: phantom reads\n\nSERIALIZABLE:\nâ€¢ Highest isolation, lowest performance\nâ€¢ Complete isolation from other transactions\nâ€¢ Range locks prevent phantoms\nâ€¢ No concurrency issues but lowest throughput\n\nCommon Problems:\nâ€¢ Dirty read - reading uncommitted data\nâ€¢ Non-repeatable read - data changes between reads\nâ€¢ Phantom read - new rows appear in range queries",
      "explanation": "Isolation levels define how transactions interact concurrently, trading between consistency and performance from READ UNCOMMITTED to SERIALIZABLE.",
      "difficulty": "Hard",
      "code": "-- Setting isolation level\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nBEGIN TRANSACTION;\nSELECT * FROM accounts; -- Can see uncommitted changes\nCOMMIT;\n\n-- READ UNCOMMITTED: Dirty read example\n-- Transaction 1:\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nBEGIN TRANSACTION;\nSELECT balance FROM accounts WHERE account_id = 1; -- Sees uncommitted changes\nCOMMIT;\n\n-- Transaction 2 (running concurrently):\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = 5000 WHERE account_id = 1; -- Not committed\n-- Transaction 1 might see 5000\nROLLBACK; -- Rolled back, was never valid\n\n-- READ COMMITTED: Prevents dirty reads\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN TRANSACTION;\nSELECT balance FROM accounts WHERE account_id = 1; -- 1000\n-- Another transaction updates to 2000\nSELECT balance FROM accounts WHERE account_id = 1; -- 2000 (non-repeatable)\nCOMMIT;\n\n-- REPEATABLE READ: Prevents non-repeatable reads\nSET TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nBEGIN TRANSACTION;\nSELECT balance FROM accounts WHERE account_id = 1; -- 1000\n-- Another transaction tries to update - blocked\nSELECT balance FROM accounts WHERE account_id = 1; -- Still 1000\nCOMMIT;\n\n-- SERIALIZABLE: Prevents phantom reads\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN TRANSACTION;\nSELECT COUNT(*) FROM accounts WHERE balance > 1000; -- Returns 5\n-- Another transaction tries to insert matching row - blocked\nSELECT COUNT(*) FROM accounts WHERE balance > 1000; -- Still 5\nCOMMIT;\n\n-- Comparison of issues\nCREATE TABLE isolation_comparison (\n    level VARCHAR(20),\n    dirty_read CHAR(3),\n    nonrepeatable_read CHAR(3),\n    phantom_read CHAR(3)\n);\n\nINSERT INTO isolation_comparison VALUES\n('READ UNCOMMITTED', 'Yes', 'Yes', 'Yes'),\n('READ COMMITTED', 'No', 'Yes', 'Yes'),\n('REPEATABLE READ', 'No', 'No', 'Yes'),\n('SERIALIZABLE', 'No', 'No', 'No');"
    },
    {
      "id": 30,
      "question": "What is query optimization and what techniques are used?",
      "answer": "Query optimization is the process of improving SQL query performance by choosing efficient execution plans.\n\nQuery Optimizer:\nâ€¢ Analyzes multiple execution strategies\nâ€¢ Estimates costs (I/O, CPU, memory)\nâ€¢ Chooses optimal execution plan\nâ€¢ Uses statistics and indexes\n\nOptimization Techniques:\n\nIndex Optimization:\nâ€¢ Create appropriate indexes on frequently queried columns\nâ€¢ Use covering indexes to avoid table lookups\nâ€¢ Remove unused indexes\nâ€¢ Consider index selectivity\n\nQuery Rewriting:\nâ€¢ Use EXISTS instead of IN for subqueries\nâ€¢ Replace subqueries with joins when possible\nâ€¢ Avoid functions on indexed columns in WHERE\nâ€¢ Use UNION ALL instead of UNION when duplicates acceptable\n\nExecution Plan Analysis:\nâ€¢ Use EXPLAIN/EXPLAIN ANALYZE\nâ€¢ Identify full table scans\nâ€¢ Look for missing indexes\nâ€¢ Check join order\n\nBest Practices:\nâ€¢ Select only needed columns (avoid SELECT *)\nâ€¢ Filter early with WHERE clause\nâ€¢ Use appropriate joins\nâ€¢ Limit result sets\nâ€¢ Avoid correlated subqueries\nâ€¢ Update statistics regularly",
      "explanation": "Query optimization improves performance through proper indexing, efficient query writing, and execution plan analysis to minimize resource usage.",
      "difficulty": "Medium",
      "code": "-- Inefficient query\nSELECT * FROM employees\nWHERE YEAR(hire_date) = 2023; -- Function on indexed column\n\n-- Optimized version\nSELECT employee_id, name, salary FROM employees\nWHERE hire_date >= '2023-01-01' AND hire_date < '2024-01-01';\n\n-- Inefficient subquery\nSELECT name FROM employees\nWHERE department_id IN (SELECT dept_id FROM departments WHERE location = 'NY');\n\n-- Optimized with JOIN\nSELECT DISTINCT e.name\nFROM employees e\nJOIN departments d ON e.department_id = d.dept_id\nWHERE d.location = 'NY';\n\n-- Inefficient: Multiple queries\nSELECT * FROM orders WHERE status = 'Pending';\nSELECT * FROM orders WHERE status = 'Processing';\nSELECT * FROM orders WHERE status = 'Shipped';\n\n-- Optimized: Single query\nSELECT * FROM orders\nWHERE status IN ('Pending', 'Processing', 'Shipped');\n\n-- Using EXPLAIN to analyze\nEXPLAIN SELECT e.name, d.dept_name\nFROM employees e\nJOIN departments d ON e.department_id = d.dept_id\nWHERE e.salary > 50000;\n\n-- Covering index to avoid table lookup\nCREATE INDEX idx_salary_covering\nON employees(salary)\nINCLUDE (name, department_id);\n\n-- Query uses index without table access\nSELECT name, department_id FROM employees WHERE salary > 50000;\n\n-- Avoiding correlated subquery\n-- Inefficient:\nSELECT name, salary FROM employees e1\nWHERE salary > (SELECT AVG(salary) FROM employees e2 WHERE e2.department = e1.department);\n\n-- Optimized with CTE:\nWITH dept_avg AS (\n    SELECT department, AVG(salary) as avg_salary\n    FROM employees\n    GROUP BY department\n)\nSELECT e.name, e.salary\nFROM employees e\nJOIN dept_avg da ON e.department = da.department\nWHERE e.salary > da.avg_salary;\n\n-- Update statistics for better optimization\nANALYZE TABLE employees;\nUPDATE STATISTICS employees;"
    },
    {
      "id": 31,
      "question": "What is the difference between DELETE and TRUNCATE in terms of performance?",
      "answer": "DELETE and TRUNCATE remove data but have significant performance differences due to their internal mechanisms.\n\nDELETE Performance:\nâ€¢ Logs each row deletion individually\nâ€¢ Slower for large datasets\nâ€¢ More transaction log space required\nâ€¢ Can use WHERE clause to delete specific rows\nâ€¢ Fires triggers for each row\nâ€¢ Table statistics updated incrementally\nâ€¢ Can be rolled back within transaction\nâ€¢ Maintains IDENTITY values\n\nTRUNCATE Performance:\nâ€¢ Deallocates data pages, minimal logging\nâ€¢ Much faster for large datasets (can be 100x faster)\nâ€¢ Less transaction log usage\nâ€¢ Removes all rows (no WHERE clause)\nâ€¢ Does not fire triggers\nâ€¢ Resets table statistics\nâ€¢ Cannot be rolled back in most databases\nâ€¢ Resets IDENTITY counter to seed\n\nPerformance Comparison:\nâ€¢ For 1 million rows: DELETE might take minutes, TRUNCATE seconds\nâ€¢ TRUNCATE uses minimal log space regardless of row count\nâ€¢ DELETE performance degrades with more rows\n\nWhen to Use:\nâ€¢ DELETE - removing specific rows, need rollback, need triggers\nâ€¢ TRUNCATE - removing all rows quickly, table refresh",
      "explanation": "TRUNCATE is significantly faster than DELETE for removing all rows because it deallocates pages with minimal logging instead of logging individual row deletions.",
      "difficulty": "Medium",
      "code": "-- Performance comparison\n-- DELETE: Slow for large table\nDELETE FROM large_table; -- Logs each row, fires triggers\n-- Can take minutes for millions of rows\n\n-- TRUNCATE: Fast for any size\nTRUNCATE TABLE large_table; -- Deallocates pages\n-- Usually completes in seconds\n\n-- DELETE with WHERE (only DELETE can do this)\nDELETE FROM orders WHERE order_date < '2020-01-01';\n\n-- Viewing transaction log impact\nBEGIN TRANSACTION;\nDELETE FROM test_table; -- Large log usage\nSELECT \n    database_transaction_log_bytes_used,\n    database_transaction_log_bytes_reserved\nFROM sys.dm_tran_database_transactions;\nROLLBACK;\n\nBEGIN TRANSACTION;\nTRUNCATE TABLE test_table; -- Minimal log usage\nSELECT \n    database_transaction_log_bytes_used,\n    database_transaction_log_bytes_reserved\nFROM sys.dm_tran_database_transactions;\nROLLBACK; -- Usually cannot rollback TRUNCATE\n\n-- IDENTITY behavior difference\nCREATE TABLE test_identity (\n    id INT IDENTITY(1,1),\n    data VARCHAR(50)\n);\n\nINSERT INTO test_identity VALUES ('Row1'), ('Row2'), ('Row3');\n-- id values: 1, 2, 3\n\nDELETE FROM test_identity;\nINSERT INTO test_identity VALUES ('Row4');\n-- New id: 4 (continues from last)\n\nTRUNCATE TABLE test_identity;\nINSERT INTO test_identity VALUES ('Row5');\n-- New id: 1 (reset to seed)\n\n-- Conditional removal strategy\nIF (SELECT COUNT(*) FROM orders WHERE status = 'Archived') > 1000000\n    TRUNCATE TABLE archived_orders; -- Fast for full table\nELSE\n    DELETE FROM archived_orders WHERE status = 'Archived'; -- Selective delete"
    },
    {
      "id": 32,
      "question": "What are database locks and their types?",
      "answer": "Locks are mechanisms that prevent concurrent transactions from interfering with each other, ensuring data consistency.\n\nLock Types by Granularity:\nâ€¢ Row-level locks - lock individual rows\nâ€¢ Page-level locks - lock data pages\nâ€¢ Table-level locks - lock entire table\nâ€¢ Database-level locks - lock entire database\n\nLock Modes:\n\nShared Lock (S):\nâ€¢ Read lock, multiple allowed\nâ€¢ Prevents modifications during read\nâ€¢ Compatible with other shared locks\nâ€¢ Released after read (READ COMMITTED)\n\nExclusive Lock (X):\nâ€¢ Write lock, exclusive access\nâ€¢ Prevents reads and writes by others\nâ€¢ Not compatible with any other locks\nâ€¢ Held until transaction ends\n\nUpdate Lock (U):\nâ€¢ Prevents deadlocks during updates\nâ€¢ Converts to exclusive when updating\nâ€¢ Only one update lock allowed\n\nIntent Locks:\nâ€¢ Indicate intention to acquire finer locks\nâ€¢ Intent Shared (IS), Intent Exclusive (IX)\nâ€¢ Improve locking efficiency\n\nLock Compatibility:\nâ€¢ S compatible with S and IS\nâ€¢ X incompatible with all\nâ€¢ U compatible with S and IS\n\nLocking Issues:\nâ€¢ Blocking - transaction waits for lock\nâ€¢ Deadlock - circular wait\nâ€¢ Lock escalation - fine locks become coarse",
      "explanation": "Database locks control concurrent access using shared locks for reads and exclusive locks for writes at various granularities from rows to tables.",
      "difficulty": "Medium",
      "code": "-- Shared lock (implicit during SELECT)\nBEGIN TRANSACTION;\nSELECT * FROM accounts WHERE account_id = 1; -- Acquires S lock\n-- Other transactions can read but not modify\nCOMMIT; -- S lock released\n\n-- Exclusive lock (implicit during UPDATE/DELETE)\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 1; -- Acquires X lock\n-- Other transactions blocked from reading/writing\nCOMMIT; -- X lock released\n\n-- Explicit locking hints (SQL Server)\n-- WITH (NOLOCK) - Read uncommitted\nSELECT * FROM accounts WITH (NOLOCK);\n\n-- WITH (HOLDLOCK) - Hold shared lock until transaction end\nBEGIN TRANSACTION;\nSELECT * FROM accounts WITH (HOLDLOCK) WHERE account_id = 1;\n-- Lock held until commit\nCOMMIT;\n\n-- WITH (UPDLOCK) - Update lock to prevent deadlocks\nBEGIN TRANSACTION;\nSELECT * FROM accounts WITH (UPDLOCK) WHERE account_id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 1;\nCOMMIT;\n\n-- WITH (TABLOCKX) - Exclusive table lock\nBEGIN TRANSACTION;\nSELECT * FROM accounts WITH (TABLOCKX);\n-- Entire table locked exclusively\nCOMMIT;\n\n-- Row-level locking (MySQL/PostgreSQL)\nSELECT * FROM accounts WHERE account_id = 1 FOR UPDATE; -- Lock row\n\n-- Viewing active locks (SQL Server)\nSELECT \n    resource_type,\n    resource_description,\n    request_mode,\n    request_status\nFROM sys.dm_tran_locks\nWHERE resource_database_id = DB_ID();\n\n-- Lock timeout setting\nSET LOCK_TIMEOUT 5000; -- Wait max 5 seconds for lock\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 1;\n-- If lock not available in 5 seconds, error raised\nCOMMIT;"
    },
    {
      "id": 33,
      "question": "What is the COALESCE function and how does it differ from ISNULL?",
      "answer": "COALESCE and ISNULL handle NULL values but have different capabilities and behaviors.\n\nCOALESCE:\nâ€¢ ANSI SQL standard function\nâ€¢ Accepts multiple arguments (2 or more)\nâ€¢ Returns first non-NULL value\nâ€¢ Can be used in all databases\nâ€¢ More flexible and powerful\nâ€¢ Return type: highest precedence data type from arguments\nâ€¢ Can be used with any data type\n\nISNULL:\nâ€¢ Database-specific (SQL Server, MySQL)\nâ€¢ Accepts exactly two arguments\nâ€¢ Returns first argument if not NULL, else second\nâ€¢ Limited to two values\nâ€¢ Return type: type of first argument\nâ€¢ May have better performance in some cases\n\nKey Differences:\nâ€¢ COALESCE evaluates arguments lazily (stops at first non-NULL)\nâ€¢ ISNULL evaluates both arguments\nâ€¢ COALESCE more portable across databases\nâ€¢ Data type handling differs\n\nWhen to Use:\nâ€¢ COALESCE - multiple fallback values, portability\nâ€¢ ISNULL - simple two-value case, SQL Server optimization",
      "explanation": "COALESCE is a standard function accepting multiple arguments to return first non-NULL value, while ISNULL is database-specific and limited to two arguments.",
      "difficulty": "Easy",
      "code": "-- COALESCE with multiple arguments\nSELECT \n    name,\n    COALESCE(mobile_phone, home_phone, work_phone, 'No phone') as contact_phone\nFROM customers;\n\n-- ISNULL with two arguments\nSELECT \n    name,\n    ISNULL(mobile_phone, 'No phone') as contact_phone\nFROM customers;\n\n-- COALESCE equivalent to nested ISNULL\nSELECT COALESCE(col1, col2, col3, 'default');\n-- Equivalent to:\nSELECT ISNULL(col1, ISNULL(col2, ISNULL(col3, 'default')));\n\n-- Data type handling difference\nDECLARE @num INT = NULL;\nDECLARE @str VARCHAR(10) = '123';\n\n-- ISNULL: Returns type of first argument (INT)\nSELECT ISNULL(@num, @str); -- Returns 123 as INT\n\n-- COALESCE: Returns highest precedence type\nSELECT COALESCE(@num, @str); -- May return '123' as VARCHAR\n\n-- Practical examples\n-- Replacing NULL in calculations\nSELECT \n    product_name,\n    price * COALESCE(discount, 0) as discount_amount\nFROM products;\n\n-- Default values for display\nSELECT \n    employee_id,\n    name,\n    COALESCE(email, phone, 'No contact info') as contact,\n    COALESCE(department, 'Unassigned') as dept\nFROM employees;\n\n-- Avoiding divide by zero\nSELECT \n    total_sales / COALESCE(NULLIF(total_orders, 0), 1) as avg_order_value\nFROM sales_summary;\n\n-- MySQL/PostgreSQL equivalent (IFNULL/NULLIF)\nSELECT IFNULL(column, 'default'); -- MySQL\nSELECT COALESCE(column, 'default'); -- PostgreSQL, MySQL, SQL Server"
    },
    {
      "id": 34,
      "question": "What is a cursor and when should it be used?",
      "answer": "A cursor is a database object that allows row-by-row processing of query results, moving through result sets one row at a time.\n\nCursor Types:\n\nStatic Cursor:\nâ€¢ Creates snapshot of result set\nâ€¢ Not affected by changes to underlying data\nâ€¢ Stored in tempdb\n\nDynamic Cursor:\nâ€¢ Reflects all changes to data\nâ€¢ No snapshot created\nâ€¢ Always current\n\nForward-Only Cursor:\nâ€¢ Can only move forward\nâ€¢ Most efficient type\nâ€¢ Default cursor type\n\nKeyset-Driven Cursor:\nâ€¢ Keys stored, data fetched as needed\nâ€¢ Sees updates but not inserts/deletes\n\nWhen to Use:\nâ€¢ Row-by-row processing absolutely necessary\nâ€¢ Complex business logic per row\nâ€¢ Calling stored procedures for each row\nâ€¢ Legacy system integration\n\nWhen to Avoid (usually better alternatives):\nâ€¢ Set-based operations can be used\nâ€¢ Aggregations and calculations\nâ€¢ Bulk updates or inserts\nâ€¢ Any operation possible with JOIN or subquery\n\nDrawbacks:\nâ€¢ Poor performance compared to set-based\nâ€¢ High memory usage\nâ€¢ Locks held longer\nâ€¢ Prevents parallel processing",
      "explanation": "Cursors enable row-by-row processing of result sets but should be avoided when possible in favor of set-based operations due to performance overhead.",
      "difficulty": "Medium",
      "code": "-- Declaring and using a cursor\nDECLARE @emp_id INT, @emp_name VARCHAR(100), @salary DECIMAL(10,2);\n\nDECLARE emp_cursor CURSOR FOR\nSELECT employee_id, name, salary\nFROM employees\nWHERE department = 'Sales';\n\nOPEN emp_cursor;\n\nFETCH NEXT FROM emp_cursor INTO @emp_id, @emp_name, @salary;\n\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    -- Process each row\n    PRINT 'Processing: ' + @emp_name;\n    \n    -- Example: Give 10% raise if salary < 50000\n    IF @salary < 50000\n    BEGIN\n        UPDATE employees \n        SET salary = salary * 1.10 \n        WHERE employee_id = @emp_id;\n    END\n    \n    FETCH NEXT FROM emp_cursor INTO @emp_id, @emp_name, @salary;\nEND\n\nCLOSE emp_cursor;\nDEALLOCATE emp_cursor;\n\n-- Better set-based alternative (avoid cursor)\nUPDATE employees\nSET salary = salary * 1.10\nWHERE department = 'Sales' AND salary < 50000;\n\n-- Cursor with error handling\nDECLARE @error_msg VARCHAR(200);\n\nDECLARE safe_cursor CURSOR FOR\nSELECT order_id FROM orders WHERE status = 'Pending';\n\nBEGIN TRY\n    OPEN safe_cursor;\n    \n    DECLARE @order_id INT;\n    FETCH NEXT FROM safe_cursor INTO @order_id;\n    \n    WHILE @@FETCH_STATUS = 0\n    BEGIN\n        EXEC ProcessOrder @order_id;\n        FETCH NEXT FROM safe_cursor INTO @order_id;\n    END\n    \n    CLOSE safe_cursor;\nEND TRY\nBEGIN CATCH\n    IF CURSOR_STATUS('local', 'safe_cursor') >= 0\n        CLOSE safe_cursor;\n    \n    SET @error_msg = ERROR_MESSAGE();\n    PRINT 'Error: ' + @error_msg;\nEND CATCH\n\nDEALLOCATE safe_cursor;\n\n-- Forward-only, read-only cursor (fastest)\nDECLARE fast_cursor CURSOR FAST_FORWARD FOR\nSELECT product_id FROM products;\n\n-- Dynamic cursor (reflects changes)\nDECLARE dynamic_cursor CURSOR DYNAMIC FOR\nSELECT * FROM orders;"
    },
    {
      "id": 35,
      "question": "What is the difference between RANK, DENSE_RANK, and ROW_NUMBER?",
      "answer": "These window functions assign numbers to rows but handle ties differently and serve different purposes.\n\nROW_NUMBER():\nâ€¢ Assigns unique sequential integer to each row\nâ€¢ No gaps in sequence\nâ€¢ Arbitrary order for ties (non-deterministic unless ORDER BY unique)\nâ€¢ Always gives different numbers to tied rows\nâ€¢ Use case: Pagination, unique numbering\n\nRANK():\nâ€¢ Assigns rank with gaps for ties\nâ€¢ Same rank for tied values\nâ€¢ Skips numbers after ties\nâ€¢ Gap size equals number of tied rows\nâ€¢ Use case: Competition rankings (Olympic medals style)\n\nDENSE_RANK():\nâ€¢ Assigns rank without gaps\nâ€¢ Same rank for tied values\nâ€¢ Does not skip numbers\nâ€¢ Consecutive rank numbers\nâ€¢ Use case: Dense rankings where continuity matters\n\nExample with ties:\nâ€¢ Scores: 100, 95, 95, 90\nâ€¢ ROW_NUMBER: 1, 2, 3, 4 (arbitrary for ties)\nâ€¢ RANK: 1, 2, 2, 4 (skips 3)\nâ€¢ DENSE_RANK: 1, 2, 2, 3 (no skip)\n\nAll three:\nâ€¢ Support PARTITION BY for grouping\nâ€¢ Require ORDER BY clause\nâ€¢ Window functions (no grouping)",
      "explanation": "ROW_NUMBER assigns unique sequential numbers, RANK assigns same rank to ties with gaps, and DENSE_RANK assigns same rank without gaps.",
      "difficulty": "Medium",
      "code": "-- Comparison of all three functions\nSELECT \n    name,\n    score,\n    ROW_NUMBER() OVER (ORDER BY score DESC) as row_num,\n    RANK() OVER (ORDER BY score DESC) as rank,\n    DENSE_RANK() OVER (ORDER BY score DESC) as dense_rank\nFROM students;\n\n-- Sample output:\n-- name    score  row_num  rank  dense_rank\n-- Alice   100    1        1     1\n-- Bob     95     2        2     2\n-- Carol   95     3        2     2  (tied with Bob)\n-- David   90     4        4     3  (rank skips 3, dense doesn't)\n-- Eve     85     5        5     4\n\n-- Practical use case: Top N per group\n-- Get top 3 employees by salary in each department\nWITH ranked_employees AS (\n    SELECT \n        name,\n        department,\n        salary,\n        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rn\n    FROM employees\n)\nSELECT name, department, salary\nFROM ranked_employees\nWHERE rn <= 3;\n\n-- RANK for competition-style rankings\nSELECT \n    athlete_name,\n    score,\n    RANK() OVER (ORDER BY score DESC) as rank\nFROM competition_results;\n-- If two athletes tie for 2nd, next is 4th (like Olympics)\n\n-- DENSE_RANK for grade levels\nSELECT \n    student_name,\n    gpa,\n    DENSE_RANK() OVER (ORDER BY gpa DESC) as grade_level\nFROM students;\n-- Ensures consecutive grade levels without gaps\n\n-- ROW_NUMBER for pagination\nSELECT * FROM (\n    SELECT \n        product_name,\n        price,\n        ROW_NUMBER() OVER (ORDER BY product_name) as row_num\n    FROM products\n) subquery\nWHERE row_num BETWEEN 11 AND 20; -- Page 2 (rows 11-20)\n\n-- Handling ties deterministically in ROW_NUMBER\nSELECT \n    name,\n    salary,\n    hire_date,\n    ROW_NUMBER() OVER (ORDER BY salary DESC, hire_date, employee_id) as row_num\nFROM employees;\n-- Multiple ORDER BY columns ensure deterministic ordering\n\n-- Finding duplicates with ROW_NUMBER\nWITH duplicates AS (\n    SELECT \n        email,\n        ROW_NUMBER() OVER (PARTITION BY email ORDER BY created_date) as rn\n    FROM users\n)\nDELETE FROM duplicates WHERE rn > 1; -- Keep first, delete rest"
    },
    {
      "id": 36,
      "question": "What is the MERGE statement and when is it used?",
      "answer": "MERGE is a DML statement that performs INSERT, UPDATE, or DELETE operations in a single statement based on the results of a join with a source table.\n\nKey Features:\nâ€¢ Also known as UPSERT (Update or Insert)\nâ€¢ Combines INSERT, UPDATE, DELETE in one operation\nâ€¢ Compares source and target tables\nâ€¢ Performs different actions based on match conditions\nâ€¢ Atomic operation\nâ€¢ Reduces round trips and improves performance\n\nMERGE Clauses:\nâ€¢ WHEN MATCHED - record exists in both tables (UPDATE/DELETE)\nâ€¢ WHEN NOT MATCHED BY TARGET - exists in source only (INSERT)\nâ€¢ WHEN NOT MATCHED BY SOURCE - exists in target only (DELETE)\n\nBenefits:\nâ€¢ Synchronize tables efficiently\nâ€¢ Single statement vs multiple statements\nâ€¢ Better performance for large datasets\nâ€¢ Cleaner code\nâ€¢ Atomic transaction\n\nCommon Use Cases:\nâ€¢ Data warehouse loading (slowly changing dimensions)\nâ€¢ Synchronizing tables\nâ€¢ Applying incremental updates\nâ€¢ Importing data with updates\nâ€¢ Maintaining summary tables",
      "explanation": "MERGE synchronizes two tables by performing conditional INSERT, UPDATE, or DELETE operations in a single atomic statement based on match conditions.",
      "difficulty": "Medium",
      "code": "-- Basic MERGE syntax\nMERGE INTO target_table AS target\nUSING source_table AS source\nON target.id = source.id\nWHEN MATCHED THEN\n    UPDATE SET target.value = source.value\nWHEN NOT MATCHED BY TARGET THEN\n    INSERT (id, value) VALUES (source.id, source.value)\nWHEN NOT MATCHED BY SOURCE THEN\n    DELETE;\n\n-- Practical example: Update product inventory\nMERGE INTO inventory AS inv\nUSING daily_sales AS sales\nON inv.product_id = sales.product_id\nWHEN MATCHED THEN\n    UPDATE SET \n        inv.quantity = inv.quantity - sales.quantity_sold,\n        inv.last_updated = GETDATE()\nWHEN NOT MATCHED BY TARGET THEN\n    INSERT (product_id, quantity, last_updated)\n    VALUES (sales.product_id, -sales.quantity_sold, GETDATE());\n\n-- MERGE with conditions\nMERGE INTO employees AS e\nUSING employee_updates AS u\nON e.employee_id = u.employee_id\nWHEN MATCHED AND u.status = 'Active' THEN\n    UPDATE SET \n        e.salary = u.salary,\n        e.department = u.department,\n        e.updated_date = GETDATE()\nWHEN MATCHED AND u.status = 'Terminated' THEN\n    DELETE\nWHEN NOT MATCHED BY TARGET THEN\n    INSERT (employee_id, name, salary, department, status, hire_date)\n    VALUES (u.employee_id, u.name, u.salary, u.department, u.status, GETDATE());\n\n-- MERGE with OUTPUT clause (audit trail)\nMERGE INTO customers AS c\nUSING customer_updates AS cu\nON c.customer_id = cu.customer_id\nWHEN MATCHED THEN\n    UPDATE SET c.address = cu.address, c.phone = cu.phone\nWHEN NOT MATCHED THEN\n    INSERT (customer_id, name, address, phone)\n    VALUES (cu.customer_id, cu.name, cu.address, cu.phone)\nOUTPUT \n    $action as action_type,\n    INSERTED.customer_id,\n    INSERTED.name,\n    DELETED.address as old_address,\n    INSERTED.address as new_address\nINTO customer_audit_log;\n\n-- Alternative to MERGE (for databases without MERGE)\n-- Using INSERT...ON DUPLICATE KEY UPDATE (MySQL)\nINSERT INTO inventory (product_id, quantity)\nVALUES (1, 100)\nON DUPLICATE KEY UPDATE quantity = quantity + VALUES(quantity);\n\n-- Using INSERT...ON CONFLICT (PostgreSQL)\nINSERT INTO inventory (product_id, quantity)\nVALUES (1, 100)\nON CONFLICT (product_id) \nDO UPDATE SET quantity = inventory.quantity + EXCLUDED.quantity;"
    },
    {
      "id": 37,
      "question": "What are execution plans and how do you read them?",
      "answer": "An execution plan is the step-by-step strategy the database uses to execute a query, showing how tables are accessed and joined.\n\nTypes of Plans:\n\nEstimated Execution Plan:\nâ€¢ Generated without executing query\nâ€¢ Based on statistics\nâ€¢ Shows predicted costs and row counts\nâ€¢ Fast to generate\n\nActual Execution Plan:\nâ€¢ Generated by executing query\nâ€¢ Shows real costs and row counts\nâ€¢ More accurate\nâ€¢ Takes time to execute\n\nKey Components to Analyze:\n\nOperators:\nâ€¢ Table Scan - reads entire table (bad for large tables)\nâ€¢ Index Scan - reads entire index\nâ€¢ Index Seek - efficient index lookup\nâ€¢ Nested Loops - join method for small datasets\nâ€¢ Hash Join - join method for large datasets\nâ€¢ Merge Join - join for sorted data\n\nMetrics:\nâ€¢ Cost percentage - relative cost of each operation\nâ€¢ Estimated rows vs actual rows\nâ€¢ I/O costs\nâ€¢ CPU costs\n\nProblems to Look For:\nâ€¢ Table scans on large tables\nâ€¢ Missing indexes\nâ€¢ Implicit conversions\nâ€¢ Large difference between estimated and actual rows\nâ€¢ Expensive sorts",
      "explanation": "Execution plans visualize query execution strategy, showing operations, costs, and row counts to identify performance bottlenecks and optimization opportunities.",
      "difficulty": "Hard",
      "code": "-- Getting execution plan (SQL Server)\nSET SHOWPLAN_TEXT ON;\nGO\nSELECT * FROM employees WHERE salary > 50000;\nGO\nSET SHOWPLAN_TEXT OFF;\n\n-- Actual execution plan with statistics\nSET STATISTICS TIME ON;\nSET STATISTICS IO ON;\n\nSELECT e.name, d.dept_name\nFROM employees e\nJOIN departments d ON e.department_id = d.dept_id\nWHERE e.salary > 50000;\n\nSET STATISTICS TIME OFF;\nSET STATISTICS IO OFF;\n\n-- MySQL execution plan\nEXPLAIN SELECT * FROM employees WHERE salary > 50000;\n\n-- PostgreSQL execution plan\nEXPLAIN ANALYZE SELECT * FROM employees WHERE salary > 50000;\n\n-- Reading execution plan output\n-- Look for:\n-- 1. type = ALL (Table Scan) - BAD for large tables\nEXPLAIN SELECT * FROM employees WHERE YEAR(hire_date) = 2023;\n-- Shows: Type: ALL, Rows: 1000000 (full scan)\n\n-- 2. type = ref/eq_ref (Index Seek) - GOOD\nEXPLAIN SELECT * FROM employees WHERE employee_id = 123;\n-- Shows: Type: eq_ref, Rows: 1, Key: PRIMARY\n\n-- 3. Expensive operations\nEXPLAIN\nSELECT DISTINCT department \nFROM employees \nORDER BY department;\n-- May show costly sort operation\n\n-- Analyzing specific issues\n-- Issue: Implicit conversion\nCREATE TABLE test_table (\n    id INT PRIMARY KEY,\n    code VARCHAR(10)\n);\n\nCREATE INDEX idx_code ON test_table(code);\n\n-- Bad: Implicit conversion prevents index use\nEXPLAIN SELECT * FROM test_table WHERE code = 123; -- INT vs VARCHAR\n-- Shows: Type: ALL (index not used)\n\n-- Good: Correct data type\nEXPLAIN SELECT * FROM test_table WHERE code = '123';\n-- Shows: Type: ref (index used)\n\n-- Comparing plans\n-- Before optimization\nEXPLAIN \nSELECT * FROM orders \nWHERE YEAR(order_date) = 2024; -- Function on column\n\n-- After optimization\nEXPLAIN\nSELECT * FROM orders\nWHERE order_date >= '2024-01-01' AND order_date < '2025-01-01';\n\n-- Plan with costs (PostgreSQL)\nEXPLAIN (ANALYZE, BUFFERS, COSTS)\nSELECT \n    e.name,\n    COUNT(o.order_id) as order_count\nFROM employees e\nLEFT JOIN orders o ON e.employee_id = o.employee_id\nGROUP BY e.name\nHAVING COUNT(o.order_id) > 5;"
    },
    {
      "id": 38,
      "question": "What is a composite key and when is it used?",
      "answer": "A composite key (also called compound key) is a primary key composed of two or more columns that together uniquely identify a row.\n\nCharacteristics:\nâ€¢ Multiple columns combined\nâ€¢ No single column is unique alone\nâ€¢ Together they form unique identifier\nâ€¢ All columns required to identify row\nâ€¢ Often used in junction/bridge tables\nâ€¢ Can be referenced by foreign keys\n\nWhen to Use:\n\nMany-to-Many Relationships:\nâ€¢ Junction tables typically have composite key\nâ€¢ Combines foreign keys from both parent tables\nâ€¢ Example: StudentID + CourseID in enrollment\n\nNaturally Composite Identifiers:\nâ€¢ Date + Location for weather data\nâ€¢ Country + State + City for locations\nâ€¢ Order + LineItem for order details\n\nAdvantages:\nâ€¢ Natural representation of relationships\nâ€¢ Prevents duplicate entries\nâ€¢ No need for surrogate key\nâ€¢ Enforces business rules\n\nDisadvantages:\nâ€¢ More complex joins\nâ€¢ Larger index size\nâ€¢ More columns in foreign keys\nâ€¢ Can be harder to maintain",
      "explanation": "Composite keys use multiple columns together as a unique identifier, commonly used in junction tables and for naturally multi-part identifiers.",
      "difficulty": "Easy",
      "code": "-- Composite primary key in junction table\nCREATE TABLE student_courses (\n    student_id INT,\n    course_id INT,\n    enrollment_date DATE,\n    grade VARCHAR(2),\n    PRIMARY KEY (student_id, course_id), -- Composite key\n    FOREIGN KEY (student_id) REFERENCES students(student_id),\n    FOREIGN KEY (course_id) REFERENCES courses(course_id)\n);\n\n-- Composite key with additional columns\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    unit_price DECIMAL(10,2),\n    PRIMARY KEY (order_id, product_id), -- Composite key\n    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    FOREIGN KEY (product_id) REFERENCES products(product_id)\n);\n\n-- Natural composite key example\nCREATE TABLE daily_weather (\n    date DATE,\n    location_id INT,\n    temperature DECIMAL(5,2),\n    humidity INT,\n    PRIMARY KEY (date, location_id)\n);\n\n-- Querying with composite key\nSELECT * FROM student_courses\nWHERE student_id = 101 AND course_id = 201;\n\n-- Foreign key referencing composite key\nCREATE TABLE assignment_submissions (\n    submission_id INT PRIMARY KEY,\n    student_id INT,\n    course_id INT,\n    assignment_name VARCHAR(100),\n    submission_date DATE,\n    FOREIGN KEY (student_id, course_id) \n        REFERENCES student_courses(student_id, course_id)\n);\n\n-- Composite key vs surrogate key\n-- With composite key (natural)\nCREATE TABLE enrollments (\n    student_id INT,\n    course_id INT,\n    PRIMARY KEY (student_id, course_id)\n);\n\n-- With surrogate key (alternative)\nCREATE TABLE enrollments_alt (\n    enrollment_id INT PRIMARY KEY AUTO_INCREMENT, -- Surrogate\n    student_id INT,\n    course_id INT,\n    UNIQUE (student_id, course_id) -- Still prevent duplicates\n);\n\n-- Join using composite key\nSELECT \n    s.name,\n    c.course_name,\n    sc.grade\nFROM students s\nJOIN student_courses sc ON s.student_id = sc.student_id\nJOIN courses c ON sc.course_id = c.course_id\nWHERE sc.grade IN ('A', 'B');"
    },
    {
      "id": 39,
      "question": "What are the different types of constraints and their purposes?",
      "answer": "Constraints are rules that enforce data integrity at the table level by limiting the type of data that can be inserted.\n\nNOT NULL Constraint:\nâ€¢ Ensures column cannot have NULL values\nâ€¢ Must provide value during INSERT\nâ€¢ Applied at column level\n\nUNIQUE Constraint:\nâ€¢ Ensures all values in column are different\nâ€¢ Allows one NULL value (in most databases)\nâ€¢ Can have multiple per table\nâ€¢ Automatically creates index\n\nPRIMARY KEY Constraint:\nâ€¢ Uniquely identifies each row\nâ€¢ Combines NOT NULL and UNIQUE\nâ€¢ Only one per table\nâ€¢ Automatically creates clustered index\n\nFOREIGN KEY Constraint:\nâ€¢ Links two tables together\nâ€¢ Enforces referential integrity\nâ€¢ References PRIMARY KEY or UNIQUE column\nâ€¢ Supports CASCADE options\n\nCHECK Constraint:\nâ€¢ Validates data based on condition\nâ€¢ Can reference other columns in same row\nâ€¢ Boolean expression must be true\n\nDEFAULT Constraint:\nâ€¢ Provides default value\nâ€¢ Used when no value specified\nâ€¢ Can be literal or expression",
      "explanation": "SQL constraints including NOT NULL, UNIQUE, PRIMARY KEY, FOREIGN KEY, CHECK, and DEFAULT enforce data integrity rules at the database level.",
      "difficulty": "Easy",
      "code": "-- Comprehensive table with all constraints\nCREATE TABLE employees (\n    -- PRIMARY KEY constraint\n    employee_id INT PRIMARY KEY,\n    \n    -- NOT NULL constraints\n    first_name VARCHAR(50) NOT NULL,\n    last_name VARCHAR(50) NOT NULL,\n    \n    -- UNIQUE constraint\n    email VARCHAR(100) NOT NULL UNIQUE,\n    ssn VARCHAR(11) UNIQUE,\n    \n    -- CHECK constraints\n    age INT CHECK (age >= 18 AND age <= 65),\n    salary DECIMAL(10,2) CHECK (salary > 0),\n    hire_date DATE CHECK (hire_date <= CURRENT_DATE),\n    \n    -- DEFAULT constraint\n    status VARCHAR(20) DEFAULT 'Active',\n    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    \n    -- FOREIGN KEY constraint\n    department_id INT,\n    manager_id INT,\n    FOREIGN KEY (department_id) REFERENCES departments(dept_id)\n        ON DELETE SET NULL\n        ON UPDATE CASCADE,\n    FOREIGN KEY (manager_id) REFERENCES employees(employee_id)\n);\n\n-- Adding constraints to existing table\nALTER TABLE products\nADD CONSTRAINT chk_price CHECK (price > 0);\n\nALTER TABLE products\nADD CONSTRAINT uq_sku UNIQUE (sku);\n\n-- Named constraints for easier management\nCREATE TABLE orders (\n    order_id INT,\n    order_date DATE,\n    total DECIMAL(10,2),\n    \n    CONSTRAINT pk_orders PRIMARY KEY (order_id),\n    CONSTRAINT chk_order_total CHECK (total >= 0),\n    CONSTRAINT chk_order_date CHECK (order_date <= CURRENT_DATE)\n);\n\n-- CHECK constraint with multiple conditions\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    category VARCHAR(50),\n    price DECIMAL(10,2),\n    discount_price DECIMAL(10,2),\n    \n    CONSTRAINT chk_discount CHECK (\n        discount_price IS NULL OR \n        (discount_price >= 0 AND discount_price < price)\n    )\n);\n\n-- Foreign key with CASCADE options\nCREATE TABLE order_items (\n    item_id INT PRIMARY KEY,\n    order_id INT,\n    product_id INT,\n    \n    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n        ON DELETE CASCADE,  -- Delete items when order deleted\n    FOREIGN KEY (product_id) REFERENCES products(product_id)\n        ON DELETE RESTRICT  -- Prevent product deletion if in orders\n);\n\n-- Dropping constraints\nALTER TABLE employees\nDROP CONSTRAINT chk_salary;\n\n-- Disabling and enabling constraints\nALTER TABLE employees NOCHECK CONSTRAINT chk_salary;\nALTER TABLE employees CHECK CONSTRAINT chk_salary;"
    },
    {
      "id": 40,
      "question": "What is the difference between a correlated and non-correlated subquery?",
      "answer": "Subqueries can be classified based on whether they depend on the outer query for their execution.\n\nNon-Correlated Subquery:\nâ€¢ Independent of outer query\nâ€¢ Executes once before outer query\nâ€¢ Same result for all rows\nâ€¢ Can be executed standalone\nâ€¢ Generally faster\nâ€¢ Result can be cached\nâ€¢ Also called simple subquery\n\nCorrelated Subquery:\nâ€¢ Depends on outer query\nâ€¢ References columns from outer query\nâ€¢ Executes once for each row of outer query\nâ€¢ Cannot be executed standalone\nâ€¢ Generally slower (O(n) executions)\nâ€¢ Result changes for each outer row\nâ€¢ Uses values from current row of outer query\n\nPerformance:\nâ€¢ Non-correlated typically faster (1 execution)\nâ€¢ Correlated can be slow (n executions)\nâ€¢ Modern optimizers may rewrite correlated as joins\nâ€¢ Use EXISTS for better performance with correlated\n\nWhen to Use:\nâ€¢ Non-correlated: Filtering against static set\nâ€¢ Correlated: Row-by-row comparison needed",
      "explanation": "Non-correlated subqueries execute independently once, while correlated subqueries reference outer query columns and execute for each outer row.",
      "difficulty": "Medium",
      "code": "-- Non-Correlated Subquery\n-- Executes once, returns list of IDs\nSELECT name, salary\nFROM employees\nWHERE department_id IN (\n    SELECT dept_id \n    FROM departments \n    WHERE location = 'New York'\n); -- Subquery executes once\n\n-- Correlated Subquery\n-- Executes for each employee row\nSELECT name, salary, department\nFROM employees e1\nWHERE salary > (\n    SELECT AVG(salary)\n    FROM employees e2\n    WHERE e2.department = e1.department -- References outer query\n); -- Subquery executes once per employee\n\n-- Non-Correlated with scalar value\nSELECT name, salary\nFROM employees\nWHERE salary > (SELECT AVG(salary) FROM employees);\n-- Inner query runs once, returns single value\n\n-- Correlated with EXISTS (efficient)\nSELECT c.name, c.email\nFROM customers c\nWHERE EXISTS (\n    SELECT 1\n    FROM orders o\n    WHERE o.customer_id = c.customer_id -- Correlated\n      AND o.order_date > '2024-01-01'\n);\n-- More efficient than IN for large datasets\n\n-- Converting correlated to non-correlated (optimization)\n-- Correlated (slower):\nSELECT e1.name, e1.department\nFROM employees e1\nWHERE e1.salary = (\n    SELECT MAX(e2.salary)\n    FROM employees e2\n    WHERE e2.department = e1.department\n);\n\n-- Non-correlated with JOIN (faster):\nWITH dept_max AS (\n    SELECT department, MAX(salary) as max_salary\n    FROM employees\n    GROUP BY department\n)\nSELECT e.name, e.department\nFROM employees e\nJOIN dept_max dm ON e.department = dm.department \n    AND e.salary = dm.max_salary;\n\n-- Correlated DELETE (remove duplicates keeping latest)\nDELETE FROM products p1\nWHERE p1.created_date < (\n    SELECT MAX(p2.created_date)\n    FROM products p2\n    WHERE p2.product_name = p1.product_name\n);\n\n-- Non-Correlated in FROM clause\nSELECT dept, avg_sal\nFROM (\n    SELECT department as dept, AVG(salary) as avg_sal\n    FROM employees\n    GROUP BY department\n) AS dept_averages\nWHERE avg_sal > 50000;\n\n-- Performance comparison\n-- Correlated (runs 1000 times for 1000 employees)\nSELECT name FROM employees e\nWHERE 5 < (SELECT COUNT(*) FROM orders o WHERE o.employee_id = e.employee_id);\n\n-- Better: JOIN approach\nSELECT e.name\nFROM employees e\nJOIN (SELECT employee_id, COUNT(*) as order_count FROM orders GROUP BY employee_id) o\nON e.employee_id = o.employee_id\nWHERE o.order_count > 5;"
    },
    {
      "id": 41,
      "question": "What is database partitioning and what are its types?",
      "answer": "Partitioning divides large tables into smaller, more manageable pieces called partitions, improving performance and maintenance.\n\nTypes of Partitioning:\n\nHorizontal Partitioning (Sharding):\nâ€¢ Splits rows across partitions\nâ€¢ Each partition has same columns\nâ€¢ Different rows in different partitions\nâ€¢ Most common type\nâ€¢ Based on partition key\n\nVertical Partitioning:\nâ€¢ Splits columns across partitions\nâ€¢ Each partition has different columns\nâ€¢ Primary key replicated in all partitions\nâ€¢ Less common\n\nPartitioning Methods:\n\nRange Partitioning:\nâ€¢ Based on value ranges\nâ€¢ Example: Date ranges, numeric ranges\nâ€¢ Good for time-series data\n\nList Partitioning:\nâ€¢ Based on discrete value lists\nâ€¢ Example: Country, region, category\nâ€¢ Good for categorical data\n\nHash Partitioning:\nâ€¢ Uses hash function on key\nâ€¢ Distributes data evenly\nâ€¢ Good for load balancing\n\nComposite Partitioning:\nâ€¢ Combination of methods\nâ€¢ Example: Range-hash, range-list\n\nBenefits:\nâ€¢ Improved query performance\nâ€¢ Easier maintenance\nâ€¢ Better manageability\nâ€¢ Partition pruning\nâ€¢ Parallel operations",
      "explanation": "Partitioning splits large tables into smaller pieces based on ranges, lists, or hash values to improve performance and manageability.",
      "difficulty": "Hard",
      "code": "-- Range partitioning by date\nCREATE TABLE orders (\n    order_id INT,\n    order_date DATE,\n    customer_id INT,\n    total DECIMAL(10,2)\n)\nPARTITION BY RANGE (YEAR(order_date)) (\n    PARTITION p2020 VALUES LESS THAN (2021),\n    PARTITION p2021 VALUES LESS THAN (2022),\n    PARTITION p2022 VALUES LESS THAN (2023),\n    PARTITION p2023 VALUES LESS THAN (2024),\n    PARTITION p_future VALUES LESS THAN MAXVALUE\n);\n\n-- List partitioning by region\nCREATE TABLE customers (\n    customer_id INT,\n    name VARCHAR(100),\n    country VARCHAR(50)\n)\nPARTITION BY LIST (country) (\n    PARTITION p_north_america VALUES IN ('USA', 'Canada', 'Mexico'),\n    PARTITION p_europe VALUES IN ('UK', 'France', 'Germany', 'Italy'),\n    PARTITION p_asia VALUES IN ('China', 'Japan', 'India'),\n    PARTITION p_other VALUES IN (DEFAULT)\n);\n\n-- Hash partitioning for even distribution\nCREATE TABLE products (\n    product_id INT,\n    name VARCHAR(100),\n    price DECIMAL(10,2)\n)\nPARTITION BY HASH (product_id)\nPARTITIONS 4; -- Creates 4 partitions\n\n-- Composite partitioning (range-hash)\nCREATE TABLE sales (\n    sale_id INT,\n    sale_date DATE,\n    product_id INT,\n    amount DECIMAL(10,2)\n)\nPARTITION BY RANGE (YEAR(sale_date))\nSUBPARTITION BY HASH (product_id)\nSUBPARTITIONS 4 (\n    PARTITION p2023 VALUES LESS THAN (2024),\n    PARTITION p2024 VALUES LESS THAN (2025)\n);\n\n-- Querying partitioned table (partition pruning)\nSELECT * FROM orders\nWHERE order_date BETWEEN '2023-01-01' AND '2023-12-31';\n-- Only p2023 partition scanned\n\n-- Managing partitions\n-- Add new partition\nALTER TABLE orders\nADD PARTITION (PARTITION p2025 VALUES LESS THAN (2026));\n\n-- Drop old partition\nALTER TABLE orders\nDROP PARTITION p2020;\n\n-- Truncate partition\nALTER TABLE orders\nTRUNCATE PARTITION p2021;\n\n-- Viewing partition information\nSELECT \n    TABLE_NAME,\n    PARTITION_NAME,\n    PARTITION_METHOD,\n    PARTITION_EXPRESSION,\n    TABLE_ROWS\nFROM INFORMATION_SCHEMA.PARTITIONS\nWHERE TABLE_NAME = 'orders';\n\n-- Vertical partitioning (manual approach)\n-- Original wide table\nCREATE TABLE employees_all (\n    employee_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    phone VARCHAR(20),\n    address TEXT,\n    bio TEXT,\n    photo BLOB\n);\n\n-- Split into frequently and rarely accessed columns\nCREATE TABLE employees_core (\n    employee_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    phone VARCHAR(20)\n);\n\nCREATE TABLE employees_extended (\n    employee_id INT PRIMARY KEY,\n    address TEXT,\n    bio TEXT,\n    photo BLOB,\n    FOREIGN KEY (employee_id) REFERENCES employees_core(employee_id)\n);"
    },
    {
      "id": 42,
      "question": "What is the N+1 query problem and how do you fix it?",
      "answer": "The N+1 query problem occurs when an application executes one query to fetch N records, then executes N additional queries to fetch related data for each record.\n\nThe Problem:\nâ€¢ Initial query fetches N parent records (1 query)\nâ€¢ Loop executes one query per parent for children (N queries)\nâ€¢ Total: N+1 queries instead of 1 or 2\nâ€¢ Severe performance impact\nâ€¢ Common with ORMs and lazy loading\n\nSymptoms:\nâ€¢ Slow page load times\nâ€¢ Excessive database queries\nâ€¢ High database load\nâ€¢ Linear growth with data size\n\nSolutions:\n\nEager Loading:\nâ€¢ Fetch all data in fewer queries\nâ€¢ Use JOINs to get related data\nâ€¢ Fetch children with parent in single query\n\nBatch Loading:\nâ€¢ Fetch children for all parents at once\nâ€¢ Use WHERE IN clause\nâ€¢ Two queries total instead of N+1\n\nCaching:\nâ€¢ Cache frequently accessed data\nâ€¢ Reduce database hits\n\nDataLoader Pattern:\nâ€¢ Batch and cache requests\nâ€¢ Common in GraphQL",
      "explanation": "N+1 query problem causes one query per record instead of bulk fetching, solved by eager loading with JOINs or batch loading with WHERE IN.",
      "difficulty": "Medium",
      "code": "-- N+1 Problem Example\n-- Query 1: Fetch all customers (1 query)\nSELECT * FROM customers; -- Returns 100 customers\n\n-- Then for each customer, fetch orders (100 queries)\nSELECT * FROM orders WHERE customer_id = 1;\nSELECT * FROM orders WHERE customer_id = 2;\nSELECT * FROM orders WHERE customer_id = 3;\n-- ... 97 more queries\n-- Total: 101 queries!\n\n-- Solution 1: JOIN (Eager Loading) - 1 query\nSELECT \n    c.customer_id,\n    c.name,\n    o.order_id,\n    o.order_date,\n    o.total\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id;\n-- Single query gets all data\n\n-- Solution 2: Batch Loading - 2 queries\n-- Query 1: Get customers\nSELECT * FROM customers; -- Returns customer_ids: 1,2,3...100\n\n-- Query 2: Get all orders at once\nSELECT * FROM orders \nWHERE customer_id IN (1,2,3,4,5,...100);\n-- Just 2 queries total\n\n-- Real-world example: Blog posts with authors\n-- BAD: N+1 queries\n-- 1. Fetch posts\nSELECT * FROM posts; -- 50 posts\n\n-- 2. For each post, fetch author (50 queries)\nSELECT * FROM users WHERE user_id = 1;\nSELECT * FROM users WHERE user_id = 2;\n-- ...\n\n-- GOOD: Single JOIN query\nSELECT \n    p.post_id,\n    p.title,\n    p.content,\n    u.user_id,\n    u.name as author_name,\n    u.email as author_email\nFROM posts p\nJOIN users u ON p.author_id = u.user_id;\n\n-- Complex example: Posts with comments and authors\n-- BAD: Would be many queries\n-- 1. Get posts (1 query)\n-- 2. Get author for each post (N queries)\n-- 3. Get comments for each post (N queries)\n-- 4. Get author for each comment (M queries)\n-- Total: 1 + N + N + M queries\n\n-- GOOD: Few queries with strategic JOINs\n-- Query 1: Posts with authors\nSELECT \n    p.post_id,\n    p.title,\n    u.name as author\nFROM posts p\nJOIN users u ON p.author_id = u.user_id;\n\n-- Query 2: All comments with authors for these posts\nSELECT \n    c.comment_id,\n    c.post_id,\n    c.content,\n    u.name as commenter\nFROM comments c\nJOIN users u ON c.user_id = u.user_id\nWHERE c.post_id IN (1,2,3,...); -- Post IDs from query 1\n-- Total: 2 queries\n\n-- Using CTE for complex scenarios\nWITH post_data AS (\n    SELECT p.*, u.name as author_name\n    FROM posts p\n    JOIN users u ON p.author_id = u.user_id\n),\ncomment_data AS (\n    SELECT c.*, u.name as commenter_name\n    FROM comments c\n    JOIN users u ON c.user_id = u.user_id\n    WHERE c.post_id IN (SELECT post_id FROM post_data)\n)\nSELECT \n    pd.*,\n    COUNT(cd.comment_id) as comment_count\nFROM post_data pd\nLEFT JOIN comment_data cd ON pd.post_id = cd.post_id\nGROUP BY pd.post_id;\n\n-- Application-level fix (pseudo-code approach)\n-- Bad:\n-- foreach (customer in customers)\n--     orders = db.query(\"SELECT * FROM orders WHERE customer_id = ?\", customer.id)\n\n-- Good:\n-- customerIds = customers.map(c => c.id)\n-- orders = db.query(\"SELECT * FROM orders WHERE customer_id IN (?)\", customerIds)\n-- groupedOrders = orders.groupBy(o => o.customer_id)"
    },
    {
      "id": 43,
      "question": "What are SQL injection attacks and how do you prevent them?",
      "answer": "SQL injection is a security vulnerability where attackers insert malicious SQL code into queries through user input to manipulate database operations.\n\nHow It Works:\nâ€¢ User input directly concatenated into SQL\nâ€¢ Attacker provides SQL commands as input\nâ€¢ Application executes malicious code\nâ€¢ Can read, modify, or delete data\nâ€¢ Can bypass authentication\nâ€¢ Can execute system commands\n\nCommon Attack Patterns:\nâ€¢ Login bypass: ' OR '1'='1\nâ€¢ Data extraction: ' UNION SELECT * FROM users--\nâ€¢ Data deletion: '; DROP TABLE users;--\nâ€¢ Comment injection: ' OR 1=1--\n\nPrevention Methods:\n\nParameterized Queries (Best):\nâ€¢ Use prepared statements\nâ€¢ Parameters bound separately\nâ€¢ Database treats input as data only\nâ€¢ Most effective prevention\n\nStored Procedures:\nâ€¢ Encapsulate SQL logic\nâ€¢ Use parameters\nâ€¢ Reduce direct SQL exposure\n\nInput Validation:\nâ€¢ Whitelist allowed characters\nâ€¢ Validate data types\nâ€¢ Check length and format\nâ€¢ Escape special characters\n\nLeast Privilege:\nâ€¢ Limit database permissions\nâ€¢ Use separate accounts for different operations\nâ€¢ Don't use admin accounts for applications",
      "explanation": "SQL injection exploits unsanitized user input in queries; prevent it using parameterized queries, stored procedures, input validation, and least privilege principles.",
      "difficulty": "Medium",
      "code": "-- VULNERABLE CODE (Never do this!)\n-- Direct string concatenation\n-- Python example:\n-- query = \"SELECT * FROM users WHERE username = '\" + username + \"' AND password = '\" + password + \"'\"\n\n-- Attack input:\n-- username: admin' --\n-- password: anything\n-- Resulting query:\n-- SELECT * FROM users WHERE username = 'admin' --' AND password = 'anything'\n-- The -- comments out password check, logs in as admin\n\n-- SAFE: Parameterized query (Python)\n-- cursor.execute(\n--     \"SELECT * FROM users WHERE username = ? AND password = ?\",\n--     (username, password)\n-- )\n\n-- SAFE: Prepared statement (Java)\n-- PreparedStatement stmt = conn.prepareStatement(\n--     \"SELECT * FROM users WHERE username = ? AND password = ?\"\n-- );\n-- stmt.setString(1, username);\n-- stmt.setString(2, password);\n-- ResultSet rs = stmt.executeQuery();\n\n-- SAFE: Named parameters (C#)\n-- SqlCommand cmd = new SqlCommand(\n--     \"SELECT * FROM users WHERE username = @username AND password = @password\",\n--     connection\n-- );\n-- cmd.Parameters.AddWithValue(\"@username\", username);\n-- cmd.Parameters.AddWithValue(\"@password\", password);\n\n-- VULNERABLE: Dynamic SQL in stored procedure\nCREATE PROCEDURE GetUser\n    @username VARCHAR(50)\nAS\nBEGIN\n    -- BAD: String concatenation\n    DECLARE @sql NVARCHAR(MAX)\n    SET @sql = 'SELECT * FROM users WHERE username = ''' + @username + ''''\n    EXEC(@sql) -- Vulnerable!\nEND;\n\n-- SAFE: Parameterized stored procedure\nCREATE PROCEDURE GetUserSafe\n    @username VARCHAR(50)\nAS\nBEGIN\n    -- GOOD: Parameter used directly\n    SELECT * FROM users WHERE username = @username;\nEND;\n\n-- Common injection attacks and safe handling\n\n-- Attack 1: Authentication bypass\n-- Input: username = \"admin' OR '1'='1\", password = \"anything\"\n-- Unsafe query becomes:\n-- SELECT * FROM users WHERE username = 'admin' OR '1'='1' AND password = 'anything'\n-- Returns all users (always true)\n\n-- Safe handling with parameters ensures input treated as literal string\n\n-- Attack 2: UNION attack to extract data\n-- Input: id = \"1 UNION SELECT username, password FROM admin_users--\"\n-- Unsafe query:\n-- SELECT * FROM products WHERE id = 1 UNION SELECT username, password FROM admin_users--\n\n-- Attack 3: Batch execution\n-- Input: id = \"1; DROP TABLE users;--\"\n-- Unsafe query:\n-- SELECT * FROM products WHERE id = 1; DROP TABLE users;--\n\n-- Input validation example\nCREATE PROCEDURE ValidateAndExecute\n    @userId INT,\n    @email VARCHAR(100)\nAS\nBEGIN\n    -- Validate integer\n    IF @userId <= 0\n        RETURN;\n    \n    -- Validate email format\n    IF @email NOT LIKE '%_@_%._%'\n        RETURN;\n    \n    -- Safe to proceed\n    SELECT * FROM users WHERE user_id = @userId AND email = @email;\nEND;\n\n-- Least privilege principle\n-- Create limited user for application\nCREATE USER app_user WITH PASSWORD 'strongpassword';\n\n-- Grant only necessary permissions\nGRANT SELECT, INSERT, UPDATE ON customers TO app_user;\nGRANT SELECT, INSERT, UPDATE ON orders TO app_user;\nGRANT SELECT ON products TO app_user;\n-- No DELETE, no DDL, no admin rights\n\n-- Use different accounts for different operations\nCREATE USER readonly_user WITH PASSWORD 'password';\nGRANT SELECT ON database.* TO readonly_user;"
    },
    {
      "id": 44,
      "question": "What is a database schema and what are its types?",
      "answer": "A database schema is the logical structure that defines how data is organized, including tables, views, indexes, relationships, and constraints.\n\nComponents of Schema:\nâ€¢ Table definitions and structures\nâ€¢ Column data types and constraints\nâ€¢ Primary and foreign keys\nâ€¢ Indexes\nâ€¢ Views and stored procedures\nâ€¢ Triggers and functions\nâ€¢ Relationships between tables\n\nTypes of Schemas:\n\nPhysical Schema:\nâ€¢ Describes how data is physically stored\nâ€¢ File organization, storage structures\nâ€¢ Indexing methods\nâ€¢ Partitioning strategies\nâ€¢ Database administrator concern\n\nLogical Schema:\nâ€¢ Describes logical structure\nâ€¢ Tables, columns, relationships\nâ€¢ Constraints and rules\nâ€¢ Independent of physical storage\nâ€¢ Designer and developer concern\n\nView Schema (External Schema):\nâ€¢ User-specific views of data\nâ€¢ Subset of logical schema\nâ€¢ Different views for different users\nâ€¢ Security and simplification\n\nSchema in Different Contexts:\nâ€¢ SQL Server: Container for objects (like namespace)\nâ€¢ MySQL: Synonym for database\nâ€¢ PostgreSQL: Namespace within database",
      "explanation": "A database schema defines the logical structure of data organization including tables, relationships, and constraints at physical, logical, and view levels.",
      "difficulty": "Easy",
      "code": "-- Creating schema (SQL Server/PostgreSQL)\nCREATE SCHEMA sales;\nCREATE SCHEMA hr;\nCREATE SCHEMA finance;\n\n-- Creating tables in specific schemas\nCREATE TABLE sales.customers (\n    customer_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100)\n);\n\nCREATE TABLE sales.orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date DATE,\n    FOREIGN KEY (customer_id) REFERENCES sales.customers(customer_id)\n);\n\nCREATE TABLE hr.employees (\n    employee_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    department VARCHAR(50),\n    salary DECIMAL(10,2)\n);\n\n-- Accessing tables in different schemas\nSELECT * FROM sales.customers;\nSELECT * FROM hr.employees;\n\n-- Setting default schema\nALTER USER app_user SET SEARCH_PATH = sales, public;\n\n-- Now can access without schema prefix\nSELECT * FROM customers; -- Looks in sales schema first\n\n-- Logical schema example (E-commerce)\nCREATE TABLE customers (\n    customer_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(100) UNIQUE NOT NULL,\n    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    name VARCHAR(200) NOT NULL,\n    description TEXT,\n    price DECIMAL(10,2) NOT NULL CHECK (price > 0),\n    stock_quantity INT DEFAULT 0\n);\n\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT NOT NULL,\n    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    status VARCHAR(20) DEFAULT 'Pending',\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n\nCREATE TABLE order_items (\n    item_id INT PRIMARY KEY,\n    order_id INT NOT NULL,\n    product_id INT NOT NULL,\n    quantity INT NOT NULL CHECK (quantity > 0),\n    unit_price DECIMAL(10,2) NOT NULL,\n    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,\n    FOREIGN KEY (product_id) REFERENCES products(product_id)\n);\n\n-- View schema (external schema)\n-- Create different views for different users\nCREATE VIEW customer_order_summary AS\nSELECT \n    c.customer_id,\n    c.name,\n    COUNT(o.order_id) as total_orders,\n    SUM(o.total) as total_spent\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.name;\n\n-- View for sales team (no sensitive data)\nCREATE VIEW sales_view AS\nSELECT \n    c.name,\n    c.email,\n    o.order_id,\n    o.order_date,\n    o.status\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id;\n\n-- View for finance team (includes financial data)\nCREATE VIEW finance_view AS\nSELECT \n    o.order_id,\n    o.order_date,\n    c.name as customer_name,\n    SUM(oi.quantity * oi.unit_price) as order_total\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id, o.order_date, c.name;\n\n-- Schema privileges\nGRANT USAGE ON SCHEMA sales TO sales_user;\nGRANT SELECT ON ALL TABLES IN SCHEMA sales TO sales_user;\n\nGRANT ALL PRIVILEGES ON SCHEMA hr TO hr_admin;\n\n-- Viewing schema information\nSELECT schema_name FROM information_schema.schemata;\n\nSELECT table_name, table_schema\nFROM information_schema.tables\nWHERE table_schema = 'sales';"
    },
    {
      "id": 45,
      "question": "What are the different types of NoSQL databases and when to use SQL vs NoSQL?",
      "answer": "While SQL is for relational databases, NoSQL databases use different data models for specific use cases.\n\nNoSQL Types:\n\nDocument Stores:\nâ€¢ Store JSON-like documents\nâ€¢ Flexible schema\nâ€¢ Examples: MongoDB, CouchDB\nâ€¢ Use case: Content management, catalogs\n\nKey-Value Stores:\nâ€¢ Simple key-value pairs\nâ€¢ Very fast lookups\nâ€¢ Examples: Redis, DynamoDB\nâ€¢ Use case: Caching, sessions, real-time data\n\nColumn-Family Stores:\nâ€¢ Store data in columns\nâ€¢ Good for analytics\nâ€¢ Examples: Cassandra, HBase\nâ€¢ Use case: Time-series, analytics, IoT\n\nGraph Databases:\nâ€¢ Store nodes and relationships\nâ€¢ Optimized for connected data\nâ€¢ Examples: Neo4j, Amazon Neptune\nâ€¢ Use case: Social networks, recommendations\n\nWhen to Use SQL:\nâ€¢ Complex queries and joins\nâ€¢ ACID transactions required\nâ€¢ Structured data\nâ€¢ Clear schema\nâ€¢ Data integrity critical\nâ€¢ Reporting and analytics\n\nWhen to Use NoSQL:\nâ€¢ Massive scale and distribution\nâ€¢ Flexible schema\nâ€¢ High throughput\nâ€¢ Simple queries\nâ€¢ Hierarchical data\nâ€¢ Rapid development",
      "explanation": "NoSQL databases include document, key-value, column-family, and graph types; choose SQL for structured data and complex queries, NoSQL for scale and flexibility.",
      "difficulty": "Medium",
      "code": "-- SQL (Relational) example\n-- Structured schema with relationships\nCREATE TABLE users (\n    user_id INT PRIMARY KEY,\n    username VARCHAR(50) NOT NULL,\n    email VARCHAR(100) NOT NULL\n);\n\nCREATE TABLE posts (\n    post_id INT PRIMARY KEY,\n    user_id INT,\n    title VARCHAR(200),\n    content TEXT,\n    created_date TIMESTAMP,\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\n);\n\nCREATE TABLE comments (\n    comment_id INT PRIMARY KEY,\n    post_id INT,\n    user_id INT,\n    content TEXT,\n    FOREIGN KEY (post_id) REFERENCES posts(post_id),\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\n);\n\n-- Complex JOIN query\nSELECT \n    u.username,\n    p.title,\n    COUNT(c.comment_id) as comment_count\nFROM users u\nJOIN posts p ON u.user_id = p.user_id\nLEFT JOIN comments c ON p.post_id = c.post_id\nGROUP BY u.username, p.title\nHAVING COUNT(c.comment_id) > 5;\n\n-- NoSQL (Document Store - MongoDB style) example\n-- Denormalized, embedded documents\n-- {\n--   \"_id\": \"user123\",\n--   \"username\": \"john_doe\",\n--   \"email\": \"john@example.com\",\n--   \"posts\": [\n--     {\n--       \"post_id\": \"post1\",\n--       \"title\": \"My First Post\",\n--       \"content\": \"Hello world\",\n--       \"created_date\": \"2024-01-01\",\n--       \"comments\": [\n--         {\n--           \"comment_id\": \"c1\",\n--           \"user\": \"jane_doe\",\n--           \"content\": \"Great post!\"\n--         }\n--       ]\n--     }\n--   ]\n-- }\n\n-- Simple query (no joins needed)\n-- db.users.find({ \"username\": \"john_doe\" })\n\n-- Key-Value Store (Redis style)\n-- SET user:123:name \"John Doe\"\n-- SET user:123:email \"john@example.com\"\n-- GET user:123:name\n-- HSET user:123 name \"John Doe\" email \"john@example.com\"\n\n-- Column-Family Store (Cassandra style)\n-- CREATE TABLE sensor_data (\n--     sensor_id UUID,\n--     timestamp TIMESTAMP,\n--     temperature DOUBLE,\n--     humidity DOUBLE,\n--     PRIMARY KEY (sensor_id, timestamp)\n-- ) WITH CLUSTERING ORDER BY (timestamp DESC);\n\n-- Hybrid approach (Polyglot Persistence)\n-- Use both SQL and NoSQL for different parts\n\n-- SQL for transactional data\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date DATE,\n    total DECIMAL(10,2),\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n\n-- NoSQL (Redis) for caching\n-- CACHE KEY: \"order:12345\"\n-- CACHE VALUE: {\"order_id\": 12345, \"status\": \"shipped\", \"total\": 99.99}\n-- EXPIRE order:12345 3600\n\n-- NoSQL (MongoDB) for product catalog (flexible attributes)\n-- {\n--   \"product_id\": \"prod123\",\n--   \"name\": \"Laptop\",\n--   \"price\": 999.99,\n--   \"specs\": {\n--     \"processor\": \"Intel i7\",\n--     \"ram\": \"16GB\",\n--     \"storage\": \"512GB SSD\"\n--   },\n--   \"reviews\": [...]\n-- }\n\n-- Decision matrix\n-- Use SQL when:\n-- SELECT * FROM orders\n-- WHERE customer_id = 123\n-- AND order_date BETWEEN '2024-01-01' AND '2024-12-31'\n-- AND status IN ('Shipped', 'Delivered')\n-- AND total > 100\n-- ORDER BY order_date DESC;\n-- (Complex filtering, sorting, aggregation)\n\n-- Use NoSQL when:\n-- Single document lookup by ID\n-- Flexible schema needed\n-- Horizontal scaling required\n-- Simple access patterns"
    },
    {
      "id": 46,
      "question": "What is a savepoint in SQL transactions?",
      "answer": "A savepoint is a marker within a transaction that allows you to rollback to a specific point without aborting the entire transaction.\n\nKey Features:\nâ€¢ Creates restore point within transaction\nâ€¢ Multiple savepoints allowed\nâ€¢ Rollback to specific savepoint\nâ€¢ Nested transaction-like behavior\nâ€¢ Part of transaction, not separate transaction\nâ€¢ Released or rolled back\n\nSavepoint Operations:\nâ€¢ SAVEPOINT name - creates savepoint\nâ€¢ ROLLBACK TO savepoint_name - reverts to savepoint\nâ€¢ RELEASE SAVEPOINT name - removes savepoint\n\nBenefits:\nâ€¢ Partial rollback capability\nâ€¢ Better error handling\nâ€¢ Complex transaction management\nâ€¢ Reduce need to restart entire transaction\nâ€¢ Try-catch within transaction\n\nCommon Use Cases:\nâ€¢ Multi-step operations\nâ€¢ Batch processing with error recovery\nâ€¢ Complex business logic\nâ€¢ Optional operations within transaction\nâ€¢ Testing scenarios\n\nLimitations:\nâ€¢ Not all databases support savepoints\nâ€¢ Overhead for many savepoints\nâ€¢ Still within main transaction isolation",
      "explanation": "Savepoints create markers within transactions enabling partial rollbacks to specific points without aborting the entire transaction.",
      "difficulty": "Medium",
      "code": "-- Basic savepoint usage\nBEGIN TRANSACTION;\n\n-- First operation\nINSERT INTO accounts (account_id, balance) VALUES (1, 1000);\nSAVEPOINT after_insert;\n\n-- Second operation\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1;\nSAVEPOINT after_update;\n\n-- Third operation that fails\nINSERT INTO invalid_table VALUES (1); -- Error!\n\n-- Rollback only to after_update, keeping insert and update\nROLLBACK TO after_update;\n\n-- Continue with valid operations\nINSERT INTO transactions (account_id, amount) VALUES (1, -100);\n\nCOMMIT;\n\n-- Multiple savepoints example\nBEGIN TRANSACTION;\n\nINSERT INTO orders (order_id, customer_id, total) VALUES (1, 100, 500);\nSAVEPOINT order_created;\n\nINSERT INTO order_items (order_id, product_id, quantity) VALUES (1, 1, 2);\nSAVEPOINT item1_added;\n\nINSERT INTO order_items (order_id, product_id, quantity) VALUES (1, 2, 1);\nSAVEPOINT item2_added;\n\n-- Undo last item only\nROLLBACK TO item2_added;\n\n-- Add different item\nINSERT INTO order_items (order_id, product_id, quantity) VALUES (1, 3, 1);\n\nCOMMIT;\n\n-- Batch processing with savepoints\nBEGIN TRANSACTION;\n\nDECLARE @processed INT = 0;\nDECLARE @errors INT = 0;\nDECLARE @batch_id INT;\n\nDECLARE batch_cursor CURSOR FOR\nSELECT batch_id FROM pending_batches;\n\nOPEN batch_cursor;\nFETCH NEXT FROM batch_cursor INTO @batch_id;\n\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    SAVEPOINT batch_start;\n    \n    BEGIN TRY\n        -- Process batch\n        EXEC ProcessBatch @batch_id;\n        SET @processed = @processed + 1;\n    END TRY\n    BEGIN CATCH\n        -- Rollback this batch only\n        ROLLBACK TO batch_start;\n        \n        -- Log error\n        INSERT INTO error_log (batch_id, error_message)\n        VALUES (@batch_id, ERROR_MESSAGE());\n        \n        SET @errors = @errors + 1;\n    END CATCH\n    \n    FETCH NEXT FROM batch_cursor INTO @batch_id;\nEND\n\nCLOSE batch_cursor;\nDEALLOCATE batch_cursor;\n\n-- Commit all successfully processed batches\nCOMMIT;\n\nPRINT 'Processed: ' + CAST(@processed AS VARCHAR);\nPRINT 'Errors: ' + CAST(@errors AS VARCHAR);\n\n-- Complex business logic with optional steps\nBEGIN TRANSACTION;\n\n-- Required: Create customer\nINSERT INTO customers (name, email) VALUES ('John Doe', 'john@example.com');\nDECLARE @customer_id INT = SCOPE_IDENTITY();\nSAVEPOINT customer_created;\n\n-- Optional: Add loyalty card\nBEGIN TRY\n    INSERT INTO loyalty_cards (customer_id, card_number)\n    VALUES (@customer_id, 'LC12345');\n    SAVEPOINT loyalty_added;\nEND TRY\nBEGIN CATCH\n    -- Loyalty failed but customer creation still valid\n    ROLLBACK TO customer_created;\n    PRINT 'Loyalty card creation failed';\nEND CATCH\n\n-- Optional: Send welcome email\nBEGIN TRY\n    EXEC SendWelcomeEmail @customer_id;\nEND TRY\nBEGIN CATCH\n    -- Email failed but customer still created\n    PRINT 'Welcome email failed';\nEND CATCH\n\nCOMMIT; -- Commit at least customer creation\n\n-- Releasing savepoints\nBEGIN TRANSACTION;\n\nINSERT INTO products (name, price) VALUES ('Product A', 100);\nSAVEPOINT sp1;\n\nUPDATE products SET price = 120 WHERE name = 'Product A';\nSAVEPOINT sp2;\n\n-- No longer need sp1\nRELEASE SAVEPOINT sp1;\n\n-- sp2 still available\nROLLBACK TO sp2; -- OK\n-- ROLLBACK TO sp1; -- ERROR: sp1 released\n\nCOMMIT;\n\n-- PostgreSQL savepoint syntax\nBEGIN;\n\nINSERT INTO accounts VALUES (1, 1000);\nSAVEPOINT my_savepoint;\n\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n\n-- Undo update\nROLLBACK TO SAVEPOINT my_savepoint;\n\nCOMMIT;"
    },
    {
      "id": 47,
      "question": "What is the difference between HAVING and WHERE with aggregate functions?",
      "answer": "WHERE and HAVING both filter data but operate at different stages of query execution with different capabilities regarding aggregate functions.\n\nWHERE Clause:\nâ€¢ Filters rows before grouping\nâ€¢ Operates on individual rows\nâ€¢ Cannot use aggregate functions\nâ€¢ Executed before GROUP BY\nâ€¢ Reduces rows before aggregation\nâ€¢ More efficient (filters early)\nâ€¢ Works with non-aggregated columns\n\nHAVING Clause:\nâ€¢ Filters groups after grouping\nâ€¢ Operates on grouped results\nâ€¢ Can use aggregate functions\nâ€¢ Executed after GROUP BY\nâ€¢ Filters aggregated results\nâ€¢ Applied after aggregation\nâ€¢ Works with aggregated values\n\nExecution Order:\n1. FROM - get tables\n2. WHERE - filter rows\n3. GROUP BY - group rows\n4. Aggregate functions - calculate\n5. HAVING - filter groups\n6. SELECT - choose columns\n7. ORDER BY - sort results\n\nBest Practices:\nâ€¢ Use WHERE when possible (more efficient)\nâ€¢ Use HAVING only for aggregate conditions\nâ€¢ Combine both for complex filtering\nâ€¢ Filter non-aggregated columns with WHERE",
      "explanation": "WHERE filters individual rows before grouping and cannot use aggregates, while HAVING filters grouped results after aggregation and can use aggregate functions.",
      "difficulty": "Easy",
      "code": "-- WHERE: Filter before grouping (no aggregates)\nSELECT department, COUNT(*) as emp_count\nFROM employees\nWHERE salary > 50000 -- Filter individual rows\nGROUP BY department;\n-- Only high-salary employees are grouped\n\n-- HAVING: Filter after grouping (with aggregates)\nSELECT department, AVG(salary) as avg_salary\nFROM employees\nGROUP BY department\nHAVING AVG(salary) > 60000; -- Filter grouped results\n-- Only departments with high average included\n\n-- Cannot use aggregate in WHERE (ERROR)\nSELECT department, COUNT(*) as emp_count\nFROM employees\nWHERE COUNT(*) > 5 -- ERROR!\nGROUP BY department;\n\n-- Correct: Use HAVING for aggregate\nSELECT department, COUNT(*) as emp_count\nFROM employees\nGROUP BY department\nHAVING COUNT(*) > 5; -- OK\n\n-- Combining WHERE and HAVING\nSELECT \n    department,\n    COUNT(*) as emp_count,\n    AVG(salary) as avg_salary\nFROM employees\nWHERE hire_date > '2020-01-01' -- Filter rows first\nGROUP BY department\nHAVING COUNT(*) > 10 -- Then filter groups\n   AND AVG(salary) > 55000;\n\n-- Performance comparison\n-- Less efficient: Filter after grouping\nSELECT department, COUNT(*) as emp_count\nFROM employees\nGROUP BY department\nHAVING department IN ('IT', 'Sales'); -- Processes all departments first\n\n-- More efficient: Filter before grouping\nSELECT department, COUNT(*) as emp_count\nFROM employees\nWHERE department IN ('IT', 'Sales') -- Only process IT and Sales\nGROUP BY department;\n\n-- Complex example with both\nSELECT \n    product_category,\n    COUNT(DISTINCT customer_id) as unique_customers,\n    SUM(quantity * price) as total_revenue,\n    AVG(quantity) as avg_quantity\nFROM orders\nWHERE order_date >= '2024-01-01' -- Filter: Orders from 2024\n  AND status = 'Completed' -- Filter: Completed orders only\nGROUP BY product_category\nHAVING SUM(quantity * price) > 10000 -- Filter: High revenue categories\n   AND COUNT(DISTINCT customer_id) > 50 -- Filter: Popular categories\nORDER BY total_revenue DESC;\n\n-- Multiple aggregates in HAVING\nSELECT \n    salesperson_id,\n    COUNT(*) as deal_count,\n    SUM(deal_value) as total_value,\n    AVG(deal_value) as avg_value\nFROM sales\nGROUP BY salesperson_id\nHAVING COUNT(*) >= 10 -- At least 10 deals\n   AND SUM(deal_value) > 100000 -- Total over 100K\n   AND AVG(deal_value) > 5000; -- Average over 5K\n\n-- Using alias in ORDER BY but not in HAVING\nSELECT \n    department,\n    AVG(salary) as avg_sal\nFROM employees\nGROUP BY department\nHAVING AVG(salary) > 60000 -- Must use function, not alias\nORDER BY avg_sal DESC; -- Can use alias here\n\n-- Practical example: Finding active customers\nSELECT \n    customer_id,\n    COUNT(order_id) as order_count,\n    SUM(total) as total_spent\nFROM orders\nWHERE order_date >= DATE_SUB(CURDATE(), INTERVAL 1 YEAR) -- Last year\n  AND status != 'Cancelled' -- Exclude cancelled\nGROUP BY customer_id\nHAVING COUNT(order_id) >= 5 -- At least 5 orders\n   AND SUM(total) >= 1000; -- Spent at least 1000"
    },
    {
      "id": 48,
      "question": "What is the purpose of the EXPLAIN command?",
      "answer": "EXPLAIN shows how the database will execute a query, providing insight into the query execution plan without actually running the query.\n\nPurpose:\nâ€¢ Analyze query performance\nâ€¢ Identify optimization opportunities\nâ€¢ Understand query execution strategy\nâ€¢ Find missing indexes\nâ€¢ Detect inefficient operations\nâ€¢ Compare alternative query approaches\n\nTypes of EXPLAIN:\n\nEXPLAIN (PostgreSQL/MySQL):\nâ€¢ Shows estimated execution plan\nâ€¢ Doesn't execute query\nâ€¢ Fast to generate\nâ€¢ Shows predicted costs\n\nEXPLAIN ANALYZE (PostgreSQL):\nâ€¢ Executes query\nâ€¢ Shows actual execution plan\nâ€¢ Includes actual timings and row counts\nâ€¢ More accurate but slower\n\nKey Metrics:\nâ€¢ Type: Access method (ALL, index, ref, eq_ref)\nâ€¢ Possible_keys: Indexes that could be used\nâ€¢ Key: Index actually used\nâ€¢ Rows: Estimated rows examined\nâ€¢ Extra: Additional information\nâ€¢ Cost: Estimated query cost\n\nProblems to Look For:\nâ€¢ Type: ALL (full table scan)\nâ€¢ High row count\nâ€¢ Using filesort\nâ€¢ Using temporary\nâ€¢ No index used",
      "explanation": "EXPLAIN reveals query execution plans showing how databases will process queries, helping identify performance bottlenecks and optimization opportunities.",
      "difficulty": "Medium",
      "code": "-- Basic EXPLAIN (MySQL)\nEXPLAIN SELECT * FROM employees WHERE salary > 50000;\n\n-- Output columns explained:\n-- id: Query identifier\n-- select_type: Type of SELECT (SIMPLE, SUBQUERY, etc.)\n-- table: Table being accessed\n-- type: Join type (system, const, eq_ref, ref, range, index, ALL)\n-- possible_keys: Indexes that might be used\n-- key: Index actually used\n-- key_len: Length of key used\n-- ref: Columns compared to index\n-- rows: Estimated rows to examine\n-- Extra: Additional information\n\n-- EXPLAIN with JOIN\nEXPLAIN\nSELECT e.name, d.dept_name\nFROM employees e\nJOIN departments d ON e.department_id = d.dept_id\nWHERE e.salary > 50000;\n\n-- PostgreSQL EXPLAIN with costs\nEXPLAIN (COSTS, VERBOSE)\nSELECT * FROM employees WHERE salary > 50000;\n\n-- PostgreSQL EXPLAIN ANALYZE (executes query)\nEXPLAIN ANALYZE\nSELECT e.name, COUNT(o.order_id) as order_count\nFROM employees e\nLEFT JOIN orders o ON e.employee_id = o.employee_id\nGROUP BY e.name\nHAVING COUNT(o.order_id) > 5;\n\n-- SQL Server execution plan\nSET SHOWPLAN_ALL ON;\nGO\nSELECT * FROM employees WHERE salary > 50000;\nGO\nSET SHOWPLAN_ALL OFF;\nGO\n\n-- Identifying problems\n-- Problem 1: Full table scan\nEXPLAIN SELECT * FROM employees WHERE YEAR(hire_date) = 2023;\n-- Output: type: ALL, rows: 1000000 (BAD)\n\n-- Solution: Rewrite to use index\nEXPLAIN SELECT * FROM employees \nWHERE hire_date >= '2023-01-01' AND hire_date < '2024-01-01';\n-- Output: type: range, key: idx_hire_date, rows: 10000 (GOOD)\n\n-- Problem 2: Not using index\nEXPLAIN SELECT * FROM products WHERE LOWER(name) = 'laptop';\n-- Output: type: ALL (function prevents index use)\n\n-- Solution: Use case-insensitive collation or functional index\nEXPLAIN SELECT * FROM products WHERE name = 'laptop';\n-- Output: type: ref, key: idx_name\n\n-- Comparing query alternatives\n-- Option 1: Subquery\nEXPLAIN\nSELECT name FROM employees\nWHERE department_id IN (SELECT dept_id FROM departments WHERE location = 'NY');\n\n-- Option 2: JOIN\nEXPLAIN\nSELECT DISTINCT e.name\nFROM employees e\nJOIN departments d ON e.department_id = d.dept_id\nWHERE d.location = 'NY';\n-- Compare costs and choose better option\n\n-- EXPLAIN with complex query\nEXPLAIN FORMAT=JSON\nSELECT \n    c.name,\n    COUNT(o.order_id) as order_count,\n    SUM(o.total) as total_spent\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nWHERE c.country = 'USA'\n  AND o.order_date >= '2024-01-01'\nGROUP BY c.customer_id, c.name\nHAVING COUNT(o.order_id) > 5\nORDER BY total_spent DESC\nLIMIT 10;\n\n-- Analyzing index effectiveness\n-- Without index\nEXPLAIN SELECT * FROM orders WHERE customer_id = 123;\n-- Output: type: ALL, rows: 1000000\n\n-- Create index\nCREATE INDEX idx_customer ON orders(customer_id);\n\n-- With index\nEXPLAIN SELECT * FROM orders WHERE customer_id = 123;\n-- Output: type: ref, key: idx_customer, rows: 15\n\n-- EXPLAIN with EXPLAIN options (PostgreSQL)\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE, COSTS, TIMING)\nSELECT * FROM large_table WHERE indexed_column = 'value';\n\n-- Reading EXPLAIN output\n-- Good indicators:\n-- â€¢ type: const, eq_ref, ref (index lookups)\n-- â€¢ Low row count\n-- â€¢ Using index (covering index)\n-- â€¢ Using where with index\n\n-- Bad indicators:\n-- â€¢ type: ALL (full scan)\n-- â€¢ High row count on large tables\n-- â€¢ Using filesort\n-- â€¢ Using temporary\n-- â€¢ Using join buffer (could mean missing index)"
    },
    {
      "id": 49,
      "question": "What are materialized views and how do they differ from regular views?",
      "answer": "Materialized views store query results physically, unlike regular views which are virtual and execute queries each time they are accessed.\n\nRegular Views:\nâ€¢ Virtual table (no data stored)\nâ€¢ Query executed every time view is accessed\nâ€¢ Always shows current data\nâ€¢ No storage overhead\nâ€¢ Slower for complex queries\nâ€¢ Automatically updated\nâ€¢ No indexes (except indexed views)\n\nMaterialized Views:\nâ€¢ Physical storage of query results\nâ€¢ Query executed once, results stored\nâ€¢ Shows snapshot of data\nâ€¢ Requires storage space\nâ€¢ Faster query performance\nâ€¢ Requires manual or scheduled refresh\nâ€¢ Can have indexes\nâ€¢ Also called indexed views (SQL Server)\n\nRefresh Strategies:\nâ€¢ Complete refresh - rebuild entire view\nâ€¢ Incremental refresh - update only changes\nâ€¢ On demand - manual refresh\nâ€¢ On commit - update with each transaction\nâ€¢ Scheduled - refresh at intervals\n\nWhen to Use Materialized Views:\nâ€¢ Complex aggregations\nâ€¢ Expensive joins\nâ€¢ Data doesn't change frequently\nâ€¢ Read-heavy workloads\nâ€¢ Reporting and analytics\nâ€¢ When slight staleness acceptable",
      "explanation": "Materialized views physically store query results for fast access unlike regular views which are virtual, requiring refresh to update but providing better performance.",
      "difficulty": "Medium",
      "code": "-- Regular View (MySQL/PostgreSQL/SQL Server)\nCREATE VIEW employee_summary AS\nSELECT \n    department,\n    COUNT(*) as emp_count,\n    AVG(salary) as avg_salary,\n    MAX(salary) as max_salary\nFROM employees\nGROUP BY department;\n\n-- Query executes every time\nSELECT * FROM employee_summary;\n-- Runs aggregation on employees table each time\n\n-- Materialized View (PostgreSQL)\nCREATE MATERIALIZED VIEW employee_summary_mv AS\nSELECT \n    department,\n    COUNT(*) as emp_count,\n    AVG(salary) as avg_salary,\n    MAX(salary) as max_salary\nFROM employees\nGROUP BY department;\n\n-- Query is fast (reads stored results)\nSELECT * FROM employee_summary_mv;\n-- No aggregation, just table read\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW employee_summary_mv;\n\n-- Concurrent refresh (non-blocking)\nREFRESH MATERIALIZED VIEW CONCURRENTLY employee_summary_mv;\n-- Requires unique index\n\n-- Oracle Materialized View with automatic refresh\nCREATE MATERIALIZED VIEW sales_summary\nBUILD IMMEDIATE\nREFRESH FAST ON COMMIT\nAS\nSELECT \n    product_id,\n    SUM(quantity) as total_quantity,\n    SUM(amount) as total_amount\nFROM sales\nGROUP BY product_id;\n\n-- SQL Server Indexed View (Materialized View equivalent)\nCREATE VIEW dbo.product_sales_indexed\nWITH SCHEMABINDING\nAS\nSELECT \n    p.product_id,\n    p.product_name,\n    SUM(s.quantity) as total_quantity,\n    SUM(s.amount) as total_amount,\n    COUNT_BIG(*) as row_count\nFROM dbo.products p\nJOIN dbo.sales s ON p.product_id = s.product_id\nGROUP BY p.product_id, p.product_name;\n\n-- Create clustered index (makes it materialized)\nCREATE UNIQUE CLUSTERED INDEX idx_product_sales\nON dbo.product_sales_indexed(product_id);\n\n-- Complex materialized view for reporting\nCREATE MATERIALIZED VIEW customer_analytics AS\nSELECT \n    c.customer_id,\n    c.name,\n    c.email,\n    COUNT(DISTINCT o.order_id) as total_orders,\n    SUM(o.total) as lifetime_value,\n    AVG(o.total) as avg_order_value,\n    MAX(o.order_date) as last_order_date,\n    MIN(o.order_date) as first_order_date\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.name, c.email;\n\n-- Create index on materialized view\nCREATE INDEX idx_lifetime_value \nON customer_analytics(lifetime_value DESC);\n\n-- Fast query using indexed materialized view\nSELECT * FROM customer_analytics\nWHERE lifetime_value > 10000\nORDER BY lifetime_value DESC\nLIMIT 100;\n\n-- Scheduled refresh (PostgreSQL with cron)\n-- Create refresh function\nCREATE OR REPLACE FUNCTION refresh_analytics()\nRETURNS void AS $$\nBEGIN\n    REFRESH MATERIALIZED VIEW CONCURRENTLY customer_analytics;\n    REFRESH MATERIALIZED VIEW CONCURRENTLY sales_summary;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Schedule: Call from pg_cron or external scheduler\n-- SELECT cron.schedule('refresh_analytics', '0 2 * * *', 'SELECT refresh_analytics()');\n\n-- Incremental refresh approach\n-- Track last refresh time\nCREATE TABLE mv_refresh_log (\n    view_name VARCHAR(100),\n    last_refresh_time TIMESTAMP\n);\n\n-- Insert only new/changed data\nCREATE MATERIALIZED VIEW incremental_sales AS\nSELECT \n    DATE(order_date) as order_day,\n    SUM(total) as daily_total\nFROM orders\nWHERE order_date >= (SELECT last_refresh_time FROM mv_refresh_log WHERE view_name = 'incremental_sales')\nGROUP BY DATE(order_date);\n\n-- Drop materialized view\nDROP MATERIALIZED VIEW employee_summary_mv;\n\n-- Performance comparison\n-- Regular view (slow on large dataset)\nCREATE VIEW complex_report AS\nSELECT \n    c.country,\n    p.category,\n    COUNT(*) as order_count,\n    SUM(oi.quantity * oi.price) as revenue\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nJOIN products p ON oi.product_id = p.product_id\nGROUP BY c.country, p.category;\n-- Query time: ~30 seconds on 10M rows\n\n-- Materialized view (fast)\nCREATE MATERIALIZED VIEW complex_report_mv AS\nSELECT \n    c.country,\n    p.category,\n    COUNT(*) as order_count,\n    SUM(oi.quantity * oi.price) as revenue\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nJOIN products p ON oi.product_id = p.product_id\nGROUP BY c.country, p.category;\n-- Query time: ~0.1 seconds (300x faster)"
    },
    {
      "id": 50,
      "question": "What is database replication and what are its types?",
      "answer": "Database replication is copying and maintaining database objects in multiple databases that make up a distributed database system.\n\nPurpose:\nâ€¢ High availability\nâ€¢ Load distribution\nâ€¢ Disaster recovery\nâ€¢ Geographic distribution\nâ€¢ Read scalability\nâ€¢ Backup without impacting production\n\nReplication Types:\n\nMaster-Slave (Primary-Replica):\nâ€¢ One master, multiple slaves\nâ€¢ Master handles writes\nâ€¢ Slaves handle reads\nâ€¢ One-way replication\nâ€¢ Most common type\n\nMaster-Master (Multi-Master):\nâ€¢ Multiple masters\nâ€¢ All can accept writes\nâ€¢ Bidirectional replication\nâ€¢ Conflict resolution needed\nâ€¢ Higher complexity\n\nReplication Methods:\n\nSynchronous Replication:\nâ€¢ Transaction waits for replica confirmation\nâ€¢ Strong consistency\nâ€¢ Slower writes\nâ€¢ No data loss\n\nAsynchronous Replication:\nâ€¢ Transaction doesn't wait\nâ€¢ Eventual consistency\nâ€¢ Faster writes\nâ€¢ Possible data loss\nâ€¢ Replication lag\n\nSemi-Synchronous:\nâ€¢ Wait for at least one replica\nâ€¢ Balance of both approaches",
      "explanation": "Database replication copies data across multiple databases for availability and scalability using master-slave or master-master topologies with synchronous or asynchronous methods.",
      "difficulty": "Hard",
      "code": "-- MySQL Master-Slave Replication Setup\n-- On Master Server:\n-- 1. Configure my.cnf\n-- [mysqld]\n-- server-id = 1\n-- log-bin = /var/log/mysql/mysql-bin.log\n-- binlog-do-db = production_db\n\n-- 2. Create replication user\nCREATE USER 'repl_user'@'%' IDENTIFIED BY 'password';\nGRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';\nFLUSH PRIVILEGES;\n\n-- 3. Get master status\nSHOW MASTER STATUS;\n-- Note: File and Position values\n\n-- On Slave Server:\n-- 1. Configure my.cnf\n-- [mysqld]\n-- server-id = 2\n-- relay-log = /var/log/mysql/mysql-relay-bin\n-- read-only = 1\n\n-- 2. Configure replication\nCHANGE MASTER TO\n    MASTER_HOST='master_ip',\n    MASTER_USER='repl_user',\n    MASTER_PASSWORD='password',\n    MASTER_LOG_FILE='mysql-bin.000001',\n    MASTER_LOG_POS=107;\n\n-- 3. Start replication\nSTART SLAVE;\n\n-- 4. Check status\nSHOW SLAVE STATUS\\G\n-- Look for:\n-- Slave_IO_Running: Yes\n-- Slave_SQL_Running: Yes\n-- Seconds_Behind_Master: 0\n\n-- PostgreSQL Streaming Replication\n-- On Primary Server:\n-- 1. Configure postgresql.conf\n-- wal_level = replica\n-- max_wal_senders = 3\n-- wal_keep_size = 64MB\n\n-- 2. Configure pg_hba.conf\n-- host replication repl_user standby_ip/32 md5\n\n-- 3. Create replication user\nCREATE ROLE repl_user WITH REPLICATION LOGIN PASSWORD 'password';\n\n-- On Standby Server:\n-- 1. Create base backup\n-- pg_basebackup -h primary_ip -D /var/lib/postgresql/data -U repl_user -P -v\n\n-- 2. Create recovery.conf or standby.signal\n-- primary_conninfo = 'host=primary_ip port=5432 user=repl_user password=password'\n-- promote_trigger_file = '/tmp/promote_standby'\n\n-- Monitoring replication lag\n-- PostgreSQL\nSELECT \n    client_addr,\n    state,\n    sent_lsn,\n    write_lsn,\n    flush_lsn,\n    replay_lsn,\n    sync_state,\n    pg_wal_lsn_diff(sent_lsn, replay_lsn) AS replication_lag_bytes\nFROM pg_stat_replication;\n\n-- MySQL\nSHOW SLAVE STATUS\\G\n-- Check: Seconds_Behind_Master\n\n-- Application-level read/write splitting\n-- Route writes to master, reads to slaves\n\n-- Write query (goes to master)\n-- Connection: master_db\nINSERT INTO orders (customer_id, total) VALUES (123, 99.99);\nUPDATE inventory SET quantity = quantity - 1 WHERE product_id = 456;\n\n-- Read queries (go to slaves)\n-- Connection: slave_db (round-robin or random)\nSELECT * FROM products WHERE category = 'Electronics';\nSELECT * FROM customers WHERE customer_id = 123;\n\n-- Load balancer configuration (HAProxy example)\n-- frontend mysql_front\n--     bind *:3306\n--     default_backend mysql_write\n\n-- backend mysql_write\n--     server master 10.0.0.1:3306 check\n\n-- backend mysql_read\n--     balance roundrobin\n--     server slave1 10.0.0.2:3306 check\n--     server slave2 10.0.0.3:3306 check\n--     server slave3 10.0.0.4:3306 check\n\n-- Handling replication conflicts (Multi-Master)\n-- Timestamp-based resolution\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    price DECIMAL(10,2),\n    updated_at TIMESTAMP,\n    updated_by VARCHAR(50)\n);\n\n-- Use triggers or application logic\n-- Keep version with latest timestamp\n\n-- Failover scenario\n-- 1. Detect master failure\n-- 2. Promote slave to master\nSTOP SLAVE;\nRESET SLAVE ALL;\n-- Remove read-only\nSET GLOBAL read_only = 0;\n\n-- 3. Point other slaves to new master\nCHANGE MASTER TO\n    MASTER_HOST='new_master_ip',\n    MASTER_USER='repl_user',\n    MASTER_PASSWORD='password';\nSTART SLAVE;\n\n-- Replication topologies\n-- Chain replication: Master -> Slave1 -> Slave2\n-- Fan-out: Master -> Slave1, Slave2, Slave3\n-- Circular: Server1 <-> Server2 <-> Server3\n\n-- Performance monitoring\nCREATE TABLE replication_metrics (\n    metric_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    server_name VARCHAR(50),\n    lag_seconds INT,\n    bytes_behind BIGINT\n);\n\n-- Log metrics periodically\nINSERT INTO replication_metrics (server_name, lag_seconds)\nSELECT 'slave1', Seconds_Behind_Master \nFROM (SHOW SLAVE STATUS) AS s;"
    },
    {
      "id": 51,
      "question": "What are data types in SQL and how do you choose them?",
      "answer": "Data types define the kind of data that can be stored in a column, affecting storage, performance, and data integrity.\n\nNumeric Types:\nâ€¢ INT/INTEGER - whole numbers\nâ€¢ BIGINT - large integers\nâ€¢ SMALLINT - small integers\nâ€¢ DECIMAL/NUMERIC - exact decimal numbers\nâ€¢ FLOAT/REAL - approximate decimals\nâ€¢ DOUBLE - double precision floats\n\nString Types:\nâ€¢ CHAR(n) - fixed length string\nâ€¢ VARCHAR(n) - variable length string\nâ€¢ TEXT - large text data\nâ€¢ NCHAR/NVARCHAR - Unicode strings\n\nDate/Time Types:\nâ€¢ DATE - date only\nâ€¢ TIME - time only\nâ€¢ DATETIME/TIMESTAMP - date and time\nâ€¢ YEAR - year only\n\nOther Types:\nâ€¢ BOOLEAN/BIT - true/false\nâ€¢ BLOB - binary large objects\nâ€¢ JSON - JSON documents\nâ€¢ UUID - unique identifiers\nâ€¢ ENUM - predefined set of values\n\nChoosing Data Types:\nâ€¢ Use smallest type that fits data\nâ€¢ Consider future growth\nâ€¢ Match application language types\nâ€¢ Consider storage and index size\nâ€¢ Performance implications\nâ€¢ Use appropriate precision",
      "explanation": "SQL data types define what kind of values columns can store; choose based on data requirements, storage efficiency, and performance considerations.",
      "difficulty": "Easy",
      "code": "-- Numeric types selection\nCREATE TABLE products (\n    product_id INT,                    -- Up to ~2 billion\n    product_code BIGINT,               -- Up to ~9 quintillion\n    quantity SMALLINT,                 -- Up to ~32,000\n    price DECIMAL(10,2),               -- Exact: $99999999.99\n    discount_percent DECIMAL(5,2),     -- 0.00 to 999.99\n    weight FLOAT,                      -- Approximate\n    rating DECIMAL(3,2)                -- 0.00 to 9.99\n);\n\n-- String types selection\nCREATE TABLE users (\n    user_id INT,\n    username VARCHAR(50),              -- Variable, max 50\n    password_hash CHAR(64),            -- Fixed 64 chars (SHA-256)\n    email VARCHAR(255),                -- Email addresses\n    bio TEXT,                          -- Long text\n    country_code CHAR(2),              -- Fixed 2 chars (US, UK)\n    status ENUM('active', 'inactive', 'suspended') -- Predefined values\n);\n\n-- Date/Time types\nCREATE TABLE orders (\n    order_id INT,\n    order_date DATE,                   -- 2024-01-15\n    order_time TIME,                   -- 14:30:00\n    created_at TIMESTAMP,              -- 2024-01-15 14:30:00\n    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    birth_year YEAR                    -- 1990\n);\n\n-- Boolean and binary\nCREATE TABLE settings (\n    setting_id INT,\n    is_active BOOLEAN,                 -- TRUE/FALSE\n    feature_flags BIT(8),              -- 8 bits\n    profile_image BLOB,                -- Binary data\n    preferences JSON                   -- JSON document\n);\n\n-- UUID for distributed systems\nCREATE TABLE sessions (\n    session_id UUID PRIMARY KEY,       -- Unique across systems\n    user_id INT,\n    created_at TIMESTAMP\n);\n\n-- Poor choices\nCREATE TABLE bad_example (\n    age VARCHAR(3),                    -- Should be INT or SMALLINT\n    price VARCHAR(20),                 -- Should be DECIMAL\n    is_active VARCHAR(10),             -- Should be BOOLEAN\n    created BIGINT                     -- Should be TIMESTAMP\n);\n\n-- Good choices\nCREATE TABLE good_example (\n    age SMALLINT CHECK (age > 0 AND age < 150),\n    price DECIMAL(10,2) CHECK (price >= 0),\n    is_active BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Storage size comparison\nCREATE TABLE storage_comparison (\n    tiny_num TINYINT,                  -- 1 byte\n    small_num SMALLINT,                -- 2 bytes\n    medium_num MEDIUMINT,              -- 3 bytes\n    regular_num INT,                   -- 4 bytes\n    big_num BIGINT,                    -- 8 bytes\n    char_fixed CHAR(10),               -- 10 bytes\n    varchar_var VARCHAR(10),           -- Length + data\n    decimal_num DECIMAL(10,2),         -- 5 bytes\n    float_num FLOAT,                   -- 4 bytes\n    double_num DOUBLE                  -- 8 bytes\n);\n\n-- Precision matters for money\nCREATE TABLE financial_transactions (\n    transaction_id INT PRIMARY KEY,\n    amount DECIMAL(15,2),              -- Exact precision\n    -- NOT FLOAT! 0.1 + 0.2 != 0.3 with float\n    tax DECIMAL(15,4),                 -- More precision for calculations\n    total AS (amount + tax) STORED     -- Computed column\n);\n\n-- Data type conversion\nSELECT \n    CAST(price AS INT) as price_int,\n    CAST(created_at AS DATE) as created_date,\n    CONVERT(VARCHAR(10), order_date, 120) as date_string\nFROM orders;\n\n-- Implicit conversion issues\nSELECT * FROM users WHERE user_id = '123'; -- String to INT conversion\nSELECT * FROM orders WHERE order_date = 20240115; -- INT to DATE\n\n-- Explicit conversion (better)\nSELECT * FROM users WHERE user_id = CAST('123' AS INT);\nSELECT * FROM orders WHERE order_date = CAST('2024-01-15' AS DATE);\n\n-- JSON data type usage\nCREATE TABLE products_with_json (\n    product_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    attributes JSON\n);\n\nINSERT INTO products_with_json VALUES\n(1, 'Laptop', '{\"brand\": \"Dell\", \"ram\": \"16GB\", \"storage\": \"512GB SSD\"}');\n\n-- Query JSON data\nSELECT \n    name,\n    JSON_EXTRACT(attributes, '$.brand') as brand,\n    JSON_EXTRACT(attributes, '$.ram') as ram\nFROM products_with_json;\n\n-- ENUM type usage\nCREATE TABLE orders_with_enum (\n    order_id INT PRIMARY KEY,\n    status ENUM('pending', 'processing', 'shipped', 'delivered', 'cancelled')\n);\n\n-- Efficient storage and validation\nINSERT INTO orders_with_enum VALUES (1, 'pending');\n-- INSERT INTO orders_with_enum VALUES (2, 'invalid'); -- ERROR"
    },
    {
      "id": 52,
      "question": "What is query caching and how does it work?",
      "answer": "Query caching stores the results of SELECT queries so subsequent identical queries can return cached results without re-executing.\n\nHow It Works:\nâ€¢ Query executed first time\nâ€¢ Results stored in cache with query as key\nâ€¢ Subsequent identical queries return cached results\nâ€¢ Cache invalidated when underlying tables change\nâ€¢ Exact match required (same query text)\n\nCache Types:\n\nQuery Result Cache:\nâ€¢ Stores complete result sets\nâ€¢ MySQL Query Cache (deprecated in 8.0)\nâ€¢ Oracle Result Cache\nâ€¢ Application-level caching\n\nExecution Plan Cache:\nâ€¢ Stores compiled query plans\nâ€¢ Prepared statement cache\nâ€¢ Reduces parsing overhead\nâ€¢ Most databases do this automatically\n\nBenefits:\nâ€¢ Faster response times\nâ€¢ Reduced CPU usage\nâ€¢ Lower disk I/O\nâ€¢ Better throughput\n\nLimitations:\nâ€¢ Exact query match required\nâ€¢ Invalidated on any table write\nâ€¢ Can be ineffective for write-heavy workloads\nâ€¢ Deprecated in newer MySQL versions\nâ€¢ Memory overhead\n\nAlternatives:\nâ€¢ Application-level caching (Redis, Memcached)\nâ€¢ Materialized views\nâ€¢ Read replicas",
      "explanation": "Query caching stores SELECT results to avoid re-execution, but is deprecated in modern databases in favor of application-level caching.",
      "difficulty": "Medium",
      "code": "-- MySQL Query Cache (Deprecated in 8.0)\n-- Check cache status\nSHOW VARIABLES LIKE 'query_cache%';\n-- query_cache_type: ON/OFF\n-- query_cache_size: Size in bytes\n\n-- Configure query cache (old MySQL)\nSET GLOBAL query_cache_type = ON;\nSET GLOBAL query_cache_size = 1048576; -- 1MB\n\n-- View cache statistics\nSHOW STATUS LIKE 'Qcache%';\n-- Qcache_hits: Cache hits\n-- Qcache_inserts: Queries added to cache\n-- Qcache_not_cached: Queries not cached\n\n-- Queries that use cache\nSELECT * FROM products WHERE price > 100;\n-- Second execution: returns cached result (if table unchanged)\n\n-- Queries that bypass cache\nSELECT SQL_NO_CACHE * FROM products WHERE price > 100;\n\n-- Cache invalidation\nUPDATE products SET price = 120 WHERE product_id = 1;\n-- All cached queries involving products are invalidated\n\n-- Oracle Result Cache\n-- Enable for specific query\nSELECT /*+ RESULT_CACHE */ \n    department,\n    COUNT(*) as emp_count,\n    AVG(salary) as avg_salary\nFROM employees\nGROUP BY department;\n\n-- Create table with result cache\nCREATE TABLE lookup_table (\n    id INT PRIMARY KEY,\n    value VARCHAR(100)\n) RESULT_CACHE (MODE FORCE);\n\n-- PostgreSQL: No built-in query cache\n-- Use application-level caching\n\n-- Application-level caching (pseudo-code)\n-- def get_products(category):\n--     cache_key = f\"products:{category}\"\n--     \n--     # Check cache first\n--     cached = redis.get(cache_key)\n--     if cached:\n--         return json.loads(cached)\n--     \n--     # Query database\n--     results = db.query(\"SELECT * FROM products WHERE category = ?\", category)\n--     \n--     # Store in cache with expiration\n--     redis.setex(cache_key, 3600, json.dumps(results))  # 1 hour TTL\n--     \n--     return results\n\n-- Prepared statement cache (automatic in most databases)\nPREPARE stmt FROM 'SELECT * FROM users WHERE user_id = ?';\nSET @id = 123;\nEXECUTE stmt USING @id;\n-- Execution plan cached, reused for different parameters\n\n-- Cache invalidation strategies\n-- 1. Time-based (TTL)\n-- redis.setex('key', 3600, value)  # Expire after 1 hour\n\n-- 2. Event-based (invalidate on write)\n-- After INSERT/UPDATE/DELETE:\n-- redis.delete('products:*')  # Clear all product caches\n\n-- 3. Versioning\nCREATE TABLE cache_versions (\n    table_name VARCHAR(50) PRIMARY KEY,\n    version INT\n);\n\n-- Increment version on table change\nUPDATE cache_versions SET version = version + 1 WHERE table_name = 'products';\n\n-- Include version in cache key\n-- cache_key = f\"products:v{version}:{category}\"\n\n-- Measuring cache effectiveness\nCREATE TABLE query_stats (\n    query_hash VARCHAR(64),\n    execution_count INT,\n    total_time_ms BIGINT,\n    cache_hits INT,\n    cache_misses INT,\n    hit_ratio AS (cache_hits * 100.0 / execution_count) STORED\n);\n\n-- Multi-level caching\n-- L1: Application memory (seconds)\n-- L2: Redis/Memcached (minutes)\n-- L3: Database query (fallback)\n\n-- Cache warming strategy\n-- Preload frequently accessed data\nSELECT * FROM popular_products LIMIT 100;\n-- Store in cache during off-peak hours\n\n-- Cache stampede prevention\n-- Problem: Cache expires, multiple requests hit database\n-- Solution: Lock while refreshing\n\n-- def get_with_lock(key):\n--     value = cache.get(key)\n--     if value:\n--         return value\n--     \n--     # Try to acquire lock\n--     if cache.setnx(f\"{key}:lock\", \"1\", ex=10):\n--         # This thread refreshes cache\n--         value = fetch_from_db()\n--         cache.setex(key, 3600, value)\n--         cache.delete(f\"{key}:lock\")\n--     else:\n--         # Wait and retry\n--         time.sleep(0.1)\n--         return get_with_lock(key)\n--     \n--     return value\n\n-- Monitoring cache performance\nSELECT \n    'Cache Hit Rate' as metric,\n    CONCAT(\n        ROUND(Qcache_hits * 100.0 / (Qcache_hits + Qcache_inserts), 2),\n        '%'\n    ) as value\nFROM (\n    SELECT \n        VARIABLE_VALUE as Qcache_hits\n    FROM performance_schema.global_status\n    WHERE VARIABLE_NAME = 'Qcache_hits'\n) hits,\n(\n    SELECT \n        VARIABLE_VALUE as Qcache_inserts\n    FROM performance_schema.global_status\n    WHERE VARIABLE_NAME = 'Qcache_inserts'\n) inserts;"
    },
    {
      "id": 53,
      "question": "What is the difference between OLTP and OLAP databases?",
      "answer": "OLTP and OLAP are different types of database systems optimized for different workloads and use cases.\n\nOLTP (Online Transaction Processing):\nâ€¢ Short, fast transactions\nâ€¢ INSERT, UPDATE, DELETE heavy\nâ€¢ Many small read/write operations\nâ€¢ Current data\nâ€¢ Normalized schema\nâ€¢ Row-oriented storage\nâ€¢ High concurrency\nâ€¢ ACID compliance critical\nâ€¢ Examples: Order processing, banking\n\nOLAP (Online Analytical Processing):\nâ€¢ Long, complex queries\nâ€¢ SELECT heavy, few writes\nâ€¢ Large aggregations and joins\nâ€¢ Historical data\nâ€¢ Denormalized schema (star/snowflake)\nâ€¢ Column-oriented storage\nâ€¢ Lower concurrency\nâ€¢ Eventual consistency acceptable\nâ€¢ Examples: Business intelligence, reporting\n\nKey Differences:\nâ€¢ OLTP: Operational, OLAP: Analytical\nâ€¢ OLTP: Rows, OLAP: Columns\nâ€¢ OLTP: Normalized, OLAP: Denormalized\nâ€¢ OLTP: Real-time, OLAP: Historical\nâ€¢ OLTP: Transaction speed, OLAP: Query complexity\n\nHybrid Approach:\nâ€¢ HTAP (Hybrid Transaction/Analytical Processing)\nâ€¢ Combines both capabilities\nâ€¢ Examples: SAP HANA, Oracle Database",
      "explanation": "OLTP optimizes for fast transactional operations on current data, while OLAP optimizes for complex analytical queries on historical data.",
      "difficulty": "Medium",
      "code": "-- OLTP Database Example (E-commerce)\nCREATE TABLE customers (\n    customer_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100) UNIQUE,\n    created_at TIMESTAMP\n);\n\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date TIMESTAMP,\n    status VARCHAR(20),\n    total DECIMAL(10,2),\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id),\n    INDEX idx_customer (customer_id),\n    INDEX idx_date (order_date)\n);\n\nCREATE TABLE order_items (\n    item_id INT PRIMARY KEY,\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    price DECIMAL(10,2),\n    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n);\n\n-- Typical OLTP queries (fast, specific)\n-- Insert new order\nBEGIN TRANSACTION;\nINSERT INTO orders (customer_id, order_date, status, total) \nVALUES (123, NOW(), 'pending', 99.99);\nINSERT INTO order_items (order_id, product_id, quantity, price)\nVALUES (LAST_INSERT_ID(), 456, 2, 49.99);\nCOMMIT;\n\n-- Update order status\nUPDATE orders SET status = 'shipped' WHERE order_id = 1001;\n\n-- Get customer orders\nSELECT * FROM orders WHERE customer_id = 123 ORDER BY order_date DESC LIMIT 10;\n\n-- OLAP Database Example (Data Warehouse)\n-- Star schema with fact and dimension tables\nCREATE TABLE fact_sales (\n    sale_id BIGINT PRIMARY KEY,\n    date_key INT,\n    customer_key INT,\n    product_key INT,\n    store_key INT,\n    quantity INT,\n    amount DECIMAL(12,2),\n    cost DECIMAL(12,2),\n    profit AS (amount - cost) STORED\n);\n\nCREATE TABLE dim_date (\n    date_key INT PRIMARY KEY,\n    date DATE,\n    year INT,\n    quarter INT,\n    month INT,\n    month_name VARCHAR(20),\n    day_of_week INT,\n    day_name VARCHAR(20),\n    is_weekend BOOLEAN,\n    is_holiday BOOLEAN\n);\n\nCREATE TABLE dim_customer (\n    customer_key INT PRIMARY KEY,\n    customer_id INT,\n    name VARCHAR(100),\n    segment VARCHAR(50),\n    region VARCHAR(50),\n    country VARCHAR(50)\n);\n\nCREATE TABLE dim_product (\n    product_key INT PRIMARY KEY,\n    product_id INT,\n    product_name VARCHAR(200),\n    category VARCHAR(100),\n    subcategory VARCHAR(100),\n    brand VARCHAR(100)\n);\n\n-- Typical OLAP queries (complex, analytical)\n-- Sales analysis by quarter\nSELECT \n    d.year,\n    d.quarter,\n    p.category,\n    SUM(f.quantity) as total_quantity,\n    SUM(f.amount) as total_revenue,\n    SUM(f.profit) as total_profit,\n    AVG(f.amount) as avg_sale_amount\nFROM fact_sales f\nJOIN dim_date d ON f.date_key = d.date_key\nJOIN dim_product p ON f.product_key = p.product_key\nWHERE d.year = 2024\nGROUP BY d.year, d.quarter, p.category\nORDER BY d.quarter, total_revenue DESC;\n\n-- Customer segmentation analysis\nSELECT \n    c.segment,\n    c.region,\n    COUNT(DISTINCT f.customer_key) as customer_count,\n    SUM(f.amount) as total_sales,\n    AVG(f.amount) as avg_transaction,\n    SUM(f.amount) / COUNT(DISTINCT f.customer_key) as revenue_per_customer\nFROM fact_sales f\nJOIN dim_customer c ON f.customer_key = c.customer_key\nJOIN dim_date d ON f.date_key = d.date_key\nWHERE d.year = 2024\nGROUP BY c.segment, c.region\nHAVING SUM(f.amount) > 100000\nORDER BY total_sales DESC;\n\n-- Year-over-year comparison\nWITH yearly_sales AS (\n    SELECT \n        d.year,\n        p.category,\n        SUM(f.amount) as total_sales\n    FROM fact_sales f\n    JOIN dim_date d ON f.date_key = d.date_key\n    JOIN dim_product p ON f.product_key = p.product_key\n    GROUP BY d.year, p.category\n)\nSELECT \n    y1.category,\n    y1.total_sales as sales_2024,\n    y2.total_sales as sales_2023,\n    y1.total_sales - y2.total_sales as difference,\n    ROUND((y1.total_sales - y2.total_sales) * 100.0 / y2.total_sales, 2) as pct_change\nFROM yearly_sales y1\nJOIN yearly_sales y2 ON y1.category = y2.category\nWHERE y1.year = 2024 AND y2.year = 2023\nORDER BY pct_change DESC;\n\n-- ETL process: OLTP to OLAP\n-- Extract from OLTP, Transform, Load into OLAP\nINSERT INTO fact_sales (date_key, customer_key, product_key, quantity, amount, cost)\nSELECT \n    d.date_key,\n    c.customer_key,\n    p.product_key,\n    oi.quantity,\n    oi.quantity * oi.price as amount,\n    oi.quantity * prod.cost as cost\nFROM oltp_db.orders o\nJOIN oltp_db.order_items oi ON o.order_id = oi.order_id\nJOIN dim_date d ON DATE(o.order_date) = d.date\nJOIN dim_customer c ON o.customer_id = c.customer_id\nJOIN dim_product p ON oi.product_id = p.product_id\nJOIN oltp_db.products prod ON oi.product_id = prod.product_id\nWHERE o.order_date >= CURRENT_DATE - INTERVAL 1 DAY;\n\n-- Column-store advantages for OLAP\n-- Query only needs a few columns\nSELECT category, SUM(amount)\nFROM fact_sales f\nJOIN dim_product p ON f.product_key = p.product_key\nGROUP BY category;\n-- Column-store: Reads only category and amount columns\n-- Row-store: Must read entire rows\n\n-- Indexing strategy differences\n-- OLTP: Many indexes on foreign keys, frequently queried columns\nCREATE INDEX idx_customer ON orders(customer_id);\nCREATE INDEX idx_status ON orders(status);\nCREATE INDEX idx_date ON orders(order_date);\n\n-- OLAP: Fewer indexes, focus on dimension keys\nCREATE INDEX idx_date_key ON fact_sales(date_key);\nCREATE INDEX idx_customer_key ON fact_sales(customer_key);\n-- Or use columnstore indexes (SQL Server)\nCREATE COLUMNSTORE INDEX idx_columnstore ON fact_sales;"
    },
    {
      "id": 54,
      "question": "What is database sharding?",
      "answer": "Sharding is a database partitioning technique that splits data across multiple database instances (shards) to improve scalability and performance.\n\nHow It Works:\nâ€¢ Data distributed across multiple databases\nâ€¢ Each shard is independent database\nâ€¢ Shards run on separate servers\nâ€¢ Application routes requests to correct shard\nâ€¢ Horizontal scaling technique\n\nSharding Strategies:\n\nRange-Based Sharding:\nâ€¢ Divide by value ranges\nâ€¢ Example: User IDs 1-1M on Shard1, 1M-2M on Shard2\nâ€¢ Simple but can cause hotspots\nâ€¢ Uneven distribution possible\n\nHash-Based Sharding:\nâ€¢ Use hash function on shard key\nâ€¢ Even distribution\nâ€¢ Hard to add/remove shards\nâ€¢ Example: hash(user_id) % shard_count\n\nGeographic Sharding:\nâ€¢ Divide by location\nâ€¢ Reduces latency\nâ€¢ Compliance with data residency\nâ€¢ Example: US data on US servers\n\nDirectory-Based Sharding:\nâ€¢ Lookup table maps keys to shards\nâ€¢ Flexible but adds complexity\nâ€¢ Single point of failure\n\nBenefits:\nâ€¢ Horizontal scalability\nâ€¢ Improved performance\nâ€¢ Reduced individual database size\nâ€¢ Better fault isolation\n\nChallenges:\nâ€¢ Cross-shard queries expensive\nâ€¢ Transactions across shards complex\nâ€¢ Rebalancing difficult\nâ€¢ Application complexity",
      "explanation": "Sharding horizontally partitions data across multiple independent database instances to achieve massive scale and performance improvements.",
      "difficulty": "Hard",
      "code": "-- Range-based sharding example\n-- Shard 1: User IDs 1-1,000,000\nCREATE TABLE users_shard1 (\n    user_id INT PRIMARY KEY CHECK (user_id >= 1 AND user_id < 1000000),\n    username VARCHAR(50),\n    email VARCHAR(100),\n    created_at TIMESTAMP\n) -- On server: shard1.db\n\n-- Shard 2: User IDs 1,000,000-2,000,000\nCREATE TABLE users_shard2 (\n    user_id INT PRIMARY KEY CHECK (user_id >= 1000000 AND user_id < 2000000),\n    username VARCHAR(50),\n    email VARCHAR(100),\n    created_at TIMESTAMP\n) -- On server: shard2.db\n\n-- Application logic to route queries\n-- def get_shard(user_id):\n--     if user_id < 1000000:\n--         return shard1_connection\n--     elif user_id < 2000000:\n--         return shard2_connection\n--     else:\n--         return shard3_connection\n\n-- def get_user(user_id):\n--     shard = get_shard(user_id)\n--     return shard.query(\"SELECT * FROM users WHERE user_id = ?\", user_id)\n\n-- Hash-based sharding\n-- Shard determination: hash(user_id) % 4\n-- def get_shard_by_hash(user_id):\n--     shard_num = hash(user_id) % 4\n--     return shard_connections[shard_num]\n\n-- User 123 -> hash(123) % 4 = 3 -> Shard 3\n-- User 456 -> hash(456) % 4 = 0 -> Shard 0\n\n-- Geographic sharding\nCREATE TABLE users_us (\n    user_id INT PRIMARY KEY,\n    username VARCHAR(50),\n    region VARCHAR(10) DEFAULT 'US'\n) -- On US servers\n\nCREATE TABLE users_eu (\n    user_id INT PRIMARY KEY,\n    username VARCHAR(50),\n    region VARCHAR(10) DEFAULT 'EU'\n) -- On EU servers\n\nCREATE TABLE users_asia (\n    user_id INT PRIMARY KEY,\n    username VARCHAR(50),\n    region VARCHAR(10) DEFAULT 'ASIA'\n) -- On Asia servers\n\n-- Directory-based sharding\nCREATE TABLE shard_directory (\n    user_id_start INT,\n    user_id_end INT,\n    shard_id INT,\n    shard_host VARCHAR(255),\n    shard_db VARCHAR(100)\n);\n\nINSERT INTO shard_directory VALUES\n(1, 999999, 1, 'shard1.example.com', 'users_db'),\n(1000000, 1999999, 2, 'shard2.example.com', 'users_db'),\n(2000000, 2999999, 3, 'shard3.example.com', 'users_db');\n\n-- Lookup shard for user\nSELECT shard_host, shard_db\nFROM shard_directory\nWHERE 1234567 BETWEEN user_id_start AND user_id_end;\n\n-- Cross-shard query challenge\n-- Query needs data from multiple shards\n-- SELECT COUNT(*) FROM users; -- Must query all shards\n\n-- Application-level aggregation\n-- def count_all_users():\n--     total = 0\n--     for shard in all_shards:\n--         count = shard.query(\"SELECT COUNT(*) FROM users\")\n--         total += count\n--     return total\n\n-- Cross-shard join (expensive)\n-- Get users and their orders from different shards\n-- def get_user_orders(user_id):\n--     user_shard = get_shard(user_id)\n--     user = user_shard.query(\"SELECT * FROM users WHERE user_id = ?\", user_id)\n--     \n--     # Orders might be on different shard\n--     order_shard = get_shard(user_id)  # If orders sharded same way\n--     orders = order_shard.query(\"SELECT * FROM orders WHERE user_id = ?\", user_id)\n--     \n--     return {\"user\": user, \"orders\": orders}\n\n-- Denormalization for cross-shard scenarios\n-- Instead of joining across shards, duplicate data\nCREATE TABLE orders_shard1 (\n    order_id INT PRIMARY KEY,\n    user_id INT,\n    user_email VARCHAR(100),  -- Denormalized from users\n    order_date TIMESTAMP,\n    total DECIMAL(10,2)\n);\n\n-- Rebalancing shards\n-- When Shard 1 gets too large, split it\n-- Before: Shard1 (1-1M), Shard2 (1M-2M)\n-- After: Shard1a (1-500K), Shard1b (500K-1M), Shard2 (1M-2M)\n\n-- Migration script\n-- INSERT INTO users_shard1b\n-- SELECT * FROM users_shard1 WHERE user_id >= 500000;\n\n-- DELETE FROM users_shard1 WHERE user_id >= 500000;\n\n-- Consistent hashing for dynamic sharding\n-- Minimizes data movement when adding/removing shards\n-- Uses ring structure with virtual nodes\n\n-- Global unique ID generation across shards\n-- Twitter Snowflake approach\n-- ID structure: timestamp + machine_id + sequence\nCREATE TABLE id_generator (\n    shard_id INT,\n    sequence BIGINT,\n    last_timestamp BIGINT\n);\n\n-- Generate unique ID\n-- timestamp (41 bits) + shard_id (10 bits) + sequence (12 bits)\n\n-- Sharded database schema example\n-- Database: shard1_db\nCREATE TABLE users (\n    user_id BIGINT PRIMARY KEY,\n    username VARCHAR(50),\n    email VARCHAR(100),\n    shard_key INT -- For reference\n);\n\nCREATE TABLE posts (\n    post_id BIGINT PRIMARY KEY,\n    user_id BIGINT, -- Same shard as user\n    content TEXT,\n    created_at TIMESTAMP\n);\n\nCREATE TABLE comments (\n    comment_id BIGINT PRIMARY KEY,\n    post_id BIGINT,\n    user_id BIGINT,\n    content TEXT\n);\n-- Keep related data (user, posts, comments) on same shard\n\n-- Monitoring shard health\nCREATE TABLE shard_metrics (\n    shard_id INT,\n    record_count BIGINT,\n    size_mb BIGINT,\n    avg_query_time_ms DECIMAL(10,2),\n    last_updated TIMESTAMP\n);\n\n-- Check for imbalance\nSELECT \n    shard_id,\n    record_count,\n    size_mb,\n    record_count * 100.0 / SUM(record_count) OVER() as percent_of_total\nFROM shard_metrics\nORDER BY record_count DESC;"
    },
    {
      "id": 55,
      "question": "What is eventual consistency and how does it differ from strong consistency?",
      "answer": "Consistency models define how and when changes become visible to readers in distributed databases.\n\nStrong Consistency:\nâ€¢ All readers see same data immediately\nâ€¢ Reads always return latest write\nâ€¢ Synchronous replication\nâ€¢ ACID compliance\nâ€¢ Higher latency\nâ€¢ Lower availability during network partitions\nâ€¢ Used in RDBMS, banking systems\n\nEventual Consistency:\nâ€¢ Readers may see stale data temporarily\nâ€¢ Updates propagate asynchronously\nâ€¢ Eventually all replicas converge\nâ€¢ Higher availability\nâ€¢ Lower latency\nâ€¢ Tolerates network partitions better\nâ€¢ Used in NoSQL, distributed systems\n\nCAP Theorem:\nâ€¢ Consistency, Availability, Partition tolerance\nâ€¢ Can only guarantee 2 of 3\nâ€¢ Strong: Choose C + P (sacrifice A)\nâ€¢ Eventual: Choose A + P (sacrifice C)\n\nConsistency Levels:\nâ€¢ Strong - immediate consistency\nâ€¢ Bounded staleness - stale within bounds\nâ€¢ Session - consistent within session\nâ€¢ Eventual - eventually consistent\n\nUse Cases:\nâ€¢ Strong: Financial transactions, inventory\nâ€¢ Eventual: Social media, DNS, shopping carts",
      "explanation": "Strong consistency ensures immediate visibility of writes across all nodes, while eventual consistency allows temporary inconsistency for better availability and performance.",
      "difficulty": "Hard",
      "code": "-- Strong Consistency Example (Traditional SQL)\n-- Transaction ensures all or nothing\nBEGIN TRANSACTION;\n\n-- Debit from account 1\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1;\n\n-- Credit to account 2\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 2;\n\nCOMMIT;\n-- After commit, all readers immediately see updated balances\n\n-- Reading immediately after shows correct balance\nSELECT balance FROM accounts WHERE account_id = 1;\n-- Always returns consistent value\n\n-- Strong consistency with replication\n-- Master-slave with synchronous replication\nBEGIN TRANSACTION;\n\nINSERT INTO orders (order_id, customer_id, total) VALUES (1, 123, 99.99);\n\n-- Wait for all replicas to confirm\n-- Only then COMMIT succeeds\nCOMMIT;\n\n-- Any replica read returns same data\nSELECT * FROM orders WHERE order_id = 1;\n-- Same result from master or any slave\n\n-- Eventual Consistency Example\n-- Write to master\nINSERT INTO posts (post_id, user_id, content) VALUES (1, 456, 'Hello World');\n-- Returns immediately\n\n-- Read from replica 1 (immediately after)\nSELECT * FROM posts WHERE post_id = 1;\n-- Might return: No results (replication lag)\n\n-- Read from replica 2 (1 second later)\nSELECT * FROM posts WHERE post_id = 1;\n-- Might return: The post (replicated)\n\n-- Eventually all replicas have same data\n-- Typical replication lag: milliseconds to seconds\n\n-- Quorum-based consistency\n-- Cassandra consistency levels\n-- Write with consistency level ONE\n-- INSERT INTO users (user_id, name) VALUES (1, 'John');\n-- Succeeds when 1 replica acknowledges\n\n-- Read with consistency level QUORUM\n-- SELECT * FROM users WHERE user_id = 1;\n-- Reads from majority of replicas\n-- R + W > N ensures consistency (R=read replicas, W=write replicas, N=total)\n\n-- Session consistency example\n-- Within same session, reads follow writes\nSET SESSION CHARACTERISTICS AS TRANSACTION ISOLATION LEVEL READ COMMITTED;\n\nBEGIN TRANSACTION;\nINSERT INTO cart (user_id, product_id) VALUES (123, 456);\n-- Later in same session\nSELECT * FROM cart WHERE user_id = 123;\n-- Guaranteed to see the insert\nCOMMIT;\n\n-- Different session might not see it immediately (eventual)\nSELECT * FROM cart WHERE user_id = 123;\n-- Might be empty if replication lag exists\n\n-- Versioning for conflict resolution\nCREATE TABLE documents (\n    doc_id INT,\n    version INT,\n    content TEXT,\n    updated_at TIMESTAMP,\n    updated_by VARCHAR(50),\n    PRIMARY KEY (doc_id, version)\n);\n\n-- Concurrent updates create conflicts\n-- User A updates (reads version 1)\nUPDATE documents \nSET content = 'User A changes', version = 2\nWHERE doc_id = 1 AND version = 1;\n\n-- User B updates (also read version 1)\nUPDATE documents \nSET content = 'User B changes', version = 2\nWHERE doc_id = 1 AND version = 1;\n-- Conflict! Both have version 2\n\n-- Conflict resolution strategies\n-- 1. Last Write Wins (LWW)\nSELECT * FROM documents\nWHERE doc_id = 1\nORDER BY updated_at DESC\nLIMIT 1;\n-- Keep most recent update\n\n-- 2. Vector clocks (track causality)\nCREATE TABLE documents_vc (\n    doc_id INT,\n    content TEXT,\n    vector_clock JSON  -- {\"node1\": 5, \"node2\": 3}\n);\n\n-- 3. CRDT (Conflict-free Replicated Data Type)\n-- Commutative operations that converge\n-- Example: Counter\nCREATE TABLE counters (\n    counter_id INT,\n    node_id INT,\n    value INT,\n    PRIMARY KEY (counter_id, node_id)\n);\n\n-- Each node increments independently\nINSERT INTO counters VALUES (1, 'node1', 5);\nINSERT INTO counters VALUES (1, 'node2', 3);\n\n-- Total is sum of all nodes\nSELECT counter_id, SUM(value) as total\nFROM counters\nGROUP BY counter_id;\n-- Result: 8 (always converges correctly)\n\n-- Monitoring replication lag\nCREATE TABLE replication_status (\n    replica_name VARCHAR(50),\n    master_log_file VARCHAR(100),\n    master_log_pos BIGINT,\n    slave_log_pos BIGINT,\n    seconds_behind_master INT,\n    last_check TIMESTAMP\n);\n\n-- Check lag\nSELECT replica_name, seconds_behind_master\nFROM replication_status\nWHERE seconds_behind_master > 5; -- Alert if > 5 seconds\n\n-- Handling eventual consistency in application\n-- def create_post(user_id, content):\n--     # Write to master\n--     post_id = db_master.insert(\"posts\", {\"user_id\": user_id, \"content\": content})\n--     \n--     # Read from master to see own write immediately\n--     return db_master.query(\"SELECT * FROM posts WHERE post_id = ?\", post_id)\n\n-- def get_post(post_id, user_id=None):\n--     # If user requesting their own post, read from master\n--     if is_own_post(post_id, user_id):\n--         return db_master.query(\"SELECT * FROM posts WHERE post_id = ?\", post_id)\n--     else:\n--         # Otherwise read from slave (eventual consistency OK)\n--         return db_slave.query(\"SELECT * FROM posts WHERE post_id = ?\", post_id)\n\n-- Causal consistency\n-- If A causes B, everyone sees A before B\nCREATE TABLE comments (\n    comment_id INT,\n    post_id INT,\n    parent_comment_id INT, -- Causality\n    content TEXT\n);\n\n-- Ensure parent comment visible before reply\nSELECT * FROM comments WHERE comment_id = parent_id;\n-- Must return result before inserting reply"
    },
    {
      "id": 56,
      "question": "What is a database transaction log and why is it important?",
      "answer": "A transaction log (write-ahead log) records all changes made to the database, enabling recovery, replication, and maintaining ACID properties.\n\nPurpose:\nâ€¢ Record all data modifications\nâ€¢ Enable crash recovery\nâ€¢ Support rollback operations\nâ€¢ Enable point-in-time recovery\nâ€¢ Facilitate replication\nâ€¢ Maintain data integrity\n\nHow It Works:\nâ€¢ Changes written to log before data files\nâ€¢ Write-ahead logging (WAL) protocol\nâ€¢ Sequential writes (faster than random)\nâ€¢ Log entries have sequence numbers\nâ€¢ Periodically checkpointed\n\nLog Contents:\nâ€¢ Transaction ID\nâ€¢ Before and after images of data\nâ€¢ Transaction begin/commit/rollback markers\nâ€¢ LSN (Log Sequence Number)\nâ€¢ Timestamps\n\nRecovery Process:\nâ€¢ REDO: Replay committed transactions\nâ€¢ UNDO: Rollback uncommitted transactions\nâ€¢ Restore to consistent state\n\nLog Management:\nâ€¢ Logs grow continuously\nâ€¢ Need regular backups\nâ€¢ Truncation after backup\nâ€¢ Circular logging vs archive logging\n\nPerformance Impact:\nâ€¢ Synchronous log writes for durability\nâ€¢ Can be bottleneck\nâ€¢ Log on fast storage (SSD)\nâ€¢ Batch commits to reduce I/O",
      "explanation": "Transaction logs record all database changes before they occur, enabling crash recovery, rollback, and replication while maintaining ACID properties.",
      "difficulty": "Medium",
      "code": "-- Transaction log example (conceptual)\n-- LSN: Log Sequence Number\n-- Log entries:\n-- LSN=100: BEGIN TRANSACTION txn_1\n-- LSN=101: UPDATE accounts SET balance=900 WHERE id=1 (old=1000, new=900)\n-- LSN=102: UPDATE accounts SET balance=1100 WHERE id=2 (old=1000, new=1100)\n-- LSN=103: COMMIT txn_1\n\n-- SQL Server transaction log management\n-- View log space usage\nDBCC SQLPERF(LOGSPACE);\n\n-- View active transactions\nDBCC OPENTRAN;\n\n-- Backup transaction log\nBACKUP LOG database_name TO DISK = 'C:\\Backups\\db_log.trn';\n\n-- Shrink log file (after backup)\nDBCC SHRINKFILE (log_file_name, target_size_mb);\n\n-- PostgreSQL WAL (Write-Ahead Log)\n-- Configuration in postgresql.conf\n-- wal_level = replica\n-- fsync = on  -- Ensure log written to disk\n-- synchronous_commit = on  -- Wait for WAL write\n-- wal_buffers = 16MB\n-- checkpoint_timeout = 5min\n\n-- View WAL information\nSELECT pg_current_wal_lsn();\nSELECT pg_walfile_name(pg_current_wal_lsn());\n\n-- MySQL binary log (transaction log)\n-- Enable binary logging in my.cnf\n-- log_bin = /var/log/mysql/mysql-bin.log\n-- binlog_format = ROW\n-- sync_binlog = 1  -- Sync to disk on commit\n-- innodb_flush_log_at_trx_commit = 1\n\n-- Show binary logs\nSHOW BINARY LOGS;\n\n-- View binary log contents\nSHOW BINLOG EVENTS IN 'mysql-bin.000001';\n\n-- Purge old logs\nPURGE BINARY LOGS BEFORE '2024-01-01 00:00:00';\n\n-- Recovery scenario\n-- 1. Database crashes\n-- 2. On restart, database reads transaction log\n-- 3. REDO: Replay committed transactions from log\nREDO: LSN=101: UPDATE accounts SET balance=900 WHERE id=1\nREDO: LSN=102: UPDATE accounts SET balance=1100 WHERE id=2\nREDO: LSN=103: COMMIT txn_1\n\n-- 4. UNDO: Rollback uncommitted transactions\nUNDO: LSN=205: UPDATE accounts SET balance=1000 WHERE id=3 (rollback)\nUNDO: LSN=204: BEGIN TRANSACTION txn_2 (discard)\n\n-- Point-in-time recovery\n-- Restore full backup\nRESTORE DATABASE sales FROM DISK = 'C:\\Backups\\sales_full.bak' \nWITH NORECOVERY;\n\n-- Apply transaction logs up to specific time\nRESTORE LOG sales FROM DISK = 'C:\\Backups\\sales_log1.trn'\nWITH NORECOVERY;\n\nRESTORE LOG sales FROM DISK = 'C:\\Backups\\sales_log2.trn'\nWITH RECOVERY, STOPAT = '2024-01-15 10:30:00';\n\n-- Transaction log structure\nCREATE TABLE transaction_log (\n    lsn BIGINT PRIMARY KEY,\n    transaction_id INT,\n    operation VARCHAR(20),  -- BEGIN, UPDATE, INSERT, DELETE, COMMIT, ROLLBACK\n    table_name VARCHAR(100),\n    before_image TEXT,      -- Old values\n    after_image TEXT,       -- New values\n    timestamp TIMESTAMP,\n    user_name VARCHAR(50)\n);\n\n-- Log maintenance job\nCREATE PROCEDURE maintain_transaction_log()\nAS\nBEGIN\n    -- Backup log\n    EXEC backup_transaction_log;\n    \n    -- Truncate log (after successful backup)\n    EXEC truncate_log;\n    \n    -- Archive old log files\n    EXEC archive_old_logs WHERE log_date < DATEADD(day, -30, GETDATE());\nEND;\n\n-- Monitoring log growth\nSELECT \n    name,\n    log_reuse_wait_desc,  -- Why log cannot be truncated\n    recovery_model_desc,\n    log_size_mb = size * 8 / 1024,\n    log_space_used_mb = FILEPROPERTY(name, 'SpaceUsed') * 8 / 1024\nFROM sys.databases\nWHERE database_id > 4;\n\n-- Preventing log file growth\n-- 1. Regular log backups\nCREATE SCHEDULE log_backup_schedule\nEVERY 15 MINUTES\nEXECUTE backup_transaction_log;\n\n-- 2. Avoid long-running transactions\nBEGIN TRANSACTION;\nUPDATE large_table SET status = 'processed'; -- Bad: locks and fills log\nCOMMIT;\n\n-- Better: Process in batches\nWHILE EXISTS (SELECT 1 FROM large_table WHERE status = 'pending')\nBEGIN\n    BEGIN TRANSACTION;\n    UPDATE TOP (1000) large_table SET status = 'processed'\n    WHERE status = 'pending';\n    COMMIT;\n    \n    -- Allows log truncation between batches\n    WAITFOR DELAY '00:00:01';\nEND;\n\n-- Transaction log vs data file\n-- Data file: Contains actual table data\n-- Log file: Contains transaction history\n\nCREATE DATABASE mydb\nON PRIMARY (\n    NAME = mydb_data,\n    FILENAME = 'C:\\Data\\mydb.mdf',\n    SIZE = 100MB,\n    FILEGROWTH = 10MB\n)\nLOG ON (\n    NAME = mydb_log,\n    FILENAME = 'C:\\Logs\\mydb_log.ldf',\n    SIZE = 50MB,\n    FILEGROWTH = 10MB\n);\n\n-- Log shipping for high availability\n-- Primary server: Backup log\nBACKUP LOG production_db TO DISK = '\\\\share\\logs\\prod_log.trn';\n\n-- Secondary server: Restore log\nRESTORE LOG production_db FROM DISK = '\\\\share\\logs\\prod_log.trn'\nWITH NORECOVERY; -- Keep in restoring state\n\n-- Repeat every few minutes\n-- Secondary is slightly behind primary\n\n-- Circular logging (no recovery)\n-- MySQL: binlog_expire_logs_seconds = 86400\n-- Automatically purge logs older than 24 hours\n\n-- PostgreSQL: WAL archiving\n-- archive_mode = on\n-- archive_command = 'cp %p /archive/%f'\n-- Copies WAL files to archive location\n\n-- Recovery models (SQL Server)\n-- SIMPLE: Log truncated automatically, minimal logging\nALTER DATABASE mydb SET RECOVERY SIMPLE;\n\n-- FULL: All transactions logged, point-in-time recovery\nALTER DATABASE mydb SET RECOVERY FULL;\n\n-- BULK_LOGGED: Minimal logging for bulk operations\nALTER DATABASE mydb SET RECOVERY BULK_LOGGED;"
    },
    {
      "id": 57,
      "question": "What are the different types of database backups?",
      "answer": "Database backups create copies of data for recovery from failures, corruption, or disasters.\n\nBackup Types:\n\nFull Backup:\nâ€¢ Complete copy of entire database\nâ€¢ Longest time and space\nâ€¢ Simplest to restore\nâ€¢ Starting point for other backups\nâ€¢ Independent of other backups\n\nDifferential Backup:\nâ€¢ Changes since last full backup\nâ€¢ Faster than full\nâ€¢ Restore: full + latest differential\nâ€¢ Size grows over time\nâ€¢ Each differential independent\n\nIncremental Backup:\nâ€¢ Changes since last backup (any type)\nâ€¢ Fastest and smallest\nâ€¢ Restore: full + all incrementals in sequence\nâ€¢ More complex restore\nâ€¢ Chain of backups needed\n\nTransaction Log Backup:\nâ€¢ Backup of transaction log\nâ€¢ Enables point-in-time recovery\nâ€¢ Frequent (every 15-60 minutes)\nâ€¢ Small and fast\nâ€¢ Prevents log file growth\n\nBackup Strategies:\nâ€¢ Full: Weekly\nâ€¢ Differential: Daily\nâ€¢ Incremental: Hourly\nâ€¢ Log: Every 15 minutes\n\nBest Practices:\nâ€¢ Test restores regularly\nâ€¢ Store offsite\nâ€¢ Encrypt backups\nâ€¢ Verify backup integrity\nâ€¢ Document procedures",
      "explanation": "Database backups include full (complete copy), differential (changes since full), incremental (changes since last), and log backups for disaster recovery.",
      "difficulty": "Medium",
      "code": "-- SQL Server backup examples\n-- Full backup\nBACKUP DATABASE sales\nTO DISK = 'C:\\Backups\\sales_full.bak'\nWITH \n    COMPRESSION,\n    CHECKSUM,\n    STATS = 10,\n    DESCRIPTION = 'Full backup of sales database';\n\n-- Differential backup\nBACKUP DATABASE sales\nTO DISK = 'C:\\Backups\\sales_diff.bak'\nWITH \n    DIFFERENTIAL,\n    COMPRESSION,\n    CHECKSUM;\n\n-- Transaction log backup\nBACKUP LOG sales\nTO DISK = 'C:\\Backups\\sales_log.trn'\nWITH \n    COMPRESSION,\n    CHECKSUM;\n\n-- Backup schedule example\n-- Sunday: Full backup\nIF DATEPART(weekday, GETDATE()) = 1\nBEGIN\n    BACKUP DATABASE sales TO DISK = 'C:\\Backups\\sales_full_' + \n        CONVERT(VARCHAR, GETDATE(), 112) + '.bak'\n    WITH COMPRESSION, INIT;\nEND\n\n-- Monday-Saturday: Differential backup\nELSE\nBEGIN\n    BACKUP DATABASE sales TO DISK = 'C:\\Backups\\sales_diff_' + \n        CONVERT(VARCHAR, GETDATE(), 112) + '.bak'\n    WITH DIFFERENTIAL, COMPRESSION, INIT;\nEND;\n\n-- Every hour: Log backup\nBACKUP LOG sales TO DISK = 'C:\\Backups\\sales_log_' + \n    CONVERT(VARCHAR, GETDATE(), 112) + '_' + \n    CONVERT(VARCHAR, GETDATE(), 108) + '.trn'\nWITH COMPRESSION;\n\n-- MySQL backup using mysqldump\n-- Full backup\n-- mysqldump --single-transaction --routines --triggers --events \n--   -u root -p sales > sales_full_20240115.sql\n\n-- Backup specific tables\n-- mysqldump -u root -p sales customers orders > sales_partial.sql\n\n-- MySQL binary backup (physical files)\n-- Full backup\n-- mysqlbackup --user=root --password=pass --backup-dir=/backups/full \n--   backup-and-apply-log\n\n-- Incremental backup\n-- mysqlbackup --user=root --password=pass --backup-dir=/backups/inc1 \n--   --incremental --incremental-base=dir:/backups/full \n--   backup\n\n-- PostgreSQL backup\n-- Full database backup\n-- pg_dump -U postgres -F c -f sales_full.dump sales\n\n-- Full cluster backup\n-- pg_dumpall -U postgres -f cluster_backup.sql\n\n-- Point-in-time recovery setup\n-- archive_mode = on\n-- archive_command = 'cp %p /archive/%f'\n\n-- Base backup\n-- pg_basebackup -U postgres -D /backups/base -F tar -z -P\n\n-- Restore procedures\n-- SQL Server restore from full backup\nRESTORE DATABASE sales\nFROM DISK = 'C:\\Backups\\sales_full.bak'\nWITH REPLACE, RECOVERY;\n\n-- Restore full + differential\nRESTORE DATABASE sales\nFROM DISK = 'C:\\Backups\\sales_full.bak'\nWITH NORECOVERY;  -- More restores coming\n\nRESTORE DATABASE sales\nFROM DISK = 'C:\\Backups\\sales_diff.bak'\nWITH RECOVERY;  -- Final restore\n\n-- Restore full + differential + logs\nRESTORE DATABASE sales\nFROM DISK = 'C:\\Backups\\sales_full.bak'\nWITH NORECOVERY;\n\nRESTORE DATABASE sales\nFROM DISK = 'C:\\Backups\\sales_diff.bak'\nWITH NORECOVERY;\n\nRESTORE LOG sales\nFROM DISK = 'C:\\Backups\\sales_log1.trn'\nWITH NORECOVERY;\n\nRESTORE LOG sales\nFROM DISK = 'C:\\Backups\\sales_log2.trn'\nWITH RECOVERY;\n\n-- Point-in-time recovery\nRESTORE DATABASE sales\nFROM DISK = 'C:\\Backups\\sales_full.bak'\nWITH NORECOVERY;\n\nRESTORE LOG sales\nFROM DISK = 'C:\\Backups\\sales_log.trn'\nWITH RECOVERY, STOPAT = '2024-01-15 10:30:00';\n-- Restores to specific point in time\n\n-- Backup verification\nRESTORE VERIFYONLY\nFROM DISK = 'C:\\Backups\\sales_full.bak';\n\n-- View backup history\nSELECT \n    database_name,\n    backup_start_date,\n    backup_finish_date,\n    type, -- D=Database, I=Differential, L=Log\n    backup_size,\n    compressed_backup_size,\n    physical_device_name\nFROM msdb.dbo.backupset\nWHERE database_name = 'sales'\nORDER BY backup_start_date DESC;\n\n-- Automated backup script\nCREATE PROCEDURE sp_backup_databases\nAS\nBEGIN\n    DECLARE @dbname VARCHAR(100);\n    DECLARE @path VARCHAR(500);\n    DECLARE @filename VARCHAR(500);\n    DECLARE @date VARCHAR(20);\n    \n    SET @date = CONVERT(VARCHAR, GETDATE(), 112);\n    \n    DECLARE db_cursor CURSOR FOR\n    SELECT name FROM sys.databases\n    WHERE database_id > 4 AND state = 0;\n    \n    OPEN db_cursor;\n    FETCH NEXT FROM db_cursor INTO @dbname;\n    \n    WHILE @@FETCH_STATUS = 0\n    BEGIN\n        SET @filename = 'C:\\Backups\\' + @dbname + '_' + @date + '.bak';\n        \n        BACKUP DATABASE @dbname\n        TO DISK = @filename\n        WITH COMPRESSION, CHECKSUM;\n        \n        FETCH NEXT FROM db_cursor INTO @dbname;\n    END;\n    \n    CLOSE db_cursor;\n    DEALLOCATE db_cursor;\nEND;\n\n-- Backup retention policy\nDELETE FROM backupset\nWHERE backup_start_date < DATEADD(day, -30, GETDATE());\n\n-- Delete physical files\nEXEC xp_delete_file 0, 'C:\\Backups', 'bak', \n    DATEADD(day, -30, GETDATE()), 0;\n\n-- Backup to Azure Blob Storage\nCREATE CREDENTIAL backup_credential\nWITH IDENTITY = 'storageaccount',\nSECRET = 'accesskey';\n\nBACKUP DATABASE sales\nTO URL = 'https://storageaccount.blob.core.windows.net/backups/sales.bak'\nWITH CREDENTIAL = 'backup_credential', COMPRESSION;\n\n-- Encrypted backup\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'strongpassword';\n\nCREATE CERTIFICATE backup_cert\nWITH SUBJECT = 'Backup Certificate';\n\nBACKUP DATABASE sales\nTO DISK = 'C:\\Backups\\sales_encrypted.bak'\nWITH COMPRESSION,\nENCRYPTION (ALGORITHM = AES_256, SERVER CERTIFICATE = backup_cert);\n\n-- Backup monitoring\nCREATE TABLE backup_log (\n    backup_id INT IDENTITY PRIMARY KEY,\n    database_name VARCHAR(100),\n    backup_type VARCHAR(20),\n    start_time DATETIME,\n    end_time DATETIME,\n    size_mb DECIMAL(10,2),\n    status VARCHAR(20),\n    error_message VARCHAR(MAX)\n);\n\n-- Log successful backup\nINSERT INTO backup_log (database_name, backup_type, start_time, end_time, size_mb, status)\nVALUES ('sales', 'FULL', @start, GETDATE(), @size, 'SUCCESS');"
    },
    {
      "id": 58,
      "question": "What is a full-text search and how does it differ from LIKE?",
      "answer": "Full-text search enables efficient searching of text content within database columns, offering more powerful capabilities than simple LIKE queries.\n\nLIKE Operator:\nâ€¢ Simple pattern matching\nâ€¢ Case-sensitive or insensitive\nâ€¢ Wildcard characters (%, _)\nâ€¢ Scans entire table (slow)\nâ€¢ No ranking or relevance\nâ€¢ Cannot search across multiple columns easily\nâ€¢ No linguistic features\n\nFull-Text Search:\nâ€¢ Optimized for text searching\nâ€¢ Uses inverted indexes\nâ€¢ Ranks results by relevance\nâ€¢ Supports word variations (stemming)\nâ€¢ Searches multiple columns\nâ€¢ Natural language queries\nâ€¢ Proximity searches\nâ€¢ Boolean operators\nâ€¢ Much faster for large text\n\nFull-Text Features:\nâ€¢ Word stemming (run, running, runs)\nâ€¢ Stop words (the, and, or)\nâ€¢ Proximity (words near each other)\nâ€¢ Thesaurus and synonyms\nâ€¢ Weighted ranking\nâ€¢ Phrase searching\nâ€¢ Boolean operations (AND, OR, NOT)\n\nPerformance:\nâ€¢ LIKE: O(n) - scans all rows\nâ€¢ Full-text: O(log n) - uses index\n\nUse Cases:\nâ€¢ LIKE: Simple exact pattern matching\nâ€¢ Full-text: Complex text searches, search engines",
      "explanation": "Full-text search uses specialized indexes for fast, ranked text searching with linguistic features, unlike LIKE which performs slow pattern matching.",
      "difficulty": "Medium",
      "code": "-- LIKE operator examples\n-- Simple pattern matching (slow on large tables)\nSELECT * FROM articles\nWHERE content LIKE '%database%';\n-- Scans entire table\n\n-- Multiple words with LIKE (inefficient)\nSELECT * FROM articles\nWHERE content LIKE '%database%'\n   OR content LIKE '%mysql%'\n   OR content LIKE '%postgresql%';\n\n-- Full-text search setup (MySQL)\n-- Create full-text index\nCREATE FULLTEXT INDEX idx_content ON articles(title, content);\n\n-- Basic full-text search\nSELECT * FROM articles\nWHERE MATCH(title, content) AGAINST ('database');\n-- Much faster, uses index\n\n-- Natural language search with ranking\nSELECT \n    title,\n    MATCH(title, content) AGAINST ('database mysql') as relevance\nFROM articles\nWHERE MATCH(title, content) AGAINST ('database mysql')\nORDER BY relevance DESC;\n-- Results ranked by relevance\n\n-- Boolean mode search\nSELECT * FROM articles\nWHERE MATCH(title, content) AGAINST ('+database +mysql -oracle' IN BOOLEAN MODE);\n-- + means must include\n-- - means must not include\n\n-- Phrase search\nSELECT * FROM articles\nWHERE MATCH(title, content) AGAINST ('\"relational database\"' IN BOOLEAN MODE);\n-- Exact phrase match\n\n-- Proximity search\nSELECT * FROM articles\nWHERE MATCH(title, content) AGAINST ('database NEAR mysql' IN BOOLEAN MODE);\n-- Words near each other\n\n-- Wildcard search\nSELECT * FROM articles\nWHERE MATCH(title, content) AGAINST ('data*' IN BOOLEAN MODE);\n-- Matches database, data, dataset, etc.\n\n-- SQL Server full-text search\n-- Create full-text catalog\nCREATE FULLTEXT CATALOG article_catalog;\n\n-- Create full-text index\nCREATE FULLTEXT INDEX ON articles(title, content)\nKEY INDEX PK_articles\nON article_catalog;\n\n-- Basic search\nSELECT * FROM articles\nWHERE CONTAINS(content, 'database');\n\n-- Multiple words (OR)\nSELECT * FROM articles\nWHERE CONTAINS(content, 'database OR mysql OR postgresql');\n\n-- Multiple words (AND)\nSELECT * FROM articles\nWHERE CONTAINS(content, 'database AND mysql');\n\n-- Phrase search\nSELECT * FROM articles\nWHERE CONTAINS(content, '\"relational database\"');\n\n-- Proximity search (words within 5 words of each other)\nSELECT * FROM articles\nWHERE CONTAINS(content, 'NEAR((database, optimization), 5)');\n\n-- Weighted search\nSELECT * FROM articles\nWHERE CONTAINS((title, content), 'database',\n    WEIGHT(0.8)) -- title more important\n   OR CONTAINS((title, content), 'database',\n    WEIGHT(0.2)); -- content less important\n\n-- FREETEXT search (natural language)\nSELECT * FROM articles\nWHERE FREETEXT(content, 'I want to learn about database optimization techniques');\n-- Automatically handles stop words, stemming\n\n-- PostgreSQL full-text search\n-- Create full-text column\nALTER TABLE articles ADD COLUMN tsv tsvector;\n\n-- Populate full-text column\nUPDATE articles\nSET tsv = to_tsvector('english', title || ' ' || content);\n\n-- Create index\nCREATE INDEX idx_tsv ON articles USING gin(tsv);\n\n-- Search\nSELECT title, content\nFROM articles\nWHERE tsv @@ to_tsquery('english', 'database & mysql');\n-- & means AND, | means OR, ! means NOT\n\n-- Ranked search\nSELECT \n    title,\n    ts_rank(tsv, query) as rank\nFROM articles,\n     to_tsquery('english', 'database & optimization') query\nWHERE tsv @@ query\nORDER BY rank DESC;\n\n-- Headline (excerpt) generation\nSELECT \n    title,\n    ts_headline('english', content, to_tsquery('database'),\n        'StartSel = <b>, StopSel = </b>, MaxWords=50') as excerpt\nFROM articles\nWHERE tsv @@ to_tsquery('database');\n-- Returns excerpt with matched words highlighted\n\n-- Performance comparison\n-- LIKE query (10 seconds on 1M rows)\nSELECT * FROM articles\nWHERE content LIKE '%database%' AND content LIKE '%optimization%';\n\n-- Full-text search (0.1 seconds on 1M rows)\nSELECT * FROM articles\nWHERE MATCH(content) AGAINST ('+database +optimization' IN BOOLEAN MODE);\n\n-- Custom full-text configuration\n-- Add custom stopwords\nALTER FULLTEXT INDEX ON articles\nADD STOPLIST custom_stoplist;\n\n-- Add custom thesaurus\nALTER FULLTEXT INDEX ON articles\nSET THESAURUS custom_thesaurus;\n\n-- Stemming example\n-- Search for \"run\" also matches: running, runs, ran\nSELECT * FROM articles\nWHERE MATCH(content) AGAINST ('run' IN NATURAL LANGUAGE MODE);\n-- Returns articles with any form of \"run\"\n\n-- Maintaining full-text index\n-- Rebuild index\nALTER FULLTEXT INDEX ON articles REBUILD;\n\n-- Update statistics\nALTER FULLTEXT INDEX ON articles UPDATE STATISTICS;\n\n-- Search across multiple tables\nSELECT 'article' as type, title, content FROM articles\nWHERE MATCH(title, content) AGAINST ('database')\nUNION ALL\nSELECT 'blog' as type, title, body FROM blogs\nWHERE MATCH(title, body) AGAINST ('database');\n\n-- Language-specific search\nSELECT * FROM articles\nWHERE MATCH(content) AGAINST ('databases' WITH QUERY EXPANSION);\n-- Expands query with related terms"
    },
    {
      "id": 59,
      "question": "What is database connection pooling?",
      "answer": "Connection pooling reuses database connections instead of creating new ones for each request, significantly improving application performance.\n\nHow It Works:\nâ€¢ Pool of connections created at startup\nâ€¢ Application borrows connection from pool\nâ€¢ Connection returned to pool after use\nâ€¢ Connections reused for subsequent requests\nâ€¢ Pool manages connection lifecycle\n\nBenefits:\nâ€¢ Reduced connection overhead\nâ€¢ Faster response times\nâ€¢ Better resource utilization\nâ€¢ Controlled resource usage\nâ€¢ Improved scalability\nâ€¢ Reduced database load\n\nPool Configuration:\nâ€¢ Minimum pool size - connections always open\nâ€¢ Maximum pool size - upper limit\nâ€¢ Connection timeout - max wait time\nâ€¢ Idle timeout - close idle connections\nâ€¢ Validation query - test connection health\nâ€¢ Max lifetime - maximum connection age\n\nCommon Issues:\nâ€¢ Pool exhaustion - all connections in use\nâ€¢ Connection leaks - connections not returned\nâ€¢ Stale connections - connection becomes invalid\nâ€¢ Oversized pool - too many connections\n\nBest Practices:\nâ€¢ Size pool appropriately\nâ€¢ Always close connections\nâ€¢ Use try-finally or try-with-resources\nâ€¢ Monitor pool metrics\nâ€¢ Set appropriate timeouts\nâ€¢ Validate connections before use",
      "explanation": "Connection pooling maintains reusable database connections to avoid expensive creation overhead, improving performance and scalability.",
      "difficulty": "Medium",
      "code": "-- Java connection pooling with HikariCP\n// import com.zaxxer.hikari.HikariConfig;\n// import com.zaxxer.hikari.HikariDataSource;\n\n// Configure pool\n// HikariConfig config = new HikariConfig();\n// config.setJdbcUrl(\"jdbc:mysql://localhost:3306/mydb\");\n// config.setUsername(\"user\");\n// config.setPassword(\"password\");\n\n// Pool sizing\n// config.setMinimumIdle(5);           // Min connections\n// config.setMaximumPoolSize(20);      // Max connections\n// config.setConnectionTimeout(30000); // 30 seconds\n// config.setIdleTimeout(600000);      // 10 minutes\n// config.setMaxLifetime(1800000);     // 30 minutes\n\n// Connection validation\n// config.setConnectionTestQuery(\"SELECT 1\");\n// config.setValidationTimeout(5000);\n\n// Create pool\n// HikariDataSource dataSource = new HikariDataSource(config);\n\n// Using connection (correct way)\n// try (Connection conn = dataSource.getConnection()) {\n//     Statement stmt = conn.createStatement();\n//     ResultSet rs = stmt.executeQuery(\"SELECT * FROM users\");\n//     // Process results\n// } // Connection automatically returned to pool\n\n// Without pooling (inefficient)\n// Connection conn1 = DriverManager.getConnection(url, user, pwd); // Expensive\n// // Use connection\n// conn1.close(); // Destroy connection\n\n// Connection conn2 = DriverManager.getConnection(url, user, pwd); // Expensive again\n// // Use connection\n// conn2.close(); // Destroy connection\n\n// With pooling (efficient)\n// Connection conn1 = pool.getConnection(); // Fast, reuses existing\n// // Use connection\n// conn1.close(); // Returns to pool, not destroyed\n\n// Connection conn2 = pool.getConnection(); // Fast, might reuse conn1\n// // Use connection\n// conn2.close(); // Returns to pool\n\n-- Python connection pooling with SQLAlchemy\n-- from sqlalchemy import create_engine\n-- from sqlalchemy.pool import QueuePool\n\n-- engine = create_engine(\n--     'mysql://user:password@localhost/mydb',\n--     poolclass=QueuePool,\n--     pool_size=10,           # Normal pool size\n--     max_overflow=20,        # Extra connections if needed\n--     pool_timeout=30,        # Wait time for connection\n--     pool_recycle=3600,      # Recycle connections after 1 hour\n--     pool_pre_ping=True      # Validate before use\n-- )\n\n-- Using connection\n-- with engine.connect() as conn:\n--     result = conn.execute(\"SELECT * FROM users\")\n--     # Process result\n-- # Connection returned to pool\n\n-- Node.js connection pooling with mysql2\n-- const mysql = require('mysql2/promise');\n\n-- const pool = mysql.createPool({\n--   host: 'localhost',\n--   user: 'root',\n--   password: 'password',\n--   database: 'mydb',\n--   waitForConnections: true,\n--   connectionLimit: 10,\n--   queueLimit: 0,\n--   enableKeepAlive: true,\n--   keepAliveInitialDelay: 0\n-- });\n\n-- async function getUser(userId) {\n--   const connection = await pool.getConnection();\n--   try {\n--     const [rows] = await connection.execute(\n--       'SELECT * FROM users WHERE id = ?',\n--       [userId]\n--     );\n--     return rows[0];\n--   } finally {\n--     connection.release(); // Return to pool\n--   }\n-- }\n\n-- C# connection pooling (built-in to ADO.NET)\n-- Connection string with pooling\n-- string connString = @\"Data Source=localhost;\n--                      Initial Catalog=mydb;\n--                      User ID=user;\n--                      Password=password;\n--                      Pooling=true;\n--                      Min Pool Size=5;\n--                      Max Pool Size=100;\n--                      Connection Lifetime=300;\n--                      Connection Timeout=30\";\n\n-- using (SqlConnection conn = new SqlConnection(connString))\n-- {\n--     conn.Open();\n--     SqlCommand cmd = new SqlCommand(\"SELECT * FROM users\", conn);\n--     SqlDataReader reader = cmd.ExecuteReader();\n--     // Process data\n-- } // Connection returned to pool\n\n-- Monitoring connection pool\nCREATE TABLE connection_pool_metrics (\n    metric_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    total_connections INT,\n    active_connections INT,\n    idle_connections INT,\n    waiting_threads INT,\n    average_wait_time_ms DECIMAL(10,2)\n);\n\n-- Log pool statistics\nINSERT INTO connection_pool_metrics \n(total_connections, active_connections, idle_connections)\nVALUES (20, 15, 5);\n\n-- Alert on pool exhaustion\nSELECT * FROM connection_pool_metrics\nWHERE active_connections = total_connections\n  AND waiting_threads > 0;\n\n-- Pool sizing calculation\n-- connections = ((core_count * 2) + effective_spindle_count)\n-- For CPU-bound: connections = core_count + 1\n-- For I/O-bound: connections = core_count * 2\n\n-- Connection leak detection\n-- Java example\n// config.setLeakDetectionThreshold(60000); // 60 seconds\n// If connection held longer than 60s, log warning\n\n-- Database-side connection monitoring\nSELECT \n    client_addr,\n    application_name,\n    state,\n    state_change,\n    COUNT(*) as connection_count\nFROM pg_stat_activity\nGROUP BY client_addr, application_name, state, state_change\nORDER BY connection_count DESC;\n\n-- Find long-running connections\nSELECT \n    pid,\n    usename,\n    application_name,\n    client_addr,\n    state,\n    query,\n    now() - query_start as duration\nFROM pg_stat_activity\nWHERE state != 'idle'\n  AND now() - query_start > interval '5 minutes'\nORDER BY duration DESC;\n\n-- MySQL connection statistics\nSHOW STATUS LIKE 'Threads_%';\n-- Threads_connected: Current connections\n-- Threads_running: Active connections\n-- Threads_created: Total created\n\n-- Max connections configuration\nSHOW VARIABLES LIKE 'max_connections';\n-- Default usually 151\n\n-- Set max connections\nSET GLOBAL max_connections = 500;\n\n-- Connection pool health check\nCREATE PROCEDURE check_pool_health()\nBEGIN\n    DECLARE active INT;\n    DECLARE max INT;\n    \n    -- Get active connections\n    SELECT COUNT(*) INTO active\n    FROM information_schema.processlist\n    WHERE command != 'Sleep';\n    \n    -- Get max connections\n    SELECT @@max_connections INTO max;\n    \n    -- Alert if > 80% utilized\n    IF active > (max * 0.8) THEN\n        INSERT INTO alerts (alert_type, message)\n        VALUES ('POOL_WARNING', CONCAT('Pool utilization: ', \n                ROUND(active * 100.0 / max, 2), '%'));\n    END IF;\nEND;"
    },
    {
      "id": 60,
      "question": "What is the difference between optimistic and pessimistic locking?",
      "answer": "Optimistic and pessimistic locking are concurrency control strategies that handle simultaneous data access differently.\n\nPessimistic Locking:\nâ€¢ Locks resource before modifying\nâ€¢ Prevents other users from accessing\nâ€¢ Assumes conflicts will occur\nâ€¢ Uses database locks\nâ€¢ Blocks concurrent access\nâ€¢ Good for high contention\nâ€¢ Can cause deadlocks\nâ€¢ Lower throughput\n\nOptimistic Locking:\nâ€¢ Doesn't lock resources\nâ€¢ Checks for conflicts at commit time\nâ€¢ Assumes conflicts are rare\nâ€¢ Uses version numbers or timestamps\nâ€¢ Allows concurrent reads\nâ€¢ Good for low contention\nâ€¢ No deadlocks\nâ€¢ Higher throughput\nâ€¢ Requires retry logic\n\nPessimistic Approach:\n1. Acquire lock\n2. Read data\n3. Modify data\n4. Write data\n5. Release lock\n\nOptimistic Approach:\n1. Read data (with version)\n2. Modify data\n3. Check version hasn't changed\n4. If same: commit, else retry\n\nUse Cases:\nâ€¢ Pessimistic: Banking, inventory, high contention\nâ€¢ Optimistic: Collaborative editing, low contention\n\nTrade-offs:\nâ€¢ Pessimistic: Consistent but slow\nâ€¢ Optimistic: Fast but may fail",
      "explanation": "Pessimistic locking prevents conflicts by locking resources upfront, while optimistic locking detects conflicts at commit time using versions.",
      "difficulty": "Hard",
      "code": "-- Pessimistic locking\n-- SQL Server: Lock row immediately\nBEGIN TRANSACTION;\n\nSELECT * FROM inventory\nWHERE product_id = 123\nFOR UPDATE; -- Locks the row\n-- Other transactions blocked\n\nUPDATE inventory SET quantity = quantity - 1\nWHERE product_id = 123;\n\nCOMMIT; -- Releases lock\n\n-- Exclusive lock (pessimistic)\nSELECT * FROM accounts\nWHERE account_id = 456\nFOR UPDATE NOWAIT; -- Fail immediately if locked\n\n-- Update lock (SQL Server)\nBEGIN TRANSACTION;\n\nSELECT balance FROM accounts WITH (UPDLOCK)\nWHERE account_id = 456;\n-- Prevents other UPDATE locks\n\nUPDATE accounts SET balance = balance - 100\nWHERE account_id = 456;\n\nCOMMIT;\n\n-- Pessimistic locking (PostgreSQL)\nBEGIN;\n\nSELECT * FROM products\nWHERE product_id = 789\nFOR UPDATE; -- Row-level lock\n\nUPDATE products SET stock = stock - 1\nWHERE product_id = 789;\n\nCOMMIT;\n\n-- Optimistic locking with version number\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    price DECIMAL(10,2),\n    stock INT,\n    version INT DEFAULT 0  -- Version column\n);\n\n-- Read with version\nSELECT product_id, name, stock, version\nFROM products\nWHERE product_id = 123;\n-- Returns: stock=10, version=5\n\n-- Update with version check\nUPDATE products\nSET stock = 9,\n    version = version + 1  -- Increment version\nWHERE product_id = 123\n  AND version = 5;  -- Check version hasn't changed\n\n-- Check affected rows\nIF @@ROWCOUNT = 0\n    -- Version changed, conflict detected\n    RAISERROR('Product was modified by another user', 16, 1);\nEND\n\n-- Optimistic locking with timestamp\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    total DECIMAL(10,2),\n    status VARCHAR(20),\n    last_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Read with timestamp\nSELECT order_id, status, last_modified\nFROM orders\nWHERE order_id = 456;\n-- Returns: status='pending', last_modified='2024-01-15 10:30:00'\n\n-- Update with timestamp check\nUPDATE orders\nSET status = 'shipped',\n    last_modified = CURRENT_TIMESTAMP\nWHERE order_id = 456\n  AND last_modified = '2024-01-15 10:30:00';\n\nIF @@ROWCOUNT = 0\n    -- Timestamp changed, conflict\n    THROW 50001, 'Order was modified', 1;\n\n-- Java optimistic locking (JPA)\n// @Entity\n// public class Product {\n//     @Id\n//     private Long id;\n//     \n//     private String name;\n//     private int stock;\n//     \n//     @Version  // Optimistic locking\n//     private int version;\n// }\n\n// Usage\n// Product product = em.find(Product.class, 123L);\n// product.setStock(product.getStock() - 1);\n// em.merge(product);\n// // If version changed, throws OptimisticLockException\n\n-- Application-level retry logic\nCREATE PROCEDURE update_with_retry(\n    @product_id INT,\n    @quantity INT\n)\nAS\nBEGIN\n    DECLARE @retry INT = 3;\n    DECLARE @version INT;\n    \n    WHILE @retry > 0\n    BEGIN\n        BEGIN TRY\n            -- Read current version\n            SELECT @version = version\n            FROM products\n            WHERE product_id = @product_id;\n            \n            -- Try update\n            UPDATE products\n            SET stock = stock - @quantity,\n                version = version + 1\n            WHERE product_id = @product_id\n              AND version = @version;\n            \n            IF @@ROWCOUNT = 1\n                RETURN; -- Success\n            ELSE\n                SET @retry = @retry - 1;\n        END TRY\n        BEGIN CATCH\n            SET @retry = @retry - 1;\n            WAITFOR DELAY '00:00:00.100'; -- Wait 100ms\n        END CATCH\n    END\n    \n    RAISERROR('Update failed after retries', 16, 1);\nEND;\n\n-- Comparison scenario\n-- High contention (pessimistic better)\nCREATE PROCEDURE reserve_seat(\n    @show_id INT,\n    @seat_id INT\n)\nAS\nBEGIN\n    BEGIN TRANSACTION;\n    \n    -- Pessimistic: Lock immediately\n    SELECT * FROM seats\n    WHERE show_id = @show_id AND seat_id = @seat_id\n    FOR UPDATE;\n    \n    -- Check availability\n    IF EXISTS (SELECT 1 FROM seats \n               WHERE show_id = @show_id \n                 AND seat_id = @seat_id \n                 AND status = 'available')\n    BEGIN\n        UPDATE seats SET status = 'reserved'\n        WHERE show_id = @show_id AND seat_id = @seat_id;\n        COMMIT;\n    END\n    ELSE\n    BEGIN\n        ROLLBACK;\n    END\nEND;\n\n-- Low contention (optimistic better)\nCREATE PROCEDURE update_profile(\n    @user_id INT,\n    @bio TEXT,\n    @current_version INT\n)\nAS\nBEGIN\n    -- Optimistic: No lock\n    UPDATE user_profiles\n    SET bio = @bio,\n        version = version + 1\n    WHERE user_id = @user_id\n      AND version = @current_version;\n    \n    IF @@ROWCOUNT = 0\n        RAISERROR('Profile was modified', 16, 1);\nEND;\n\n-- Deadlock with pessimistic locking\n-- Transaction 1:\nBEGIN TRANSACTION;\nSELECT * FROM table1 WHERE id = 1 FOR UPDATE; -- Lock 1\nSELECT * FROM table2 WHERE id = 2 FOR UPDATE; -- Wait for lock 2\n\n-- Transaction 2 (concurrent):\nBEGIN TRANSACTION;\nSELECT * FROM table2 WHERE id = 2 FOR UPDATE; -- Lock 2\nSELECT * FROM table1 WHERE id = 1 FOR UPDATE; -- Wait for lock 1\n-- DEADLOCK!\n\n-- No deadlock with optimistic locking\n-- Transaction 1:\nSELECT * FROM table1 WHERE id = 1; -- No lock\nUPDATE table1 SET... WHERE id = 1 AND version = 5;\n\n-- Transaction 2:\nSELECT * FROM table1 WHERE id = 1; -- No lock\nUPDATE table1 SET... WHERE id = 1 AND version = 5;\n-- One succeeds, other gets conflict and retries\n\n-- Hybrid approach\nCREATE PROCEDURE process_order(\n    @order_id INT\n)\nAS\nBEGIN\n    DECLARE @version INT;\n    \n    -- Optimistic read\n    SELECT @version = version FROM orders WHERE order_id = @order_id;\n    \n    -- Process business logic (no locks)\n    \n    -- Pessimistic write\n    BEGIN TRANSACTION;\n    \n    SELECT * FROM orders WHERE order_id = @order_id FOR UPDATE;\n    \n    UPDATE orders\n    SET status = 'completed',\n        version = version + 1\n    WHERE order_id = @order_id\n      AND version = @version;\n    \n    COMMIT;\nEND;"
    },
    {
      "id": 61,
      "question": "What are prepared statements and why are they important?",
      "answer": "Prepared statements are precompiled SQL statements that separate SQL code from data, improving security and performance.\n\nHow They Work:\nâ€¢ SQL structure sent to database once\nâ€¢ Database compiles and optimizes\nâ€¢ Parameters sent separately\nâ€¢ Same statement reused with different parameters\nâ€¢ Execution plan cached\n\nBenefits:\n\nSecurity:\nâ€¢ Prevents SQL injection completely\nâ€¢ Parameters never interpreted as SQL\nâ€¢ Database handles escaping\nâ€¢ Most important benefit\n\nPerformance:\nâ€¢ Compilation done once\nâ€¢ Execution plan reused\nâ€¢ Reduced parsing overhead\nâ€¢ Network efficiency (binary protocol)\n\nMaintainability:\nâ€¢ Cleaner code\nâ€¢ Separation of SQL and data\nâ€¢ Easier to read and maintain\n\nVs Regular Statements:\nâ€¢ Regular: SQL + data combined as string\nâ€¢ Prepared: SQL and data sent separately\nâ€¢ Regular: Vulnerable to injection\nâ€¢ Prepared: Safe from injection\n\nUse Cases:\nâ€¢ All user input queries\nâ€¢ Repeated queries with different parameters\nâ€¢ Dynamic queries\nâ€¢ Security-critical operations",
      "explanation": "Prepared statements separate SQL structure from data, preventing SQL injection and improving performance through cached execution plans.",
      "difficulty": "Easy",
      "code": "-- Java prepared statement\n// Unsafe: SQL injection vulnerable\n// String sql = \"SELECT * FROM users WHERE username = '\" + username + \"' AND password = '\" + password + \"'\";\n// Statement stmt = conn.createStatement();\n// ResultSet rs = stmt.executeQuery(sql);\n\n// Safe: Prepared statement\n// String sql = \"SELECT * FROM users WHERE username = ? AND password = ?\";\n// PreparedStatement pstmt = conn.prepareStatement(sql);\n// pstmt.setString(1, username);  // Parameter 1\n// pstmt.setString(2, password);  // Parameter 2\n// ResultSet rs = pstmt.executeQuery();\n\n// Reusing prepared statement\n// PreparedStatement pstmt = conn.prepareStatement(\"INSERT INTO logs (user_id, action) VALUES (?, ?)\");\n// for (int i = 0; i < 1000; i++) {\n//     pstmt.setInt(1, userId);\n//     pstmt.setString(2, \"action\" + i);\n//     pstmt.executeUpdate();\n// }\n// // Compiled once, executed 1000 times\n\n-- Python prepared statement (with parameterization)\n-- # Unsafe\n-- cursor.execute(f\"SELECT * FROM users WHERE email = '{email}'\")\n\n-- # Safe\n-- cursor.execute(\"SELECT * FROM users WHERE email = %s\", (email,))\n\n-- # Or with named parameters\n-- cursor.execute(\n--     \"SELECT * FROM users WHERE email = %(email)s AND status = %(status)s\",\n--     {'email': email, 'status': 'active'}\n-- )\n\n-- Node.js prepared statement\n-- // Unsafe\n-- connection.query(`SELECT * FROM products WHERE category = '${category}'`);\n\n-- // Safe\n-- connection.execute(\n--     'SELECT * FROM products WHERE category = ? AND price > ?',\n--     [category, minPrice]\n-- );\n\n-- C# prepared statement\n-- // Unsafe\n-- SqlCommand cmd = new SqlCommand(\n--     \"SELECT * FROM employees WHERE department = '\" + dept + \"'\",\n--     conn\n-- );\n\n-- // Safe\n-- SqlCommand cmd = new SqlCommand(\n--     \"SELECT * FROM employees WHERE department = @dept AND salary > @minSalary\",\n--     conn\n-- );\n-- cmd.Parameters.AddWithValue(\"@dept\", department);\n-- cmd.Parameters.AddWithValue(\"@minSalary\", minSalary);\n\n-- PHP prepared statement (PDO)\n-- // Unsafe\n-- $sql = \"SELECT * FROM users WHERE username = '$username'\";\n-- $result = $conn->query($sql);\n\n-- // Safe\n-- $stmt = $conn->prepare(\"SELECT * FROM users WHERE username = :username AND email = :email\");\n-- $stmt->bindParam(':username', $username);\n-- $stmt->bindParam(':email', $email);\n-- $stmt->execute();\n\n-- SQL Server named parameters\nDECLARE @sql NVARCHAR(MAX);\nDECLARE @username VARCHAR(50) = 'john_doe';\nDECLARE @age INT = 25;\n\nSET @sql = N'SELECT * FROM users WHERE username = @p1 AND age > @p2';\n\nEXECUTE sp_executesql @sql,\n    N'@p1 VARCHAR(50), @p2 INT',\n    @p1 = @username,\n    @p2 = @age;\n\n-- MySQL prepared statements\nPREPARE stmt FROM 'SELECT * FROM products WHERE category = ? AND price BETWEEN ? AND ?';\nSET @category = 'Electronics';\nSET @min_price = 100;\nSET @max_price = 1000;\nEXECUTE stmt USING @category, @min_price, @max_price;\nDEALLOCATE PREPARE stmt;\n\n-- PostgreSQL prepared statements\nPREPARE user_query (VARCHAR, INT) AS\nSELECT * FROM users WHERE email = $1 AND age > $2;\n\nEXECUTE user_query('john@example.com', 18);\n\n-- SQL injection attack prevented\n-- Attacker input: username = \"admin' OR '1'='1\"\n-- Without prepared statement:\n-- SELECT * FROM users WHERE username = 'admin' OR '1'='1'\n-- Returns all users! (SQL injection success)\n\n-- With prepared statement:\n-- SQL: SELECT * FROM users WHERE username = ?\n-- Parameter: \"admin' OR '1'='1\"\n-- Database treats entire input as literal string\n-- Looks for username = \"admin' OR '1'='1\" (no such user)\n-- Returns nothing (SQL injection failed)\n\n-- Performance benefit demonstration\n-- Without prepared statement (slow)\nCREATE PROCEDURE insert_without_prepared\nAS\nBEGIN\n    DECLARE @i INT = 0;\n    WHILE @i < 1000\n    BEGIN\n        -- Parsed and compiled 1000 times\n        EXEC('INSERT INTO logs (message) VALUES (''Log message ' + CAST(@i AS VARCHAR) + ''')');\n        SET @i = @i + 1;\n    END\nEND;\n\n-- With prepared statement (fast)\nCREATE PROCEDURE insert_with_prepared\nAS\nBEGIN\n    DECLARE @i INT = 0;\n    DECLARE @sql NVARCHAR(MAX) = N'INSERT INTO logs (message) VALUES (@msg)';\n    \n    -- Parsed and compiled once\n    WHILE @i < 1000\n    BEGIN\n        EXEC sp_executesql @sql, N'@msg VARCHAR(100)', @msg = CONCAT('Log message ', @i);\n        SET @i = @i + 1;\n    END\nEND;\n\n-- Data type handling\n-- Strong typing in prepared statements\n-- pstmt.setInt(1, 123);        // Integer\n-- pstmt.setString(2, \"text\");  // String\n-- pstmt.setDate(3, date);      // Date\n-- pstmt.setDouble(4, 99.99);   // Double\n-- pstmt.setNull(5, Types.VARCHAR); // NULL\n\n-- Batch execution with prepared statements\n-- PreparedStatement pstmt = conn.prepareStatement(\n--     \"INSERT INTO products (name, price) VALUES (?, ?)\"\n-- );\n\n-- for (Product product : products) {\n--     pstmt.setString(1, product.getName());\n--     pstmt.setDouble(2, product.getPrice());\n--     pstmt.addBatch();\n-- }\n\n-- int[] results = pstmt.executeBatch();\n\n-- Common mistake: Building SQL string\n-- String sql = \"SELECT * FROM users WHERE \"; // Base query\n-- if (filter != null) {\n--     sql += \"status = '\" + filter + \"' AND \";  // WRONG!\n-- }\n-- sql += \"age > \" + minAge;  // WRONG!\n\n-- Correct: Use prepared statement with dynamic SQL\n-- StringBuilder sql = new StringBuilder(\"SELECT * FROM users WHERE 1=1\");\n-- List<Object> params = new ArrayList<>();\n\n-- if (filter != null) {\n--     sql.append(\" AND status = ?\");\n--     params.add(filter);\n-- }\n-- sql.append(\" AND age > ?\");\n-- params.add(minAge);\n\n-- PreparedStatement pstmt = conn.prepareStatement(sql.toString());\n-- for (int i = 0; i < params.size(); i++) {\n--     pstmt.setObject(i + 1, params.get(i));\n-- }\n\n-- Stored procedure as prepared statement\nCREATE PROCEDURE get_user_orders\n    @user_id INT,\n    @start_date DATE,\n    @end_date DATE\nAS\nBEGIN\n    SELECT * FROM orders\n    WHERE user_id = @user_id\n      AND order_date BETWEEN @start_date AND @end_date;\nEND;\n\n-- Call with parameters\nEXEC get_user_orders @user_id = 123, @start_date = '2024-01-01', @end_date = '2024-12-31';"
    },
    {
      "id": 62,
      "question": "What is database mirroring and how does it differ from replication?",
      "answer": "Database mirroring and replication are both high availability solutions but serve different purposes and work differently.\n\nDatabase Mirroring:\nâ€¢ Two servers: principal and mirror\nâ€¢ Automatic failover\nâ€¢ Database-level operation\nâ€¢ Synchronous or asynchronous\nâ€¢ Mirror is standby (not readable)\nâ€¢ Exact copy at all times\nâ€¢ Transaction-level replication\nâ€¢ SQL Server specific\n\nReplication:\nâ€¢ Multiple subscribers possible\nâ€¢ Manual or automatic failover\nâ€¢ Table/object-level operation\nâ€¢ Usually asynchronous\nâ€¢ Subscribers can be readable\nâ€¢ May have differences\nâ€¢ Row-level replication\nâ€¢ Cross-platform possible\n\nMirroring Modes:\n\nHigh Safety (Synchronous):\nâ€¢ Transaction waits for mirror commit\nâ€¢ No data loss\nâ€¢ Automatic failover with witness\nâ€¢ Higher latency\n\nHigh Performance (Asynchronous):\nâ€¢ Transaction doesn't wait\nâ€¢ Possible data loss\nâ€¢ Manual failover\nâ€¢ Better performance\n\nWhen to Use:\nâ€¢ Mirroring: DR, failover, high availability\nâ€¢ Replication: Distribution, reporting, load balancing",
      "explanation": "Mirroring maintains exact database copy for failover, while replication distributes data subsets to multiple locations for read scaling.",
      "difficulty": "Hard",
      "code": "-- SQL Server Database Mirroring Setup\n-- On Principal Server:\n-- 1. Full backup\nBACKUP DATABASE production\nTO DISK = 'C:\\Backups\\production.bak';\n\nBACKUP LOG production\nTO DISK = 'C:\\Backups\\production_log.trn';\n\n-- 2. Create endpoint\nCREATE ENDPOINT mirror_endpoint\nSTATE = STARTED\nAS TCP (LISTENER_PORT = 5022)\nFOR DATABASE_MIRRORING (\n    ROLE = PARTNER,\n    AUTHENTICATION = WINDOWS NEGOTIATE,\n    ENCRYPTION = REQUIRED ALGORITHM AES\n);\n\n-- On Mirror Server:\n-- 1. Restore backups with NORECOVERY\nRESTORE DATABASE production\nFROM DISK = 'C:\\Backups\\production.bak'\nWITH NORECOVERY;\n\nRESTORE LOG production\nFROM DISK = 'C:\\Backups\\production_log.trn'\nWITH NORECOVERY;\n\n-- 2. Create endpoint\nCREATE ENDPOINT mirror_endpoint\nSTATE = STARTED\nAS TCP (LISTENER_PORT = 5022)\nFOR DATABASE_MIRRORING (\n    ROLE = PARTNER,\n    AUTHENTICATION = WINDOWS NEGOTIATE,\n    ENCRYPTION = REQUIRED ALGORITHM AES\n);\n\n-- 3. Set mirror partner\nALTER DATABASE production\nSET PARTNER = 'TCP://principal.server.com:5022';\n\n-- On Principal Server:\n-- Set mirror partner\nALTER DATABASE production\nSET PARTNER = 'TCP://mirror.server.com:5022';\n\n-- Optional: Add witness for automatic failover\n-- On Witness Server:\nCREATE ENDPOINT witness_endpoint\nSTATE = STARTED\nAS TCP (LISTENER_PORT = 5022)\nFOR DATABASE_MIRRORING (ROLE = WITNESS);\n\n-- On Principal:\nALTER DATABASE production\nSET WITNESS = 'TCP://witness.server.com:5022';\n\n-- Set operating mode\n-- High Safety with automatic failover\nALTER DATABASE production\nSET PARTNER SAFETY FULL;\n\n-- High Performance (asynchronous)\nALTER DATABASE production\nSET PARTNER SAFETY OFF;\n\n-- Monitor mirroring status\nSELECT \n    database_name,\n    mirroring_state_desc,\n    mirroring_role_desc,\n    mirroring_safety_level_desc,\n    mirroring_partner_name,\n    mirroring_witness_name,\n    mirroring_witness_state_desc\nFROM sys.database_mirroring\nWHERE mirroring_guid IS NOT NULL;\n\n-- Manual failover\nALTER DATABASE production SET PARTNER FAILOVER;\n\n-- SQL Server Transactional Replication\n-- Step 1: Configure Distributor\nEXEC sp_adddistributor \n    @distributor = 'distributor.server.com',\n    @password = 'strongpassword';\n\nEXEC sp_adddistributiondb \n    @database = 'distribution';\n\n-- Step 2: Configure Publisher\nEXEC sp_addpublication \n    @publication = 'ProductPublication',\n    @database = 'production',\n    @sync_method = 'concurrent',\n    @repl_freq = 'continuous',\n    @allow_push = 'true',\n    @allow_pull = 'true';\n\n-- Step 3: Add articles (tables to replicate)\nEXEC sp_addarticle \n    @publication = 'ProductPublication',\n    @article = 'products',\n    @source_object = 'products',\n    @destination_table = 'products';\n\nEXEC sp_addarticle \n    @publication = 'ProductPublication',\n    @article = 'orders',\n    @source_object = 'orders',\n    @destination_table = 'orders';\n\n-- Step 4: Create subscription\nEXEC sp_addsubscription \n    @publication = 'ProductPublication',\n    @subscriber = 'subscriber.server.com',\n    @destination_db = 'production_replica',\n    @subscription_type = 'push';\n\n-- Monitor replication\nEXEC sp_replmonitorhelppublication;\nEXEC sp_replmonitorhelpsubscription;\n\n-- Check replication lag\nSELECT \n    publisher_db,\n    publication,\n    subscriber,\n    subscriber_db,\n    latency,\n    time\nFROM distribution.dbo.MSdistribution_history;\n\n-- MySQL Master-Slave Replication\n-- On Master:\nCREATE USER 'repl_user'@'%' IDENTIFIED BY 'password';\nGRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%';\nFLUSH PRIVILEGES;\n\nSHOW MASTER STATUS;\n-- Note File and Position\n\n-- On Slave:\nCHANGE MASTER TO\n    MASTER_HOST='master.server.com',\n    MASTER_USER='repl_user',\n    MASTER_PASSWORD='password',\n    MASTER_LOG_FILE='mysql-bin.000001',\n    MASTER_LOG_POS=107;\n\nSTART SLAVE;\n\nSHOW SLAVE STATUS\\G\n\n-- PostgreSQL Logical Replication\n-- On Publisher:\nCREATE PUBLICATION product_pub FOR TABLE products, orders;\n\n-- On Subscriber:\nCREATE SUBSCRIPTION product_sub\nCONNECTION 'host=publisher.server.com dbname=production user=repl_user'\nPUBLICATION product_pub;\n\n-- Monitor subscription\nSELECT * FROM pg_stat_subscription;\n\n-- Comparison table\nCREATE TABLE ha_comparison (\n    feature VARCHAR(50),\n    mirroring VARCHAR(100),\n    replication VARCHAR(100)\n);\n\nINSERT INTO ha_comparison VALUES\n('Granularity', 'Database level', 'Table/object level'),\n('Failover', 'Automatic (with witness)', 'Usually manual'),\n('Read Access', 'Mirror not readable', 'Subscribers readable'),\n('Latency', 'Low (synchronous option)', 'Higher (async)'),\n('Data Guarantee', 'Exact copy always', 'May have differences'),\n('Use Case', 'DR, HA', 'Distribution, reporting');\n\n-- Failover scenario - Mirroring\n-- Automatic failover (high safety with witness)\n-- 1. Principal fails\n-- 2. Mirror detects failure via witness\n-- 3. Mirror automatically becomes principal\n-- 4. Applications reconnect to new principal\n\n-- Application connection string with failover\n-- Data Source=principal.server.com;Failover Partner=mirror.server.com;\n-- Initial Catalog=production;\n\n-- Failover scenario - Replication\n-- Manual failover\n-- 1. Publisher fails\n-- 2. Promote subscriber to publisher\nSTOP SLAVE;\nRESET SLAVE ALL;\nSET GLOBAL read_only = 0;\n\n-- 3. Reconfigure application to use new publisher\n-- 4. Setup replication from new publisher\n\n-- Hybrid approach: Mirroring + Replication\n-- Principal <-> Mirror (Mirroring for HA)\n-- Principal -> Subscribers (Replication for read scale)\n\n-- Performance monitoring\nCREATE TABLE mirror_performance (\n    sample_time DATETIME,\n    send_queue_size INT,\n    redo_queue_size INT,\n    mirroring_role VARCHAR(20),\n    mirroring_state VARCHAR(20)\n);\n\nINSERT INTO mirror_performance\nSELECT \n    GETDATE(),\n    mirroring_send_queue_size,\n    mirroring_redo_queue_size,\n    mirroring_role_desc,\n    mirroring_state_desc\nFROM sys.database_mirroring\nWHERE database_id = DB_ID('production');"
    },
    {
      "id": 63,
      "question": "What is a covering index and when should you use it?",
      "answer": "A covering index (index with included columns) contains all columns needed by a query, eliminating the need to access the table data.\n\nHow It Works:\nâ€¢ Index includes query columns\nâ€¢ Query satisfied entirely from index\nâ€¢ No table lookup required\nâ€¢ Avoids key lookup operation\nâ€¢ Reads only index pages\n\nComponents:\n\nKey Columns:\nâ€¢ Used for searching and sorting\nâ€¢ Ordered in B-tree\nâ€¢ Part of index key\nâ€¢ Affect index size and ordering\n\nIncluded Columns:\nâ€¢ Additional data stored in leaf level\nâ€¢ Not part of index key\nâ€¢ Not used for searching\nâ€¢ Only for covering queries\nâ€¢ Don't affect index ordering\n\nBenefits:\nâ€¢ Dramatically faster queries\nâ€¢ Reduced I/O operations\nâ€¢ No table access needed\nâ€¢ Lower resource usage\nâ€¢ Better for SELECT queries\n\nTrade-offs:\nâ€¢ Larger index size\nâ€¢ Slower INSERT/UPDATE/DELETE\nâ€¢ More disk space\nâ€¢ More maintenance overhead\n\nWhen to Use:\nâ€¢ Frequently run queries\nâ€¢ SELECT with specific columns\nâ€¢ High read, low write scenarios\nâ€¢ Performance critical queries",
      "explanation": "Covering indexes include all columns needed by a query in the index itself, eliminating table lookups for maximum query performance.",
      "difficulty": "Medium",
      "code": "-- Non-covering index (requires table lookup)\nCREATE INDEX idx_lastname ON employees(last_name);\n\nSELECT last_name, first_name, email, salary\nFROM employees\nWHERE last_name = 'Smith';\n-- 1. Index seek on idx_lastname finds matching rows\n-- 2. Key lookup to table for first_name, email, salary (slow)\n-- 3. Returns results\n\n-- Covering index (no table lookup)\nCREATE INDEX idx_covering ON employees(last_name)\nINCLUDE (first_name, email, salary);\n\nSELECT last_name, first_name, email, salary\nFROM employees\nWHERE last_name = 'Smith';\n-- 1. Index seek on idx_covering\n-- 2. All data available in index (fast)\n-- 3. Returns results (no table access)\n\n-- SQL Server covering index\nCREATE NONCLUSTERED INDEX idx_employee_details\nON employees(department_id, last_name)\nINCLUDE (first_name, email, phone, hire_date);\n\n-- This query is fully covered\nSELECT first_name, last_name, email, phone\nFROM employees\nWHERE department_id = 10\nORDER BY last_name;\n-- Execution plan shows: Index Seek (no Key Lookup)\n\n-- MySQL covering index\nCREATE INDEX idx_product_search\nON products(category, price, name, description);\n\nSELECT name, price, description\nFROM products\nWHERE category = 'Electronics'\n  AND price BETWEEN 100 AND 500;\n-- Uses covering index, no table access\n-- EXPLAIN shows: Using index (covering)\n\n-- Composite key with included columns\nCREATE INDEX idx_order_details\nON orders(customer_id, order_date)\nINCLUDE (total, status, shipping_address);\n\n-- Covered queries\nSELECT order_date, total, status\nFROM orders\nWHERE customer_id = 123;\n-- Fully covered\n\nSELECT customer_id, COUNT(*), SUM(total)\nFROM orders\nWHERE order_date >= '2024-01-01'\nGROUP BY customer_id;\n-- Fully covered\n\n-- Non-covered query (requires table access)\nSELECT order_date, total, status, detailed_notes\nFROM orders\nWHERE customer_id = 123;\n-- detailed_notes not in index, requires key lookup\n\n-- Identifying missing covering indexes\n-- Check execution plan for key lookups\nSET STATISTICS IO ON;\n\nSELECT name, email, phone\nFROM users\nWHERE username = 'john_doe';\n\n-- Output shows:\n-- Logical reads: users = 100 (table access)\n-- Logical reads: idx_username = 3 (index)\n\nSET STATISTICS IO OFF;\n\n-- Create covering index to eliminate table reads\nCREATE INDEX idx_username_covering\nON users(username)\nINCLUDE (name, email, phone);\n\n-- Query again\nSET STATISTICS IO ON;\n\nSELECT name, email, phone\nFROM users\nWHERE username = 'john_doe';\n\n-- Output shows:\n-- Logical reads: idx_username_covering = 3 (only index)\n-- No table access!\n\n-- Finding key lookup operations\nSELECT \n    object_name(ips.object_id) as table_name,\n    i.name as index_name,\n    ips.index_type_desc,\n    ips.page_count,\n    ips.avg_fragmentation_in_percent\nFROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, 'DETAILED') ips\nJOIN sys.indexes i ON ips.object_id = i.object_id AND ips.index_id = i.index_id\nWHERE ips.index_level = 0;\n\n-- DMV for missing indexes\nSELECT \n    migs.avg_total_user_cost * (migs.avg_user_impact / 100.0) * (migs.user_seeks + migs.user_scans) as improvement_measure,\n    'CREATE INDEX idx_' + \n        REPLACE(REPLACE(REPLACE(mid.equality_columns,'[',''),']',''),', ','_') + '_covering' +\n        ' ON ' + mid.statement + \n        ' (' + ISNULL(mid.equality_columns,'') + ') ' +\n        CASE WHEN mid.inequality_columns IS NOT NULL \n             THEN 'INCLUDE (' + mid.inequality_columns + ')' \n             ELSE '' END as create_statement\nFROM sys.dm_db_missing_index_groups mig\nJOIN sys.dm_db_missing_index_group_stats migs ON mig.index_group_handle = migs.group_handle\nJOIN sys.dm_db_missing_index_details mid ON mig.index_handle = mid.index_handle\nORDER BY improvement_measure DESC;\n\n-- Covering index for JOIN query\nCREATE INDEX idx_order_items_covering\nON order_items(order_id)\nINCLUDE (product_id, quantity, price);\n\nCREATE INDEX idx_products_covering\nON products(product_id)\nINCLUDE (product_name, category);\n\nSELECT \n    p.product_name,\n    p.category,\n    oi.quantity,\n    oi.price\nFROM order_items oi\nJOIN products p ON oi.product_id = p.product_id\nWHERE oi.order_id = 12345;\n-- Both indexes are covering, no table access\n\n-- Covering index for aggregation\nCREATE INDEX idx_sales_summary\nON sales(product_id, sale_date)\nINCLUDE (quantity, amount);\n\nSELECT \n    product_id,\n    COUNT(*) as sale_count,\n    SUM(quantity) as total_quantity,\n    SUM(amount) as total_amount\nFROM sales\nWHERE sale_date >= '2024-01-01'\nGROUP BY product_id;\n-- Fully covered, very fast\n\n-- Too many included columns (bad)\nCREATE INDEX idx_too_wide\nON employees(department_id)\nINCLUDE (first_name, last_name, email, phone, address, city, state, zip, \n         ssn, birth_date, hire_date, salary, bonus, notes, photo);\n-- Index too large, slow writes, wasted space\n\n-- Better: Multiple targeted covering indexes\nCREATE INDEX idx_dept_names\nON employees(department_id)\nINCLUDE (first_name, last_name, email);\n\nCREATE INDEX idx_dept_contact\nON employees(department_id)\nINCLUDE (phone, email, address);\n\nCREATE INDEX idx_dept_hr\nON employees(department_id)\nINCLUDE (hire_date, salary, status);\n\n-- Filtered covering index\nCREATE INDEX idx_active_employees\nON employees(department_id)\nINCLUDE (name, email, phone)\nWHERE status = 'Active' AND employment_type = 'Full-Time';\n-- Smaller index, faster queries for active employees\n\n-- Monitoring covering index usage\nSELECT \n    OBJECT_NAME(s.object_id) as table_name,\n    i.name as index_name,\n    s.user_seeks,\n    s.user_scans,\n    s.user_lookups,  -- Key lookups (bad for covering)\n    s.user_updates\nFROM sys.dm_db_index_usage_stats s\nJOIN sys.indexes i ON s.object_id = i.object_id AND s.index_id = i.index_id\nWHERE s.database_id = DB_ID()\nORDER BY s.user_lookups DESC;\n-- High user_lookups indicates non-covering index"
    },
    {
      "id": 64,
      "question": "What is a recursive query and how does it work?",
      "answer": "Recursive queries allow a query to reference itself, essential for querying hierarchical or tree-structured data.\n\nStructure:\n\nAnchor Member:\nâ€¢ Non-recursive part\nâ€¢ Initial result set\nâ€¢ Executed once\nâ€¢ Starting point\n\nRecursive Member:\nâ€¢ References CTE itself\nâ€¢ Executed repeatedly\nâ€¢ Joins with previous iteration\nâ€¢ Continues until no rows returned\n\nFinal Result:\nâ€¢ UNION of all iterations\nâ€¢ Anchor + recursive results\nâ€¢ Stops when recursive returns empty\n\nCommon Use Cases:\nâ€¢ Organizational hierarchies\nâ€¢ Bill of materials\nâ€¢ Graph traversal\nâ€¢ Tree structures\nâ€¢ Pathfinding\nâ€¢ Hierarchical menus\nâ€¢ Category trees\n\nTermination:\nâ€¢ No more rows from recursive part\nâ€¢ MAXRECURSION option (SQL Server)\nâ€¢ Prevents infinite loops\n\nPerformance:\nâ€¢ Can be expensive\nâ€¢ Watch for infinite loops\nâ€¢ Consider materialized paths\nâ€¢ Use appropriate indexes",
      "explanation": "Recursive queries use CTEs that reference themselves to traverse hierarchical data like org charts, repeatedly joining results until complete.",
      "difficulty": "Hard",
      "code": "-- Basic recursive CTE structure\nWITH RECURSIVE cte_name AS (\n    -- Anchor: Non-recursive base case\n    SELECT columns FROM table WHERE condition\n    \n    UNION ALL\n    \n    -- Recursive: References CTE itself\n    SELECT columns FROM table\n    JOIN cte_name ON join_condition\n)\nSELECT * FROM cte_name;\n\n-- Employee hierarchy (manager-employee)\nCREATE TABLE employees (\n    employee_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    manager_id INT,\n    title VARCHAR(100)\n);\n\nINSERT INTO employees VALUES\n(1, 'CEO', NULL, 'Chief Executive Officer'),\n(2, 'VP Sales', 1, 'Vice President'),\n(3, 'VP Engineering', 1, 'Vice President'),\n(4, 'Sales Manager', 2, 'Manager'),\n(5, 'Developer', 3, 'Software Engineer'),\n(6, 'Sales Rep', 4, 'Representative');\n\n-- Find all employees under CEO\nWITH RECURSIVE employee_hierarchy AS (\n    -- Anchor: CEO (no manager)\n    SELECT employee_id, name, manager_id, title, 0 as level,\n           CAST(name AS VARCHAR(1000)) as path\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive: All direct reports\n    SELECT e.employee_id, e.name, e.manager_id, e.title, eh.level + 1,\n           CAST(eh.path || ' > ' || e.name AS VARCHAR(1000))\n    FROM employees e\n    JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n)\nSELECT \n    REPEAT('  ', level) || name as indented_name,\n    title,\n    level,\n    path\nFROM employee_hierarchy\nORDER BY path;\n\n-- Output:\n-- CEO                          Chief Executive Officer  0  CEO\n--   VP Sales                    Vice President          1  CEO > VP Sales\n--     Sales Manager             Manager                 2  CEO > VP Sales > Sales Manager\n--       Sales Rep               Representative          3  CEO > VP Sales > Sales Manager > Sales Rep\n--   VP Engineering              Vice President          1  CEO > VP Engineering\n--     Developer                 Software Engineer       2  CEO > VP Engineering > Developer\n\n-- SQL Server recursive CTE\nWITH EmployeeHierarchy AS (\n    -- Anchor\n    SELECT employee_id, name, manager_id, title, 0 as level\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    -- Recursive\n    SELECT e.employee_id, e.name, e.manager_id, e.title, eh.level + 1\n    FROM employees e\n    INNER JOIN EmployeeHierarchy eh ON e.manager_id = eh.employee_id\n)\nSELECT * FROM EmployeeHierarchy\nORDER BY level, name\nOPTION (MAXRECURSION 100); -- Limit to 100 levels\n\n-- Category tree (nested categories)\nCREATE TABLE categories (\n    category_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    parent_id INT\n);\n\nINSERT INTO categories VALUES\n(1, 'Electronics', NULL),\n(2, 'Computers', 1),\n(3, 'Laptops', 2),\n(4, 'Desktops', 2),\n(5, 'Gaming Laptops', 3),\n(6, 'Business Laptops', 3);\n\n-- Get all subcategories of Electronics\nWITH RECURSIVE category_tree AS (\n    SELECT category_id, name, parent_id, 0 as depth\n    FROM categories\n    WHERE name = 'Electronics'\n    \n    UNION ALL\n    \n    SELECT c.category_id, c.name, c.parent_id, ct.depth + 1\n    FROM categories c\n    JOIN category_tree ct ON c.parent_id = ct.category_id\n)\nSELECT \n    category_id,\n    REPEAT('-', depth * 2) || name as category_name,\n    depth\nFROM category_tree\nORDER BY category_id;\n\n-- Bill of Materials (BOM)\nCREATE TABLE parts (\n    part_id INT,\n    part_name VARCHAR(100),\n    parent_part_id INT,\n    quantity INT\n);\n\nINSERT INTO parts VALUES\n(1, 'Bicycle', NULL, 1),\n(2, 'Frame', 1, 1),\n(3, 'Wheel', 1, 2),\n(4, 'Tire', 3, 1),\n(5, 'Tube', 3, 1),\n(6, 'Rim', 3, 1);\n\n-- Calculate total parts needed\nWITH RECURSIVE bom AS (\n    SELECT part_id, part_name, parent_part_id, quantity, 1 as total_quantity\n    FROM parts\n    WHERE parent_part_id IS NULL\n    \n    UNION ALL\n    \n    SELECT p.part_id, p.part_name, p.parent_part_id, p.quantity,\n           bom.total_quantity * p.quantity\n    FROM parts p\n    JOIN bom ON p.parent_part_id = bom.part_id\n)\nSELECT part_name, SUM(total_quantity) as total_needed\nFROM bom\nGROUP BY part_name\nORDER BY part_name;\n\n-- Number sequence generation\nWITH RECURSIVE numbers AS (\n    SELECT 1 as n\n    UNION ALL\n    SELECT n + 1 FROM numbers WHERE n < 100\n)\nSELECT * FROM numbers;\n-- Generates 1 to 100\n\n-- Date sequence generation\nWITH RECURSIVE dates AS (\n    SELECT CAST('2024-01-01' AS DATE) as date\n    UNION ALL\n    SELECT date + INTERVAL 1 DAY\n    FROM dates\n    WHERE date < '2024-12-31'\n)\nSELECT * FROM dates;\n-- All dates in 2024\n\n-- Graph traversal (finding paths)\nCREATE TABLE connections (\n    from_node INT,\n    to_node INT,\n    distance INT\n);\n\nINSERT INTO connections VALUES\n(1, 2, 10),\n(2, 3, 20),\n(1, 3, 50),\n(3, 4, 15),\n(2, 4, 30);\n\n-- Find all paths from node 1 to node 4\nWITH RECURSIVE paths AS (\n    SELECT from_node, to_node, distance,\n           ARRAY[from_node, to_node] as path,\n           distance as total_distance\n    FROM connections\n    WHERE from_node = 1\n    \n    UNION ALL\n    \n    SELECT c.from_node, c.to_node, c.distance,\n           p.path || c.to_node,\n           p.total_distance + c.distance\n    FROM connections c\n    JOIN paths p ON c.from_node = p.to_node\n    WHERE NOT (c.to_node = ANY(p.path)) -- Avoid cycles\n)\nSELECT path, total_distance\nFROM paths\nWHERE to_node = 4\nORDER BY total_distance;\n\n-- Fibonacci sequence\nWITH RECURSIVE fibonacci (n, fib_n, fib_n_plus_1) AS (\n    SELECT 1, 0, 1\n    UNION ALL\n    SELECT n + 1, fib_n_plus_1, fib_n + fib_n_plus_1\n    FROM fibonacci\n    WHERE n < 20\n)\nSELECT n, fib_n FROM fibonacci;\n\n-- Detecting cycles in hierarchy\nWITH RECURSIVE cycle_detect AS (\n    SELECT employee_id, manager_id,\n           ARRAY[employee_id] as path,\n           false as cycle\n    FROM employees\n    WHERE manager_id IS NULL\n    \n    UNION ALL\n    \n    SELECT e.employee_id, e.manager_id,\n           cd.path || e.employee_id,\n           e.employee_id = ANY(cd.path)\n    FROM employees e\n    JOIN cycle_detect cd ON e.manager_id = cd.employee_id\n    WHERE NOT cd.cycle\n)\nSELECT * FROM cycle_detect WHERE cycle = true;\n-- Reports any cycles found\n\n-- Performance: Non-recursive alternative (materialized path)\nCREATE TABLE employees_with_path (\n    employee_id INT,\n    name VARCHAR(100),\n    path VARCHAR(255)  -- e.g., '/1/2/4/6'\n);\n\n-- Query is simple and fast\nSELECT * FROM employees_with_path\nWHERE path LIKE '/1/2/%';\n-- All descendants of employee 2"
    },
    {
      "id": 65,
      "question": "What is database auditing and how is it implemented?",
      "answer": "Database auditing tracks and records database activities for security, compliance, and troubleshooting purposes.\n\nWhat to Audit:\nâ€¢ Login attempts (success/failure)\nâ€¢ Data modifications (INSERT, UPDATE, DELETE)\nâ€¢ Schema changes (DDL operations)\nâ€¢ Permission changes\nâ€¢ Query execution\nâ€¢ Administrative actions\nâ€¢ Access to sensitive data\n\nAuditing Methods:\n\nDatabase-Level Auditing:\nâ€¢ Built-in audit features\nâ€¢ Minimal performance impact\nâ€¢ Automatic logging\nâ€¢ SQL Server Audit, Oracle Audit\n\nTrigger-Based Auditing:\nâ€¢ Custom audit logic\nâ€¢ Flexible but slower\nâ€¢ DML triggers on tables\nâ€¢ Application-specific rules\n\nApplication-Level Auditing:\nâ€¢ Logged by application\nâ€¢ Complete business context\nâ€¢ Easier to customize\nâ€¢ Requires code changes\n\nLog Files:\nâ€¢ Transaction logs\nâ€¢ Audit trails\nâ€¢ Error logs\nâ€¢ Performance logs\n\nCompliance:\nâ€¢ GDPR, HIPAA, SOX, PCI-DSS\nâ€¢ Regulatory requirements\nâ€¢ Data protection laws\nâ€¢ Industry standards\n\nBest Practices:\nâ€¢ Audit critical operations only\nâ€¢ Secure audit logs\nâ€¢ Regular review\nâ€¢ Retention policies\nâ€¢ Performance monitoring",
      "explanation": "Database auditing tracks database activities through built-in features, triggers, or application logging to ensure security, compliance, and accountability.",
      "difficulty": "Medium",
      "code": "-- SQL Server Audit setup\n-- 1. Create server audit\nCREATE SERVER AUDIT production_audit\nTO FILE (\n    FILEPATH = 'C:\\Audits\\',\n    MAXSIZE = 100 MB,\n    MAX_ROLLOVER_FILES = 10,\n    RESERVE_DISK_SPACE = OFF\n)\nWITH (QUEUE_DELAY = 1000, ON_FAILURE = CONTINUE);\n\nALTER SERVER AUDIT production_audit WITH (STATE = ON);\n\n-- 2. Create database audit specification\nCREATE DATABASE AUDIT SPECIFICATION audit_data_changes\nFOR SERVER AUDIT production_audit\nADD (DELETE, INSERT, UPDATE ON dbo.customers BY public),\nADD (DELETE, INSERT, UPDATE ON dbo.orders BY public),\nADD (SELECT ON dbo.credit_cards BY public)\nWITH (STATE = ON);\n\n-- 3. Create server audit specification\nCREATE SERVER AUDIT SPECIFICATION audit_logins\nFOR SERVER AUDIT production_audit\nADD (FAILED_LOGIN_GROUP),\nADD (SUCCESSFUL_LOGIN_GROUP),\nADD (LOGOUT_GROUP)\nWITH (STATE = ON);\n\n-- Query audit logs\nSELECT \n    event_time,\n    action_id,\n    succeeded,\n    session_server_principal_name as username,\n    database_name,\n    schema_name,\n    object_name,\n    statement\nFROM sys.fn_get_audit_file('C:\\Audits\\*.sqlaudit', NULL, NULL)\nWHERE event_time > DATEADD(day, -7, GETDATE())\nORDER BY event_time DESC;\n\n-- Trigger-based auditing\n-- Create audit table\nCREATE TABLE audit_log (\n    audit_id BIGINT IDENTITY PRIMARY KEY,\n    table_name VARCHAR(100),\n    operation VARCHAR(10),\n    user_name VARCHAR(100),\n    operation_date DATETIME DEFAULT GETDATE(),\n    old_values TEXT,\n    new_values TEXT\n);\n\n-- Audit trigger for data changes\nCREATE TRIGGER trg_customers_audit\nON customers\nAFTER INSERT, UPDATE, DELETE\nAS\nBEGIN\n    DECLARE @operation VARCHAR(10);\n    \n    IF EXISTS (SELECT * FROM inserted) AND EXISTS (SELECT * FROM deleted)\n        SET @operation = 'UPDATE';\n    ELSE IF EXISTS (SELECT * FROM inserted)\n        SET @operation = 'INSERT';\n    ELSE\n        SET @operation = 'DELETE';\n    \n    INSERT INTO audit_log (table_name, operation, user_name, old_values, new_values)\n    SELECT \n        'customers',\n        @operation,\n        SYSTEM_USER,\n        (SELECT * FROM deleted FOR JSON AUTO),\n        (SELECT * FROM inserted FOR JSON AUTO);\nEND;\n\n-- Testing the audit trigger\nUPDATE customers SET email = 'new@email.com' WHERE customer_id = 123;\n\n-- View audit log\nSELECT * FROM audit_log WHERE table_name = 'customers' ORDER BY operation_date DESC;\n\n-- MySQL audit setup\n-- Enable general log (all queries)\nSET GLOBAL general_log = 'ON';\nSET GLOBAL general_log_file = '/var/log/mysql/general.log';\n\n-- Enable slow query log\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL slow_query_log_file = '/var/log/mysql/slow-query.log';\nSET GLOBAL long_query_time = 2; -- Log queries > 2 seconds\n\n-- MySQL Enterprise Audit Plugin\n-- INSTALL PLUGIN audit_log SONAME 'audit_log.so';\n-- SET GLOBAL audit_log_policy = 'ALL';\n-- SET GLOBAL audit_log_format = 'JSON';\n\n-- PostgreSQL auditing with triggers\nCREATE TABLE audit_trail (\n    audit_id SERIAL PRIMARY KEY,\n    schema_name TEXT,\n    table_name TEXT,\n    operation TEXT,\n    user_name TEXT,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    old_data JSONB,\n    new_data JSONB\n);\n\n-- Generic audit trigger function\nCREATE OR REPLACE FUNCTION audit_trigger_func()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF (TG_OP = 'DELETE') THEN\n        INSERT INTO audit_trail (schema_name, table_name, operation, user_name, old_data)\n        VALUES (TG_TABLE_SCHEMA, TG_TABLE_NAME, TG_OP, current_user, row_to_json(OLD));\n        RETURN OLD;\n    ELSIF (TG_OP = 'UPDATE') THEN\n        INSERT INTO audit_trail (schema_name, table_name, operation, user_name, old_data, new_data)\n        VALUES (TG_TABLE_SCHEMA, TG_TABLE_NAME, TG_OP, current_user, row_to_json(OLD), row_to_json(NEW));\n        RETURN NEW;\n    ELSIF (TG_OP = 'INSERT') THEN\n        INSERT INTO audit_trail (schema_name, table_name, operation, user_name, new_data)\n        VALUES (TG_TABLE_SCHEMA, TG_TABLE_NAME, TG_OP, current_user, row_to_json(NEW));\n        RETURN NEW;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Attach to table\nCREATE TRIGGER audit_customers\nAFTER INSERT OR UPDATE OR DELETE ON customers\nFOR EACH ROW EXECUTE FUNCTION audit_trigger_func();\n\n-- Login audit table\nCREATE TABLE login_audit (\n    login_id BIGINT IDENTITY PRIMARY KEY,\n    login_name VARCHAR(100),\n    login_time DATETIME,\n    login_success BIT,\n    client_ip VARCHAR(50),\n    application_name VARCHAR(200)\n);\n\n-- Logon trigger (SQL Server)\nCREATE TRIGGER trg_logon_audit\nON ALL SERVER\nFOR LOGON\nAS\nBEGIN\n    INSERT INTO AuditDB.dbo.login_audit (login_name, login_time, login_success, client_ip, application_name)\n    VALUES (\n        ORIGINAL_LOGIN(),\n        GETDATE(),\n        1,\n        CONVERT(VARCHAR(50), CONNECTIONPROPERTY('client_net_address')),\n        APP_NAME()\n    );\nEND;\n\n-- Schema change audit\nCREATE TABLE ddl_audit (\n    audit_id BIGINT IDENTITY PRIMARY KEY,\n    event_time DATETIME,\n    login_name VARCHAR(100),\n    database_name VARCHAR(100),\n    object_name VARCHAR(200),\n    ddl_statement TEXT,\n    event_type VARCHAR(50)\n);\n\n-- DDL trigger\nCREATE TRIGGER trg_ddl_audit\nON DATABASE\nFOR CREATE_TABLE, ALTER_TABLE, DROP_TABLE, CREATE_INDEX, DROP_INDEX\nAS\nBEGIN\n    DECLARE @data XML = EVENTDATA();\n    \n    INSERT INTO ddl_audit (event_time, login_name, database_name, object_name, ddl_statement, event_type)\n    VALUES (\n        GETDATE(),\n        @data.value('(/EVENT_INSTANCE/LoginName)[1]', 'VARCHAR(100)'),\n        @data.value('(/EVENT_INSTANCE/DatabaseName)[1]', 'VARCHAR(100)'),\n        @data.value('(/EVENT_INSTANCE/ObjectName)[1]', 'VARCHAR(200)'),\n        @data.value('(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]', 'VARCHAR(MAX)'),\n        @data.value('(/EVENT_INSTANCE/EventType)[1]', 'VARCHAR(50)')\n    );\nEND;\n\n-- Audit reports\n-- Failed login attempts\nSELECT \n    login_name,\n    COUNT(*) as failed_attempts,\n    MAX(login_time) as last_attempt,\n    client_ip\nFROM login_audit\nWHERE login_success = 0\n  AND login_time > DATEADD(day, -1, GETDATE())\nGROUP BY login_name, client_ip\nHAVING COUNT(*) > 5\nORDER BY failed_attempts DESC;\n\n-- Data access audit (sensitive tables)\nSELECT \n    user_name,\n    table_name,\n    operation,\n    COUNT(*) as access_count,\n    MAX(operation_date) as last_access\nFROM audit_log\nWHERE table_name IN ('credit_cards', 'ssn_data', 'salaries')\n  AND operation_date > DATEADD(day, -7, GETDATE())\nGROUP BY user_name, table_name, operation\nORDER BY access_count DESC;\n\n-- Compliance report\nSELECT \n    CONVERT(DATE, operation_date) as date,\n    table_name,\n    operation,\n    COUNT(*) as operations\nFROM audit_log\nWHERE operation_date >= DATEADD(month, -1, GETDATE())\nGROUP BY CONVERT(DATE, operation_date), table_name, operation\nORDER BY date DESC, operations DESC;\n\n-- Audit log maintenance\n-- Archive old logs\nINSERT INTO audit_log_archive\nSELECT * FROM audit_log\nWHERE operation_date < DATEADD(year, -1, GETDATE());\n\n-- Delete archived logs\nDELETE FROM audit_log\nWHERE operation_date < DATEADD(year, -1, GETDATE());\n\n-- Application-level auditing example\n-- CREATE PROCEDURE sp_update_customer\n--     @customer_id INT,\n--     @email VARCHAR(100),\n--     @user_id INT\n-- AS\n-- BEGIN\n--     BEGIN TRANSACTION;\n--     \n--     -- Get old values\n--     DECLARE @old_email VARCHAR(100);\n--     SELECT @old_email = email FROM customers WHERE customer_id = @customer_id;\n--     \n--     -- Update\n--     UPDATE customers SET email = @email WHERE customer_id = @customer_id;\n--     \n--     -- Log change\n--     INSERT INTO audit_log (table_name, operation, user_name, old_values, new_values)\n--     VALUES ('customers', 'UPDATE', @user_id, \n--             JSON_OBJECT('email', @old_email),\n--             JSON_OBJECT('email', @email));\n--     \n--     COMMIT;\n-- END;"
    },
    {
      "id": 66,
      "question": "What is a database index fragmentation and how do you fix it?",
      "answer": "Index fragmentation occurs when index pages are not stored contiguously or data within pages becomes scattered, degrading performance.\n\nTypes of Fragmentation:\n\nExternal (Logical) Fragmentation:\nâ€¢ Index pages not physically sequential\nâ€¢ Affects range scans\nâ€¢ Caused by page splits\nâ€¢ Measured as avg_fragmentation_in_percent\n\nInternal Fragmentation:\nâ€¢ Wasted space within pages\nâ€¢ Pages not fully utilized\nâ€¢ Affects storage efficiency\nâ€¢ Measured as avg_page_space_used_in_percent\n\nCauses:\nâ€¢ INSERT, UPDATE, DELETE operations\nâ€¢ Random data insertion\nâ€¢ Page splits\nâ€¢ Poor fill factor\nâ€¢ Growing tables\n\nEffects:\nâ€¢ Slower query performance\nâ€¢ More disk I/O\nâ€¢ Larger index size\nâ€¢ Inefficient caching\nâ€¢ Slower range scans\n\nSolutions:\n\nREBUILD:\nâ€¢ Drops and recreates index\nâ€¢ Removes all fragmentation\nâ€¢ Can be done online\nâ€¢ Acquires schema locks\nâ€¢ Resets statistics\n\nREORGANIZE:\nâ€¢ Defragments leaf level\nâ€¢ Always online operation\nâ€¢ Minimal locks\nâ€¢ Less resource intensive\nâ€¢ Updates statistics\n\nGuidelines:\nâ€¢ < 10% fragmentation - Ignore\nâ€¢ 10-30% - REORGANIZE\nâ€¢ > 30% - REBUILD",
      "explanation": "Index fragmentation scatters index pages reducing performance; fix with REBUILD for heavy fragmentation or REORGANIZE for moderate fragmentation.",
      "difficulty": "Medium"
    },
    {
      "id": 67,
      "question": "What is a cross-database query and what are its considerations?",
      "answer": "Cross-database queries access data from multiple databases within a single SQL statement, requiring special considerations.\n\nBasic Syntax:\nâ€¢ Three-part naming: database.schema.table\nâ€¢ Four-part naming: server.database.schema.table\nâ€¢ Linked servers for remote databases\nâ€¢ Database context switching\n\nTypes:\n\nSame Server:\nâ€¢ Direct three-part names\nâ€¢ Fast and efficient\nâ€¢ Minimal overhead\nâ€¢ Common in enterprise apps\n\nDifferent Servers:\nâ€¢ Linked servers required\nâ€¢ Network latency\nâ€¢ Distributed queries\nâ€¢ More complex\n\nConsiderations:\n\nPerformance:\nâ€¢ Network overhead\nâ€¢ Can't optimize across databases easily\nâ€¢ No cross-database statistics\nâ€¢ Transaction coordination overhead\n\nSecurity:\nâ€¢ Permissions on both databases\nâ€¢ Cross-database ownership chaining\nâ€¢ Trust relationships\nâ€¢ Credential mapping\n\nTransactions:\nâ€¢ Distributed transactions\nâ€¢ Two-phase commit\nâ€¢ MSDTC coordination\nâ€¢ Higher failure risk\n\nMaintenance:\nâ€¢ Dependencies harder to track\nâ€¢ Schema changes affect multiple databases\nâ€¢ Backup/restore coordination\nâ€¢ Testing complexity\n\nAlternatives:\nâ€¢ Database consolidation\nâ€¢ Data replication\nâ€¢ ETL processes\nâ€¢ Microservices APIs",
      "explanation": "Cross-database queries span multiple databases using multi-part names, requiring consideration of performance, security, transactions, and maintenance.",
      "difficulty": "Medium"
    },
    {
      "id": 68,
      "question": "What are database isolation problems: dirty reads, non-repeatable reads, and phantom reads?",
      "answer": "Isolation problems occur when transactions interact in undesirable ways due to concurrent execution.\n\nDirty Read:\nâ€¢ Reading uncommitted data\nâ€¢ Transaction reads changes from uncommitted transaction\nâ€¢ If rolled back, data was never valid\nâ€¢ Most severe problem\nâ€¢ Prevented by READ COMMITTED or higher\n\nNon-Repeatable Read:\nâ€¢ Same query returns different results\nâ€¢ Another transaction modifies data between reads\nâ€¢ Data changes within transaction\nâ€¢ Less severe than dirty read\nâ€¢ Prevented by REPEATABLE READ or higher\n\nPhantom Read:\nâ€¢ New rows appear in result set\nâ€¢ Another transaction inserts matching rows\nâ€¢ Row count changes\nâ€¢ Different from non-repeatable (new vs modified)\nâ€¢ Prevented by SERIALIZABLE\n\nRelationship to Isolation Levels:\nâ€¢ READ UNCOMMITTED: Allows all three\nâ€¢ READ COMMITTED: Prevents dirty reads\nâ€¢ REPEATABLE READ: Prevents dirty, non-repeatable\nâ€¢ SERIALIZABLE: Prevents all three\n\nReal-World Impact:\nâ€¢ Dirty: Reporting on invalid data\nâ€¢ Non-repeatable: Calculations incorrect\nâ€¢ Phantom: Counts and aggregates wrong\n\nSolutions:\nâ€¢ Higher isolation levels\nâ€¢ Explicit locking\nâ€¢ Optimistic concurrency\nâ€¢ Application logic\nâ€¢ Transaction retry logic",
      "explanation": "Isolation problems include dirty reads (uncommitted data), non-repeatable reads (data changes), and phantom reads (new rows), prevented by increasing isolation levels.",
      "difficulty": "Hard"
    },
    {
      "id": 69,
      "question": "What is a database view materialization strategy?",
      "answer": "View materialization determines how and when view data is computed and stored, affecting performance and freshness.\n\nStrategies:\n\nVirtual Views (Standard):\nâ€¢ Query executed every access\nâ€¢ Always current data\nâ€¢ No storage overhead\nâ€¢ Slower for complex queries\nâ€¢ Most common type\n\nMaterialized Views:\nâ€¢ Results stored physically\nâ€¢ Fast queries\nâ€¢ Requires refresh\nâ€¢ Storage overhead\nâ€¢ Good for reporting\n\nRefresh Methods:\n\nComplete Refresh:\nâ€¢ Rebuild entire view\nâ€¢ Consistent but slow\nâ€¢ Simple to implement\nâ€¢ Good for small views\n\nIncremental/Fast Refresh:\nâ€¢ Update only changes\nâ€¢ Faster than complete\nâ€¢ Requires materialized view logs\nâ€¢ Complex to maintain\n\nOn Demand:\nâ€¢ Manual refresh\nâ€¢ Full control\nâ€¢ Can schedule\nâ€¢ May be stale\n\nOn Commit:\nâ€¢ Auto-refresh after changes\nâ€¢ Always current\nâ€¢ Performance impact\nâ€¢ Good for critical data\n\nDeferred:\nâ€¢ Refresh at specific time\nâ€¢ Batch processing\nâ€¢ Less overhead\nâ€¢ Scheduled maintenance\n\nTrade-offs:\nâ€¢ Performance vs Freshness\nâ€¢ Storage vs Computation\nâ€¢ Complexity vs Simplicity\n\nBest Practices:\nâ€¢ Use for expensive queries\nâ€¢ Refresh during off-hours\nâ€¢ Index materialized views\nâ€¢ Monitor staleness\nâ€¢ Document refresh strategy",
      "explanation": "View materialization strategies balance query performance and data freshness through virtual (always fresh) or materialized (fast but stale) approaches with various refresh methods.",
      "difficulty": "Hard"
    },
    {
      "id": 70,
      "question": "What is database compression and when should it be used?",
      "answer": "Database compression reduces storage space by encoding data more efficiently, with trade-offs between space and performance.\n\nTypes of Compression:\n\nRow Compression:\nâ€¢ Variable-length storage\nâ€¢ Removes trailing zeros/spaces\nâ€¢ Minimal CPU overhead\nâ€¢ 20-40% space savings\nâ€¢ Good starting point\n\nPage Compression:\nâ€¢ Includes row compression\nâ€¢ Column prefix compression\nâ€¢ Page dictionary compression\nâ€¢ Higher CPU usage\nâ€¢ 40-80% space savings\n\nColumnstore Compression:\nâ€¢ Column-oriented storage\nâ€¢ Extreme compression ratios\nâ€¢ 10x or more savings\nâ€¢ Best for analytics\nâ€¢ Read-optimized\n\nBackup Compression:\nâ€¢ Compresses backup files\nâ€¢ Faster backups\nâ€¢ Less storage\nâ€¢ Minimal restore overhead\n\nBenefits:\nâ€¢ Reduced storage costs\nâ€¢ Lower I/O requirements\nâ€¢ Faster data scans\nâ€¢ More data in memory\nâ€¢ Reduced backup size\n\nDrawbacks:\nâ€¢ Increased CPU usage\nâ€¢ Compression overhead\nâ€¢ Some operations slower\nâ€¢ Not all data compresses well\n\nWhen to Use:\nâ€¢ Large historical tables\nâ€¢ Read-heavy workloads\nâ€¢ Data warehouses\nâ€¢ Archival data\nâ€¢ Storage constrained\n\nWhen to Avoid:\nâ€¢ Write-intensive workloads\nâ€¢ Small tables\nâ€¢ Already compressed data\nâ€¢ CPU constrained systems\nâ€¢ Frequently updated data",
      "explanation": "Database compression reduces storage using row, page, or columnstore methods, trading CPU overhead for space savings, best for large read-heavy tables.",
      "difficulty": "Medium"
    },
    {
      "id": 71,
      "question": "What are database statistics and why are they important?",
      "answer": "Database statistics contain information about data distribution that the query optimizer uses to create efficient execution plans.\n\nWhat Statistics Track:\nâ€¢ Number of rows in table\nâ€¢ Data distribution in columns\nâ€¢ Cardinality estimates\nâ€¢ Density of values\nâ€¢ Histogram of data ranges\nâ€¢ Index selectivity\nâ€¢ NULL value counts\n\nTypes:\n\nColumn Statistics:\nâ€¢ Distribution of values\nâ€¢ Unique value count\nâ€¢ Most/least common values\nâ€¢ Created automatically\n\nIndex Statistics:\nâ€¢ Index key distribution\nâ€¢ Index depth\nâ€¢ Page counts\nâ€¢ Fragmentation levels\n\nHow Optimizer Uses:\nâ€¢ Chooses index vs table scan\nâ€¢ Selects join order\nâ€¢ Estimates row counts\nâ€¢ Determines join methods\nâ€¢ Allocates memory\n\nOutdated Statistics:\nâ€¢ Poor execution plans\nâ€¢ Inefficient queries\nâ€¢ Wrong index choices\nâ€¢ Performance degradation\n\nMaintenance:\nâ€¢ Auto-update statistics\nâ€¢ Manual updates after bulk loads\nâ€¢ Update on percentage changes\nâ€¢ Sampling rates\nâ€¢ Full scan vs sample\n\nBest Practices:\nâ€¢ Enable auto-create statistics\nâ€¢ Enable auto-update statistics\nâ€¢ Update after large data changes\nâ€¢ Use full scan for critical tables\nâ€¢ Monitor statistics age\nâ€¢ Include in maintenance plans",
      "explanation": "Database statistics track data distribution enabling the query optimizer to make informed decisions about execution plans, requiring regular updates for accuracy.",
      "difficulty": "Medium"
    },
    {
      "id": 72,
      "question": "What is a database deadlock victim selection process?",
      "answer": "When a deadlock occurs, the database must choose which transaction to roll back (the deadlock victim) to break the cycle.\n\nSelection Criteria:\n\nTransaction Priority:\nâ€¢ DEADLOCK_PRIORITY setting\nâ€¢ Low priority chosen first\nâ€¢ Range: -10 (lowest) to 10 (highest)\nâ€¢ Default: 0 (normal)\n\nTransaction Cost:\nâ€¢ Work already done\nâ€¢ Resource usage\nâ€¢ Transaction duration\nâ€¢ Least costly transaction chosen\n\nLock Count:\nâ€¢ Number of locks held\nâ€¢ Easier to rollback fewer locks\n\nNested Trigger Level:\nâ€¢ Transaction with fewer triggers\nâ€¢ Simpler rollback\n\nSession Settings:\nâ€¢ Application name\nâ€¢ Login name\nâ€¢ Connection time\n\nPrevention Strategies:\nâ€¢ Consistent resource ordering\nâ€¢ Short transactions\nâ€¢ Appropriate isolation levels\nâ€¢ Index optimization\nâ€¢ Lock hints\nâ€¢ Retry logic\n\nDetection:\nâ€¢ Lock monitor thread\nâ€¢ Waits-for graph\nâ€¢ Cycle detection\nâ€¢ Automatic resolution\n\nRecovery:\nâ€¢ Victim receives error 1205\nâ€¢ Automatic rollback\nâ€¢ Application catches error\nâ€¢ Retry transaction\nâ€¢ Log for analysis\n\nBest Practices:\nâ€¢ Handle deadlock errors gracefully\nâ€¢ Implement retry logic\nâ€¢ Log deadlock occurrences\nâ€¢ Analyze deadlock graphs\nâ€¢ Optimize queries\nâ€¢ Use DEADLOCK_PRIORITY for critical transactions",
      "explanation": "Deadlock victim selection chooses the least expensive transaction to rollback based on priority, cost, and complexity to break the deadlock cycle.",
      "difficulty": "Hard"
    },
    {
      "id": 73,
      "question": "What is database connection string and what are its components?",
      "answer": "A connection string contains configuration information needed to establish a database connection from an application.\n\nCommon Components:\n\nServer/Host:\nâ€¢ Database server address\nâ€¢ IP address or hostname\nâ€¢ Port number (optional)\nâ€¢ Named instances\n\nDatabase:\nâ€¢ Target database name\nâ€¢ Initial catalog\nâ€¢ Default database\n\nAuthentication:\nâ€¢ Username/password\nâ€¢ Integrated/Windows authentication\nâ€¢ Certificate-based\nâ€¢ OAuth/token\n\nSecurity:\nâ€¢ Encryption (SSL/TLS)\nâ€¢ Trust server certificate\nâ€¢ Encrypted connections\n\nConnection Pooling:\nâ€¢ Pool size (min/max)\nâ€¢ Connection lifetime\nâ€¢ Pool timeout\nâ€¢ Enable pooling\n\nTimeout Settings:\nâ€¢ Connection timeout\nâ€¢ Command timeout\nâ€¢ Login timeout\n\nApplication Settings:\nâ€¢ Application name\nâ€¢ Workstation ID\nâ€¢ Client library version\n\nBest Practices:\nâ€¢ Store securely (not in code)\nâ€¢ Use environment variables\nâ€¢ Encrypt sensitive data\nâ€¢ Use connection pooling\nâ€¢ Set appropriate timeouts\nâ€¢ Validate connection strings\nâ€¢ Use least privilege accounts\nâ€¢ Don't log connection strings\nâ€¢ Rotate credentials regularly\nâ€¢ Use managed identities when possible",
      "explanation": "Connection strings define database connection parameters including server, credentials, security, pooling, and timeouts, requiring secure storage and configuration.",
      "difficulty": "Easy"
    },
    {
      "id": 74,
      "question": "What are database sequence objects and how do they differ from identity columns?",
      "answer": "Sequences are database objects that generate sequential numeric values, similar to identity columns but more flexible.\n\nSequence Features:\nâ€¢ Independent database object\nâ€¢ Not tied to specific table\nâ€¢ Can be shared across tables\nâ€¢ Cached for performance\nâ€¢ Cycle or no cycle option\nâ€¢ Custom start, increment, min, max\n\nIdentity Column:\nâ€¢ Table column property\nâ€¢ Tied to specific column\nâ€¢ One per table\nâ€¢ Automatic assignment\nâ€¢ Less flexible\nâ€¢ Simpler to use\n\nAdvantages of Sequences:\nâ€¢ Reusable across tables\nâ€¢ Generate value before INSERT\nâ€¢ More control over values\nâ€¢ Can reset/alter\nâ€¢ Coordinate IDs across tables\nâ€¢ Pre-allocate ranges\n\nAdvantages of Identity:\nâ€¢ Simpler syntax\nâ€¢ Automatic\nâ€¢ Guaranteed unique in table\nâ€¢ No gaps from aborted transactions\nâ€¢ Integrated with table\n\nGaps in Values:\nâ€¢ Sequences: Gaps from cache, rollback\nâ€¢ Identity: Gaps from DELETE, rollback\nâ€¢ Both can have gaps\nâ€¢ Not guaranteed continuous\n\nPerformance:\nâ€¢ Sequences: Cache improves performance\nâ€¢ Identity: Slight overhead\nâ€¢ Both very fast\n\nUse Cases:\nâ€¢ Sequences: Multiple tables, pre-generation, custom numbering\nâ€¢ Identity: Simple auto-increment, single table",
      "explanation": "Sequences are independent objects generating sequential numbers sharable across tables, while identity columns are simpler table-specific auto-increment properties.",
      "difficulty": "Medium"
    },
    {
      "id": 75,
      "question": "What is database collation and how does it affect queries?",
      "answer": "Collation defines rules for character comparison, sorting, and case sensitivity, affecting string operations and query results.\n\nCollation Determines:\nâ€¢ Character set encoding\nâ€¢ Sorting order\nâ€¢ Case sensitivity (CS/CI)\nâ€¢ Accent sensitivity (AS/AI)\nâ€¢ Kana sensitivity\nâ€¢ Width sensitivity\n\nLevels:\n\nServer Collation:\nâ€¢ Default for server\nâ€¢ System databases\nâ€¢ Template for new databases\n\nDatabase Collation:\nâ€¢ Default for database\nâ€¢ Metadata\nâ€¢ Temporary tables\n\nColumn Collation:\nâ€¢ Specific column\nâ€¢ Override database default\nâ€¢ Most granular\n\nExpression Collation:\nâ€¢ Query-time override\nâ€¢ COLLATE clause\nâ€¢ Temporary change\n\nCase Sensitivity Examples:\nâ€¢ CI (Case Insensitive): 'John' = 'JOHN'\nâ€¢ CS (Case Sensitive): 'John' != 'JOHN'\n\nAccent Sensitivity:\nâ€¢ AI (Accent Insensitive): 'cafÃ©' = 'cafe'\nâ€¢ AS (Accent Sensitive): 'cafÃ©' != 'cafe'\n\nImpact on Queries:\nâ€¢ WHERE clause matching\nâ€¢ ORDER BY sorting\nâ€¢ JOIN conditions\nâ€¢ DISTINCT results\nâ€¢ Index usage\nâ€¢ String comparisons\n\nPerformance:\nâ€¢ Case-insensitive may be slower\nâ€¢ Index must match collation\nâ€¢ Collation conflicts expensive\n\nBest Practices:\nâ€¢ Choose at database creation\nâ€¢ Consistent across database\nâ€¢ Document collation choices\nâ€¢ Test queries with collation\nâ€¢ Consider international requirements",
      "explanation": "Collation defines character comparison rules including case and accent sensitivity, affecting sorting, matching, and index usage in string operations.",
      "difficulty": "Medium"
    },
    {
      "id": 76,
      "question": "What is query plan caching and how does it work?",
      "answer": "Query plan caching stores compiled execution plans to avoid recompilation overhead, significantly improving performance.\n\nHow It Works:\nâ€¢ Query parsed and compiled\nâ€¢ Execution plan created\nâ€¢ Plan stored in plan cache\nâ€¢ Subsequent identical queries reuse plan\nâ€¢ Plans evicted based on memory pressure\n\nPlan Cache Structure:\nâ€¢ Plan hash\nâ€¢ Query hash\nâ€¢ Execution statistics\nâ€¢ Parameter metadata\nâ€¢ Compilation cost\nâ€¢ Last execution time\n\nPlan Reuse Conditions:\nâ€¢ Identical query text\nâ€¢ Same database context\nâ€¢ Same schema version\nâ€¢ Compatible SET options\nâ€¢ Same object IDs\n\nParameter Sniffing:\nâ€¢ Plans optimized for first parameters\nâ€¢ May be suboptimal for different parameters\nâ€¢ Can cause performance issues\nâ€¢ RECOMPILE hint forces new plan\nâ€¢ OPTIMIZE FOR hint provides guidance\n\nCache Pollution:\nâ€¢ Ad-hoc queries fill cache\nâ€¢ Plans not reused\nâ€¢ Memory wasted\nâ€¢ Use parameterized queries\n\nCache Invalidation:\nâ€¢ Schema changes\nâ€¢ Statistics updates\nâ€¢ Index changes\nâ€¢ Manual clearing\nâ€¢ Memory pressure\n\nBest Practices:\nâ€¢ Use prepared statements\nâ€¢ Parameterize queries\nâ€¢ Avoid dynamic SQL when possible\nâ€¢ Monitor plan cache\nâ€¢ Clear cache after major changes\nâ€¢ Use forced parameterization\nâ€¢ Set appropriate memory limits",
      "explanation": "Query plan caching stores compiled execution plans for reuse, avoiding expensive recompilation, but requires identical queries and proper parameterization.",
      "difficulty": "Hard"
    },
    {
      "id": 77,
      "question": "What are table hints in SQL and when should they be used?",
      "answer": "Table hints override query optimizer decisions, forcing specific behaviors like index usage or locking strategies.\n\nCommon Hints:\n\nIndex Hints:\nâ€¢ INDEX(index_name) - Use specific index\nâ€¢ FORCESEEK - Force index seek\nâ€¢ FORCESCAN - Force index/table scan\n\nLocking Hints:\nâ€¢ NOLOCK (READUNCOMMITTED) - No locks, dirty reads\nâ€¢ HOLDLOCK (SERIALIZABLE) - Hold locks until transaction end\nâ€¢ UPDLOCK - Update lock\nâ€¢ TABLOCK - Table-level lock\nâ€¢ TABLOCKX - Exclusive table lock\nâ€¢ ROWLOCK - Row-level lock\nâ€¢ PAGLOCK - Page-level lock\n\nQuery Hints:\nâ€¢ MAXDOP - Max degree of parallelism\nâ€¢ RECOMPILE - Force recompilation\nâ€¢ FAST n - Optimize for first n rows\nâ€¢ MAXRECURSION - Max recursion depth\n\nJoin Hints:\nâ€¢ LOOP - Force nested loop join\nâ€¢ MERGE - Force merge join\nâ€¢ HASH - Force hash join\n\nWhen to Use:\nâ€¢ Optimizer makes poor choice\nâ€¢ Known better plan exists\nâ€¢ Temporary workaround\nâ€¢ Specific business requirements\nâ€¢ Testing scenarios\n\nWhen to Avoid:\nâ€¢ As first solution\nâ€¢ Without understanding why\nâ€¢ Permanent fixes (usually)\nâ€¢ Across changing data\nâ€¢ Without monitoring\n\nRisks:\nâ€¢ Locks in suboptimal plans\nâ€¢ Breaks with data changes\nâ€¢ Masks real issues\nâ€¢ Maintenance burden\nâ€¢ Hard to troubleshoot\n\nBetter Alternatives:\nâ€¢ Update statistics\nâ€¢ Add/modify indexes\nâ€¢ Rewrite query\nâ€¢ Change schema\nâ€¢ Adjust database settings",
      "explanation": "Table hints force specific query behaviors overriding the optimizer, useful for specific scenarios but risky as permanent solutions that can lock in poor performance.",
      "difficulty": "Hard"
    },
    {
      "id": 78,
      "question": "What is database change data capture (CDC)?",
      "answer": "CDC tracks and captures data changes (INSERT, UPDATE, DELETE) in source tables, enabling downstream systems to process only changes.\n\nHow CDC Works:\nâ€¢ Reads transaction log\nâ€¢ Captures changes asynchronously\nâ€¢ Stores in change tables\nâ€¢ Minimal impact on source\nâ€¢ Near real-time\n\nComponents:\n\nCapture Instance:\nâ€¢ Configuration for tracking\nâ€¢ One per source table\nâ€¢ Change table created\nâ€¢ Capture job\n\nChange Tables:\nâ€¢ Mirror source structure\nâ€¢ Additional CDC columns\nâ€¢ Operation type (I/U/D)\nâ€¢ LSN (Log Sequence Number)\nâ€¢ Change time\n\nCDC Jobs:\nâ€¢ Capture job - Reads log\nâ€¢ Cleanup job - Purges old data\nâ€¢ SQL Server Agent required\n\nUse Cases:\nâ€¢ Data warehouse ETL\nâ€¢ Real-time analytics\nâ€¢ Audit trails\nâ€¢ Data synchronization\nâ€¢ Event-driven architectures\nâ€¢ Microservices data replication\n\nBenefits:\nâ€¢ Low performance impact\nâ€¢ No trigger overhead\nâ€¢ Captures all changes\nâ€¢ Historical tracking\nâ€¢ No source modification\n\nLimitations:\nâ€¢ SQL Server Enterprise (before 2016)\nâ€¢ Requires additional storage\nâ€¢ Cleanup management needed\nâ€¢ Not for all table types\nâ€¢ DDL changes require reconfiguration\n\nVs Triggers:\nâ€¢ CDC: Asynchronous, log-based, lower impact\nâ€¢ Triggers: Synchronous, higher overhead, immediate\n\nVs Change Tracking:\nâ€¢ CDC: Full data, what changed\nâ€¢ Change Tracking: Only that change occurred",
      "explanation": "CDC captures data changes from transaction logs to change tables, enabling downstream processing of only modified data with minimal source impact.",
      "difficulty": "Hard"
    },
    {
      "id": 79,
      "question": "What are temporal tables in SQL?",
      "answer": "Temporal tables (system-versioned tables) automatically track the full history of data changes, enabling point-in-time queries and historical analysis.\n\nFeatures:\nâ€¢ Automatic history tracking\nâ€¢ No application changes needed\nâ€¢ Two tables: current and history\nâ€¢ System-managed\nâ€¢ Temporal query extensions\n\nComponents:\n\nCurrent Table:\nâ€¢ Contains current data\nâ€¢ Normal DML operations\nâ€¢ Period columns (start/end)\nâ€¢ System-versioned\n\nHistory Table:\nâ€¢ Stores all versions\nâ€¢ Read-only\nâ€¢ Same schema as current\nâ€¢ Compressed recommended\n\nPeriod Columns:\nâ€¢ ValidFrom (row start time)\nâ€¢ ValidTo (row end time)\nâ€¢ GENERATED ALWAYS\nâ€¢ DateTime2 type\nâ€¢ System-managed\n\nTemporal Queries:\nâ€¢ AS OF - Point in time\nâ€¢ FROM...TO - Time range\nâ€¢ BETWEEN...AND - Inclusive range\nâ€¢ CONTAINED IN - Fully within range\n\nUse Cases:\nâ€¢ Audit requirements\nâ€¢ Data forensics\nâ€¢ Slowly changing dimensions\nâ€¢ Compliance tracking\nâ€¢ Historical reporting\nâ€¢ Undo/recovery operations\n\nBenefits:\nâ€¢ Automatic tracking\nâ€¢ No trigger overhead\nâ€¢ Transparent to application\nâ€¢ Query historical data easily\nâ€¢ Built-in feature\n\nConsiderations:\nâ€¢ Additional storage\nâ€¢ History table grows\nâ€¢ Retention policies needed\nâ€¢ Impact on transactions\nâ€¢ Backup/restore complexity\n\nVs Manual History:\nâ€¢ Temporal: Automatic, consistent\nâ€¢ Manual: Custom, flexible, complex",
      "explanation": "Temporal tables automatically maintain full data history with system-managed versioning, enabling time-travel queries for audit, compliance, and analysis.",
      "difficulty": "Hard"
    },
    {
      "id": 80,
      "question": "What is database schema migration and what tools are used?",
      "answer": "Schema migration manages and tracks database schema changes across environments, ensuring consistent evolution and deployability.\n\nMigration Approaches:\n\nForward-Only:\nâ€¢ Each migration moves forward\nâ€¢ No rollback scripts\nâ€¢ Simple and safe\nâ€¢ Most common\n\nUp/Down:\nâ€¢ Migrations with rollback\nâ€¢ Can undo changes\nâ€¢ More complex\nâ€¢ Useful for development\n\nState-Based:\nâ€¢ Compare desired vs current\nâ€¢ Generate migration automatically\nâ€¢ Less control\nâ€¢ Easier to understand\n\nComponents:\n\nMigration Files:\nâ€¢ Versioned SQL scripts\nâ€¢ Sequential numbering\nâ€¢ Timestamp-based naming\nâ€¢ Descriptive names\n\nMigration Table:\nâ€¢ Tracks applied migrations\nâ€¢ Version history\nâ€¢ Applied timestamp\nâ€¢ Checksum validation\n\nPopular Tools:\n\nFlyway:\nâ€¢ Java-based\nâ€¢ SQL and Java migrations\nâ€¢ Command line and API\nâ€¢ Wide database support\n\nLiquibase:\nâ€¢ XML/YAML/JSON/SQL\nâ€¢ Database-independent\nâ€¢ Rollback support\nâ€¢ Complex changesets\n\nAlembic (Python):\nâ€¢ SQLAlchemy integration\nâ€¢ Auto-generation\nâ€¢ Branching support\n\nEntity Framework (. NET):\nâ€¢ Code-first migrations\nâ€¢ Automatic from models\nâ€¢ C# based\n\nBest Practices:\nâ€¢ Version control migrations\nâ€¢ One change per migration\nâ€¢ Test on copy first\nâ€¢ Backup before migration\nâ€¢ Idempotent scripts\nâ€¢ Forward-compatible changes\nâ€¢ Separate DDL and data\nâ€¢ Document breaking changes",
      "explanation": "Schema migration systematically manages database changes through versioned scripts tracked in migration tables, using tools like Flyway or Liquibase for consistency.",
      "difficulty": "Medium"
    },
    {
      "id": 81,
      "question": "What are database lock escalation and lock modes?",
      "answer": "Lock escalation converts many fine-grained locks into fewer coarse-grained locks to conserve memory, affecting concurrency and performance.\n\nLock Escalation:\nâ€¢ Automatic process\nâ€¢ Conserves memory\nâ€¢ Reduces lock overhead\nâ€¢ May reduce concurrency\nâ€¢ Threshold-based (typically 5000 locks)\n\nLock Hierarchy:\nâ€¢ Database\nâ€¢ Table\nâ€¢ Page\nâ€¢ Row\nâ€¢ Key\n\nEscalation Path:\nâ€¢ Row locks â†’ Page locks\nâ€¢ Page locks â†’ Table locks\nâ€¢ Skips intermediate levels\n\nLock Modes:\n\nShared (S):\nâ€¢ Read locks\nâ€¢ Multiple allowed\nâ€¢ Blocks exclusive\nâ€¢ SELECT statements\n\nExclusive (X):\nâ€¢ Write locks\nâ€¢ Single transaction\nâ€¢ Blocks all others\nâ€¢ UPDATE, INSERT, DELETE\n\nUpdate (U):\nâ€¢ Intent to modify\nâ€¢ Prevents deadlocks\nâ€¢ Converts to X when updating\nâ€¢ One per resource\n\nIntent Locks:\nâ€¢ IS, IX, IU\nâ€¢ Indicate intent for finer locks\nâ€¢ Improve locking efficiency\nâ€¢ Hierarchy maintenance\n\nSchema Locks:\nâ€¢ Sch-S - Schema stability\nâ€¢ Sch-M - Schema modification\nâ€¢ DDL operations\n\nBulk Update (BU):\nâ€¢ Bulk operations\nâ€¢ Minimally logged\nâ€¢ Special scenarios\n\nControlling Escalation:\nâ€¢ LOCK_ESCALATION table option\nâ€¢ INDEX hints\nâ€¢ Partition-level escalation\nâ€¢ Transaction optimization\n\nPreventing Issues:\nâ€¢ Keep transactions short\nâ€¢ Use appropriate isolation\nâ€¢ Optimize queries\nâ€¢ Add indexes\nâ€¢ Consider partitioning",
      "explanation": "Lock escalation converts fine-grained row locks to coarse table locks to save memory, using various lock modes (shared, exclusive, update) to balance concurrency and resource usage.",
      "difficulty": "Hard"
    },
    {
      "id": 82,
      "question": "What is database resource governor?",
      "answer": "Resource Governor limits SQL Server resource consumption (CPU, memory, I/O) for different workloads, ensuring fair resource allocation and preventing resource monopolization.\n\nComponents:\n\nResource Pools:\nâ€¢ Portion of resources\nâ€¢ MIN/MAX CPU percentage\nâ€¢ MIN/MAX memory percentage\nâ€¢ MIN/MAX IOPS\n\nWorkload Groups:\nâ€¢ Collection of sessions\nâ€¢ Associated with pool\nâ€¢ Request limits\nâ€¢ Execution timeout\nâ€¢ Memory grant limits\n\nClassifier Function:\nâ€¢ User-defined function\nâ€¢ Routes requests to groups\nâ€¢ Based on connection properties\nâ€¢ LOGIN, APP_NAME, HOST_NAME\nâ€¢ Database name, time of day\n\nUse Cases:\n\nMulti-Tenant:\nâ€¢ Isolate tenant resources\nâ€¢ Prevent noisy neighbors\nâ€¢ Fair resource sharing\n\nMixed Workloads:\nâ€¢ OLTP vs Reporting\nâ€¢ Production vs Development\nâ€¢ Interactive vs Batch\n\nResource Control:\nâ€¢ Limit runaway queries\nâ€¢ Protect critical operations\nâ€¢ Enforce SLAs\n\nBenefits:\nâ€¢ Predictable performance\nâ€¢ Resource isolation\nâ€¢ Better utilization\nâ€¢ Prevent resource starvation\nâ€¢ No application changes\n\nLimitations:\nâ€¢ SQL Server Enterprise only\nâ€¢ Can't limit disk space\nâ€¢ Configuration complexity\nâ€¢ Monitoring overhead\n\nBest Practices:\nâ€¢ Start with monitoring\nâ€¢ Conservative limits initially\nâ€¢ Regular review and adjustment\nâ€¢ Document classification logic\nâ€¢ Test before production\nâ€¢ Monitor resource pool usage",
      "explanation": "Resource Governor allocates and limits SQL Server resources (CPU, memory, I/O) across workloads using pools and groups, ensuring fair resource distribution and preventing monopolization.",
      "difficulty": "Hard"
    },
    {
      "id": 83,
      "question": "What are database query store and its benefits?",
      "answer": "Query Store captures query execution history and plans, providing performance insights and plan regression detection.\n\nWhat It Captures:\nâ€¢ Query text\nâ€¢ Execution plans\nâ€¢ Runtime statistics\nâ€¢ Execution count\nâ€¢ Duration\nâ€¢ CPU time\nâ€¢ Memory usage\nâ€¢ I/O statistics\n\nComponents:\n\nPlan Store:\nâ€¢ Execution plans\nâ€¢ Plan hash\nâ€¢ Query hash\nâ€¢ Plan variations\n\nRuntime Stats Store:\nâ€¢ Execution metrics\nâ€¢ Aggregated intervals\nâ€¢ Statistical summaries\n\nQuery Store Catalog:\nâ€¢ Query text\nâ€¢ Configuration\nâ€¢ Retention settings\n\nBenefits:\n\nPerformance Troubleshooting:\nâ€¢ Identify regressed queries\nâ€¢ Compare plan performance\nâ€¢ Historical analysis\nâ€¢ Root cause analysis\n\nPlan Management:\nâ€¢ Force specific plans\nâ€¢ Prevent plan regression\nâ€¢ A/B testing plans\nâ€¢ Stable performance\n\nMonitoring:\nâ€¢ Top resource consumers\nâ€¢ Query trends\nâ€¢ Workload patterns\nâ€¢ Performance baselines\n\nConfiguration:\nâ€¢ Operation mode (Read-Write, Read-Only, Off)\nâ€¢ Size limit\nâ€¢ Retention period\nâ€¢ Capture interval\nâ€¢ Statistics collection interval\n\nUse Cases:\nâ€¢ Upgrade testing\nâ€¢ Performance regression detection\nâ€¢ Query tuning\nâ€¢ Workload analysis\nâ€¢ Capacity planning\n\nBest Practices:\nâ€¢ Enable on all databases\nâ€¢ Set appropriate size\nâ€¢ Configure retention\nâ€¢ Regular monitoring\nâ€¢ Force plans cautiously\nâ€¢ Integrate with monitoring tools",
      "explanation": "Query Store tracks query execution plans and statistics, enabling performance regression detection, plan forcing, and historical analysis for troubleshooting and optimization.",
      "difficulty": "Medium"
    },
    {
      "id": 84,
      "question": "What is database write-ahead logging (WAL)?",
      "answer": "Write-Ahead Logging ensures data integrity by writing all changes to a log before modifying database pages, enabling recovery and maintaining ACID properties.\n\nWAL Protocol:\n\n1. Change written to log buffer\n2. Log buffer flushed to disk\n3. Data page modified in memory\n4. Eventually flushed to disk\n\nKey Principle:\nâ€¢ Log record on disk before data page\nâ€¢ Ensures recoverability\nâ€¢ Fundamental to ACID\n\nComponents:\n\nLog Buffer:\nâ€¢ In-memory log cache\nâ€¢ Fast writes\nâ€¢ Batch flushing\nâ€¢ Circular buffer\n\nLog Files:\nâ€¢ Sequential writes\nâ€¢ Transaction log\nâ€¢ Archived for recovery\nâ€¢ Replay for recovery\n\nCheckpoints:\nâ€¢ Flush dirty pages\nâ€¢ Reduce recovery time\nâ€¢ Create recovery point\nâ€¢ Periodic process\n\nBenefits:\n\nDurability:\nâ€¢ Committed changes persist\nâ€¢ Survive crashes\nâ€¢ No data loss\n\nPerformance:\nâ€¢ Sequential log writes\nâ€¢ Batch commits\nâ€¢ Deferred page writes\nâ€¢ Reduced random I/O\n\nRecovery:\nâ€¢ Replay log entries\nâ€¢ Redo committed\nâ€¢ Undo uncommitted\nâ€¢ Point-in-time recovery\n\nRecovery Process:\n\n1. Analysis - Identify transactions\n2. Redo - Replay committed\n3. Undo - Rollback uncommitted\n\nConfiguration:\n\nSync Modes:\nâ€¢ Synchronous - Wait for disk\nâ€¢ Asynchronous - Don't wait\nâ€¢ Group commit - Batch\n\nLog Settings:\nâ€¢ Log file size\nâ€¢ Number of files\nâ€¢ Archive location\nâ€¢ Sync frequency\n\nPerformance Impact:\nâ€¢ Log on fast storage (SSD)\nâ€¢ Separate from data files\nâ€¢ Appropriate log size\nâ€¢ Monitor log waits",
      "explanation": "WAL writes all changes to sequential log files before modifying data pages, ensuring durability and enabling recovery through log replay.",
      "difficulty": "Hard"
    },
    {
      "id": 85,
      "question": "What are database performance metrics to monitor?",
      "answer": "Key performance metrics provide visibility into database health, performance, and resource utilization for proactive management.\n\nQuery Performance:\n\nExecution Time:\nâ€¢ Query duration\nâ€¢ Average, median, P95, P99\nâ€¢ Slow query identification\nâ€¢ Response time trends\n\nQuery Throughput:\nâ€¢ Queries per second\nâ€¢ Transactions per second\nâ€¢ Batch operations\nâ€¢ Peak vs average\n\nResource Utilization:\n\nCPU Usage:\nâ€¢ Overall CPU percentage\nâ€¢ Per-query CPU time\nâ€¢ CPU wait times\nâ€¢ Parallel query usage\n\nMemory:\nâ€¢ Buffer cache hit ratio\nâ€¢ Page life expectancy\nâ€¢ Memory grants\nâ€¢ Out of memory errors\n\nDisk I/O:\nâ€¢ Read/write IOPS\nâ€¢ Disk latency\nâ€¢ Average I/O time\nâ€¢ Disk queue length\n\nLocking and Blocking:\n\nWait Statistics:\nâ€¢ Wait types\nâ€¢ Wait duration\nâ€¢ Signal wait time\nâ€¢ Resource waits\n\nDeadlocks:\nâ€¢ Deadlock count\nâ€¢ Deadlock victims\nâ€¢ Deadlock chains\nâ€¢ Frequency trends\n\nBlocking:\nâ€¢ Blocking sessions\nâ€¢ Blocked duration\nâ€¢ Lock escalations\nâ€¢ Lock timeouts\n\nConnection Metrics:\n\nConnections:\nâ€¢ Active connections\nâ€¢ Connection errors\nâ€¢ Connection timeouts\nâ€¢ Pool utilization\n\nDatabase Health:\n\nIndex Statistics:\nâ€¢ Fragmentation percentage\nâ€¢ Missing indexes\nâ€¢ Unused indexes\nâ€¢ Index usage stats\n\nStatistics:\nâ€¢ Statistics age\nâ€¢ Auto-update events\nâ€¢ Statistics accuracy\n\nBackup/Recovery:\nâ€¢ Last backup time\nâ€¢ Backup duration\nâ€¢ Restore test status\nâ€¢ RPO/RTO metrics\n\nMonitoring Tools:\nâ€¢ Native DMVs\nâ€¢ Extended events\nâ€¢ Query Store\nâ€¢ Third-party APM\nâ€¢ Custom dashboards",
      "explanation": "Monitor query performance (execution time, throughput), resource utilization (CPU, memory, I/O), locking/blocking, connections, and database health metrics for comprehensive database performance management.",
      "difficulty": "Medium"
    },
    {
      "id": 86,
      "question": "What is database query optimization process?",
      "answer": "Query optimization is the systematic process of analyzing and improving SQL query performance through various techniques and best practices.\n\nOptimization Phases:\n\n1. Query Analysis:\nâ€¢ Understand business requirement\nâ€¢ Review query structure\nâ€¢ Identify bottlenecks\nâ€¢ Analyze execution plan\nâ€¢ Measure baseline performance\n\n2. Data Analysis:\nâ€¢ Table sizes and row counts\nâ€¢ Data distribution\nâ€¢ Cardinality estimates\nâ€¢ Join relationships\nâ€¢ Index availability\n\n3. Optimization Techniques:\n\nIndexing:\nâ€¢ Create missing indexes\nâ€¢ Remove unused indexes\nâ€¢ Covering indexes\nâ€¢ Index consolidation\nâ€¢ Statistics updates\n\nQuery Rewriting:\nâ€¢ Eliminate unnecessary joins\nâ€¢ Optimize subqueries\nâ€¢ Use EXISTS vs IN appropriately\nâ€¢ Avoid functions on indexed columns\nâ€¢ Reduce result set early\n\nJoin Optimization:\nâ€¢ Optimal join order\nâ€¢ Appropriate join types\nâ€¢ Reduce Cartesian products\nâ€¢ Filter before joining\nâ€¢ Index join columns\n\nSet Operations:\nâ€¢ UNION ALL vs UNION\nâ€¢ Eliminate DISTINCT when possible\nâ€¢ Use window functions\nâ€¢ Batch operations\n\n4. Testing and Validation:\nâ€¢ Compare execution times\nâ€¢ Verify result accuracy\nâ€¢ Test with various data volumes\nâ€¢ Production-like environment\nâ€¢ Load testing\n\nCommon Anti-Patterns:\nâ€¢ SELECT *\nâ€¢ N+1 queries\nâ€¢ Missing WHERE clauses\nâ€¢ Implicit conversions\nâ€¢ OR in WHERE clause\nâ€¢ Functions on indexed columns\nâ€¢ Correlated subqueries\nâ€¢ Large IN lists\n\nOptimization Checklist:\nâ€¢ Indexes on foreign keys\nâ€¢ Statistics up-to-date\nâ€¢ Appropriate data types\nâ€¢ Normalized schema\nâ€¢ Query hints as last resort\nâ€¢ Connection pooling\nâ€¢ Batch operations\n\nDocumentation:\nâ€¢ Performance baseline\nâ€¢ Optimization changes\nâ€¢ Before/after metrics\nâ€¢ Execution plans\nâ€¢ Business justification",
      "explanation": "Query optimization involves systematic analysis of execution plans, indexing strategies, query rewriting, and testing to improve performance through multiple techniques.",
      "difficulty": "Hard"
    },
    {
      "id": 87,
      "question": "What are database design patterns and anti-patterns?",
      "answer": "Database design patterns are proven solutions to common design problems, while anti-patterns are common mistakes that should be avoided.\n\nCommon Patterns:\n\nStar Schema:\nâ€¢ Fact table with dimension tables\nâ€¢ Data warehousing\nâ€¢ Denormalized for performance\nâ€¢ Simple queries\n\nSnowflake Schema:\nâ€¢ Normalized dimensions\nâ€¢ Reduced redundancy\nâ€¢ More complex joins\nâ€¢ Better for updates\n\nSoftly Changing Dimensions:\nâ€¢ Track historical changes\nâ€¢ Type 1: Overwrite\nâ€¢ Type 2: Add row with version\nâ€¢ Type 3: Add column\n\nTable Partitioning:\nâ€¢ Divide large tables\nâ€¢ Improve query performance\nâ€¢ Easier maintenance\nâ€¢ Parallel operations\n\nAudit Trail Pattern:\nâ€¢ Separate audit tables\nâ€¢ Triggers for tracking\nâ€¢ Immutable logs\nâ€¢ Temporal tables\n\nCommon Anti-Patterns:\n\nEAV (Entity-Attribute-Value):\nâ€¢ Generic flexible structure\nâ€¢ Difficult to query\nâ€¢ Poor performance\nâ€¢ Type safety issues\n\nPolymorphic Associations:\nâ€¢ Single foreign key to multiple tables\nâ€¢ Referential integrity issues\nâ€¢ Complex queries\nâ€¢ Maintenance nightmare\n\nKeyless Tables:\nâ€¢ No primary key\nâ€¢ Duplicate prevention difficult\nâ€¢ Poor relationships\nâ€¢ Indexing issues\n\nCatch-All Columns:\nâ€¢ TEXT/VARCHAR(MAX) for everything\nâ€¢ No data validation\nâ€¢ Storage waste\nâ€¢ Query inefficiency\n\nFear of NULL:\nâ€¢ Using magic values (0, -1, '')\nâ€¢ Semantic confusion\nâ€¢ Complicates queries\nâ€¢ Data integrity issues\n\nBest Practice Patterns:\n\nSurrogate Keys:\nâ€¢ Auto-increment IDs\nâ€¢ Independence from business logic\nâ€¢ Stability\nâ€¢ Performance\n\nNatural Keys:\nâ€¢ Business meaningful\nâ€¢ Data validation\nâ€¢ Uniqueness\nâ€¢ Consider changeability\n\nJunction Tables:\nâ€¢ Many-to-many relationships\nâ€¢ Clean design\nâ€¢ Flexible\nâ€¢ Additional attributes\n\nArchitecture Patterns:\n\nCQRS:\nâ€¢ Separate read/write models\nâ€¢ Optimized independently\nâ€¢ Scalability\nâ€¢ Complexity trade-off\n\nEvent Sourcing:\nâ€¢ Store events not state\nâ€¢ Complete history\nâ€¢ Audit trail\nâ€¢ Replay capability",
      "explanation": "Database design patterns like star schema and junction tables solve common problems effectively, while anti-patterns like EAV and polymorphic associations create maintenance and performance issues.",
      "difficulty": "Hard"
    },
    {
      "id": 88,
      "question": "What is database capacity planning?",
      "answer": "Capacity planning ensures databases have sufficient resources to meet current and future performance and availability requirements.\n\nPlanning Dimensions:\n\nStorage Capacity:\nâ€¢ Current database size\nâ€¢ Growth rate\nâ€¢ Retention requirements\nâ€¢ Backup storage\nâ€¢ Index overhead\nâ€¢ Temp space requirements\n\nCompute Capacity:\nâ€¢ CPU utilization trends\nâ€¢ Peak vs average usage\nâ€¢ Query complexity growth\nâ€¢ Concurrent user growth\nâ€¢ Batch processing needs\n\nMemory Capacity:\nâ€¢ Buffer pool usage\nâ€¢ Query memory grants\nâ€¢ Connection memory\nâ€¢ Cached plans\nâ€¢ Temp objects\n\nI/O Capacity:\nâ€¢ IOPS requirements\nâ€¢ Throughput needs\nâ€¢ Latency targets\nâ€¢ Read vs write ratio\nâ€¢ Storage technology\n\nPlanning Process:\n\n1. Baseline Current State:\nâ€¢ Resource utilization\nâ€¢ Performance metrics\nâ€¢ Workload patterns\nâ€¢ Peak loads\nâ€¢ Bottlenecks\n\n2. Analyze Trends:\nâ€¢ Historical growth\nâ€¢ Seasonal patterns\nâ€¢ Business projections\nâ€¢ Feature additions\nâ€¢ User growth\n\n3. Project Requirements:\nâ€¢ Future capacity needs\nâ€¢ Timeline\nâ€¢ Growth scenarios\nâ€¢ Performance targets\nâ€¢ Budget constraints\n\n4. Design Solutions:\nâ€¢ Vertical scaling\nâ€¢ Horizontal scaling\nâ€¢ Architecture changes\nâ€¢ Technology upgrades\nâ€¢ Cloud migration\n\n5. Implementation Plan:\nâ€¢ Phased approach\nâ€¢ Testing strategy\nâ€¢ Rollback plans\nâ€¢ Monitoring\nâ€¢ Validation\n\nCapacity Metrics:\n\nUtilization Targets:\nâ€¢ CPU: 60-70% average\nâ€¢ Memory: 70-80% usage\nâ€¢ Disk: 80% capacity\nâ€¢ IOPS: 70% capacity\n\nGrowth Calculations:\nâ€¢ Linear projection\nâ€¢ Exponential growth\nâ€¢ Seasonal adjustment\nâ€¢ Safety margins (20-30%)\n\nCost Optimization:\nâ€¢ Right-sizing\nâ€¢ Reserved capacity\nâ€¢ Auto-scaling\nâ€¢ Archive old data\nâ€¢ Compression\nâ€¢ Efficient indexing\n\nMonitoring:\nâ€¢ Real-time metrics\nâ€¢ Trend analysis\nâ€¢ Alerting thresholds\nâ€¢ Capacity dashboards\nâ€¢ Regular reviews",
      "explanation": "Capacity planning analyzes current resource usage, growth trends, and future requirements to ensure adequate database storage, compute, memory, and I/O capacity.",
      "difficulty": "Medium"
    },
    {
      "id": 89,
      "question": "What are database testing strategies?",
      "answer": "Database testing validates functionality, performance, security, and data integrity through various testing approaches and techniques.\n\nTesting Types:\n\nUnit Testing:\nâ€¢ Stored procedures\nâ€¢ Functions\nâ€¢ Triggers\nâ€¢ Individual queries\nâ€¢ Test frameworks (tSQLt, pgTAP)\n\nIntegration Testing:\nâ€¢ Application-database interaction\nâ€¢ Multiple components\nâ€¢ Data flow\nâ€¢ Transaction handling\n\nPerformance Testing:\nâ€¢ Load testing\nâ€¢ Stress testing\nâ€¢ Query performance\nâ€¢ Scalability\nâ€¢ Response times\n\nData Validation:\nâ€¢ Data accuracy\nâ€¢ Referential integrity\nâ€¢ Constraint enforcement\nâ€¢ Data migration\nâ€¢ ETL processes\n\nSecurity Testing:\nâ€¢ Access controls\nâ€¢ SQL injection prevention\nâ€¢ Encryption validation\nâ€¢ Privilege testing\nâ€¢ Audit logging\n\nTest Data Strategies:\n\nProduction Copy:\nâ€¢ Realistic data\nâ€¢ Anonymize sensitive data\nâ€¢ Size management\nâ€¢ Refresh strategy\n\nSynthetic Data:\nâ€¢ Generated data\nâ€¢ Privacy compliant\nâ€¢ Scalable\nâ€¢ Consistent patterns\n\nMinimal Dataset:\nâ€¢ Small subset\nâ€¢ Edge cases\nâ€¢ Fast tests\nâ€¢ CI/CD friendly\n\nTest Automation:\n\nCI/CD Integration:\nâ€¢ Automated test execution\nâ€¢ Schema migration testing\nâ€¢ Regression testing\nâ€¢ Pre-deployment validation\n\nTools:\nâ€¢ Database unit test frameworks\nâ€¢ Load testing tools\nâ€¢ Schema comparison\nâ€¢ Data generation\n\nBest Practices:\n\nTest Environment:\nâ€¢ Isolated from production\nâ€¢ Production-like configuration\nâ€¢ Automated provisioning\nâ€¢ Version controlled\n\nTest Coverage:\nâ€¢ Critical paths\nâ€¢ Edge cases\nâ€¢ Error scenarios\nâ€¢ Rollback scenarios\n\nDocumentation:\nâ€¢ Test cases\nâ€¢ Expected results\nâ€¢ Test data\nâ€¢ Environment setup\n\nPerformance Baseline:\nâ€¢ Measure current performance\nâ€¢ Compare after changes\nâ€¢ Track trends\nâ€¢ Alert on regression",
      "explanation": "Database testing includes unit, integration, performance, and security testing using various data strategies and automation to ensure functionality, performance, and integrity.",
      "difficulty": "Medium"
    },
    {
      "id": 90,
      "question": "What is database disaster recovery planning?",
      "answer": "Disaster recovery planning prepares for and enables recovery from catastrophic events, ensuring business continuity and minimal data loss.\n\nKey Metrics:\n\nRTO (Recovery Time Objective):\nâ€¢ Maximum acceptable downtime\nâ€¢ Time to restore service\nâ€¢ Impacts solution design\nâ€¢ Business requirement\n\nRPO (Recovery Point Objective):\nâ€¢ Maximum acceptable data loss\nâ€¢ Point-in-time to recover to\nâ€¢ Determines backup frequency\nâ€¢ Business requirement\n\nDR Strategies:\n\nBackup and Restore:\nâ€¢ Full, differential, log backups\nâ€¢ Slowest RTO\nâ€¢ Lowest cost\nâ€¢ Off-site storage\nâ€¢ Regular testing\n\nLog Shipping:\nâ€¢ Transaction log backups\nâ€¢ Restore to standby\nâ€¢ Minutes to hours RTO\nâ€¢ Moderate cost\nâ€¢ Simple setup\n\nDatabase Mirroring:\nâ€¢ Synchronous/asynchronous\nâ€¢ Automatic failover\nâ€¢ Seconds to minutes RTO\nâ€¢ Higher cost\nâ€¢ SQL Server specific\n\nReplication:\nâ€¢ Continuous data sync\nâ€¢ Multiple copies\nâ€¢ Fast recovery\nâ€¢ Can be read-enabled\nâ€¢ Complex setup\n\nAlways On AG:\nâ€¢ Multiple replicas\nâ€¢ Automatic failover\nâ€¢ Seconds RTO\nâ€¢ Read-scale replicas\nâ€¢ Enterprise feature\n\nCloud Backup:\nâ€¢ Geographic distribution\nâ€¢ Automated\nâ€¢ Scalable\nâ€¢ Pay-per-use\nâ€¢ Network dependency\n\nDR Plan Components:\n\nInventory:\nâ€¢ All databases\nâ€¢ Dependencies\nâ€¢ Criticality\nâ€¢ Current state\n\nProcedures:\nâ€¢ Step-by-step recovery\nâ€¢ Contact information\nâ€¢ Escalation paths\nâ€¢ Communication plan\n\nTesting:\nâ€¢ Regular DR drills\nâ€¢ Scheduled tests\nâ€¢ Document results\nâ€¢ Update procedures\nâ€¢ Verify RTO/RPO\n\nMonitoring:\nâ€¢ Backup success\nâ€¢ Replication lag\nâ€¢ Storage capacity\nâ€¢ Alert configuration\n\nDocumentation:\nâ€¢ Architecture diagrams\nâ€¢ Recovery procedures\nâ€¢ Configuration details\nâ€¢ Test results\nâ€¢ Lessons learned\n\nBest Practices:\nâ€¢ 3-2-1 backup rule\nâ€¢ Geographic separation\nâ€¢ Automate processes\nâ€¢ Regular testing\nâ€¢ Document everything\nâ€¢ Train staff\nâ€¢ Monitor continuously\nâ€¢ Review annually",
      "explanation": "Disaster recovery planning defines RTO/RPO targets and implements strategies like backup, replication, or mirroring to ensure rapid recovery from catastrophic events.",
      "difficulty": "Hard"
    },
    {
      "id": 91,
      "question": "What are database monitoring best practices?",
      "answer": "Effective database monitoring proactively identifies issues, ensures optimal performance, and prevents outages through comprehensive observation and alerting.\n\nMonitoring Layers:\n\nInfrastructure:\nâ€¢ CPU utilization\nâ€¢ Memory usage\nâ€¢ Disk space\nâ€¢ Disk I/O\nâ€¢ Network bandwidth\n\nDatabase Engine:\nâ€¢ Connection count\nâ€¢ Active sessions\nâ€¢ Lock waits\nâ€¢ Buffer cache hit ratio\nâ€¢ Transaction log usage\n\nQuery Performance:\nâ€¢ Execution time\nâ€¢ Query throughput\nâ€¢ Slow queries\nâ€¢ Execution plans\nâ€¢ Resource consumption\n\nApplication:\nâ€¢ Response times\nâ€¢ Error rates\nâ€¢ User experience\nâ€¢ Transaction success rate\n\nKey Metrics:\n\nHealth Indicators:\nâ€¢ Availability uptime\nâ€¢ Failed login attempts\nâ€¢ Deadlock frequency\nâ€¢ Index fragmentation\nâ€¢ Statistics freshness\n\nPerformance Indicators:\nâ€¢ Average query time\nâ€¢ Queries per second\nâ€¢ Cache hit ratios\nâ€¢ Wait statistics\nâ€¢ Blocking sessions\n\nCapacity Indicators:\nâ€¢ Storage growth rate\nâ€¢ Connection pool usage\nâ€¢ Memory pressure\nâ€¢ Temp space usage\n\nMonitoring Tools:\n\nNative Tools:\nâ€¢ DMVs (Dynamic Management Views)\nâ€¢ Extended Events\nâ€¢ Query Store\nâ€¢ Performance Monitor\nâ€¢ Database Mail alerts\n\nThird-Party APM:\nâ€¢ Datadog\nâ€¢ New Relic\nâ€¢ Dynatrace\nâ€¢ AppDynamics\nâ€¢ SolarWinds\n\nOpen Source:\nâ€¢ Prometheus + Grafana\nâ€¢ Zabbix\nâ€¢ Nagios\nâ€¢ Elasticsearch + Kibana\n\nAlert Strategy:\n\nSeverity Levels:\nâ€¢ Critical - Immediate action\nâ€¢ Warning - Investigation needed\nâ€¢ Info - Awareness only\n\nThreshold Setting:\nâ€¢ Based on baselines\nâ€¢ Avoid alert fatigue\nâ€¢ Business impact driven\nâ€¢ Adjust over time\n\nAlert Channels:\nâ€¢ Email\nâ€¢ SMS/Phone\nâ€¢ Slack/Teams\nâ€¢ PagerDuty\nâ€¢ Ticketing systems\n\nDashboards:\n\nReal-Time:\nâ€¢ Current metrics\nâ€¢ Active queries\nâ€¢ Resource utilization\nâ€¢ Connection status\n\nHistorical:\nâ€¢ Trend analysis\nâ€¢ Capacity planning\nâ€¢ Performance over time\nâ€¢ Incident correlation\n\nExecutive:\nâ€¢ SLA compliance\nâ€¢ Availability statistics\nâ€¢ Performance trends\nâ€¢ Business metrics\n\nAutomation:\nâ€¢ Automated remediation\nâ€¢ Self-healing scripts\nâ€¢ Alert suppression\nâ€¢ Maintenance windows\nâ€¢ Report generation\n\nBest Practices:\nâ€¢ Define baseline metrics\nâ€¢ Set realistic thresholds\nâ€¢ Document alerts\nâ€¢ Regular review\nâ€¢ Incident response plans\nâ€¢ Continuous improvement\nâ€¢ Centralized logging\nâ€¢ Retention policies",
      "explanation": "Database monitoring tracks infrastructure, engine, query, and application metrics using native and third-party tools with proper alerting, dashboards, and automation for proactive management.",
      "difficulty": "Medium"
    },
    {
      "id": 92,
      "question": "What is database security hardening?",
      "answer": "Database security hardening involves implementing multiple layers of defense to protect against unauthorized access, data breaches, and attacks.\n\nSecurity Layers:\n\nNetwork Security:\nâ€¢ Firewall rules\nâ€¢ Network segmentation\nâ€¢ VPN access\nâ€¢ Private endpoints\nâ€¢ TLS/SSL encryption\n\nAccess Control:\nâ€¢ Principle of least privilege\nâ€¢ Role-based access (RBAC)\nâ€¢ Strong authentication\nâ€¢ Multi-factor authentication\nâ€¢ Service accounts\n\nData Protection:\nâ€¢ Encryption at rest\nâ€¢ Encryption in transit\nâ€¢ Column-level encryption\nâ€¢ Transparent Data Encryption (TDE)\nâ€¢ Key management\n\nHardening Steps:\n\nInstallation:\nâ€¢ Minimal installation\nâ€¢ Disable unnecessary services\nâ€¢ Remove sample databases\nâ€¢ Strong SA password\nâ€¢ Non-default ports\n\nConfiguration:\nâ€¢ Disable xp_cmdshell\nâ€¢ Disable CLR integration\nâ€¢ Enable login auditing\nâ€¢ Set password policies\nâ€¢ Configure SSL/TLS\n\nAccounts:\nâ€¢ Rename/disable SA\nâ€¢ Remove guest access\nâ€¢ Delete unused accounts\nâ€¢ Regular access reviews\nâ€¢ Principle of least privilege\n\nPermissions:\nâ€¢ Grant minimum required\nâ€¢ Deny > Grant\nâ€¢ Avoid dbo ownership\nâ€¢ Schema separation\nâ€¢ Application roles\n\nAuditing:\nâ€¢ Enable audit logging\nâ€¢ Monitor failed logins\nâ€¢ Track privilege changes\nâ€¢ Data access monitoring\nâ€¢ Regular audit review\n\nPatch Management:\nâ€¢ Regular patching\nâ€¢ Test patches\nâ€¢ Security updates\nâ€¢ Version upgrades\nâ€¢ Maintenance windows\n\nVulnerability Prevention:\n\nSQL Injection:\nâ€¢ Parameterized queries\nâ€¢ Input validation\nâ€¢ Stored procedures\nâ€¢ ORM usage\nâ€¢ Code reviews\n\nPrivilege Escalation:\nâ€¢ Restrict permissions\nâ€¢ Monitor privilege changes\nâ€¢ Disable dangerous features\nâ€¢ Regular audits\n\nData Exposure:\nâ€¢ Encrypt sensitive data\nâ€¢ Mask in non-production\nâ€¢ Access logging\nâ€¢ Data classification\nâ€¢ DLP tools\n\nCompliance:\nâ€¢ GDPR compliance\nâ€¢ HIPAA requirements\nâ€¢ PCI-DSS standards\nâ€¢ SOX controls\nâ€¢ Industry standards\n\nMonitoring:\nâ€¢ Failed login attempts\nâ€¢ Permission changes\nâ€¢ Schema modifications\nâ€¢ Unusual query patterns\nâ€¢ Data exfiltration\n\nBest Practices:\nâ€¢ Defense in depth\nâ€¢ Regular security audits\nâ€¢ Vulnerability assessments\nâ€¢ Penetration testing\nâ€¢ Security training\nâ€¢ Incident response plan\nâ€¢ Document security config\nâ€¢ Regular backup testing",
      "explanation": "Database security hardening implements multiple defensive layers including network security, access control, encryption, auditing, and vulnerability prevention to protect against threats.",
      "difficulty": "Hard"
    },
    {
      "id": 93,
      "question": "What are database cloud migration strategies?",
      "answer": "Cloud migration moves databases from on-premises to cloud platforms using various strategies based on requirements, complexity, and business goals.\n\nMigration Strategies:\n\nRehost (Lift and Shift):\nâ€¢ Move as-is to cloud\nâ€¢ Minimal changes\nâ€¢ Fastest migration\nâ€¢ IaaS VMs\nâ€¢ Least optimization\n\nReplatform:\nâ€¢ Move to managed service\nâ€¢ Minor optimizations\nâ€¢ PaaS databases\nâ€¢ Reduced management\nâ€¢ Some refactoring\n\nRefactor:\nâ€¢ Rearchitect for cloud\nâ€¢ Microservices\nâ€¢ Cloud-native features\nâ€¢ Maximum benefit\nâ€¢ Highest complexity\n\nRetain:\nâ€¢ Keep on-premises\nâ€¢ Not ready for cloud\nâ€¢ Compliance reasons\nâ€¢ Legacy systems\n\nRetire:\nâ€¢ Decommission\nâ€¢ No longer needed\nâ€¢ Consolidate\n\nCloud Platforms:\n\nAWS:\nâ€¢ RDS (managed databases)\nâ€¢ Aurora (MySQL/PostgreSQL)\nâ€¢ DynamoDB (NoSQL)\nâ€¢ Redshift (data warehouse)\nâ€¢ Database Migration Service\n\nAzure:\nâ€¢ SQL Database\nâ€¢ Cosmos DB\nâ€¢ MySQL/PostgreSQL\nâ€¢ Synapse Analytics\nâ€¢ Database Migration Service\n\nGCP:\nâ€¢ Cloud SQL\nâ€¢ Cloud Spanner\nâ€¢ BigQuery\nâ€¢ Firestore\nâ€¢ Database Migration Service\n\nMigration Process:\n\n1. Assessment:\nâ€¢ Current environment inventory\nâ€¢ Dependency mapping\nâ€¢ Performance baseline\nâ€¢ Cost analysis\nâ€¢ Risk assessment\n\n2. Planning:\nâ€¢ Target architecture\nâ€¢ Migration strategy\nâ€¢ Timeline\nâ€¢ Resource allocation\nâ€¢ Rollback plan\n\n3. Preparation:\nâ€¢ Network connectivity\nâ€¢ Security setup\nâ€¢ Access provisioning\nâ€¢ Tool selection\nâ€¢ Test environment\n\n4. Migration:\nâ€¢ Schema migration\nâ€¢ Data migration\nâ€¢ Application updates\nâ€¢ Testing\nâ€¢ Validation\n\n5. Cutover:\nâ€¢ Final sync\nâ€¢ Switch traffic\nâ€¢ Monitor\nâ€¢ Rollback if needed\n\n6. Optimization:\nâ€¢ Performance tuning\nâ€¢ Cost optimization\nâ€¢ Security review\nâ€¢ Documentation\n\nMigration Tools:\nâ€¢ AWS DMS\nâ€¢ Azure Database Migration Service\nâ€¢ Google Database Migration Service\nâ€¢ Native backup/restore\nâ€¢ Third-party tools\n\nChallenges:\nâ€¢ Downtime requirements\nâ€¢ Data synchronization\nâ€¢ Application compatibility\nâ€¢ Network bandwidth\nâ€¢ Cost management\nâ€¢ Skill gaps\n\nBest Practices:\nâ€¢ Start with non-critical\nâ€¢ Test thoroughly\nâ€¢ Phased approach\nâ€¢ Monitor continuously\nâ€¢ Document everything\nâ€¢ Train team\nâ€¢ Plan for rollback\nâ€¢ Validate data integrity",
      "explanation": "Cloud migration uses strategies like rehost, replatform, or refactor to move databases to AWS, Azure, or GCP, requiring careful planning, testing, and phased execution.",
      "difficulty": "Hard"
    },
    {
      "id": 94,
      "question": "What is database DevOps and CI/CD?",
      "answer": "Database DevOps applies DevOps principles to database development, enabling automated, repeatable, and reliable database changes through CI/CD pipelines.\n\nCore Principles:\n\nVersion Control:\nâ€¢ Database schema in Git\nâ€¢ Migration scripts versioned\nâ€¢ Branching strategy\nâ€¢ Code reviews\nâ€¢ History tracking\n\nAutomation:\nâ€¢ Automated testing\nâ€¢ Automated deployment\nâ€¢ Automated rollback\nâ€¢ Infrastructure as Code\nâ€¢ Continuous monitoring\n\nContinuous Integration:\nâ€¢ Automated builds\nâ€¢ Unit testing\nâ€¢ Integration testing\nâ€¢ Static analysis\nâ€¢ Code quality checks\n\nContinuous Deployment:\nâ€¢ Automated deployment\nâ€¢ Environment promotion\nâ€¢ Blue-green deployment\nâ€¢ Canary releases\nâ€¢ Rollback capability\n\nCI/CD Pipeline:\n\n1. Source Control:\nâ€¢ Schema changes\nâ€¢ Stored procedures\nâ€¢ Migration scripts\nâ€¢ Test data\nâ€¢ Configuration\n\n2. Build Stage:\nâ€¢ Validate syntax\nâ€¢ Run linters\nâ€¢ Package artifacts\nâ€¢ Version tagging\n\n3. Test Stage:\nâ€¢ Unit tests\nâ€¢ Integration tests\nâ€¢ Performance tests\nâ€¢ Security scans\nâ€¢ Data validation\n\n4. Deploy Stage:\nâ€¢ DEV deployment\nâ€¢ QA deployment\nâ€¢ Staging deployment\nâ€¢ Production deployment\nâ€¢ Smoke tests\n\n5. Monitor:\nâ€¢ Deployment status\nâ€¢ Performance metrics\nâ€¢ Error tracking\nâ€¢ Rollback triggers\n\nTools:\n\nVersion Control:\nâ€¢ Git\nâ€¢ GitHub/GitLab/Bitbucket\nâ€¢ Azure Repos\n\nCI/CD Platforms:\nâ€¢ Jenkins\nâ€¢ GitLab CI\nâ€¢ GitHub Actions\nâ€¢ Azure DevOps\nâ€¢ CircleCI\n\nDatabase Tools:\nâ€¢ Flyway\nâ€¢ Liquibase\nâ€¢ Redgate\nâ€¢ DBmaestro\nâ€¢ SSDT\n\nTesting:\nâ€¢ tSQLt\nâ€¢ pgTAP\nâ€¢ DBUnit\nâ€¢ Custom frameworks\n\nBest Practices:\n\nDatabase Changes:\nâ€¢ Small, incremental\nâ€¢ Backward compatible\nâ€¢ Tested thoroughly\nâ€¢ Documented\nâ€¢ Reversible\n\nDeployment Strategy:\nâ€¢ Non-breaking changes first\nâ€¢ Feature flags\nâ€¢ Gradual rollout\nâ€¢ Monitoring\nâ€¢ Quick rollback\n\nEnvironment Management:\nâ€¢ Environment parity\nâ€¢ Automated provisioning\nâ€¢ Configuration management\nâ€¢ Data masking\nâ€¢ Refresh strategy\n\nSecurity:\nâ€¢ Secrets management\nâ€¢ Access control\nâ€¢ Audit logging\nâ€¢ Compliance checks\nâ€¢ Vulnerability scanning\n\nChallenges:\nâ€¢ State management\nâ€¢ Data migration\nâ€¢ Downtime requirements\nâ€¢ Rollback complexity\nâ€¢ Performance testing\nâ€¢ Cultural change",
      "explanation": "Database DevOps automates database changes through CI/CD pipelines with version control, automated testing, and deployment, enabling reliable and repeatable database releases.",
      "difficulty": "Hard"
    },
    {
      "id": 95,
      "question": "What are database observability and monitoring differences?",
      "answer": "Observability is the ability to understand system internal state from external outputs, while monitoring tracks predefined metrics.\n\nMonitoring:\n\nDefinition:\nâ€¢ Tracks known metrics\nâ€¢ Predefined dashboards\nâ€¢ Threshold-based alerts\nâ€¢ Reactive approach\nâ€¢ Answers known questions\n\nCharacteristics:\nâ€¢ Metrics collection\nâ€¢ Health checks\nâ€¢ Performance counters\nâ€¢ Uptime tracking\nâ€¢ SLA monitoring\n\nLimitations:\nâ€¢ Can't discover unknowns\nâ€¢ Requires predefinition\nâ€¢ Limited context\nâ€¢ Point-in-time view\n\nObservability:\n\nDefinition:\nâ€¢ Understand internal state\nâ€¢ Explore and discover\nâ€¢ Context-rich data\nâ€¢ Proactive approach\nâ€¢ Answers unknown questions\n\nThree Pillars:\n\nMetrics:\nâ€¢ Time-series data\nâ€¢ Aggregated measurements\nâ€¢ Resource utilization\nâ€¢ Performance indicators\n\nLogs:\nâ€¢ Event records\nâ€¢ Error messages\nâ€¢ Audit trails\nâ€¢ Detailed context\n\nTraces:\nâ€¢ Request flow\nâ€¢ Distributed tracing\nâ€¢ Latency breakdown\nâ€¢ Dependency mapping\n\nCapabilities:\nâ€¢ Root cause analysis\nâ€¢ Pattern detection\nâ€¢ Correlation analysis\nâ€¢ Anomaly detection\nâ€¢ Predictive insights\n\nDatabase Observability:\n\nQuery Analysis:\nâ€¢ Execution patterns\nâ€¢ Performance trends\nâ€¢ Resource consumption\nâ€¢ Query relationships\nâ€¢ Plan evolution\n\nConnection Tracing:\nâ€¢ Request paths\nâ€¢ Multi-database queries\nâ€¢ Service dependencies\nâ€¢ Latency sources\n\nState Insights:\nâ€¢ Buffer pool behavior\nâ€¢ Lock contention patterns\nâ€¢ Transaction flows\nâ€¢ Resource bottlenecks\n\nTools:\n\nMonitoring Tools:\nâ€¢ Prometheus\nâ€¢ Grafana\nâ€¢ CloudWatch\nâ€¢ Azure Monitor\nâ€¢ Native DMVs\n\nObservability Platforms:\nâ€¢ Datadog\nâ€¢ New Relic\nâ€¢ Honeycomb\nâ€¢ Lightstep\nâ€¢ Elastic APM\n\nImplementation:\n\nMonitoring Setup:\nâ€¢ Define KPIs\nâ€¢ Set thresholds\nâ€¢ Create dashboards\nâ€¢ Configure alerts\nâ€¢ Regular review\n\nObservability Setup:\nâ€¢ Instrument application\nâ€¢ Collect telemetry\nâ€¢ Centralize data\nâ€¢ Enable exploration\nâ€¢ Context correlation\n\nBest Practices:\nâ€¢ Combine both approaches\nâ€¢ Monitor for knowns\nâ€¢ Observe for unknowns\nâ€¢ Rich context logging\nâ€¢ Distributed tracing\nâ€¢ Continuous improvement\nâ€¢ Incident learning",
      "explanation": "Monitoring tracks predefined metrics reactively, while observability enables exploring and understanding system behavior proactively through metrics, logs, and traces.",
      "difficulty": "Hard"
    },
    {
      "id": 96,
      "question": "What are modern SQL features in recent database versions?",
      "answer": "Recent SQL standards and database versions introduce advanced features improving productivity, performance, and functionality.\n\nSQL:2016 Features:\n\nJSON Support:\nâ€¢ JSON data type\nâ€¢ JSON functions\nâ€¢ Path expressions\nâ€¢ JSON aggregation\nâ€¢ Native indexing\n\nRow Pattern Matching:\nâ€¢ MATCH_RECOGNIZE clause\nâ€¢ Pattern detection in sequences\nâ€¢ Time-series analysis\nâ€¢ Complex event processing\n\nPolymorphic Table Functions:\nâ€¢ Dynamic result schemas\nâ€¢ Runtime determination\nâ€¢ Flexible output\n\nSQL:2019 Features:\n\nMulti-Dimensional Arrays:\nâ€¢ Array operations\nâ€¢ Array comparisons\nâ€¢ Enhanced array functions\n\nPart-based Flags:\nâ€¢ Enhanced windowing\nâ€¢ Frame specifications\nâ€¢ Range operations\n\nDatabase-Specific Modern Features:\n\nPostgreSQL 14+:\nâ€¢ Multiranges\nâ€¢ Query parallelism improvements\nâ€¢ Logical replication enhancements\nâ€¢ JSON subscripting\nâ€¢ Stored procedures with transactions\n\nMySQL 8.0+:\nâ€¢ Window functions\nâ€¢ CTEs (WITH clause)\nâ€¢ JSON functions\nâ€¢ Descending indexes\nâ€¢ Invisible indexes\nâ€¢ Instant ADD COLUMN\n\nSQL Server 2022:\nâ€¢ Intelligent Query Processing\nâ€¢ Parameter Sensitive Plans\nâ€¢ Approximate percentiles\nâ€¢ JSON improvements\nâ€¢ Ledger functionality\n\nOracle 23c:\nâ€¢ JavaScript in database\nâ€¢ SQL Domains\nâ€¢ Schema annotations\nâ€¢ JSON Relational Duality\nâ€¢ Property Graphs\n\nPerformance Features:\n\nAdaptive Query Processing:\nâ€¢ Runtime plan adjustments\nâ€¢ Batch mode improvements\nâ€¢ Memory grant feedback\nâ€¢ Adaptive joins\n\nIntelligent Caching:\nâ€¢ Result set caching\nâ€¢ Accelerated queries\nâ€¢ Automatic optimization\n\nColumnstore Enhancements:\nâ€¢ Better compression\nâ€¢ Batch mode operations\nâ€¢ Online operations\n\nDevelopment Features:\n\nGenerated Columns:\nâ€¢ Computed always\nâ€¢ Virtual vs stored\nâ€¢ Index support\n\nTemporal Tables:\nâ€¢ System versioning\nâ€¢ Historical queries\nâ€¢ Point-in-time recovery\n\nGraph Features:\nâ€¢ Graph tables\nâ€¢ MATCH clause\nâ€¢ Shortest path\nâ€¢ Node/edge queries\n\nSecurity Features:\n\nDynamic Data Masking:\nâ€¢ On-the-fly masking\nâ€¢ Role-based\nâ€¢ No schema changes\n\nRow-Level Security:\nâ€¢ Per-row access control\nâ€¢ Predicate-based\nâ€¢ Multi-tenant support\n\nAlways Encrypted:\nâ€¢ Client-side encryption\nâ€¢ Transparent to database\nâ€¢ Key management\n\nCloud-Native Features:\n\nServerless:\nâ€¢ Auto-scaling\nâ€¢ Pay-per-use\nâ€¢ Automatic pause/resume\n\nMulti-Region:\nâ€¢ Global distribution\nâ€¢ Cross-region replication\nâ€¢ Low latency\n\nElastic Pools:\nâ€¢ Resource sharing\nâ€¢ Cost optimization\nâ€¢ Dynamic scaling",
      "explanation": "Modern SQL features include JSON support, window functions, CTEs, temporal tables, intelligent query processing, and cloud-native capabilities improving functionality and performance.",
      "difficulty": "Medium"
    },
    {
      "id": 97,
      "question": "What are database multi-tenancy patterns?",
      "answer": "Multi-tenancy allows a single database to serve multiple tenants (customers) with different isolation, resource sharing, and management approaches.\n\nIsolation Models:\n\nDatabase Per Tenant:\nâ€¢ Separate database for each\nâ€¢ Complete isolation\nâ€¢ Easy to scale\nâ€¢ Higher cost\nâ€¢ Complex management\n\nBenefits:\nâ€¢ Maximum isolation\nâ€¢ Independent scaling\nâ€¢ Custom schema per tenant\nâ€¢ Easy backup/restore\nâ€¢ Regulatory compliance\n\nDrawbacks:\nâ€¢ Higher resource usage\nâ€¢ More databases to manage\nâ€¢ Schema updates complex\nâ€¢ Higher hosting costs\n\nSchema Per Tenant:\nâ€¢ Shared database, separate schemas\nâ€¢ Medium isolation\nâ€¢ Moderate scaling\nâ€¢ Moderate cost\nâ€¢ Manageable complexity\n\nBenefits:\nâ€¢ Better resource utilization\nâ€¢ Easier management than separate DBs\nâ€¢ Schema customization possible\nâ€¢ Good isolation\n\nDrawbacks:\nâ€¢ Schema proliferation\nâ€¢ Shared resource contention\nâ€¢ Database size limits\nâ€¢ Backup/restore complexity\n\nShared Schema:\nâ€¢ All tenants same schema\nâ€¢ TenantID column\nâ€¢ Minimal isolation\nâ€¢ Best resource sharing\nâ€¢ Lowest cost\nâ€¢ Simplest management\n\nBenefits:\nâ€¢ Maximum density\nâ€¢ Easiest maintenance\nâ€¢ Simple schema changes\nâ€¢ Lowest cost\nâ€¢ Best resource utilization\n\nDrawbacks:\nâ€¢ No physical isolation\nâ€¢ Noisy neighbor issues\nâ€¢ Query complexity\nâ€¢ Accidental cross-tenant access risk\n\nImplementation Patterns:\n\nRow-Level Security:\nâ€¢ Tenant filter at row level\nâ€¢ Transparent to application\nâ€¢ Database-enforced\nâ€¢ Policy-based\n\nApplication-Level:\nâ€¢ WHERE TenantID = ?\nâ€¢ Application enforces\nâ€¢ Flexible\nâ€¢ Performance overhead\n\nHybrid Approach:\nâ€¢ Mix of patterns\nâ€¢ Premium vs standard\nâ€¢ Based on tenant size\nâ€¢ Tiered service\n\nTenant Management:\n\nTenant Provisioning:\nâ€¢ Automated onboarding\nâ€¢ Resource allocation\nâ€¢ Configuration\nâ€¢ Initial data\n\nTenant Discovery:\nâ€¢ Tenant routing\nâ€¢ Connection strings\nâ€¢ Database mapping\nâ€¢ Load balancing\n\nResource Limits:\nâ€¢ CPU quotas\nâ€¢ Memory limits\nâ€¢ Storage caps\nâ€¢ Connection limits\nâ€¢ Query timeout\n\nMonitoring:\nâ€¢ Per-tenant metrics\nâ€¢ Resource usage\nâ€¢ Performance tracking\nâ€¢ Cost allocation\nâ€¢ SLA compliance\n\nChallenges:\n\nData Isolation:\nâ€¢ Prevent cross-tenant access\nâ€¢ Security policies\nâ€¢ Audit logging\nâ€¢ Compliance\n\nPerformance:\nâ€¢ Noisy neighbors\nâ€¢ Resource contention\nâ€¢ Query optimization\nâ€¢ Caching strategies\n\nScaling:\nâ€¢ Tenant distribution\nâ€¢ Shard rebalancing\nâ€¢ Migration\nâ€¢ Elastic scaling\n\nBest Practices:\nâ€¢ Choose based on requirements\nâ€¢ Security first\nâ€¢ Monitor tenant health\nâ€¢ Automate operations\nâ€¢ Plan for growth\nâ€¢ Cost optimization\nâ€¢ Clear SLAs\nâ€¢ Regular audits",
      "explanation": "Multi-tenancy patterns include database-per-tenant (max isolation), schema-per-tenant (medium), and shared-schema (max efficiency), each balancing isolation, cost, and complexity.",
      "difficulty": "Hard"
    },
    {
      "id": 98,
      "question": "What are database performance tuning methodologies?",
      "answer": "Performance tuning uses systematic approaches to identify and resolve database performance issues through analysis, optimization, and validation.\n\nMethodologies:\n\nTop-Down Approach:\nâ€¢ Start with business requirements\nâ€¢ Identify slow transactions\nâ€¢ Drill down to queries\nâ€¢ Analyze execution plans\nâ€¢ Optimize bottlenecks\n\nBottom-Up Approach:\nâ€¢ Start with system metrics\nâ€¢ Identify resource bottlenecks\nâ€¢ Find expensive queries\nâ€¢ Optimize infrastructure\nâ€¢ Validate improvements\n\nWait-Based Analysis:\nâ€¢ Identify wait types\nâ€¢ Analyze wait statistics\nâ€¢ Address wait causes\nâ€¢ Monitor wait reduction\nâ€¢ Most effective method\n\nTuning Process:\n\n1. Establish Baseline:\nâ€¢ Current performance metrics\nâ€¢ Response times\nâ€¢ Resource utilization\nâ€¢ Query patterns\nâ€¢ User complaints\n\n2. Identify Bottlenecks:\n\nCommon Bottlenecks:\nâ€¢ CPU saturation\nâ€¢ Memory pressure\nâ€¢ Disk I/O\nâ€¢ Network latency\nâ€¢ Lock contention\nâ€¢ Poor queries\nâ€¢ Missing indexes\n\nAnalysis Tools:\nâ€¢ Execution plans\nâ€¢ DMVs/System views\nâ€¢ Extended Events\nâ€¢ Query Store\nâ€¢ Profiler/Trace\nâ€¢ Wait statistics\n\n3. Prioritize Issues:\nâ€¢ Impact on users\nâ€¢ Frequency of occurrence\nâ€¢ Fix complexity\nâ€¢ Resource requirements\nâ€¢ Quick wins first\n\n4. Apply Optimizations:\n\nQuery Level:\nâ€¢ Rewrite queries\nâ€¢ Add indexes\nâ€¢ Update statistics\nâ€¢ Optimize joins\nâ€¢ Reduce result sets\n\nDatabase Level:\nâ€¢ Configuration tuning\nâ€¢ Memory allocation\nâ€¢ Parallelism settings\nâ€¢ TempDB optimization\nâ€¢ File placement\n\nSchema Level:\nâ€¢ Normalization review\nâ€¢ Partitioning\nâ€¢ Archiving old data\nâ€¢ Data type optimization\n\nInfrastructure Level:\nâ€¢ Hardware upgrades\nâ€¢ Storage optimization\nâ€¢ Network improvements\nâ€¢ Resource allocation\n\n5. Validate Changes:\nâ€¢ Measure improvements\nâ€¢ Compare to baseline\nâ€¢ Load testing\nâ€¢ Monitor for regressions\nâ€¢ Document results\n\n6. Monitor Continuously:\nâ€¢ Track metrics\nâ€¢ Alert on degradation\nâ€¢ Regular reviews\nâ€¢ Proactive optimization\n\nTuning Techniques:\n\nProactive:\nâ€¢ Regular maintenance\nâ€¢ Index optimization\nâ€¢ Statistics updates\nâ€¢ Capacity planning\nâ€¢ Performance testing\n\nReactive:\nâ€¢ Incident response\nâ€¢ Quick fixes\nâ€¢ Emergency optimization\nâ€¢ Root cause analysis\n\nIterative:\nâ€¢ Continuous improvement\nâ€¢ Small changes\nâ€¢ Test and measure\nâ€¢ Gradual optimization\n\nBest Practices:\nâ€¢ One change at a time\nâ€¢ Measure before and after\nâ€¢ Document everything\nâ€¢ Test in non-production\nâ€¢ Have rollback plan\nâ€¢ Involve stakeholders\nâ€¢ Focus on biggest impact\nâ€¢ Sustainable solutions\n\nCommon Pitfalls:\nâ€¢ Optimizing prematurely\nâ€¢ Ignoring root cause\nâ€¢ Over-indexing\nâ€¢ Copying production to test\nâ€¢ Tuning without measurement\nâ€¢ Quick fixes without understanding\nâ€¢ Ignoring application code",
      "explanation": "Performance tuning uses systematic methodologies like top-down, bottom-up, or wait-based analysis to identify bottlenecks, apply optimizations, and validate improvements iteratively.",
      "difficulty": "Hard"
    },
    {
      "id": 99,
      "question": "What are database scalability patterns and strategies?",
      "answer": "Database scalability enables systems to handle growing workloads through vertical (scale-up) and horizontal (scale-out) approaches with various architectural patterns.\n\nScalability Types:\n\nVertical Scaling (Scale-Up):\nâ€¢ Add more resources to server\nâ€¢ CPU, memory, storage, I/O\nâ€¢ Simple implementation\nâ€¢ Limited by hardware\nâ€¢ Higher cost per unit\nâ€¢ Single point of failure\nâ€¢ No application changes\n\nHorizontal Scaling (Scale-Out):\nâ€¢ Add more servers\nâ€¢ Distribute load\nâ€¢ Near-infinite scalability\nâ€¢ Complexity increases\nâ€¢ Better cost efficiency\nâ€¢ Fault tolerance\nâ€¢ Application changes required\n\nScalability Patterns:\n\nRead Replicas:\nâ€¢ Primary for writes\nâ€¢ Replicas for reads\nâ€¢ Asynchronous replication\nâ€¢ Eventual consistency\nâ€¢ Scale read traffic\n\nBenefits:\nâ€¢ Distribute read load\nâ€¢ Geographic distribution\nâ€¢ Reporting offload\nâ€¢ High availability\n\nLimitations:\nâ€¢ Replication lag\nâ€¢ Write scalability unchanged\nâ€¢ Consistency challenges\n\nSharding:\nâ€¢ Horizontal data partitioning\nâ€¢ Data split across servers\nâ€¢ Each shard independent\nâ€¢ Application-aware routing\n\nSharding Keys:\nâ€¢ Customer ID\nâ€¢ Geographic region\nâ€¢ Date ranges\nâ€¢ Hash-based\n\nBenefits:\nâ€¢ Scales reads and writes\nâ€¢ Unlimited growth potential\nâ€¢ Isolated failures\nâ€¢ Parallel processing\n\nChallenges:\nâ€¢ Cross-shard queries\nâ€¢ Distributed transactions\nâ€¢ Rebalancing complexity\nâ€¢ Application complexity\n\nCQRS (Command Query Responsibility Segregation):\nâ€¢ Separate read and write models\nâ€¢ Optimized independently\nâ€¢ Different data stores\nâ€¢ Event-driven sync\n\nBenefits:\nâ€¢ Independent scaling\nâ€¢ Optimized models\nâ€¢ Flexibility\n\nDatabase Clustering:\nâ€¢ Multiple nodes\nâ€¢ Shared-nothing architecture\nâ€¢ Automatic failover\nâ€¢ Load balancing\n\nExamples:\nâ€¢ PostgreSQL with Patroni\nâ€¢ MySQL Cluster\nâ€¢ Oracle RAC\nâ€¢ SQL Server Always On\n\nCaching Strategies:\n\nApplication Cache:\nâ€¢ Redis, Memcached\nâ€¢ Reduce database load\nâ€¢ Fast response times\nâ€¢ Cache invalidation\n\nQuery Result Cache:\nâ€¢ Cache query results\nâ€¢ Materialized views\nâ€¢ Periodic refresh\n\nDatabase Cache:\nâ€¢ Buffer pool optimization\nâ€¢ Query plan cache\nâ€¢ Native caching\n\nPartitioning:\n\nTable Partitioning:\nâ€¢ Split large tables\nâ€¢ Range, list, hash\nâ€¢ Parallel query execution\nâ€¢ Easier maintenance\n\nVertical Partitioning:\nâ€¢ Split columns\nâ€¢ Hot vs cold data\nâ€¢ Frequently vs rarely accessed\n\nArchitecture Patterns:\n\nMicroservices:\nâ€¢ Database per service\nâ€¢ Independent scaling\nâ€¢ Polyglot persistence\nâ€¢ Data consistency challenges\n\nEvent Sourcing:\nâ€¢ Store events\nâ€¢ Rebuild state\nâ€¢ Audit trail\nâ€¢ Temporal queries\n\nCloud-Native:\nâ€¢ Managed services\nâ€¢ Auto-scaling\nâ€¢ Serverless\nâ€¢ Multi-region\n\nBest Practices:\nâ€¢ Plan for scale early\nâ€¢ Measure before scaling\nâ€¢ Start with vertical\nâ€¢ Add horizontal as needed\nâ€¢ Monitor continuously\nâ€¢ Test at scale\nâ€¢ Automate operations\nâ€¢ Consider trade-offs",
      "explanation": "Database scalability uses vertical scaling (add resources), horizontal scaling (add servers), patterns like read replicas and sharding, and caching strategies to handle growth.",
      "difficulty": "Hard"
    },
    {
      "id": 100,
      "question": "What are emerging trends and future of databases?",
      "answer": "Database technology evolves with trends in cloud-native, AI integration, specialized databases, and new data models shaping the future.\n\nEmerging Technologies:\n\nCloud-Native Databases:\nâ€¢ Built for cloud\nâ€¢ Serverless architectures\nâ€¢ Auto-scaling\nâ€¢ Pay-per-use\nâ€¢ Multi-region by default\nâ€¢ Distributed by design\n\nExamples:\nâ€¢ Amazon Aurora Serverless\nâ€¢ Google Cloud Spanner\nâ€¢ Azure Cosmos DB\nâ€¢ PlanetScale\nâ€¢ CockroachDB\n\nAI and Machine Learning Integration:\n\nIn-Database ML:\nâ€¢ Built-in ML models\nâ€¢ SQL-based ML\nâ€¢ Automated predictions\nâ€¢ Feature engineering\n\nAutonomous Databases:\nâ€¢ Self-tuning\nâ€¢ Self-securing\nâ€¢ Self-repairing\nâ€¢ Automated patching\nâ€¢ Oracle Autonomous Database\n\nAI-Powered Optimization:\nâ€¢ Query optimization\nâ€¢ Index recommendations\nâ€¢ Workload analysis\nâ€¢ Predictive scaling\n\nSpecialized Databases:\n\nTime-Series:\nâ€¢ IoT data\nâ€¢ Metrics storage\nâ€¢ High write throughput\nâ€¢ InfluxDB, TimescaleDB\n\nGraph Databases:\nâ€¢ Relationship-focused\nâ€¢ Social networks\nâ€¢ Recommendation engines\nâ€¢ Neo4j, Amazon Neptune\n\nVector Databases:\nâ€¢ Similarity search\nâ€¢ ML embeddings\nâ€¢ Semantic search\nâ€¢ Pinecone, Weaviate\n\nLedger Databases:\nâ€¢ Immutable\nâ€¢ Cryptographic verification\nâ€¢ Audit trail\nâ€¢ Amazon QLDB\n\nMulti-Model:\nâ€¢ Document + Graph + Key-Value\nâ€¢ Single database\nâ€¢ Flexible schema\nâ€¢ ArangoDB, Azure Cosmos DB\n\nData Distribution:\n\nEdge Computing:\nâ€¢ Data at edge\nâ€¢ Low latency\nâ€¢ Local processing\nâ€¢ Sync to cloud\n\nGlobal Distribution:\nâ€¢ Multi-region active-active\nâ€¢ Low latency worldwide\nâ€¢ Conflict resolution\nâ€¢ High availability\n\nNew Paradigms:\n\nSeparation of Storage and Compute:\nâ€¢ Independent scaling\nâ€¢ Cost optimization\nâ€¢ Elastic compute\nâ€¢ Shared storage layer\n\nData Lakehouse:\nâ€¢ Combine data lake and warehouse\nâ€¢ ACID transactions\nâ€¢ BI and ML workloads\nâ€¢ Databricks, Snowflake\n\nReal-Time Analytics:\nâ€¢ Stream processing\nâ€¢ Instant insights\nâ€¢ Real-time dashboards\nâ€¢ Apache Kafka integration\n\nSecurity Evolution:\n\nConfidential Computing:\nâ€¢ Encrypted in-use\nâ€¢ Hardware-based security\nâ€¢ Zero-trust\n\nPrivacy-Preserving:\nâ€¢ Differential privacy\nâ€¢ Homomorphic encryption\nâ€¢ Secure multi-party computation\n\nQuantum-Resistant:\nâ€¢ Post-quantum cryptography\nâ€¢ Future-proof security\n\nDevelopment Trends:\n\nLow-Code/No-Code:\nâ€¢ Visual query builders\nâ€¢ Automated workflows\nâ€¢ Citizen developers\n\nGitOps for Databases:\nâ€¢ Infrastructure as Code\nâ€¢ Version-controlled schemas\nâ€¢ Automated deployments\nâ€¢ CI/CD integration\n\nObservability Native:\nâ€¢ Built-in monitoring\nâ€¢ Distributed tracing\nâ€¢ Metrics and logs\n\nFuture Predictions:\nâ€¢ More specialization\nâ€¢ AI-first databases\nâ€¢ Quantum databases\nâ€¢ Blockchain integration\nâ€¢ Unified data platforms\nâ€¢ Sustainability focus\nâ€¢ Privacy by design",
      "explanation": "Future databases trend toward cloud-native serverless, AI integration, specialized types (time-series, graph, vector), multi-model support, and enhanced security with real-time analytics.",
      "difficulty": "Medium"
    }
  ]
}
