{
  "topic": "System Design",
  "questions": [
    {
      "id": 1,
      "question": "What is scalability and what are its types?",
      "answer": "Scalability is the ability of a system to handle increased load by adding resources.\n\nTypes of Scalability:\n• Vertical Scaling (Scale Up): Adding more power (CPU, RAM) to existing machines\n• Horizontal Scaling (Scale Out): Adding more machines to distribute load\n• Diagonal Scaling: Combining both vertical and horizontal scaling\n\nVertical Scaling Characteristics:\n• Simpler to implement (no code changes needed)\n• Limited by hardware capacity\n• Single point of failure\n• Cost increases exponentially\n\nHorizontal Scaling Characteristics:\n• More complex (requires load balancing, state management)\n• Virtually unlimited scalability\n• Better fault tolerance\n• Cost increases linearly\n\nWhen to Use:\n• Vertical: Legacy apps, databases, quick fixes\n• Horizontal: Web servers, microservices, cloud-native apps",
      "explanation": "Scalability ensures systems can grow by either adding more powerful hardware (vertical) or more machines (horizontal), with horizontal scaling being preferred for modern distributed systems.",
      "difficulty": "Easy",
      "code": "# Vertical Scaling Example\n# Single powerful server\nserver:\n  cpu: 32 cores\n  ram: 128GB\n  capacity: 10,000 users\n\n# Horizontal Scaling Example\n# Multiple smaller servers behind load balancer\nload_balancer:\n  algorithm: round-robin\n  servers:\n    - server1: { cpu: 8 cores, ram: 32GB, capacity: 2,500 users }\n    - server2: { cpu: 8 cores, ram: 32GB, capacity: 2,500 users }\n    - server3: { cpu: 8 cores, ram: 32GB, capacity: 2,500 users }\n    - server4: { cpu: 8 cores, ram: 32GB, capacity: 2,500 users }\n  total_capacity: 10,000 users\n  benefits:\n    - fault_tolerance: true\n    - can_add_more_servers: true\n    - cost_effective: true\n\n# AWS Auto Scaling Group Example\nresource \"aws_autoscaling_group\" \"web\" {\n  name = \"web-asg\"\n  min_size = 2\n  max_size = 10\n  desired_capacity = 4\n  \n  target_group_arns = [aws_lb_target_group.web.arn]\n  vpc_zone_identifier = [aws_subnet.private.*.id]\n  \n  # Scale based on CPU usage\n  tag {\n    key = \"Environment\"\n    value = \"production\"\n    propagate_at_launch = true\n  }\n}\n\n# Kubernetes Horizontal Pod Autoscaler\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: web-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: web-app\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70"
    },
    {
      "id": 2,
      "question": "What is the CAP theorem and how does it affect system design?",
      "answer": "CAP theorem states that a distributed system can only guarantee two out of three properties: Consistency, Availability, and Partition Tolerance.\n\nThe Three Properties:\n• Consistency: All nodes see the same data at the same time\n• Availability: Every request receives a response (success or failure)\n• Partition Tolerance: System continues despite network partitions\n\nPractical Implications:\n• CP Systems: Sacrifice availability for consistency (MongoDB, HBase, Redis)\n• AP Systems: Sacrifice consistency for availability (Cassandra, CouchDB, DynamoDB)\n• CA Systems: Theoretical only, not partition tolerant (Single-node RDBMS)\n\nReality Check:\n• Network partitions will happen, so Partition Tolerance is mandatory\n• Real choice is between CP and AP\n• Modern systems use tunable consistency (eventual consistency)\n\nDesign Decisions:\n• Financial systems: Choose CP (consistency critical)\n• Social media: Choose AP (availability critical)\n• E-commerce: Hybrid approach with eventual consistency",
      "explanation": "CAP theorem forces distributed systems to choose between strong consistency and high availability during network partitions, with partition tolerance being mandatory in practice.",
      "difficulty": "Easy",
      "code": "// CP System Example (Strong Consistency)\n// MongoDB with majority write concern\nconst { MongoClient } = require('mongodb');\n\nconst client = new MongoClient('mongodb://localhost:27017', {\n  replicaSet: 'rs0',\n  writeConcern: {\n    w: 'majority',  // Wait for majority of nodes\n    j: true,        // Wait for journal commit\n    wtimeout: 5000  // Timeout after 5s (sacrifice availability)\n  }\n});\n\nasync function writeWithStrongConsistency(data) {\n  try {\n    const db = client.db('myapp');\n    const result = await db.collection('users').insertOne(data);\n    // If partition occurs, this will fail (no availability)\n    return result;\n  } catch (error) {\n    // Operation failed, but data is consistent across nodes\n    throw new Error('Write failed - maintaining consistency');\n  }\n}\n\n// AP System Example (High Availability)\n// DynamoDB with eventual consistency\nconst { DynamoDBClient, PutItemCommand } = require('@aws-sdk/client-dynamodb');\n\nconst dynamodb = new DynamoDBClient({ region: 'us-east-1' });\n\nasync function writeWithHighAvailability(data) {\n  const params = {\n    TableName: 'Users',\n    Item: data,\n    // Eventually consistent reads (default)\n    // Writes always succeed even during partitions\n  };\n  \n  try {\n    await dynamodb.send(new PutItemCommand(params));\n    // Write succeeds immediately (high availability)\n    // But replicas may not have data yet (eventual consistency)\n    return { success: true };\n  } catch (error) {\n    // Rare failure case\n    return { success: false, error };\n  }\n}\n\n// Tunable Consistency in Cassandra\nconst cassandra = require('cassandra-driver');\n\nconst client = new cassandra.Client({\n  contactPoints: ['host1', 'host2', 'host3'],\n  localDataCenter: 'datacenter1'\n});\n\n// Strong Consistency (CP)\nawait client.execute(query, params, {\n  consistency: cassandra.types.consistencies.quorum\n  // Requires majority of nodes to respond\n  // Slower but consistent\n});\n\n// High Availability (AP)\nawait client.execute(query, params, {\n  consistency: cassandra.types.consistencies.one\n  // Only one node needs to respond\n  // Faster but may return stale data\n});\n\n// Trade-off visualization\nconst systemChoices = {\n  banking: { type: 'CP', reason: 'Account balance must be consistent' },\n  twitter: { type: 'AP', reason: 'Tweet availability more important than instant consistency' },\n  inventory: { type: 'Hybrid', reason: 'Eventual consistency with conflict resolution' }\n};"
    },
    {
      "id": 3,
      "question": "What is a load balancer and what are common load balancing algorithms?",
      "answer": "A load balancer distributes incoming network traffic across multiple servers to ensure no single server bears too much load.\n\nTypes of Load Balancers:\n• Layer 4 (Transport): Works at TCP/UDP level, faster but less flexible\n• Layer 7 (Application): Works at HTTP level, content-aware routing\n• Global Load Balancing: Geographic distribution using DNS\n\nCommon Algorithms:\n• Round Robin: Distributes requests equally in sequential order\n• Least Connections: Routes to server with fewest active connections\n• Least Response Time: Routes to server with fastest response\n• IP Hash: Routes based on client IP for session persistence\n• Weighted Round Robin: Distributes based on server capacity weights\n• Random: Routes to random server (surprisingly effective)\n\nHealth Checks:\n• Active: Load balancer pings servers regularly\n• Passive: Monitor actual traffic and remove failing servers\n• Custom: Application-specific health endpoints\n\nBenefits:\n• High availability through redundancy\n• Horizontal scalability\n• Maintenance without downtime\n• SSL termination\n• Request routing and filtering",
      "explanation": "Load balancers distribute traffic across multiple servers using various algorithms, improving reliability, scalability, and performance while enabling zero-downtime deployments.",
      "difficulty": "Easy",
      "code": "# NGINX Load Balancer Configuration\n# Layer 7 HTTP Load Balancing\n\nupstream backend {\n    # Round Robin (default)\n    server web1.example.com:8080;\n    server web2.example.com:8080;\n    server web3.example.com:8080;\n}\n\nupstream backend_weighted {\n    # Weighted Round Robin\n    server web1.example.com:8080 weight=3;  # 3x more traffic\n    server web2.example.com:8080 weight=2;\n    server web3.example.com:8080 weight=1;\n}\n\nupstream backend_least_conn {\n    # Least Connections algorithm\n    least_conn;\n    server web1.example.com:8080;\n    server web2.example.com:8080;\n    server web3.example.com:8080;\n}\n\nupstream backend_ip_hash {\n    # IP Hash for session persistence\n    ip_hash;\n    server web1.example.com:8080;\n    server web2.example.com:8080;\n    server web3.example.com:8080;\n}\n\nserver {\n    listen 80;\n    server_name example.com;\n\n    location / {\n        proxy_pass http://backend;\n        \n        # Health checks\n        proxy_next_upstream error timeout http_500;\n        proxy_connect_timeout 2s;\n        \n        # Headers\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n    \n    # Content-based routing (Layer 7)\n    location /api/ {\n        proxy_pass http://backend_api;\n    }\n    \n    location /images/ {\n        proxy_pass http://backend_static;\n    }\n}\n\n# AWS Application Load Balancer with Terraform\nresource \"aws_lb\" \"main\" {\n  name = \"web-alb\"\n  load_balancer_type = \"application\"\n  subnets = aws_subnet.public.*.id\n  security_groups = [aws_security_group.lb.id]\n  \n  enable_deletion_protection = true\n  enable_http2 = true\n}\n\nresource \"aws_lb_target_group\" \"web\" {\n  name = \"web-tg\"\n  port = 80\n  protocol = \"HTTP\"\n  vpc_id = aws_vpc.main.id\n  \n  health_check {\n    enabled = true\n    path = \"/health\"\n    interval = 30\n    timeout = 5\n    healthy_threshold = 2\n    unhealthy_threshold = 2\n    matcher = \"200-299\"\n  }\n  \n  stickiness {\n    type = \"lb_cookie\"\n    cookie_duration = 86400\n    enabled = true\n  }\n}\n\nresource \"aws_lb_listener\" \"web\" {\n  load_balancer_arn = aws_lb.main.arn\n  port = \"443\"\n  protocol = \"HTTPS\"\n  ssl_policy = \"ELBSecurityPolicy-2016-08\"\n  certificate_arn = aws_acm_certificate.main.arn\n  \n  default_action {\n    type = \"forward\"\n    target_group_arn = aws_lb_target_group.web.arn\n  }\n}\n\n# HAProxy Configuration with Advanced Features\nfrontend http_front\n    bind *:80\n    bind *:443 ssl crt /etc/ssl/certs/example.com.pem\n    \n    # ACL-based routing\n    acl is_api path_beg /api/\n    acl is_static path_beg /static/\n    acl is_admin hdr(host) -i admin.example.com\n    \n    use_backend api_servers if is_api\n    use_backend static_servers if is_static\n    use_backend admin_servers if is_admin\n    default_backend web_servers\n\nbackend web_servers\n    balance roundrobin\n    option httpchk GET /health\n    http-check expect status 200\n    \n    server web1 192.168.1.10:8080 check inter 5s fall 3 rise 2\n    server web2 192.168.1.11:8080 check inter 5s fall 3 rise 2\n    server web3 192.168.1.12:8080 check inter 5s fall 3 rise 2\n\nbackend api_servers\n    balance leastconn\n    option httpchk GET /api/health\n    server api1 192.168.1.20:8080 check\n    server api2 192.168.1.21:8080 check"
    },
    {
      "id": 4,
      "question": "What is caching and what are different caching strategies?",
      "answer": "Caching stores frequently accessed data in fast storage to reduce latency and database load.\n\nCache Levels:\n• Client-side: Browser cache, localStorage\n• CDN: Content Delivery Network for static assets\n• Application: Redis, Memcached for API responses\n• Database: Query cache, result cache\n\nCaching Strategies:\n• Cache-Aside (Lazy Loading): App checks cache, loads from DB on miss\n• Write-Through: Write to cache and DB simultaneously\n• Write-Behind (Write-Back): Write to cache, async write to DB\n• Refresh-Ahead: Preemptively refresh before expiration\n\nEviction Policies:\n• LRU (Least Recently Used): Remove oldest accessed items\n• LFU (Least Frequently Used): Remove least accessed items\n• FIFO (First In First Out): Remove oldest items by time\n• TTL (Time To Live): Auto-expire after time period\n\nCache Invalidation:\n• Time-based: Set expiration time\n• Event-based: Invalidate on data changes\n• Manual: Explicit cache clear\n\nCommon Pitfalls:\n• Cache stampede: Multiple requests hit DB on cache miss\n• Stale data: Cache not updated when DB changes\n• Hot keys: Popular items overwhelming cache nodes",
      "explanation": "Caching improves performance by storing frequently accessed data in faster storage layers, using strategies like cache-aside and write-through with eviction policies like LRU.",
      "difficulty": "Easy",
      "code": "// Cache-Aside Pattern (Most Common)\nconst redis = require('redis');\nconst client = redis.createClient();\n\nasync function getUserCacheAside(userId) {\n  // 1. Try to get from cache\n  const cached = await client.get(`user:${userId}`);\n  if (cached) {\n    console.log('Cache hit');\n    return JSON.parse(cached);\n  }\n  \n  // 2. Cache miss - load from database\n  console.log('Cache miss - loading from DB');\n  const user = await db.users.findById(userId);\n  \n  // 3. Store in cache for future requests\n  await client.setEx(\n    `user:${userId}`,\n    3600, // TTL: 1 hour\n    JSON.stringify(user)\n  );\n  \n  return user;\n}\n\n// Write-Through Pattern\nasync function updateUserWriteThrough(userId, data) {\n  // 1. Update database\n  const user = await db.users.update(userId, data);\n  \n  // 2. Update cache immediately (synchronous)\n  await client.setEx(\n    `user:${userId}`,\n    3600,\n    JSON.stringify(user)\n  );\n  \n  return user;\n}\n\n// Write-Behind Pattern\nconst writeQueue = [];\n\nasync function updateUserWriteBehind(userId, data) {\n  // 1. Update cache immediately\n  await client.setEx(\n    `user:${userId}`,\n    3600,\n    JSON.stringify(data)\n  );\n  \n  // 2. Queue database write (asynchronous)\n  writeQueue.push({ userId, data });\n  \n  return data;\n}\n\n// Periodic flush to database\nsetInterval(async () => {\n  while (writeQueue.length > 0) {\n    const { userId, data } = writeQueue.shift();\n    await db.users.update(userId, data);\n  }\n}, 5000); // Flush every 5 seconds\n\n// Preventing Cache Stampede\nconst ongoingRequests = new Map();\n\nasync function getUserWithStampedeProtection(userId) {\n  const cacheKey = `user:${userId}`;\n  \n  // Check cache\n  const cached = await client.get(cacheKey);\n  if (cached) return JSON.parse(cached);\n  \n  // Check if request already in progress\n  if (ongoingRequests.has(cacheKey)) {\n    return await ongoingRequests.get(cacheKey);\n  }\n  \n  // Create new request promise\n  const promise = (async () => {\n    try {\n      const user = await db.users.findById(userId);\n      await client.setEx(cacheKey, 3600, JSON.stringify(user));\n      return user;\n    } finally {\n      ongoingRequests.delete(cacheKey);\n    }\n  })();\n  \n  ongoingRequests.set(cacheKey, promise);\n  return await promise;\n}\n\n// Multi-Level Caching\nclass MultiLevelCache {\n  constructor() {\n    this.l1Cache = new Map(); // In-memory (fast, small)\n    this.l2Cache = redis.createClient(); // Redis (slower, larger)\n  }\n  \n  async get(key) {\n    // Try L1 cache\n    if (this.l1Cache.has(key)) {\n      return this.l1Cache.get(key);\n    }\n    \n    // Try L2 cache\n    const l2Value = await this.l2Cache.get(key);\n    if (l2Value) {\n      // Promote to L1\n      this.l1Cache.set(key, l2Value);\n      return l2Value;\n    }\n    \n    // Cache miss\n    return null;\n  }\n  \n  async set(key, value, ttl) {\n    // Write to both levels\n    this.l1Cache.set(key, value);\n    await this.l2Cache.setEx(key, ttl, value);\n  }\n}\n\n// LRU Cache Implementation\nclass LRUCache {\n  constructor(capacity) {\n    this.capacity = capacity;\n    this.cache = new Map();\n  }\n  \n  get(key) {\n    if (!this.cache.has(key)) return null;\n    \n    // Move to end (most recently used)\n    const value = this.cache.get(key);\n    this.cache.delete(key);\n    this.cache.set(key, value);\n    return value;\n  }\n  \n  set(key, value) {\n    // Remove if exists\n    if (this.cache.has(key)) {\n      this.cache.delete(key);\n    }\n    \n    // Remove oldest if at capacity\n    if (this.cache.size >= this.capacity) {\n      const firstKey = this.cache.keys().next().value;\n      this.cache.delete(firstKey);\n    }\n    \n    this.cache.set(key, value);\n  }\n}\n\n// Cache warming on startup\nasync function warmCache() {\n  const popularUsers = await db.users.find({ popular: true });\n  \n  for (const user of popularUsers) {\n    await client.setEx(\n      `user:${user.id}`,\n      3600,\n      JSON.stringify(user)\n    );\n  }\n  \n  console.log(`Warmed cache with ${popularUsers.length} users`);\n}"
    },
    {
      "id": 5,
      "question": "What is a CDN (Content Delivery Network) and how does it work?",
      "answer": "A CDN is a distributed network of servers that delivers content to users from the geographically closest server.\n\nHow CDN Works:\n• User requests content from website\n• DNS resolves to nearest CDN edge server\n• Edge server serves cached content if available\n• If not cached, edge fetches from origin server\n• Content is cached at edge for future requests\n\nCDN Architecture:\n• Edge Servers: Distributed worldwide, serve users\n• Origin Server: Your main server with original content\n• PoP (Point of Presence): CDN data center locations\n• Routing: Anycast or GeoDNS to find nearest edge\n\nContent Types:\n• Static: Images, CSS, JS, videos (perfect for CDN)\n• Dynamic: API responses, personalized content (can use CDN with care)\n• Streaming: Video/audio with adaptive bitrate\n\nBenefits:\n• Reduced latency (geographic proximity)\n• Reduced bandwidth costs on origin server\n• DDoS protection and security\n• High availability and redundancy\n• Improved SEO (faster load times)\n\nCDN Features:\n• Cache purging and invalidation\n• SSL/TLS termination\n• Image optimization and compression\n• Edge computing (run code at edge)\n• Real-time analytics",
      "explanation": "CDNs cache and serve content from servers closest to users, dramatically reducing latency, bandwidth costs, and improving availability by distributing load globally.",
      "difficulty": "Easy",
      "code": "// CloudFront CDN Configuration (AWS)\nresource \"aws_cloudfront_distribution\" \"main\" {\n  enabled = true\n  is_ipv6_enabled = true\n  default_root_object = \"index.html\"\n  \n  # Origin - Your main server\n  origin {\n    domain_name = \"origin.example.com\"\n    origin_id = \"myOrigin\"\n    \n    custom_origin_config {\n      http_port = 80\n      https_port = 443\n      origin_protocol_policy = \"https-only\"\n      origin_ssl_protocols = [\"TLSv1.2\"]\n    }\n    \n    # Custom headers to identify CDN requests\n    custom_header {\n      name = \"X-CDN-Secret\"\n      value = \"secret-token-12345\"\n    }\n  }\n  \n  # Cache behavior for static assets\n  ordered_cache_behavior {\n    path_pattern = \"/static/*\"\n    target_origin_id = \"myOrigin\"\n    viewer_protocol_policy = \"redirect-to-https\"\n    \n    # Cache settings\n    min_ttl = 0\n    default_ttl = 86400      # 1 day\n    max_ttl = 31536000       # 1 year\n    compress = true\n    \n    # Allowed methods\n    allowed_methods = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    cached_methods = [\"GET\", \"HEAD\"]\n    \n    forwarded_values {\n      query_string = false\n      cookies {\n        forward = \"none\"\n      }\n      headers = [\"Origin\", \"Access-Control-Request-Method\"]\n    }\n  }\n  \n  # Cache behavior for API (minimal caching)\n  ordered_cache_behavior {\n    path_pattern = \"/api/*\"\n    target_origin_id = \"myOrigin\"\n    viewer_protocol_policy = \"https-only\"\n    \n    min_ttl = 0\n    default_ttl = 0\n    max_ttl = 0\n    \n    allowed_methods = [\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\", \"PATCH\", \"POST\", \"PUT\"]\n    cached_methods = [\"GET\", \"HEAD\"]\n    \n    forwarded_values {\n      query_string = true\n      cookies {\n        forward = \"all\"\n      }\n      headers = [\"*\"]\n    }\n  }\n  \n  # Geographic restrictions\n  restrictions {\n    geo_restriction {\n      restriction_type = \"none\"\n    }\n  }\n  \n  # SSL certificate\n  viewer_certificate {\n    acm_certificate_arn = aws_acm_certificate.main.arn\n    ssl_support_method = \"sni-only\"\n    minimum_protocol_version = \"TLSv1.2_2021\"\n  }\n  \n  # Custom error responses\n  custom_error_response {\n    error_code = 404\n    response_code = 200\n    response_page_path = \"/404.html\"\n    error_caching_min_ttl = 300\n  }\n}\n\n// Cache invalidation\nconst AWS = require('aws-sdk');\nconst cloudfront = new AWS.CloudFront();\n\nasync function invalidateCDNCache(paths) {\n  const params = {\n    DistributionId: 'E1234EXAMPLE',\n    InvalidationBatch: {\n      CallerReference: Date.now().toString(),\n      Paths: {\n        Quantity: paths.length,\n        Items: paths // ['/index.html', '/static/*']\n      }\n    }\n  };\n  \n  const result = await cloudfront.createInvalidation(params).promise();\n  console.log('Invalidation created:', result.Invalidation.Id);\n}\n\n// Using CDN with versioned assets\n// Instead of invalidation, use versioned URLs\nconst assets = {\n  css: '/static/style.v123.css',\n  js: '/static/app.v456.js',\n  logo: '/static/logo.v789.png'\n};\n\n// Generate HTML with versioned assets\nfunction generateHTML() {\n  return `\n    <!DOCTYPE html>\n    <html>\n      <head>\n        <link rel=\"stylesheet\" href=\"${assets.css}\">\n      </head>\n      <body>\n        <img src=\"${assets.logo}\" alt=\"Logo\">\n        <script src=\"${assets.js}\"></script>\n      </body>\n    </html>\n  `;\n}\n\n// Cloudflare CDN with Workers (Edge Computing)\n// Runs JavaScript at CDN edge servers\naddEventListener('fetch', event => {\n  event.respondWith(handleRequest(event.request));\n});\n\nasync function handleRequest(request) {\n  const url = new URL(request.url);\n  \n  // Edge caching logic\n  if (url.pathname.startsWith('/api/')) {\n    // Bypass cache for API\n    return fetch(request, { cf: { cacheTtl: 0 } });\n  }\n  \n  // Check cache\n  const cache = caches.default;\n  let response = await cache.match(request);\n  \n  if (!response) {\n    // Fetch from origin\n    response = await fetch(request);\n    \n    // Cache static assets\n    if (url.pathname.startsWith('/static/')) {\n      response = new Response(response.body, response);\n      response.headers.set('Cache-Control', 'public, max-age=86400');\n      await cache.put(request, response.clone());\n    }\n  }\n  \n  return response;\n}\n\n// Monitoring CDN performance\nfunction logCDNMetrics() {\n  // Check PerformanceNavigationTiming API\n  const perfData = performance.getEntriesByType('navigation')[0];\n  \n  console.log('CDN Metrics:', {\n    dns: perfData.domainLookupEnd - perfData.domainLookupStart,\n    tcp: perfData.connectEnd - perfData.connectStart,\n    ttfb: perfData.responseStart - perfData.requestStart,\n    download: perfData.responseEnd - perfData.responseStart,\n    total: perfData.loadEventEnd - perfData.fetchStart\n  });\n}"
    },
    {
      "id": 6,
      "question": "What is database sharding and when should you use it?",
      "answer": "Sharding is a database partitioning technique that splits data across multiple database instances to improve scalability and performance.\n\nSharding Strategies:\n• Horizontal Sharding: Split rows across shards based on key\n• Vertical Sharding: Split columns/tables across shards\n• Hash-based: Use hash function on shard key (user_id % num_shards)\n• Range-based: Partition by value ranges (A-M on shard1, N-Z on shard2)\n• Geographic: Partition by region/location\n• Directory-based: Lookup table maps keys to shards\n\nShard Key Selection:\n• High cardinality: Many unique values\n• Even distribution: Avoid hot spots\n• Query-friendly: Minimize cross-shard queries\n• Stable: Rarely changes to avoid resharding\n\nChallenges:\n• Cross-shard queries are expensive\n• Distributed transactions complexity\n• Resharding is difficult\n• Uneven data distribution (hotspots)\n• Application complexity increases\n\nWhen to Shard:\n• Data size exceeds single server capacity\n• Write throughput hits limits\n• Read replicas insufficient\n• Geographic data locality needed\n\nAlternatives to Consider First:\n• Vertical scaling\n• Read replicas\n• Better indexing\n• Caching layers",
      "explanation": "Sharding splits a database horizontally across multiple servers to scale beyond single-server limits, but introduces complexity in queries, transactions, and data distribution.",
      "difficulty": "Medium",
      "code": "// Hash-based Sharding Implementation\nclass ShardedDatabase {\n  constructor(shards) {\n    this.shards = shards; // Array of database connections\n    this.numShards = shards.length;\n  }\n  \n  // Determine shard using hash\n  getShardIndex(userId) {\n    // Simple modulo hash\n    return this.hashCode(userId) % this.numShards;\n  }\n  \n  hashCode(str) {\n    let hash = 0;\n    for (let i = 0; i < str.length; i++) {\n      hash = ((hash << 5) - hash) + str.charCodeAt(i);\n      hash = hash & hash; // Convert to 32-bit integer\n    }\n    return Math.abs(hash);\n  }\n  \n  async getUserById(userId) {\n    const shardIndex = this.getShardIndex(userId);\n    const db = this.shards[shardIndex];\n    return await db.query('SELECT * FROM users WHERE id = ?', [userId]);\n  }\n  \n  async createUser(user) {\n    const shardIndex = this.getShardIndex(user.id);\n    const db = this.shards[shardIndex];\n    return await db.query('INSERT INTO users SET ?', user);\n  }\n  \n  // Cross-shard query (expensive!)\n  async getAllUsers() {\n    const promises = this.shards.map(db => \n      db.query('SELECT * FROM users')\n    );\n    const results = await Promise.all(promises);\n    return results.flat();\n  }\n}\n\n// Range-based Sharding\nclass RangeShardedDatabase {\n  constructor() {\n    this.shardRanges = [\n      { min: 'A', max: 'F', db: db1 },\n      { min: 'G', max: 'M', db: db2 },\n      { min: 'N', max: 'S', db: db3 },\n      { min: 'T', max: 'Z', db: db4 }\n    ];\n  }\n  \n  getShardByName(name) {\n    const firstChar = name[0].toUpperCase();\n    for (const range of this.shardRanges) {\n      if (firstChar >= range.min && firstChar <= range.max) {\n        return range.db;\n      }\n    }\n    throw new Error('No shard found for name: ' + name);\n  }\n  \n  async getUserByName(name) {\n    const db = this.getShardByName(name);\n    return await db.query('SELECT * FROM users WHERE name = ?', [name]);\n  }\n}\n\n// Geographic Sharding\nclass GeoShardedDatabase {\n  constructor() {\n    this.shardMap = {\n      'US': { region: 'us-east-1', db: usDb },\n      'EU': { region: 'eu-west-1', db: euDb },\n      'ASIA': { region: 'ap-south-1', db: asiaDb }\n    };\n  }\n  \n  getShardByRegion(region) {\n    return this.shardMap[region]?.db || this.shardMap['US'].db;\n  }\n  \n  async getUserById(userId, region) {\n    const db = this.getShardByRegion(region);\n    return await db.query('SELECT * FROM users WHERE id = ?', [userId]);\n  }\n}\n\n// Consistent Hashing (for resharding)\nclass ConsistentHashSharding {\n  constructor(shards, virtualNodes = 150) {\n    this.ring = new Map();\n    this.shards = shards;\n    this.virtualNodes = virtualNodes;\n    this.initRing();\n  }\n  \n  initRing() {\n    // Create virtual nodes for better distribution\n    for (const shard of this.shards) {\n      for (let i = 0; i < this.virtualNodes; i++) {\n        const virtualKey = `${shard.name}:${i}`;\n        const hash = this.hash(virtualKey);\n        this.ring.set(hash, shard);\n      }\n    }\n    // Sort ring by hash values\n    this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n  }\n  \n  hash(key) {\n    // Use better hash function in production (murmurhash, etc.)\n    let hash = 0;\n    for (let i = 0; i < key.length; i++) {\n      hash = ((hash << 5) - hash) + key.charCodeAt(i);\n    }\n    return Math.abs(hash);\n  }\n  \n  getShard(key) {\n    const keyHash = this.hash(key);\n    \n    // Find first node with hash >= keyHash\n    for (const nodeHash of this.sortedKeys) {\n      if (nodeHash >= keyHash) {\n        return this.ring.get(nodeHash);\n      }\n    }\n    \n    // Wrap around to first node\n    return this.ring.get(this.sortedKeys[0]);\n  }\n  \n  addShard(shard) {\n    this.shards.push(shard);\n    // Add virtual nodes for new shard\n    for (let i = 0; i < this.virtualNodes; i++) {\n      const virtualKey = `${shard.name}:${i}`;\n      const hash = this.hash(virtualKey);\n      this.ring.set(hash, shard);\n    }\n    this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n  }\n}\n\n// Vitess-style Sharding (YouTube's approach)\nconst vitessConfig = {\n  keyspace: 'users',\n  shards: [\n    { name: '-40', range: { start: '', end: '40' } },\n    { name: '40-80', range: { start: '40', end: '80' } },\n    { name: '80-c0', range: { start: '80', end: 'c0' } },\n    { name: 'c0-', range: { start: 'c0', end: '' } }\n  ],\n  vschema: {\n    tables: {\n      users: {\n        column_vindexes: [\n          { column: 'user_id', name: 'hash' }\n        ]\n      },\n      orders: {\n        column_vindexes: [\n          { column: 'user_id', name: 'hash' }\n        ]\n      }\n    }\n  }\n};\n\n// Handling Distributed Transactions Across Shards\nasync function transferFunds(fromUserId, toUserId, amount) {\n  const fromShard = getShardIndex(fromUserId);\n  const toShard = getShardIndex(toUserId);\n  \n  if (fromShard === toShard) {\n    // Same shard - simple transaction\n    const db = shards[fromShard];\n    await db.transaction(async (trx) => {\n      await trx('accounts').where({ user_id: fromUserId }).decrement('balance', amount);\n      await trx('accounts').where({ user_id: toUserId }).increment('balance', amount);\n    });\n  } else {\n    // Cross-shard - use 2-phase commit or saga pattern\n    try {\n      // Phase 1: Prepare\n      await shards[fromShard].query('BEGIN');\n      await shards[toShard].query('BEGIN');\n      \n      await shards[fromShard].query(\n        'UPDATE accounts SET balance = balance - ? WHERE user_id = ?',\n        [amount, fromUserId]\n      );\n      \n      await shards[toShard].query(\n        'UPDATE accounts SET balance = balance + ? WHERE user_id = ?',\n        [amount, toUserId]\n      );\n      \n      // Phase 2: Commit\n      await shards[fromShard].query('COMMIT');\n      await shards[toShard].query('COMMIT');\n    } catch (error) {\n      // Rollback both\n      await shards[fromShard].query('ROLLBACK');\n      await shards[toShard].query('ROLLBACK');\n      throw error;\n    }\n  }\n}"
    },
    {
      "id": 7,
      "question": "What is database replication and what are different replication strategies?",
      "answer": "Database replication copies data from one database server (primary) to one or more servers (replicas) for redundancy, scalability, and availability.\n\nReplication Types:\n• Master-Slave (Primary-Replica): One write node, multiple read nodes\n• Master-Master (Multi-Master): Multiple nodes accept writes\n• Synchronous: Wait for replica confirmation (slower, consistent)\n• Asynchronous: Don't wait for replicas (faster, eventual consistency)\n• Semi-Synchronous: Wait for at least one replica\n\nReplication Methods:\n• Statement-based: Replay SQL statements on replicas\n• Row-based: Copy actual changed rows\n• Mixed: Combination of statement and row-based\n\nUse Cases:\n• Read scaling: Route reads to replicas\n• High availability: Failover to replica if primary fails\n• Backup: Use replica for backups without affecting primary\n• Analytics: Run reports on replica\n• Geographic distribution: Replicas in multiple regions\n\nChallenges:\n• Replication lag: Replicas behind primary\n• Write conflicts in multi-master\n• Failover complexity\n• Data consistency guarantees\n\nBest Practices:\n• Monitor replication lag\n• Use connection pools with read/write splitting\n• Handle replication lag in application\n• Plan failover procedures\n• Test disaster recovery",
      "explanation": "Database replication creates copies of data on multiple servers to improve read scalability, availability, and disaster recovery, with trade-offs between consistency and performance.",
      "difficulty": "Medium",
      "code": "# MySQL Master-Slave Replication Setup\n\n# On Master Server (my.cnf)\n[mysqld]\nserver-id = 1\nlog-bin = mysql-bin\nbinlog-format = ROW\nbinlog-do-db = myapp_production\nautocomplicate-on-fail = 1\n\n# Create replication user\nCREATE USER 'replicator'@'%' IDENTIFIED BY 'strong_password';\nGRANT REPLICATION SLAVE ON *.* TO 'replicator'@'%';\nFLUSH PRIVILEGES;\n\n# Get master status\nSHOW MASTER STATUS;\n# Note the File and Position values\n\n# On Slave Server (my.cnf)\n[mysqld]\nserver-id = 2\nrelay-log = mysql-relay-bin\nread_only = 1\nlog-bin = mysql-bin\nbinlog-format = ROW\n\n# Configure slave\nCHANGE MASTER TO\n  MASTER_HOST='master-db.example.com',\n  MASTER_USER='replicator',\n  MASTER_PASSWORD='strong_password',\n  MASTER_LOG_FILE='mysql-bin.000001',\n  MASTER_LOG_POS=107;\n\nSTART SLAVE;\n\n# Check slave status\nSHOW SLAVE STATUS\\G\n\n# PostgreSQL Streaming Replication Setup\n# On Primary Server (postgresql.conf)\nwal_level = replica\nmax_wal_senders = 10\nwal_keep_size = 1GB\narchive_mode = on\narchive_command = 'cp %p /var/lib/postgresql/wal_archive/%f'\n\n# Create replication user (pg_hba.conf)\nhost replication replicator 10.0.0.0/8 md5\n\n# SQL command\nCREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'strong_password';\n\n# On Replica Server\n# Create standby.signal file\ntouch /var/lib/postgresql/data/standby.signal\n\n# postgresql.conf\nprimary_conninfo = 'host=primary-db.example.com port=5432 user=replicator password=strong_password'\nrestore_command = 'cp /var/lib/postgresql/wal_archive/%f %p'\nhot_standby = on\n\n// Application-Level Read/Write Splitting\nconst { Pool } = require('pg');\n\nclass DatabasePool {\n  constructor() {\n    this.primary = new Pool({\n      host: 'primary-db.example.com',\n      database: 'myapp',\n      user: 'app_user',\n      password: 'password',\n      max: 20\n    });\n    \n    this.replicas = [\n      new Pool({\n        host: 'replica1-db.example.com',\n        database: 'myapp',\n        user: 'app_user',\n        password: 'password',\n        max: 20\n      }),\n      new Pool({\n        host: 'replica2-db.example.com',\n        database: 'myapp',\n        user: 'app_user',\n        password: 'password',\n        max: 20\n      })\n    ];\n    \n    this.replicaIndex = 0;\n  }\n  \n  // Write to primary\n  async write(query, params) {\n    return await this.primary.query(query, params);\n  }\n  \n  // Read from replica (round-robin)\n  async read(query, params) {\n    const replica = this.replicas[this.replicaIndex];\n    this.replicaIndex = (this.replicaIndex + 1) % this.replicas.length;\n    \n    try {\n      return await replica.query(query, params);\n    } catch (error) {\n      // Fallback to primary if replica fails\n      console.log('Replica failed, using primary');\n      return await this.primary.query(query, params);\n    }\n  }\n  \n  // Transaction must use primary\n  async transaction(callback) {\n    const client = await this.primary.connect();\n    try {\n      await client.query('BEGIN');\n      const result = await callback(client);\n      await client.query('COMMIT');\n      return result;\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n}\n\nconst db = new DatabasePool();\n\n// Usage\nasync function getUser(userId) {\n  // Read from replica\n  return await db.read('SELECT * FROM users WHERE id = $1', [userId]);\n}\n\nasync function createUser(user) {\n  // Write to primary\n  return await db.write(\n    'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING *',\n    [user.name, user.email]\n  );\n}\n\n// Handling Read-After-Write Consistency\nclass ConsistentReadPool extends DatabasePool {\n  constructor() {\n    super();\n    this.recentWrites = new Map(); // Track recent writes per user\n  }\n  \n  async write(query, params, userId) {\n    const result = await this.primary.query(query, params);\n    \n    // Mark write timestamp for this user\n    if (userId) {\n      this.recentWrites.set(userId, Date.now());\n    }\n    \n    return result;\n  }\n  \n  async read(query, params, userId) {\n    // Check if user recently wrote\n    if (userId && this.recentWrites.has(userId)) {\n      const writeTime = this.recentWrites.get(userId);\n      const elapsed = Date.now() - writeTime;\n      \n      // If write was recent (< 1s), read from primary\n      if (elapsed < 1000) {\n        return await this.primary.query(query, params);\n      }\n      \n      // Clean up old entries\n      this.recentWrites.delete(userId);\n    }\n    \n    // Normal replica read\n    return await super.read(query, params);\n  }\n}\n\n// Monitoring Replication Lag\nasync function checkReplicationLag() {\n  // PostgreSQL\n  const result = await replica.query(`\n    SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) AS lag_seconds\n  `);\n  \n  const lagSeconds = result.rows[0].lag_seconds;\n  \n  if (lagSeconds > 60) {\n    console.error(`High replication lag: ${lagSeconds}s`);\n    // Alert operations team\n  }\n  \n  return lagSeconds;\n}\n\n// MySQL replication lag check\nasync function checkMySQLReplicationLag() {\n  const result = await replica.query('SHOW SLAVE STATUS');\n  const secondsBehindMaster = result[0].Seconds_Behind_Master;\n  \n  if (secondsBehindMaster > 60) {\n    console.error(`High replication lag: ${secondsBehindMaster}s`);\n  }\n  \n  return secondsBehindMaster;\n}\n\n// Automatic Failover with ProxySQL\n# proxysql.cnf\nmysql_servers =\n(\n    { address=\"primary-db.example.com\", port=3306, hostgroup=0, max_connections=100 },\n    { address=\"replica1-db.example.com\", port=3306, hostgroup=1, max_connections=200 },\n    { address=\"replica2-db.example.com\", port=3306, hostgroup=1, max_connections=200 }\n)\n\nmysql_query_rules =\n(\n    { rule_id=1, active=1, match_pattern=\"^SELECT.*FOR UPDATE\", destination_hostgroup=0, apply=1 },\n    { rule_id=2, active=1, match_pattern=\"^SELECT\", destination_hostgroup=1, apply=1 }\n)"
    },
    {
      "id": 8,
      "question": "What is a message queue and when should you use it?",
      "answer": "A message queue is a form of asynchronous service-to-service communication that decouples producers and consumers of messages.\n\nCore Concepts:\n• Producer: Sends messages to queue\n• Consumer: Receives and processes messages\n• Queue: Stores messages temporarily\n• Message: Unit of work/data\n• Broker: Manages queues and message routing\n\nCommon Message Queue Systems:\n• RabbitMQ: Feature-rich, AMQP protocol, flexible routing\n• Apache Kafka: High throughput, log-based, stream processing\n• AWS SQS: Managed, simple, scalable\n• Redis: In-memory, simple pub/sub\n• Apache ActiveMQ: JMS-based, enterprise features\n\nUse Cases:\n• Async processing: Send emails, generate reports\n• Load leveling: Smooth traffic spikes\n• Decoupling services: Microservices communication\n• Reliable delivery: Guarantee message processing\n• Event-driven architecture: React to domain events\n• Task distribution: Distribute work across workers\n\nMessage Queue Patterns:\n• Point-to-Point: One producer, one consumer\n• Publish-Subscribe: One producer, multiple consumers\n• Request-Reply: Synchronous-like async communication\n• Priority Queue: Process high-priority messages first\n• Dead Letter Queue: Handle failed messages\n\nBenefits:\n• Decoupling: Services don't need to know about each other\n• Scalability: Add consumers independently\n• Reliability: Messages persist until processed\n• Buffering: Handle traffic spikes\n• Async processing: Don't block user requests",
      "explanation": "Message queues enable asynchronous communication between services by temporarily storing messages, improving scalability, reliability, and decoupling in distributed systems.",
      "difficulty": "Medium",
      "code": "// RabbitMQ Producer\nconst amqp = require('amqplib');\n\nclass RabbitMQProducer {\n  async connect() {\n    this.connection = await amqp.connect('amqp://localhost');\n    this.channel = await this.connection.createChannel();\n  }\n  \n  async sendToQueue(queueName, message) {\n    // Ensure queue exists\n    await this.channel.assertQueue(queueName, {\n      durable: true // Persist queue across restarts\n    });\n    \n    // Send message\n    this.channel.sendToQueue(\n      queueName,\n      Buffer.from(JSON.stringify(message)),\n      {\n        persistent: true, // Persist message to disk\n        contentType: 'application/json'\n      }\n    );\n    \n    console.log('Message sent:', message);\n  }\n  \n  async publishToExchange(exchange, routingKey, message) {\n    await this.channel.assertExchange(exchange, 'topic', {\n      durable: true\n    });\n    \n    this.channel.publish(\n      exchange,\n      routingKey,\n      Buffer.from(JSON.stringify(message)),\n      { persistent: true }\n    );\n  }\n  \n  async close() {\n    await this.channel.close();\n    await this.connection.close();\n  }\n}\n\n// RabbitMQ Consumer\nclass RabbitMQConsumer {\n  async connect() {\n    this.connection = await amqp.connect('amqp://localhost');\n    this.channel = await this.connection.createChannel();\n  }\n  \n  async consume(queueName, handler) {\n    await this.channel.assertQueue(queueName, { durable: true });\n    \n    // Prefetch: Only process one message at a time per worker\n    this.channel.prefetch(1);\n    \n    this.channel.consume(queueName, async (msg) => {\n      if (msg !== null) {\n        try {\n          const content = JSON.parse(msg.content.toString());\n          console.log('Processing message:', content);\n          \n          // Process message\n          await handler(content);\n          \n          // Acknowledge successful processing\n          this.channel.ack(msg);\n        } catch (error) {\n          console.error('Message processing failed:', error);\n          \n          // Reject and requeue if processing fails\n          this.channel.nack(msg, false, true);\n        }\n      }\n    });\n  }\n}\n\n// Usage Example\nconst producer = new RabbitMQProducer();\nconst consumer = new RabbitMQConsumer();\n\nawait producer.connect();\nawait consumer.connect();\n\n// Producer: Send email task\nawait producer.sendToQueue('email-queue', {\n  to: 'user@example.com',\n  subject: 'Welcome!',\n  body: 'Thank you for signing up'\n});\n\n// Consumer: Process emails\nawait consumer.consume('email-queue', async (message) => {\n  await sendEmail(message.to, message.subject, message.body);\n});\n\n// Apache Kafka Producer\nconst { Kafka } = require('kafkajs');\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['kafka1:9092', 'kafka2:9092', 'kafka3:9092']\n});\n\nconst producer = kafka.producer();\n\nawait producer.connect();\n\n// Send message\nawait producer.send({\n  topic: 'user-events',\n  messages: [\n    {\n      key: 'user-123', // Partition key\n      value: JSON.stringify({\n        event: 'user.created',\n        userId: '123',\n        email: 'user@example.com',\n        timestamp: Date.now()\n      }),\n      headers: {\n        'correlation-id': 'abc-123'\n      }\n    }\n  ]\n});\n\n// Kafka Consumer\nconst consumer = kafka.consumer({ groupId: 'user-service' });\n\nawait consumer.connect();\nawait consumer.subscribe({ topic: 'user-events', fromBeginning: false });\n\nawait consumer.run({\n  eachMessage: async ({ topic, partition, message }) => {\n    const event = JSON.parse(message.value.toString());\n    console.log('Received event:', event);\n    \n    // Process event\n    if (event.event === 'user.created') {\n      await sendWelcomeEmail(event.userId);\n    }\n  }\n});\n\n// AWS SQS Example\nconst { SQSClient, SendMessageCommand, ReceiveMessageCommand, DeleteMessageCommand } = require('@aws-sdk/client-sqs');\n\nconst sqs = new SQSClient({ region: 'us-east-1' });\n\n// Send message\nasync function sendToSQS(queueUrl, message) {\n  const command = new SendMessageCommand({\n    QueueUrl: queueUrl,\n    MessageBody: JSON.stringify(message),\n    DelaySeconds: 0,\n    MessageAttributes: {\n      'Priority': {\n        DataType: 'String',\n        StringValue: 'high'\n      }\n    }\n  });\n  \n  const result = await sqs.send(command);\n  console.log('Message sent:', result.MessageId);\n}\n\n// Receive and process messages\nasync function processSQSMessages(queueUrl) {\n  const command = new ReceiveMessageCommand({\n    QueueUrl: queueUrl,\n    MaxNumberOfMessages: 10,\n    WaitTimeSeconds: 20, // Long polling\n    VisibilityTimeout: 30\n  });\n  \n  const result = await sqs.send(command);\n  \n  if (result.Messages) {\n    for (const message of result.Messages) {\n      try {\n        const body = JSON.parse(message.Body);\n        console.log('Processing:', body);\n        \n        // Process message\n        await handleMessage(body);\n        \n        // Delete message after successful processing\n        await sqs.send(new DeleteMessageCommand({\n          QueueUrl: queueUrl,\n          ReceiptHandle: message.ReceiptHandle\n        }));\n      } catch (error) {\n        console.error('Processing failed:', error);\n        // Message will become visible again after VisibilityTimeout\n      }\n    }\n  }\n}\n\n// Dead Letter Queue Pattern\nconst mainQueue = 'https://sqs.us-east-1.amazonaws.com/123456789/main-queue';\nconst dlq = 'https://sqs.us-east-1.amazonaws.com/123456789/main-queue-dlq';\n\n// Configure redrive policy on main queue\nconst redrivePolicy = {\n  deadLetterTargetArn: 'arn:aws:sqs:us-east-1:123456789:main-queue-dlq',\n  maxReceiveCount: 3 // Move to DLQ after 3 failures\n};\n\n// Priority Queue Implementation\nclass PriorityQueue {\n  constructor() {\n    this.highPriority = [];\n    this.normalPriority = [];\n    this.lowPriority = [];\n  }\n  \n  enqueue(message, priority = 'normal') {\n    switch (priority) {\n      case 'high':\n        this.highPriority.push(message);\n        break;\n      case 'low':\n        this.lowPriority.push(message);\n        break;\n      default:\n        this.normalPriority.push(message);\n    }\n  }\n  \n  dequeue() {\n    if (this.highPriority.length > 0) {\n      return this.highPriority.shift();\n    }\n    if (this.normalPriority.length > 0) {\n      return this.normalPriority.shift();\n    }\n    if (this.lowPriority.length > 0) {\n      return this.lowPriority.shift();\n    }\n    return null;\n  }\n}"
    },
    {
      "id": 9,
      "question": "What is API Gateway and what are its key features?",
      "answer": "An API Gateway is a server that acts as a single entry point for all client requests to backend microservices, handling routing, composition, and protocol translation.\n\nCore Functions:\n• Request routing: Direct requests to appropriate services\n• Protocol translation: HTTP to gRPC, WebSocket, etc.\n• Request/response transformation: Modify payloads\n• Authentication & Authorization: Validate tokens, enforce policies\n• Rate limiting: Prevent abuse and overload\n• Load balancing: Distribute traffic across instances\n\nKey Features:\n• API Composition: Aggregate multiple service calls\n• Caching: Store responses to reduce backend load\n• Logging & Monitoring: Centralized observability\n• Circuit breaking: Prevent cascading failures\n• Service discovery: Find backend services dynamically\n• SSL termination: Handle HTTPS encryption\n\nCommon API Gateways:\n• Kong: Open-source, plugin-based, high performance\n• AWS API Gateway: Managed, serverless, scales automatically\n• NGINX: Lightweight, reverse proxy capabilities\n• Envoy: Modern, service mesh integration\n• Apigee: Enterprise, analytics, developer portal\n• Azure API Management: Microsoft cloud solution\n\nBenefits:\n• Simplified client code: Single endpoint\n• Cross-cutting concerns: Centralized security, logging\n• Backend flexibility: Change services without affecting clients\n• Protocol flexibility: Support multiple client types\n\nChallenges:\n• Single point of failure: Need redundancy\n• Performance bottleneck: Must be highly optimized\n• Complexity: Another component to manage\n• Configuration overhead: Routing rules, policies",
      "explanation": "API Gateway provides a unified entry point for microservices, handling routing, security, rate limiting, and protocol translation while abstracting backend complexity from clients.",
      "difficulty": "Medium",
      "code": "// Express-based API Gateway\nconst express = require('express');\nconst axios = require('axios');\nconst rateLimit = require('express-rate-limit');\nconst jwt = require('jsonwebtoken');\nconst { createProxyMiddleware } = require('http-proxy-middleware');\n\nconst app = express();\n\n// Service registry\nconst services = {\n  users: 'http://user-service:3001',\n  orders: 'http://order-service:3002',\n  products: 'http://product-service:3003'\n};\n\n// Authentication middleware\nfunction authenticate(req, res, next) {\n  const token = req.headers.authorization?.split(' ')[1];\n  \n  if (!token) {\n    return res.status(401).json({ error: 'No token provided' });\n  }\n  \n  try {\n    const decoded = jwt.verify(token, process.env.JWT_SECRET);\n    req.user = decoded;\n    next();\n  } catch (error) {\n    return res.status(401).json({ error: 'Invalid token' });\n  }\n}\n\n// Rate limiting\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100, // Limit each IP to 100 requests per windowMs\n  message: 'Too many requests, please try again later'\n});\n\napp.use(limiter);\n\n// Request logging\napp.use((req, res, next) => {\n  console.log(`${new Date().toISOString()} ${req.method} ${req.url}`);\n  next();\n});\n\n// Simple proxy routing\napp.use('/api/users', authenticate, createProxyMiddleware({\n  target: services.users,\n  changeOrigin: true,\n  pathRewrite: { '^/api/users': '' },\n  onProxyReq: (proxyReq, req) => {\n    // Add user context to backend request\n    proxyReq.setHeader('X-User-Id', req.user.id);\n  }\n}));\n\napp.use('/api/orders', authenticate, createProxyMiddleware({\n  target: services.orders,\n  changeOrigin: true,\n  pathRewrite: { '^/api/orders': '' }\n}));\n\napp.use('/api/products', createProxyMiddleware({\n  target: services.products,\n  changeOrigin: true,\n  pathRewrite: { '^/api/products': '' }\n}));\n\n// API Composition - Aggregate multiple services\napp.get('/api/user-dashboard/:userId', authenticate, async (req, res) => {\n  try {\n    const { userId } = req.params;\n    \n    // Fetch data from multiple services in parallel\n    const [user, orders, recommendations] = await Promise.all([\n      axios.get(`${services.users}/${userId}`),\n      axios.get(`${services.orders}/user/${userId}`),\n      axios.get(`${services.products}/recommendations/${userId}`)\n    ]);\n    \n    // Compose response\n    res.json({\n      user: user.data,\n      recentOrders: orders.data.slice(0, 5),\n      recommendations: recommendations.data\n    });\n  } catch (error) {\n    console.error('Dashboard aggregation failed:', error);\n    res.status(500).json({ error: 'Failed to load dashboard' });\n  }\n});\n\n// Response caching\nconst NodeCache = require('node-cache');\nconst cache = new NodeCache({ stdTTL: 300 }); // 5 minute TTL\n\nfunction cacheMiddleware(duration) {\n  return (req, res, next) => {\n    const key = req.originalUrl;\n    const cached = cache.get(key);\n    \n    if (cached) {\n      return res.json(cached);\n    }\n    \n    // Override res.json to cache the response\n    const originalJson = res.json.bind(res);\n    res.json = (body) => {\n      cache.set(key, body);\n      return originalJson(body);\n    };\n    \n    next();\n  };\n}\n\napp.get('/api/products', cacheMiddleware(300), createProxyMiddleware({\n  target: services.products,\n  changeOrigin: true\n}));\n\n// Circuit breaker pattern\nconst CircuitBreaker = require('opossum');\n\nconst options = {\n  timeout: 3000, // 3 seconds\n  errorThresholdPercentage: 50,\n  resetTimeout: 30000 // 30 seconds\n};\n\nconst makeRequest = (url) => axios.get(url);\nconst breaker = new CircuitBreaker(makeRequest, options);\n\nbreaker.fallback(() => ({\n  data: { message: 'Service temporarily unavailable' }\n}));\n\napp.get('/api/external-service', async (req, res) => {\n  try {\n    const result = await breaker.fire('http://external-api.com/data');\n    res.json(result.data);\n  } catch (error) {\n    res.status(503).json({ error: 'Service unavailable' });\n  }\n});\n\napp.listen(8080, () => {\n  console.log('API Gateway running on port 8080');\n});\n\n// Kong API Gateway Configuration\n// Create service\ncurl -i -X POST http://localhost:8001/services/ \\\n  --data name=user-service \\\n  --data url='http://user-service:3001'\n\n// Create route\ncurl -i -X POST http://localhost:8001/services/user-service/routes \\\n  --data 'paths[]=/api/users' \\\n  --data name=user-route\n\n// Add JWT authentication plugin\ncurl -i -X POST http://localhost:8001/services/user-service/plugins \\\n  --data name=jwt\n\n// Add rate limiting plugin\ncurl -i -X POST http://localhost:8001/services/user-service/plugins \\\n  --data name=rate-limiting \\\n  --data config.minute=100 \\\n  --data config.policy=local\n\n// Add request transformer plugin\ncurl -i -X POST http://localhost:8001/services/user-service/plugins \\\n  --data name=request-transformer \\\n  --data config.add.headers=X-Gateway-Version:1.0\n\n// AWS API Gateway with Terraform\nresource \"aws_api_gateway_rest_api\" \"main\" {\n  name = \"my-api-gateway\"\n  description = \"API Gateway for microservices\"\n}\n\nresource \"aws_api_gateway_resource\" \"users\" {\n  rest_api_id = aws_api_gateway_rest_api.main.id\n  parent_id = aws_api_gateway_rest_api.main.root_resource_id\n  path_part = \"users\"\n}\n\nresource \"aws_api_gateway_method\" \"users_get\" {\n  rest_api_id = aws_api_gateway_rest_api.main.id\n  resource_id = aws_api_gateway_resource.users.id\n  http_method = \"GET\"\n  authorization = \"COGNITO_USER_POOLS\"\n  authorizer_id = aws_api_gateway_authorizer.main.id\n}\n\nresource \"aws_api_gateway_integration\" \"users\" {\n  rest_api_id = aws_api_gateway_rest_api.main.id\n  resource_id = aws_api_gateway_resource.users.id\n  http_method = aws_api_gateway_method.users_get.http_method\n  \n  type = \"HTTP_PROXY\"\n  uri = \"http://user-service.internal/users\"\n  integration_http_method = \"GET\"\n}\n\n// NGINX as API Gateway\nhttp {\n  upstream user_service {\n    server user-service:3001;\n  }\n  \n  upstream order_service {\n    server order-service:3002;\n  }\n  \n  limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;\n  \n  server {\n    listen 80;\n    \n    location /api/users {\n      limit_req zone=api_limit burst=20;\n      \n      # JWT validation\n      auth_jwt \"API\";\n      auth_jwt_key_file /etc/nginx/jwt-key.pem;\n      \n      proxy_pass http://user_service;\n      proxy_set_header Host $host;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header X-User-Id $jwt_claim_sub;\n    }\n    \n    location /api/orders {\n      limit_req zone=api_limit burst=20;\n      auth_jwt \"API\";\n      auth_jwt_key_file /etc/nginx/jwt-key.pem;\n      \n      proxy_pass http://order_service;\n    }\n  }\n}"
    },
    {
      "id": 10,
      "question": "What is microservices architecture and what are its trade-offs?",
      "answer": "Microservices architecture structures an application as a collection of small, independent services that communicate over network protocols.\n\nCore Principles:\n• Single Responsibility: Each service does one thing well\n• Independent Deployment: Services deployed separately\n• Decentralized Data: Each service owns its database\n• Technology Diversity: Different tech stacks per service\n• Organized Around Business Capabilities: Service boundaries align with business domains\n\nAdvantages:\n• Independent scaling: Scale services individually\n• Technology flexibility: Choose best tool for each service\n• Fault isolation: Failures don't cascade\n• Team autonomy: Teams own services end-to-end\n• Faster deployment: Small, independent releases\n• Better modularity: Clear boundaries\n\nDisadvantages:\n• Increased complexity: Distributed system challenges\n• Network latency: Inter-service communication overhead\n• Data consistency: No ACID transactions across services\n• Testing complexity: Integration testing harder\n• Deployment coordination: Multiple services to manage\n• Monitoring overhead: Distributed tracing needed\n\nWhen to Use:\n• Large, complex applications\n• Multiple teams working independently\n• Different scaling requirements per component\n• Need for technology diversity\n• Frequent deployments required\n\nWhen NOT to Use:\n• Small applications (monolith simpler)\n• Single small team\n• Tight coupling in business logic\n• ACID transactions critical\n• Limited operational capacity",
      "explanation": "Microservices split applications into small, independent services for better scalability and team autonomy, but introduce complexity in distributed system management, networking, and data consistency.",
      "difficulty": "Hard",
      "code": "// Microservices Example: E-commerce System\n\n// User Service (Node.js)\n// Handles user authentication and profile management\nconst express = require('express');\nconst app = express();\n\nconst userDatabase = require('./database');\nconst { publishEvent } = require('./messageQueue');\n\napp.post('/users', async (req, res) => {\n  const user = await userDatabase.create(req.body);\n  \n  // Publish event for other services\n  await publishEvent('user.created', {\n    userId: user.id,\n    email: user.email,\n    timestamp: new Date()\n  });\n  \n  res.json(user);\n});\n\napp.get('/users/:id', async (req, res) => {\n  const user = await userDatabase.findById(req.params.id);\n  res.json(user);\n});\n\napp.listen(3001);\n\n// Order Service (Python Flask)\n# Handles order processing\nfrom flask import Flask, request, jsonify\nimport requests\nimport pika\n\napp = Flask(__name__)\n\n@app.route('/orders', methods=['POST'])\ndef create_order():\n    order_data = request.json\n    \n    # Call user service to validate user\n    user_response = requests.get(\n        f'http://user-service:3001/users/{order_data[\"user_id\"]}'\n    )\n    \n    if user_response.status_code != 200:\n        return jsonify({'error': 'User not found'}), 404\n    \n    # Call product service to check inventory\n    product_response = requests.post(\n        'http://product-service:3003/inventory/reserve',\n        json={'product_ids': order_data['products']}\n    )\n    \n    if not product_response.json()['available']:\n        return jsonify({'error': 'Products not available'}), 400\n    \n    # Create order\n    order = db.create_order(order_data)\n    \n    # Publish order created event\n    publish_event('order.created', {\n        'order_id': order['id'],\n        'user_id': order['user_id'],\n        'total': order['total']\n    })\n    \n    return jsonify(order), 201\n\ndef publish_event(event_type, data):\n    connection = pika.BlockingConnection(\n        pika.ConnectionParameters('rabbitmq')\n    )\n    channel = connection.channel()\n    channel.exchange_declare('events', 'topic')\n    channel.basic_publish(\n        exchange='events',\n        routing_key=event_type,\n        body=json.dumps(data)\n    )\n    connection.close()\n\nif __name__ == '__main__':\n    app.run(port=3002)\n\n// Product Service (Java Spring Boot)\n// Handles product catalog and inventory\n@RestController\n@RequestMapping(\"/products\")\npublic class ProductController {\n    \n    @Autowired\n    private ProductService productService;\n    \n    @Autowired\n    private KafkaTemplate<String, String> kafkaTemplate;\n    \n    @GetMapping\n    public List<Product> getAllProducts() {\n        return productService.findAll();\n    }\n    \n    @PostMapping(\"/inventory/reserve\")\n    public ResponseEntity<InventoryResponse> reserveInventory(\n            @RequestBody ReserveRequest request) {\n        \n        boolean available = productService.checkAvailability(\n            request.getProductIds()\n        );\n        \n        if (available) {\n            productService.reserveProducts(request.getProductIds());\n            \n            // Publish event\n            kafkaTemplate.send(\"inventory-events\", \n                new InventoryReservedEvent(\n                    request.getProductIds(),\n                    request.getOrderId()\n                ).toJson()\n            );\n            \n            return ResponseEntity.ok(\n                new InventoryResponse(true, \"Reserved\")\n            );\n        }\n        \n        return ResponseEntity.ok(\n            new InventoryResponse(false, \"Not available\")\n        );\n    }\n}\n\n// Notification Service (Go)\n// Listens to events and sends notifications\npackage main\n\nimport (\n    \"encoding/json\"\n    \"log\"\n    \"github.com/streadway/amqp\"\n)\n\ntype OrderCreatedEvent struct {\n    OrderID string  `json:\"order_id\"`\n    UserID  string  `json:\"user_id\"`\n    Total   float64 `json:\"total\"`\n}\n\nfunc main() {\n    conn, err := amqp.Dial(\"amqp://rabbitmq:5672\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer conn.Close()\n    \n    ch, err := conn.Channel()\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer ch.Close()\n    \n    // Subscribe to order events\n    q, err := ch.QueueDeclare(\"notifications\", true, false, false, false, nil)\n    err = ch.QueueBind(q.Name, \"order.*\", \"events\", false, nil)\n    \n    msgs, err := ch.Consume(q.Name, \"\", false, false, false, false, nil)\n    \n    for msg := range msgs {\n        var event OrderCreatedEvent\n        json.Unmarshal(msg.Body, &event)\n        \n        // Send email notification\n        sendOrderConfirmation(event.UserID, event.OrderID)\n        \n        msg.Ack(false)\n    }\n}\n\nfunc sendOrderConfirmation(userID, orderID string) {\n    // Send email via SMTP or email service\n    log.Printf(\"Sending order confirmation for %s to user %s\", \n        orderID, userID)\n}\n\n// API Gateway (routing to microservices)\nconst express = require('express');\nconst { createProxyMiddleware } = require('http-proxy-middleware');\n\nconst app = express();\n\n// Service registry with health checks\nconst services = {\n    users: { url: 'http://user-service:3001', healthy: true },\n    orders: { url: 'http://order-service:3002', healthy: true },\n    products: { url: 'http://product-service:3003', healthy: true }\n};\n\n// Health check middleware\nsetInterval(async () => {\n    for (const [name, service] of Object.entries(services)) {\n        try {\n            await axios.get(`${service.url}/health`);\n            service.healthy = true;\n        } catch (error) {\n            service.healthy = false;\n            console.error(`Service ${name} is unhealthy`);\n        }\n    }\n}, 10000);\n\n// Route requests to services\napp.use('/api/users', (req, res, next) => {\n    if (!services.users.healthy) {\n        return res.status(503).json({ error: 'Service unavailable' });\n    }\n    createProxyMiddleware({\n        target: services.users.url,\n        changeOrigin: true,\n        pathRewrite: { '^/api/users': '' }\n    })(req, res, next);\n});\n\napp.use('/api/orders', (req, res, next) => {\n    if (!services.orders.healthy) {\n        return res.status(503).json({ error: 'Service unavailable' });\n    }\n    createProxyMiddleware({\n        target: services.orders.url,\n        changeOrigin: true,\n        pathRewrite: { '^/api/orders': '' }\n    })(req, res, next);\n});\n\napp.use('/api/products', (req, res, next) => {\n    if (!services.products.healthy) {\n        return res.status(503).json({ error: 'Service unavailable' });\n    }\n    createProxyMiddleware({\n        target: services.products.url,\n        changeOrigin: true,\n        pathRewrite: { '^/api/products': '' }\n    })(req, res, next);\n});\n\napp.listen(8080);\n\n// Docker Compose for Microservices\nversion: '3.8'\n\nservices:\n  api-gateway:\n    build: ./api-gateway\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - user-service\n      - order-service\n      - product-service\n  \n  user-service:\n    build: ./user-service\n    environment:\n      - DB_HOST=user-db\n      - RABBITMQ_HOST=rabbitmq\n    depends_on:\n      - user-db\n      - rabbitmq\n  \n  order-service:\n    build: ./order-service\n    environment:\n      - DB_HOST=order-db\n      - RABBITMQ_HOST=rabbitmq\n    depends_on:\n      - order-db\n      - rabbitmq\n  \n  product-service:\n    build: ./product-service\n    environment:\n      - DB_HOST=product-db\n      - KAFKA_HOST=kafka\n    depends_on:\n      - product-db\n      - kafka\n  \n  notification-service:\n    build: ./notification-service\n    environment:\n      - RABBITMQ_HOST=rabbitmq\n    depends_on:\n      - rabbitmq\n  \n  user-db:\n    image: postgres:14\n  \n  order-db:\n    image: postgres:14\n  \n  product-db:\n    image: postgres:14\n  \n  rabbitmq:\n    image: rabbitmq:3-management\n  \n  kafka:\n    image: confluentinc/cp-kafka:latest"
    },
    {
      "id": 11,
      "question": "How do you design a URL shortening service like bit.ly?",
      "answer": "A URL shortening service converts long URLs into short, unique codes for easier sharing.\n\nCore Requirements:\n• Functional: Shorten URL, redirect to original, optional custom aliases, expiration\n• Non-functional: High availability, low latency, scalable to billions of URLs\n\nSystem Components:\n• API Service: Handle URL shortening and redirects\n• Database: Store URL mappings (short_code -> long_url)\n• Cache: Store popular URLs for fast lookup\n• Analytics Service: Track clicks, referrers, geography\n\nKey Design Decisions:\n• Short Code Generation: Base62 encoding (a-zA-Z0-9) gives 62^7 = 3.5 trillion combinations\n• Database Choice: NoSQL (Cassandra, DynamoDB) for scalability\n• Caching Strategy: LRU cache for hot URLs\n• Collision Handling: Use auto-increment ID or check uniqueness\n\nScalability:\n• Read-heavy system: 100:1 read-to-write ratio\n• Use CDN for redirects\n• Database sharding by hash of short code\n• Master-slave replication for reads\n\nEstimations:\n• 500M new URLs per month\n• 50B redirects per month\n• Storage: 500 bytes per URL = 250GB per month\n• Bandwidth: 50B requests * 500 bytes = 25TB\n\nAPI Endpoints:\n• POST /shorten: Create short URL\n• GET /:shortCode: Redirect to original\n• GET /analytics/:shortCode: Get statistics",
      "explanation": "URL shorteners map long URLs to short codes using base62 encoding, with heavy caching for reads, NoSQL for scale, and analytics tracking for business value.",
      "difficulty": "Medium",
      "code": "// URL Shortener Implementation\n\nconst express = require('express');\nconst redis = require('redis');\nconst { MongoClient } = require('mongodb');\nconst crypto = require('crypto');\n\nconst app = express();\nconst cache = redis.createClient();\nconst mongoClient = new MongoClient('mongodb://localhost:27017');\n\n// Base62 encoding for short codes\nconst BASE62 = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ';\n\nfunction encodeBase62(num) {\n  if (num === 0) return BASE62[0];\n  \n  let encoded = '';\n  while (num > 0) {\n    encoded = BASE62[num % 62] + encoded;\n    num = Math.floor(num / 62);\n  }\n  return encoded;\n}\n\nfunction decodeBase62(str) {\n  let decoded = 0;\n  for (let i = 0; i < str.length; i++) {\n    decoded = decoded * 62 + BASE62.indexOf(str[i]);\n  }\n  return decoded;\n}\n\n// Shorten URL endpoint\napp.post('/shorten', async (req, res) => {\n  const { longUrl, customAlias, expiresAt } = req.body;\n  \n  // Validate URL\n  try {\n    new URL(longUrl);\n  } catch (error) {\n    return res.status(400).json({ error: 'Invalid URL' });\n  }\n  \n  const db = mongoClient.db('urlshortener');\n  const urls = db.collection('urls');\n  \n  let shortCode;\n  \n  if (customAlias) {\n    // Check if custom alias is available\n    const existing = await urls.findOne({ shortCode: customAlias });\n    if (existing) {\n      return res.status(409).json({ error: 'Alias already taken' });\n    }\n    shortCode = customAlias;\n  } else {\n    // Generate unique short code using auto-increment ID\n    const counter = await db.collection('counters').findOneAndUpdate(\n      { _id: 'url_id' },\n      { $inc: { seq: 1 } },\n      { upsert: true, returnDocument: 'after' }\n    );\n    \n    shortCode = encodeBase62(counter.value.seq);\n  }\n  \n  // Store in database\n  const urlDoc = {\n    shortCode,\n    longUrl,\n    createdAt: new Date(),\n    expiresAt: expiresAt ? new Date(expiresAt) : null,\n    clicks: 0,\n    clickData: []\n  };\n  \n  await urls.insertOne(urlDoc);\n  \n  // Cache for quick access\n  await cache.setEx(\n    `url:${shortCode}`,\n    3600, // 1 hour TTL\n    longUrl\n  );\n  \n  res.json({\n    shortUrl: `https://short.ly/${shortCode}`,\n    shortCode,\n    longUrl\n  });\n});\n\n// Redirect endpoint\napp.get('/:shortCode', async (req, res) => {\n  const { shortCode } = req.params;\n  \n  // Try cache first\n  let longUrl = await cache.get(`url:${shortCode}`);\n  \n  if (!longUrl) {\n    // Cache miss - query database\n    const db = mongoClient.db('urlshortener');\n    const urlDoc = await db.collection('urls').findOne({ shortCode });\n    \n    if (!urlDoc) {\n      return res.status(404).send('URL not found');\n    }\n    \n    // Check expiration\n    if (urlDoc.expiresAt && new Date() > urlDoc.expiresAt) {\n      return res.status(410).send('URL expired');\n    }\n    \n    longUrl = urlDoc.longUrl;\n    \n    // Update cache\n    await cache.setEx(`url:${shortCode}`, 3600, longUrl);\n  }\n  \n  // Track analytics asynchronously (don't block redirect)\n  trackClick(shortCode, req).catch(err => \n    console.error('Analytics tracking failed:', err)\n  );\n  \n  // Redirect\n  res.redirect(301, longUrl);\n});\n\n// Track click analytics\nasync function trackClick(shortCode, req) {\n  const db = mongoClient.db('urlshortener');\n  \n  const clickData = {\n    timestamp: new Date(),\n    ip: req.ip,\n    userAgent: req.get('user-agent'),\n    referrer: req.get('referrer') || 'direct'\n  };\n  \n  // Increment click counter and store click data\n  await db.collection('urls').updateOne(\n    { shortCode },\n    {\n      $inc: { clicks: 1 },\n      $push: {\n        clickData: {\n          $each: [clickData],\n          $slice: -1000 // Keep only last 1000 clicks\n        }\n      }\n    }\n  );\n}\n\n// Analytics endpoint\napp.get('/analytics/:shortCode', async (req, res) => {\n  const { shortCode } = req.params;\n  \n  const db = mongoClient.db('urlshortener');\n  const urlDoc = await db.collection('urls').findOne(\n    { shortCode },\n    { projection: { clicks: 1, clickData: 1, createdAt: 1 } }\n  );\n  \n  if (!urlDoc) {\n    return res.status(404).json({ error: 'URL not found' });\n  }\n  \n  // Aggregate analytics\n  const analytics = {\n    totalClicks: urlDoc.clicks,\n    createdAt: urlDoc.createdAt,\n    clicksByDay: {},\n    topReferrers: {},\n    uniqueIPs: new Set()\n  };\n  \n  urlDoc.clickData.forEach(click => {\n    // Clicks by day\n    const day = click.timestamp.toISOString().split('T')[0];\n    analytics.clicksByDay[day] = (analytics.clicksByDay[day] || 0) + 1;\n    \n    // Top referrers\n    analytics.topReferrers[click.referrer] = \n      (analytics.topReferrers[click.referrer] || 0) + 1;\n    \n    // Unique visitors\n    analytics.uniqueIPs.add(click.ip);\n  });\n  \n  analytics.uniqueVisitors = analytics.uniqueIPs.size;\n  delete analytics.uniqueIPs;\n  \n  res.json(analytics);\n});\n\n// Database Schema (MongoDB)\nconst urlSchema = {\n  shortCode: { type: 'string', index: true, unique: true },\n  longUrl: { type: 'string', required: true },\n  createdAt: { type: 'date', default: Date.now },\n  expiresAt: { type: 'date', default: null },\n  clicks: { type: 'number', default: 0 },\n  clickData: [{\n    timestamp: 'date',\n    ip: 'string',\n    userAgent: 'string',\n    referrer: 'string'\n  }]\n};\n\n// Create indexes\ndb.collection('urls').createIndex({ shortCode: 1 }, { unique: true });\ndb.collection('urls').createIndex({ expiresAt: 1 }, { expireAfterSeconds: 0 });\n\n// Cassandra Schema (for scale)\nCREATE KEYSPACE urlshortener WITH replication = {\n  'class': 'SimpleStrategy',\n  'replication_factor': 3\n};\n\nCREATE TABLE urls (\n  short_code text PRIMARY KEY,\n  long_url text,\n  created_at timestamp,\n  expires_at timestamp,\n  clicks counter\n);\n\nCREATE TABLE analytics (\n  short_code text,\n  click_date date,\n  click_hour int,\n  ip text,\n  user_agent text,\n  referrer text,\n  PRIMARY KEY ((short_code, click_date), click_hour, ip)\n) WITH CLUSTERING ORDER BY (click_hour DESC, ip ASC);\n\n// Distributed ID Generation (Snowflake-like)\nclass IDGenerator {\n  constructor(datacenterId, workerId) {\n    this.epoch = 1609459200000; // 2021-01-01\n    this.datacenterIdBits = 5;\n    this.workerIdBits = 5;\n    this.sequenceBits = 12;\n    \n    this.datacenterId = datacenterId;\n    this.workerId = workerId;\n    this.sequence = 0;\n    this.lastTimestamp = -1;\n  }\n  \n  generate() {\n    let timestamp = Date.now();\n    \n    if (timestamp < this.lastTimestamp) {\n      throw new Error('Clock moved backwards');\n    }\n    \n    if (timestamp === this.lastTimestamp) {\n      // Same millisecond - increment sequence\n      this.sequence = (this.sequence + 1) & ((1 << this.sequenceBits) - 1);\n      if (this.sequence === 0) {\n        // Sequence exhausted - wait for next millisecond\n        while (timestamp <= this.lastTimestamp) {\n          timestamp = Date.now();\n        }\n      }\n    } else {\n      this.sequence = 0;\n    }\n    \n    this.lastTimestamp = timestamp;\n    \n    // Combine timestamp, datacenter, worker, and sequence\n    const id = \n      ((timestamp - this.epoch) << (this.datacenterIdBits + this.workerIdBits + this.sequenceBits)) |\n      (this.datacenterId << (this.workerIdBits + this.sequenceBits)) |\n      (this.workerId << this.sequenceBits) |\n      this.sequence;\n    \n    return id;\n  }\n}\n\nconst idGen = new IDGenerator(1, 1);\nconst uniqueId = idGen.generate();\nconst shortCode = encodeBase62(uniqueId);\n\napp.listen(3000);\n\n// High-Level Architecture\n/*\nClient Request\n     |\n     v\nLoad Balancer (NGINX/ALB)\n     |\n     v\nAPI Servers (Auto-scaled)\n     |\n     +-- Redis Cache (Read-through)\n     |\n     +-- Database Cluster (Cassandra/DynamoDB)\n     |\n     +-- Analytics Queue (Kafka)\n     |\n     v\nAnalytics Workers\n     |\n     v\nAnalytics Database (ClickHouse/TimescaleDB)\n\nEstimations:\n- 500M new URLs/month = 200 writes/sec\n- 50B redirects/month = 20K reads/sec\n- Cache hit rate: 80% (16K cache hits, 4K DB queries)\n- Database: Cassandra 3-node cluster\n- Cache: Redis cluster with 10GB memory\n- API servers: 10 instances behind load balancer\n*/"
    },
    {
      "id": 12,
      "question": "Design a chat system like WhatsApp or Slack",
      "answer": "A real-time chat system requires low-latency message delivery, presence tracking, and persistent storage.\n\nCore Requirements:\n• One-on-one chat: Direct messages between users\n• Group chat: Multiple participants\n• Online status: Show who's online\n• Message history: Persistent storage\n• Read receipts: Delivery and read confirmation\n• Push notifications: Alert offline users\n• Media sharing: Images, videos, files\n\nTechnical Challenges:\n• Real-time delivery: WebSocket connections\n• Message ordering: Timestamps and sequence numbers\n• Scalability: Millions of concurrent connections\n• Consistency: Same message order for all users\n• Offline support: Queue messages for offline users\n\nCore Components:\n• WebSocket servers: Maintain persistent connections\n• Message queue: RabbitMQ or Kafka for message routing\n• Database: Cassandra or MongoDB for message storage\n• Cache: Redis for online status and recent messages\n• Push notification service: Firebase Cloud Messaging\n• Media storage: S3 or CDN for files\n\nMessage Flow:\n• Sender sends message via WebSocket\n• Server validates and assigns message ID\n• Message stored in database\n• Message queued for delivery to recipients\n• Online recipients receive via WebSocket\n• Offline recipients get push notification\n• Read receipts sent back to sender\n\nDatabase Design:\n• Messages table: Partitioned by conversation ID\n• Users table: User profiles and settings\n• Conversations table: Chat metadata\n• Key-value cache for user status",
      "explanation": "Chat systems use WebSockets for real-time delivery, message queues for routing, partitioned databases for storage, and push notifications for offline users, with careful attention to message ordering and scalability.",
      "difficulty": "Hard",
      "code": "// WebSocket Chat Server (Node.js)\nconst express = require('express');\nconst http = require('http');\nconst { Server } = require('socket.io');\nconst redis = require('redis');\nconst { Kafka } = require('kafkajs');\n\nconst app = express();\nconst server = http.createServer(app);\nconst io = new Server(server);\n\nconst cache = redis.createClient();\nconst kafka = new Kafka({\n  brokers: ['kafka1:9092', 'kafka2:9092']\n});\n\nconst producer = kafka.producer();\nconst consumer = kafka.consumer({ groupId: 'chat-servers' });\n\nconst onlineUsers = new Map(); // userId -> socket\n\nio.on('connection', (socket) => {\n  console.log('User connected:', socket.id);\n  \n  // Authenticate user\n  socket.on('authenticate', async (token) => {\n    try {\n      const userId = await verifyToken(token);\n      socket.userId = userId;\n      onlineUsers.set(userId, socket);\n      \n      // Update online status\n      await cache.setEx(`user:${userId}:online`, 300, '1');\n      \n      // Notify contacts\n      const contacts = await getContactIds(userId);\n      contacts.forEach(contactId => {\n        const contactSocket = onlineUsers.get(contactId);\n        if (contactSocket) {\n          contactSocket.emit('user_online', { userId });\n        }\n      });\n      \n      // Load offline messages\n      const offlineMessages = await loadOfflineMessages(userId);\n      socket.emit('offline_messages', offlineMessages);\n      \n      socket.emit('authenticated', { userId });\n    } catch (error) {\n      socket.emit('auth_error', { message: 'Invalid token' });\n      socket.disconnect();\n    }\n  });\n  \n  // Send message\n  socket.on('send_message', async (data) => {\n    const { conversationId, content, type = 'text' } = data;\n    \n    // Create message\n    const message = {\n      id: generateMessageId(),\n      conversationId,\n      senderId: socket.userId,\n      content,\n      type,\n      timestamp: Date.now(),\n      status: 'sent'\n    };\n    \n    // Store in database\n    await db.messages.insert(message);\n    \n    // Publish to Kafka for routing\n    await producer.send({\n      topic: 'chat-messages',\n      messages: [{\n        key: conversationId,\n        value: JSON.stringify(message)\n      }]\n    });\n    \n    // Cache recent message\n    await cache.lPush(\n      `conversation:${conversationId}:messages`,\n      JSON.stringify(message)\n    );\n    await cache.lTrim(`conversation:${conversationId}:messages`, 0, 99);\n    \n    // Send acknowledgment to sender\n    socket.emit('message_sent', { \n      tempId: data.tempId,\n      messageId: message.id,\n      timestamp: message.timestamp\n    });\n  });\n  \n  // Typing indicator\n  socket.on('typing', async (data) => {\n    const { conversationId } = data;\n    const participants = await getConversationParticipants(conversationId);\n    \n    participants.forEach(userId => {\n      if (userId !== socket.userId) {\n        const userSocket = onlineUsers.get(userId);\n        if (userSocket) {\n          userSocket.emit('user_typing', {\n            conversationId,\n            userId: socket.userId\n          });\n        }\n      }\n    });\n  });\n  \n  // Mark as read\n  socket.on('mark_read', async (data) => {\n    const { conversationId, messageId } = data;\n    \n    await db.messages.update(\n      { id: messageId },\n      { $addToSet: { readBy: socket.userId } }\n    );\n    \n    // Notify sender\n    const message = await db.messages.findOne({ id: messageId });\n    const senderSocket = onlineUsers.get(message.senderId);\n    if (senderSocket) {\n      senderSocket.emit('message_read', {\n        messageId,\n        conversationId,\n        readBy: socket.userId,\n        timestamp: Date.now()\n      });\n    }\n  });\n  \n  // Disconnect\n  socket.on('disconnect', async () => {\n    if (socket.userId) {\n      onlineUsers.delete(socket.userId);\n      await cache.del(`user:${socket.userId}:online`);\n      \n      // Notify contacts\n      const contacts = await getContactIds(socket.userId);\n      contacts.forEach(contactId => {\n        const contactSocket = onlineUsers.get(contactId);\n        if (contactSocket) {\n          contactSocket.emit('user_offline', { \n            userId: socket.userId,\n            lastSeen: Date.now()\n          });\n        }\n      });\n    }\n  });\n});\n\n// Kafka consumer for message routing\nasync function startMessageRouter() {\n  await consumer.connect();\n  await consumer.subscribe({ topic: 'chat-messages' });\n  \n  await consumer.run({\n    eachMessage: async ({ message }) => {\n      const chatMessage = JSON.parse(message.value.toString());\n      const participants = await getConversationParticipants(\n        chatMessage.conversationId\n      );\n      \n      // Deliver to online users\n      let deliveredCount = 0;\n      participants.forEach(userId => {\n        if (userId !== chatMessage.senderId) {\n          const socket = onlineUsers.get(userId);\n          if (socket) {\n            socket.emit('new_message', chatMessage);\n            deliveredCount++;\n          }\n        }\n      });\n      \n      // Queue for offline users\n      participants.forEach(async userId => {\n        if (userId !== chatMessage.senderId && !onlineUsers.has(userId)) {\n          // Store in offline message queue\n          await cache.lPush(\n            `user:${userId}:offline_messages`,\n            JSON.stringify(chatMessage)\n          );\n          \n          // Send push notification\n          await sendPushNotification(userId, {\n            title: await getUserName(chatMessage.senderId),\n            body: chatMessage.content,\n            data: { conversationId: chatMessage.conversationId }\n          });\n        }\n      });\n      \n      // Update message status\n      await db.messages.update(\n        { id: chatMessage.id },\n        { status: deliveredCount > 0 ? 'delivered' : 'sent' }\n      );\n    }\n  });\n}\n\n// Database Schema (MongoDB)\nconst messageSchema = {\n  id: { type: 'string', index: true },\n  conversationId: { type: 'string', index: true },\n  senderId: { type: 'string', index: true },\n  content: { type: 'string' },\n  type: { type: 'string', enum: ['text', 'image', 'video', 'file'] },\n  mediaUrl: { type: 'string' },\n  timestamp: { type: 'number', index: true },\n  status: { type: 'string', enum: ['sent', 'delivered', 'read'] },\n  readBy: [{ type: 'string' }],\n  replyTo: { type: 'string' }\n};\n\nconst conversationSchema = {\n  id: { type: 'string', index: true },\n  type: { type: 'string', enum: ['direct', 'group'] },\n  participants: [{ type: 'string', index: true }],\n  name: { type: 'string' },\n  createdAt: { type: 'number' },\n  lastMessageAt: { type: 'number', index: true },\n  lastMessage: { type: 'object' }\n};\n\n// Create indexes for efficient queries\ndb.messages.createIndex({ conversationId: 1, timestamp: -1 });\ndb.messages.createIndex({ senderId: 1, timestamp: -1 });\ndb.conversations.createIndex({ participants: 1, lastMessageAt: -1 });\n\n// Cassandra Schema (for high scale)\nCREATE TABLE messages_by_conversation (\n  conversation_id text,\n  message_id timeuuid,\n  sender_id text,\n  content text,\n  type text,\n  media_url text,\n  status text,\n  PRIMARY KEY (conversation_id, message_id)\n) WITH CLUSTERING ORDER BY (message_id DESC);\n\nCREATE TABLE messages_by_user (\n  user_id text,\n  conversation_id text,\n  message_id timeuuid,\n  content text,\n  PRIMARY KEY (user_id, conversation_id, message_id)\n) WITH CLUSTERING ORDER BY (conversation_id ASC, message_id DESC);\n\n// REST API for chat history\napp.get('/conversations/:conversationId/messages', async (req, res) => {\n  const { conversationId } = req.params;\n  const { before, limit = 50 } = req.query;\n  \n  // Check cache first\n  const cached = await cache.lRange(\n    `conversation:${conversationId}:messages`,\n    0,\n    limit - 1\n  );\n  \n  if (cached.length >= limit && !before) {\n    return res.json({\n      messages: cached.map(msg => JSON.parse(msg))\n    });\n  }\n  \n  // Query database\n  const query = { conversationId };\n  if (before) {\n    query.timestamp = { $lt: parseInt(before) };\n  }\n  \n  const messages = await db.messages\n    .find(query)\n    .sort({ timestamp: -1 })\n    .limit(parseInt(limit));\n  \n  res.json({ messages });\n});\n\n// Media upload endpoint\nconst multer = require('multer');\nconst { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');\n\nconst upload = multer({ storage: multer.memoryStorage() });\nconst s3 = new S3Client({ region: 'us-east-1' });\n\napp.post('/media/upload', upload.single('file'), async (req, res) => {\n  const fileKey = `${Date.now()}-${req.file.originalname}`;\n  \n  await s3.send(new PutObjectCommand({\n    Bucket: 'chat-media',\n    Key: fileKey,\n    Body: req.file.buffer,\n    ContentType: req.file.mimetype\n  }));\n  \n  const mediaUrl = `https://chat-media.s3.amazonaws.com/${fileKey}`;\n  res.json({ mediaUrl });\n});\n\nserver.listen(3000);\nstartMessageRouter();\n\n// Group Chat Management\napp.post('/conversations/group', async (req, res) => {\n  const { name, participants } = req.body;\n  \n  const conversation = {\n    id: generateUUID(),\n    type: 'group',\n    name,\n    participants,\n    createdAt: Date.now(),\n    createdBy: req.user.id\n  };\n  \n  await db.conversations.insert(conversation);\n  \n  // Notify all participants\n  participants.forEach(userId => {\n    const socket = onlineUsers.get(userId);\n    if (socket) {\n      socket.emit('new_conversation', conversation);\n    }\n  });\n  \n  res.json(conversation);\n});"
    },
    {
      "id": 13,
      "question": "What is eventual consistency and how is it different from strong consistency?",
      "answer": "Eventual consistency is a consistency model where the system guarantees that if no new updates are made, eventually all replicas will converge to the same value.\n\nStrong Consistency:\n• Every read returns the most recent write\n• All nodes see same data at same time\n• Writes block until all replicas updated\n• Higher latency, lower availability\n• Example: Bank transactions, inventory systems\n\nEventual Consistency:\n• Reads may return stale data temporarily\n• Replicas update asynchronously\n• Eventually all replicas converge\n• Lower latency, higher availability\n• Example: Social media feeds, DNS, shopping carts\n\nConsistency Levels (Weakest to Strongest):\n• Eventual: No guarantees on timing\n• Causal: Reads respect causality (if A caused B, all see A before B)\n• Session: Reads see writes in same session\n• Linearizable: Strongest, appears instantaneous\n\nWhen to Use Eventual Consistency:\n• High availability critical\n• Can tolerate temporary inconsistencies\n• Partition tolerance required\n• Global distribution needed\n• Social features (likes, follows)\n\nConflict Resolution:\n• Last Write Wins: Use timestamp\n• Version Vectors: Track causality\n• Custom merge: Business logic decides\n• CRDTs: Conflict-free replicated data types\n\nReal-World Examples:\n• DNS: Takes time to propagate changes\n• DynamoDB: Eventually consistent reads by default\n• Cassandra: Tunable consistency\n• S3: Eventually consistent for overwrites/deletes",
      "explanation": "Eventual consistency allows temporary inconsistencies across replicas for better availability and performance, with guaranteed convergence over time, unlike strong consistency which ensures immediate consistency at the cost of latency.",
      "difficulty": "Medium",
      "code": "// DynamoDB - Eventual vs Strong Consistency\nconst { DynamoDBClient, GetItemCommand, PutItemCommand } = require('@aws-sdk/client-dynamodb');\n\nconst dynamodb = new DynamoDBClient({ region: 'us-east-1' });\n\n// Write to DynamoDB (always strongly consistent)\nasync function writeUser(userId, data) {\n  await dynamodb.send(new PutItemCommand({\n    TableName: 'Users',\n    Item: {\n      userId: { S: userId },\n      name: { S: data.name },\n      email: { S: data.email },\n      version: { N: Date.now().toString() }\n    }\n  }));\n}\n\n// Eventually consistent read (default, faster, cheaper)\nasync function getUserEventuallyConsistent(userId) {\n  const result = await dynamodb.send(new GetItemCommand({\n    TableName: 'Users',\n    Key: { userId: { S: userId } },\n    ConsistentRead: false // Eventually consistent (default)\n  }));\n  \n  // May return stale data if read immediately after write\n  return result.Item;\n}\n\n// Strongly consistent read (slower, more expensive)\nasync function getUserStronglyConsistent(userId) {\n  const result = await dynamodb.send(new GetItemCommand({\n    TableName: 'Users',\n    Key: { userId: { S: userId } },\n    ConsistentRead: true // Strong consistency\n  }));\n  \n  // Always returns latest data\n  return result.Item;\n}\n\n// Example: Read-your-writes consistency\nclass UserService {\n  constructor() {\n    this.recentWrites = new Map(); // Track recent writes\n  }\n  \n  async updateUser(userId, data) {\n    await writeUser(userId, data);\n    \n    // Track write for this user\n    this.recentWrites.set(userId, {\n      data,\n      timestamp: Date.now()\n    });\n    \n    // Clean up old entries after 2 seconds\n    setTimeout(() => this.recentWrites.delete(userId), 2000);\n  }\n  \n  async getUser(userId) {\n    // Check if we recently wrote to this user\n    if (this.recentWrites.has(userId)) {\n      const write = this.recentWrites.get(userId);\n      const age = Date.now() - write.timestamp;\n      \n      // If write was recent (< 1s), use strong consistency\n      if (age < 1000) {\n        return await getUserStronglyConsistent(userId);\n      }\n    }\n    \n    // Normal case: use eventual consistency\n    return await getUserEventuallyConsistent(userId);\n  }\n}\n\n// Cassandra - Tunable Consistency\nconst cassandra = require('cassandra-driver');\n\nconst client = new cassandra.Client({\n  contactPoints: ['node1', 'node2', 'node3'],\n  localDataCenter: 'datacenter1',\n  keyspace: 'myapp'\n});\n\n// Strong consistency: Quorum reads and writes\nasync function writeWithStrongConsistency(userId, data) {\n  const query = 'INSERT INTO users (user_id, name, email) VALUES (?, ?, ?)';\n  await client.execute(query, [userId, data.name, data.email], {\n    consistency: cassandra.types.consistencies.quorum // Majority of nodes\n  });\n}\n\nasync function readWithStrongConsistency(userId) {\n  const query = 'SELECT * FROM users WHERE user_id = ?';\n  const result = await client.execute(query, [userId], {\n    consistency: cassandra.types.consistencies.quorum // Majority of nodes\n  });\n  return result.rows[0];\n}\n\n// Eventual consistency: ONE read/write\nasync function writeWithEventualConsistency(userId, data) {\n  const query = 'INSERT INTO users (user_id, name, email) VALUES (?, ?, ?)';\n  await client.execute(query, [userId, data.name, data.email], {\n    consistency: cassandra.types.consistencies.one // Any single node\n  });\n}\n\nasync function readWithEventualConsistency(userId) {\n  const query = 'SELECT * FROM users WHERE user_id = ?';\n  const result = await client.execute(query, [userId], {\n    consistency: cassandra.types.consistencies.one // Any single node\n  });\n  return result.rows[0];\n}\n\n// Last Write Wins (LWW) Conflict Resolution\nclass LWWRegister {\n  constructor() {\n    this.value = null;\n    this.timestamp = 0;\n  }\n  \n  set(value, timestamp) {\n    if (timestamp > this.timestamp) {\n      this.value = value;\n      this.timestamp = timestamp;\n      return true;\n    }\n    return false; // Ignore older writes\n  }\n  \n  get() {\n    return this.value;\n  }\n  \n  // Merge with replica\n  merge(other) {\n    if (other.timestamp > this.timestamp) {\n      this.value = other.value;\n      this.timestamp = other.timestamp;\n    }\n  }\n}\n\n// Example usage\nconst replica1 = new LWWRegister();\nconst replica2 = new LWWRegister();\n\n// Different writes on different replicas\nreplica1.set('Alice', 1000);\nreplica2.set('Bob', 1001); // Newer timestamp\n\n// Merge replicas - Bob wins (later timestamp)\nreplica1.merge(replica2);\nconsole.log(replica1.get()); // 'Bob'\n\n// CRDT: Grow-Only Counter\nclass GCounter {\n  constructor(nodeId) {\n    this.nodeId = nodeId;\n    this.counts = {}; // nodeId -> count\n  }\n  \n  increment() {\n    this.counts[this.nodeId] = (this.counts[this.nodeId] || 0) + 1;\n  }\n  \n  value() {\n    return Object.values(this.counts).reduce((sum, count) => sum + count, 0);\n  }\n  \n  merge(other) {\n    for (const [nodeId, count] of Object.entries(other.counts)) {\n      this.counts[nodeId] = Math.max(\n        this.counts[nodeId] || 0,\n        count\n      );\n    }\n  }\n}\n\n// Example: Multiple replicas counting likes\nconst counter1 = new GCounter('node1');\nconst counter2 = new GCounter('node2');\n\ncounter1.increment(); // node1: 1\ncounter1.increment(); // node1: 2\ncounter2.increment(); // node2: 1\n\n// Merge - no conflicts!\ncounter1.merge(counter2);\nconsole.log(counter1.value()); // 3\n\n// CRDT: Last-Write-Wins Map\nclass LWWMap {\n  constructor() {\n    this.data = new Map(); // key -> {value, timestamp}\n  }\n  \n  set(key, value, timestamp = Date.now()) {\n    const current = this.data.get(key);\n    if (!current || timestamp > current.timestamp) {\n      this.data.set(key, { value, timestamp });\n    }\n  }\n  \n  get(key) {\n    return this.data.get(key)?.value;\n  }\n  \n  delete(key, timestamp = Date.now()) {\n    this.set(key, null, timestamp); // Tombstone\n  }\n  \n  merge(other) {\n    for (const [key, entry] of other.data.entries()) {\n      const current = this.data.get(key);\n      if (!current || entry.timestamp > current.timestamp) {\n        this.data.set(key, entry);\n      }\n    }\n  }\n}\n\n// Vector Clocks for Causal Consistency\nclass VectorClock {\n  constructor(nodeId) {\n    this.nodeId = nodeId;\n    this.clock = {}; // nodeId -> counter\n  }\n  \n  increment() {\n    this.clock[this.nodeId] = (this.clock[this.nodeId] || 0) + 1;\n  }\n  \n  update(other) {\n    for (const [nodeId, count] of Object.entries(other.clock)) {\n      this.clock[nodeId] = Math.max(this.clock[nodeId] || 0, count);\n    }\n    this.increment();\n  }\n  \n  // Compare two vector clocks\n  compareTo(other) {\n    let less = false;\n    let greater = false;\n    \n    const allNodes = new Set([\n      ...Object.keys(this.clock),\n      ...Object.keys(other.clock)\n    ]);\n    \n    for (const node of allNodes) {\n      const thisCount = this.clock[node] || 0;\n      const otherCount = other.clock[node] || 0;\n      \n      if (thisCount < otherCount) less = true;\n      if (thisCount > otherCount) greater = true;\n    }\n    \n    if (less && !greater) return -1; // this happened before other\n    if (greater && !less) return 1;  // other happened before this\n    if (!less && !greater) return 0; // identical\n    return null; // concurrent (conflict)\n  }\n}\n\n// Example: Detect concurrent writes\nconst clock1 = new VectorClock('node1');\nconst clock2 = new VectorClock('node2');\n\nclock1.increment(); // {node1: 1}\nclock2.increment(); // {node2: 1}\n\nconst comparison = clock1.compareTo(clock2);\nif (comparison === null) {\n  console.log('Concurrent writes detected - need conflict resolution');\n}"
    },
    {
      "id": 14,
      "question": "What is database indexing and what are different types of indexes?",
      "answer": "Database indexes are data structures that improve query performance by providing fast lookups, similar to a book's index.\n\nHow Indexes Work:\n• Create sorted reference to data locations\n• Trade storage space for query speed\n• Speed up SELECT but slow down INSERT/UPDATE/DELETE\n• Must be maintained on data changes\n\nCommon Index Types:\n• B-Tree (Balanced Tree): Default for most databases, good for range queries\n• Hash Index: Fast equality lookups, no range queries\n• Bitmap Index: Low-cardinality columns (gender, status)\n• Full-Text Index: Text search capabilities\n• Spatial Index: Geographic queries (R-tree)\n• Covering Index: Includes all queried columns\n• Composite Index: Multiple columns together\n\nWhen to Create Indexes:\n• Columns in WHERE clauses\n• JOIN columns\n• ORDER BY columns\n• Foreign keys\n• Frequently queried columns\n\nWhen NOT to Index:\n• Small tables\n• Columns with many NULLs\n• Frequently updated columns\n• Low cardinality columns (few unique values)\n• Tables with heavy writes\n\nIndex Strategies:\n• Selective indexes: High cardinality columns\n• Composite indexes: Match query patterns\n• Covering indexes: Avoid table lookups\n• Partial indexes: Filter specific rows\n• Index hints: Force specific index usage\n\nPerformance Impact:\n• Reads: 10x-100x faster with proper indexes\n• Writes: 10-20% slower with many indexes\n• Storage: ~10-20% of table size per index",
      "explanation": "Indexes dramatically speed up queries by creating sorted lookup structures, but require storage overhead and slow down writes, necessitating careful selection based on query patterns.",
      "difficulty": "Medium",
      "code": "-- Creating Different Types of Indexes\n\n-- Simple B-Tree Index (default)\nCREATE INDEX idx_users_email ON users(email);\n\n-- Unique Index\nCREATE UNIQUE INDEX idx_users_username ON users(username);\n\n-- Composite Index (multiple columns)\nCREATE INDEX idx_orders_user_date ON orders(user_id, created_at DESC);\n\n-- Partial Index (filtered)\nCREATE INDEX idx_active_users ON users(email) WHERE active = true;\n\n-- Covering Index (includes additional columns)\nCREATE INDEX idx_users_covered ON users(email) INCLUDE (name, created_at);\n\n-- Full-Text Index (PostgreSQL)\nCREATE INDEX idx_posts_search ON posts USING GIN(to_tsvector('english', title || ' ' || body));\n\n-- Spatial Index (PostGIS)\nCREATE INDEX idx_locations_geom ON locations USING GIST(geom);\n\n-- Hash Index (PostgreSQL)\nCREATE INDEX idx_users_id_hash ON users USING HASH(id);\n\n-- Index on Expression\nCREATE INDEX idx_users_lower_email ON users(LOWER(email));\n\n-- Query Performance Analysis\n\n-- Before Index\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = 'john@example.com';\n/*\nSeq Scan on users  (cost=0.00..1693.00 rows=1 width=100)\n                   (actual time=15.234..15.237 rows=1 loops=1)\n  Filter: (email = 'john@example.com')\n  Rows Removed by Filter: 99999\nPlanning Time: 0.123 ms\nExecution Time: 15.250 ms\n*/\n\n-- After Index\nCREATE INDEX idx_users_email ON users(email);\n\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = 'john@example.com';\n/*\nIndex Scan using idx_users_email on users  (cost=0.42..8.44 rows=1 width=100)\n                                           (actual time=0.034..0.036 rows=1 loops=1)\n  Index Cond: (email = 'john@example.com')\nPlanning Time: 0.145 ms\nExecution Time: 0.065 ms  -- 234x faster!\n*/\n\n-- Composite Index Usage\n\n-- Good: Uses index efficiently\nSELECT * FROM orders \nWHERE user_id = 123 \nAND created_at >= '2023-01-01'\nORDER BY created_at DESC;\n-- Uses: idx_orders_user_date (user_id, created_at DESC)\n\n-- Bad: Cannot use composite index\nSELECT * FROM orders \nWHERE created_at >= '2023-01-01';  -- Skips first column\n-- Cannot use idx_orders_user_date, needs separate index on created_at\n\n-- Index Cardinality Analysis\nSELECT \n  schemaname,\n  tablename,\n  attname AS column_name,\n  n_distinct AS cardinality,\n  null_frac AS null_percentage\nFROM pg_stats\nWHERE tablename = 'users'\nORDER BY n_distinct DESC;\n\n-- Find Missing Indexes (PostgreSQL)\nSELECT \n  schemaname,\n  tablename,\n  seq_scan,\n  seq_tup_read,\n  idx_scan,\n  seq_tup_read / seq_scan AS avg_seq_read,\n  'CREATE INDEX idx_' || tablename || '_missing ON ' || tablename || '(column_name);' AS suggestion\nFROM pg_stat_user_tables\nWHERE seq_scan > 0\n  AND idx_scan = 0\n  AND seq_tup_read > 10000\nORDER BY seq_tup_read DESC;\n\n-- Find Unused Indexes\nSELECT\n  schemaname,\n  tablename,\n  indexname,\n  idx_scan AS index_scans,\n  pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,\n  'DROP INDEX ' || indexname || ';' AS drop_statement\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\n  AND indexrelname NOT LIKE '%_pkey'\nORDER BY pg_relation_size(indexrelid) DESC;\n\n-- MySQL Index Hints\n\n-- Force specific index\nSELECT * FROM users \nUSE INDEX (idx_users_email)\nWHERE email = 'john@example.com';\n\n-- Ignore specific index\nSELECT * FROM users \nIGNORE INDEX (idx_users_name)\nWHERE name LIKE 'John%';\n\n-- Force index scan (avoid full table scan)\nSELECT * FROM users \nFORCE INDEX (idx_users_created)\nWHERE created_at > '2023-01-01';\n\n-- Application-Level Index Optimization\n\nconst { Pool } = require('pg');\nconst pool = new Pool();\n\n// Good: Uses index on email\nasync function getUserByEmail(email) {\n  const result = await pool.query(\n    'SELECT * FROM users WHERE email = $1',\n    [email]\n  );\n  return result.rows[0];\n}\n\n// Bad: Function in WHERE prevents index usage\nasync function getUserByEmailBad(email) {\n  const result = await pool.query(\n    'SELECT * FROM users WHERE LOWER(email) = $1',\n    [email.toLowerCase()]\n  );\n  return result.rows[0];\n}\n\n// Better: Use functional index\n// CREATE INDEX idx_users_lower_email ON users(LOWER(email));\n\n// Covering Index Example\nCREATE INDEX idx_users_email_covering \nON users(email) \nINCLUDE (id, name, created_at);\n\n-- Query now uses index-only scan (no table access)\nSELECT id, name, created_at \nFROM users \nWHERE email = 'john@example.com';\n\n-- Partial Index for Active Users\nCREATE INDEX idx_active_users_email \nON users(email) \nWHERE active = true AND deleted_at IS NULL;\n\n-- Much smaller index, faster for active user queries\nSELECT * FROM users \nWHERE email = 'john@example.com' \n  AND active = true \n  AND deleted_at IS NULL;\n\n-- Multi-Column Index Order Matters\n\n-- Index: (user_id, created_at)\n-- Good queries:\nWHERE user_id = 123\nWHERE user_id = 123 AND created_at > '2023-01-01'\nWHERE user_id = 123 ORDER BY created_at\n\n-- Cannot use index efficiently:\nWHERE created_at > '2023-01-01'  -- Skips first column\nORDER BY created_at  -- user_id not filtered\n\n-- Index Maintenance\n\n-- Rebuild index (PostgreSQL)\nREINDEX INDEX idx_users_email;\n\n-- Rebuild all indexes on table\nREINDEX TABLE users;\n\n-- Vacuum and analyze for stats\nVACUUM ANALYZE users;\n\n-- Check index bloat\nSELECT\n  schemaname,\n  tablename,\n  indexname,\n  pg_size_pretty(pg_relation_size(indexrelid)) AS size,\n  idx_scan,\n  idx_tup_read,\n  idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public'\nORDER BY pg_relation_size(indexrelid) DESC;\n\n-- Monitor Index Usage Over Time\nCREATE TABLE index_stats_history (\n  recorded_at timestamp DEFAULT now(),\n  indexname text,\n  idx_scan bigint,\n  idx_tup_read bigint,\n  idx_tup_fetch bigint\n);\n\n-- Collect stats periodically\nINSERT INTO index_stats_history (indexname, idx_scan, idx_tup_read, idx_tup_fetch)\nSELECT indexname, idx_scan, idx_tup_read, idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public';\n\n-- Analyze index growth\nSELECT \n  indexname,\n  MAX(idx_scan) - MIN(idx_scan) AS scans_last_day,\n  (MAX(idx_scan) - MIN(idx_scan)) / EXTRACT(EPOCH FROM (MAX(recorded_at) - MIN(recorded_at))) * 86400 AS scans_per_day\nFROM index_stats_history\nWHERE recorded_at > now() - interval '1 day'\nGROUP BY indexname\nORDER BY scans_per_day DESC;"
    },
    {
      "id": 15,
      "question": "What are design patterns for handling rate limiting?",
      "answer": "Rate limiting controls the number of requests a user can make in a time period to prevent abuse and ensure fair resource allocation.\n\nCommon Rate Limiting Algorithms:\n• Token Bucket: Tokens added at fixed rate, requests consume tokens\n• Leaky Bucket: Queue requests, process at fixed rate\n• Fixed Window: Count requests in fixed time windows\n• Sliding Window Log: Store timestamp of each request\n• Sliding Window Counter: Hybrid of fixed window and sliding log\n\nImplementation Levels:\n• Application: In-process rate limiting\n• API Gateway: Centralized at entry point\n• Load Balancer: Distributed rate limiting\n• CDN: Edge-level protection\n\nDistributed Rate Limit Challenges:\n• Synchronization: Share state across  servers\n• Atomic operations: Prevent race conditions\n• Performance: Low latency requirement\n• Accuracy: Trade-off with performance\n\nResponse to Rate Limit:\n• HTTP 429 (Too Many Requests)\n• Retry-After header with wait time\n• Rate limit headers (X-RateLimit-Limit, X-RateLimit-Remaining)\n• Exponential backoff for retries\n\nStrategies:\n• Per-user limits: Based on user ID\n• Per-IP limits: Based on IP address\n• Per-API-key limits: Based on API credentials\n• Tiered limits: Different limits per plan\n• Burst allowance: Temporary higher limits\n\nBest Practices:\n• Clear documentation of limits\n• Gradual enforcement (warnings first)\n• Whitelist trusted users\n• Monitor for abuse patterns\n• Graceful degradation",
      "explanation": "Rate limiting prevents abuse by controlling request frequency using algorithms like token bucket or sliding window, typically implemented at the API gateway with Redis for distributed tracking.",
      "difficulty": "Medium",
      "code": "// Token Bucket Rate Limiter\nclass TokenBucket {\n  constructor(capacity, refillRate) {\n    this.capacity = capacity;           // Maximum tokens\n    this.tokens = capacity;             // Current tokens\n    this.refillRate = refillRate;       // Tokens per second\n    this.lastRefill = Date.now();\n  }\n  \n  refill() {\n    const now = Date.now();\n    const timePassed = (now - this.lastRefill) / 1000;\n    const tokensToAdd = timePassed * this.refillRate;\n    \n    this.tokens = Math.min(this.capacity, this.tokens + tokensToAdd);\n    this.lastRefill = now;\n  }\n  \n  tryConsume(tokens = 1) {\n    this.refill();\n    \n    if (this.tokens >= tokens) {\n      this.tokens -= tokens;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  getWaitTime() {\n    this.refill();\n    if (this.tokens >= 1) return 0;\n    \n    const tokensNeeded = 1 - this.tokens;\n    return (tokensNeeded / this.refillRate) * 1000;\n  }\n}\n\n// Usage\nconst bucket = new TokenBucket(10, 1); // 10 capacity, 1 token/sec\n\nif (bucket.tryConsume()) {\n  // Process request\n  res.json({ success: true });\n} else {\n  const waitTime = Math.ceil(bucket.getWaitTime() / 1000);\n  res.status(429).json({\n    error: 'Too many requests',\n    retryAfter: waitTime\n  });\n}\n\n// Redis-based Distributed Token Bucket\nconst redis = require('redis');\nconst client = redis.createClient();\n\nclass DistributedTokenBucket {\n  constructor(key, capacity, refillRate) {\n    this.key = key;\n    this.capacity = capacity;\n    this.refillRate = refillRate;\n  }\n  \n  async tryConsume(tokens = 1) {\n    const script = `\n      local key = KEYS[1]\n      local capacity = tonumber(ARGV[1])\n      local refill_rate = tonumber(ARGV[2])\n      local requested = tonumber(ARGV[3])\n      local now = tonumber(ARGV[4])\n      \n      local bucket = redis.call('HMGET', key, 'tokens', 'last_refill')\n      local current_tokens = tonumber(bucket[1]) or capacity\n      local last_refill = tonumber(bucket[2]) or now\n      \n      -- Refill tokens\n      local time_passed = (now - last_refill) / 1000\n      local tokens_to_add = time_passed * refill_rate\n      current_tokens = math.min(capacity, current_tokens + tokens_to_add)\n      \n      -- Try to consume\n      if current_tokens >= requested then\n        current_tokens = current_tokens - requested\n        redis.call('HMSET', key, 'tokens', current_tokens, 'last_refill', now)\n        redis.call('EXPIRE', key, 3600)\n        return 1\n      end\n      \n      redis.call('HMSET', key, 'tokens', current_tokens, 'last_refill', now)\n      redis.call('EXPIRE', key, 3600)\n      return 0\n    `;\n    \n    const result = await client.eval(script, {\n      keys: [this.key],\n      arguments: [\n        this.capacity.toString(),\n        this.refillRate.toString(),\n        tokens.toString(),\n        Date.now().toString()\n      ]\n    });\n    \n    return result === 1;\n  }\n}\n\n// Fixed Window Counter\nasync function fixedWindowRateLimit(userId, limit, windowSeconds) {\n  const now = Date.now();\n  const window = Math.floor(now / (windowSeconds * 1000));\n  const key = `rate_limit:${userId}:${window}`;\n  \n  const current = await client.incr(key);\n  \n  if (current === 1) {\n    await client.expire(key, windowSeconds);\n  }\n  \n  if (current > limit) {\n    return {\n      allowed: false,\n      remaining: 0,\n      resetAt: (window + 1) * windowSeconds * 1000\n    };\n  }\n  \n  return {\n    allowed: true,\n    remaining: limit - current,\n    resetAt: (window + 1) * windowSeconds * 1000\n  };\n}\n\n// Sliding Window Log\nclass SlidingWindowLog {\n  constructor(userId, limit, windowMs) {\n    this.key = `rate_limit:log:${userId}`;\n    this.limit = limit;\n    this.windowMs = windowMs;\n  }\n  \n  async tryRequest() {\n    const now = Date.now();\n    const windowStart = now - this.windowMs;\n    \n    // Remove old requests\n    await client.zRemRangeByScore(this.key, 0, windowStart);\n    \n    // Count requests in window\n    const count = await client.zCard(this.key);\n    \n    if (count < this.limit) {\n      // Add new request\n      await client.zAdd(this.key, { score: now, value: now.toString() });\n      await client.expire(this.key, Math.ceil(this.windowMs / 1000));\n      \n      return {\n        allowed: true,\n        remaining: this.limit - count - 1\n      };\n    }\n    \n    // Get oldest request to calculate retry time\n    const oldest = await client.zRange(this.key, 0, 0, { withScores: true });\n    const retryAfter = oldest[0] ? oldest[0].score + this.windowMs - now : 0;\n    \n    return {\n      allowed: false,\n      remaining: 0,\n      retryAfter: Math.max(0, Math.ceil(retryAfter / 1000))\n    };\n  }\n}\n\n// Sliding Window Counter (Hybrid)\nasync function slidingWindowCounter(userId, limit, windowSeconds) {\n  const now = Date.now();\n  const currentWindow = Math.floor(now / (windowSeconds * 1000));\n  const previousWindow = currentWindow - 1;\n  \n  const currentKey = `rate_limit:${userId}:${currentWindow}`;\n  const previousKey = `rate_limit:${userId}:${previousWindow}`;\n  \n  const [currentCount, previousCount] = await Promise.all([\n    client.get(currentKey).then(v => parseInt(v) || 0),\n    client.get(previousKey).then(v => parseInt(v) || 0)\n  ]);\n  \n  // Calculate position in current window\n  const windowProgress = (now % (windowSeconds * 1000)) / (windowSeconds * 1000);\n  \n  // Weighted count from previous window\n  const weightedCount = \n    previousCount * (1 - windowProgress) + currentCount;\n  \n  if (weightedCount >= limit) {\n    return { allowed: false, remaining: 0 };\n  }\n  \n  // Increment current window\n  const newCount = await client.incr(currentKey);\n  if (newCount === 1) {\n    await client.expire(currentKey, windowSeconds * 2);\n  }\n  \n  return {\n    allowed: true,\n    remaining: Math.max(0, limit - Math.ceil(weightedCount) - 1)\n  };\n}\n\n// Express Middleware\nconst rateLimit = (options) => {\n  const {\n    windowMs = 15 * 60 * 1000,  // 15 minutes\n    max = 100,                   // 100 requests\n    keyGenerator = (req) => req.ip,\n    handler = (req, res) => {\n      res.status(429).json({\n        error: 'Too many requests, please try again later'\n      });\n    }\n  } = options;\n  \n  return async (req, res, next) => {\n    const key = keyGenerator(req);\n    const limiter = new SlidingWindowLog(key, max, windowMs);\n    \n    try {\n      const result = await limiter.tryRequest();\n      \n      // Add rate limit headers\n      res.set({\n        'X-RateLimit-Limit': max,\n        'X-RateLimit-Remaining': result.remaining,\n        'X-RateLimit-Reset': Date.now() + windowMs\n      });\n      \n      if (result.allowed) {\n        next();\n      } else {\n        res.set('Retry-After', result.retryAfter);\n        handler(req, res);\n      }\n    } catch (error) {\n      console.error('Rate limiting error:', error);\n      next(); // Fail open\n    }\n  };\n};\n\n// Usage\nconst express = require('express');\nconst app = express();\n\napp.use('/api/', rateLimit({\n  windowMs: 60 * 1000, // 1 minute\n  max: 60,              // 60 requests per minute\n  keyGenerator: (req) => req.user?.id || req.ip\n}));\n\n// Tiered Rate Limiting\nconst tieredRateLimit = async (req, res, next) => {\n  const user = req.user;\n  \n  const limits = {\n    free: { max: 100, windowMs: 60 * 60 * 1000 },      // 100/hour\n    basic: { max: 1000, windowMs: 60 * 60 * 1000 },    // 1000/hour\n    premium: { max: 10000, windowMs: 60 * 60 * 1000 }, // 10000/hour\n    enterprise: { max: Infinity }                      // Unlimited\n  };\n  \n  const userTier = user?.tier || 'free';\n  const config = limits[userTier];\n  \n  if (config.max === Infinity) {\n    return next();\n  }\n  \n  const limiter = rateLimit({\n    windowMs: config.windowMs,\n    max: config.max,\n    keyGenerator: () => user?.id || req.ip\n  });\n  \n  limiter(req, res, next);\n};\n\napp.use('/api/', tieredRateLimit);\n\n// NGINX Rate Limiting\nhttp {\n  limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;\n  \n  server {\n    location /api/ {\n      limit_req zone=api_limit burst=20 nodelay;\n      limit_req_status 429;\n      \n      proxy_pass http://backend;\n    }\n  }\n}\n\n// Leaky Bucket Algorithm\nclass LeakyBucket {\n  constructor(capacity, leakRate) {\n    this.capacity = capacity;\n    this.queue = [];\n    this.leakRate = leakRate; // Items per second\n    this.lastLeak = Date.now();\n    \n    this.startLeaking();\n  }\n  \n  startLeaking() {\n    setInterval(() => {\n      if (this.queue.length > 0) {\n        const toRemove = Math.floor(this.leakRate);\n        this.queue.splice(0, toRemove);\n      }\n    }, 1000);\n  }\n  \n  tryAdd(item) {\n    if (this.queue.length < this.capacity) {\n      this.queue.push(item);\n      return true;\n    }\n    return false;\n  }\n}"
    },
    {
      "id": 16,
      "question": "What is the difference between SQL and NoSQL databases and when to use each?",
      "answer": "SQL and NoSQL databases represent different approaches to data storage with distinct trade-offs in consistency, scalability, and flexibility.\n\nSQL (Relational) Databases:\n• Structured schema with tables, rows, columns\n• ACID transactions (Atomicity, Consistency, Isolation, Durability)\n• Strong consistency and data integrity\n• Vertical scaling primarily\n• Complex queries with JOINs\n• Examples: MySQL, PostgreSQL, Oracle, SQL Server\n\nNoSQL Databases:\n• Flexible schema or schema-less\n• BASE properties (Basically Available, Soft state, Eventually consistent)\n• Horizontal scaling\n• Simple queries, denormalized data\n• Types: Document, Key-Value, Column-Family, Graph\n• Examples: MongoDB, Cassandra, Redis, DynamoDB\n\nNoSQL Types:\n• Document: MongoDB, CouchDB (JSON documents)\n• Key-Value: Redis, DynamoDB (simple lookups)\n• Column-Family: Cassandra, HBase (wide tables)\n• Graph: Neo4j, Neptune (relationships)\n\nWhen to Use SQL:\n• Complex queries with multiple JOINs\n• Transactions critical (banking, e-commerce orders)\n• Structured data with known schema\n• Strong consistency required\n• ACID compliance mandatory\n\nWhen to Use NoSQL:\n• Massive scale (billions of records)\n• Unstructured or semi-structured data\n• High write throughput\n• Geographic distribution\n• Flexible schema evolution\n• Eventual consistency acceptable\n\nHybrid Approach (Polyglot Persistence):\n• Use both SQL and NoSQL\n• SQL for transactional data\n• NoSQL for caching, sessions, logs\n• Choose database per use case",
      "explanation": "SQL databases prioritize consistency and complex queries with fixed schemas, while NoSQL offers flexibility and horizontal scalability for massive datasets with simpler access patterns, choosing between them depends on consistency needs and scale requirements.",
      "difficulty": "Easy",
      "code": "// SQL Example (PostgreSQL with Node.js)\nconst { Pool } = require('pg');\nconst pool = new Pool({ connectionString: process.env.DATABASE_URL });\n\n// Complex query with JOINs\nasync function getUserOrdersSql(userId) {\n  const query = `\n    SELECT \n      u.id, u.name, u.email,\n      o.id as order_id, o.total, o.created_at,\n      json_agg(json_build_object(\n        'product_id', oi.product_id,\n        'product_name', p.name,\n        'quantity', oi.quantity,\n        'price', oi.price\n      )) as items\n    FROM users u\n    JOIN orders o ON u.id = o.user_id\n    JOIN order_items oi ON o.id = oi.order_id\n    JOIN products p ON oi.product_id = p.id\n    WHERE u.id = $1\n    GROUP BY u.id, o.id\n    ORDER BY o.created_at DESC\n    LIMIT 10\n  `;\n  \n  const result = await pool.query(query, [userId]);\n  return result.rows;\n}\n\n// ACID Transaction\nasync function transferFundsSql(fromUserId, toUserId, amount) {\n  const client = await pool.connect();\n  \n  try {\n    await client.query('BEGIN');\n    \n    // Check balance\n    const { rows: [fromAccount] } = await client.query(\n      'SELECT balance FROM accounts WHERE user_id = $1 FOR UPDATE',\n      [fromUserId]\n    );\n    \n    if (fromAccount.balance < amount) {\n      throw new Error('Insufficient funds');\n    }\n    \n    // Debit\n    await client.query(\n      'UPDATE accounts SET balance = balance - $1 WHERE user_id = $2',\n      [amount, fromUserId]\n    );\n    \n    // Credit\n    await client.query(\n      'UPDATE accounts SET balance = balance + $1 WHERE user_id = $2',\n      [amount, toUserId]\n    );\n    \n    // Record transaction\n    await client.query(\n      'INSERT INTO transactions (from_user, to_user, amount, type) VALUES ($1, $2, $3, $4)',\n      [fromUserId, toUserId, amount, 'transfer']\n    );\n    \n    await client.query('COMMIT');\n    return { success: true };\n  } catch (error) {\n    await client.query('ROLLBACK');\n    throw error;\n  } finally {\n    client.release();\n  }\n}\n\n// NoSQL Example (MongoDB)\nconst { MongoClient } = require('mongodb');\nconst mongo = new MongoClient('mongodb://localhost:27017');\nconst db = mongo.db('myapp');\n\n// Document database - denormalized data\nconst userDocument = {\n  _id: ObjectId('507f1f77bcf86cd799439011'),\n  name: 'John Doe',\n  email: 'john@example.com',\n  addresses: [\n    {\n      type: 'home',\n      street: '123 Main St',\n      city: 'New York',\n      zip: '10001'\n    },\n    {\n      type: 'work',\n      street: '456 Office Blvd',\n      city: 'New York',\n      zip: '10002'\n    }\n  ],\n  orders: [\n    {\n      orderId: '12345',\n      total: 99.99,\n      items: [\n        { productId: 'A1', name: 'Widget', price: 49.99, quantity: 2 }\n      ],\n      createdAt: new Date('2023-01-15')\n    }\n  ],\n  preferences: {\n    newsletter: true,\n    theme: 'dark'\n  }\n};\n\n// Simple lookup (no JOINS needed)\nasync function getUserWithOrdersMongo(userId) {\n  return await db.collection('users').findOne(\n    { _id: ObjectId(userId) },\n    { projection: { password: 0 } } // Exclude sensitive fields\n  );\n}\n\n// Flexible schema - add fields without migration\nasync function addUserPreference(userId, preference, value) {\n  await db.collection('users').updateOne(\n    { _id: ObjectId(userId) },\n    { $set: { [`preferences.${preference}`]: value } }\n  );\n}\n\n// Array operations\nasync function addUserAddress(userId, address) {\n  await db.collection('users').updateOne(\n    { _id: ObjectId(userId) },\n    { $push: { addresses: address } }\n  );\n}\n\n// Redis (Key-Value NoSQL)\nconst redis = require('redis');\nconst cacheClient = redis.createClient();\n\n// Simple key-value operations\nasync function cacheUserSession(sessionId, userData) {\n  await cacheClient.setEx(\n    `session:${sessionId}`,\n    3600, // TTL: 1 hour\n    JSON.stringify(userData)\n  );\n}\n\nasync function getUserSession(sessionId) {\n  const data = await cacheClient.get(`session:${sessionId}`);\n  return data ? JSON.parse(data) : null;\n}\n\n// Sorted sets for leaderboards\nasync function updateLeaderboard(userId, score) {\n  await cacheClient.zAdd('leaderboard:global', {\n    score,\n    value: userId\n  });\n}\n\nasync function getTopPlayers(limit = 10) {\n  return await cacheClient.zRange('leaderboard:global', 0, limit - 1, {\n    REV: true,\n    WITHSCORES: true\n  });\n}\n\n// Cassandra (Column-Family NoSQL)\nconst cassandra = require('cassandra-driver');\nconst cassandraClient = new cassandra.Client({\n  contactPoints: ['127.0.0.1'],\n  localDataCenter: 'datacenter1',\n  keyspace: 'myapp'\n});\n\n// Wide column store - optimized for time-series\nconst createTimeSeriesTable = `\n  CREATE TABLE sensor_data (\n    sensor_id text,\n    date date,\n    timestamp timestamp,\n    temperature double,\n    humidity double,\n    PRIMARY KEY ((sensor_id, date), timestamp)\n  ) WITH CLUSTERING ORDER BY (timestamp DESC)\n`;\n\nasync function insertSensorReading(sensorId, temperature, humidity) {\n  const query = 'INSERT INTO sensor_data (sensor_id, date, timestamp, temperature, humidity) VALUES (?, ?, ?, ?, ?)';\n  const date = new Date().toISOString().split('T')[0];\n  \n  await cassandraClient.execute(query, [\n    sensorId,\n    date,\n    new Date(),\n    temperature,\n    humidity\n  ]);\n}\n\nasync function getSensorReadings(sensorId, date) {\n  const query = 'SELECT * FROM sensor_data WHERE sensor_id = ? AND date = ?';\n  const result = await cassandraClient.execute(query, [sensorId, date]);\n  return result.rows;\n}\n\n// Polyglot Persistence Example\nclass UserService {\n  constructor() {\n    this.postgres = pool;      // Primary data\n    this.mongo = db;           // User profiles, logs\n    this.redis = cacheClient;  // Sessions, cache\n  }\n  \n  async createUser(userData) {\n    const client = await this.postgres.connect();\n    \n    try {\n      await client.query('BEGIN');\n      \n      // Store critical data in PostgreSQL\n      const { rows: [user] } = await client.query(\n        'INSERT INTO users (email, password_hash) VALUES ($1, $2) RETURNING id',\n        [userData.email, userData.passwordHash]\n      );\n      \n      await client.query('COMMIT');\n      \n      // Store profile data in MongoDB\n      await this.mongo.collection('user_profiles').insertOne({\n        userId: user.id,\n        name: userData.name,\n        bio: userData.bio,\n        avatar: userData.avatar,\n        preferences: userData.preferences || {}\n      });\n      \n      return user;\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n  \n  async getUserComplete(userId) {\n    // Try cache first\n    const cached = await this.redis.get(`user:${userId}`);\n    if (cached) return JSON.parse(cached);\n    \n    // Get from databases\n    const [authData, profile] = await Promise.all([\n      this.postgres.query('SELECT id, email, created_at FROM users WHERE id = $1', [userId]),\n      this.mongo.collection('user_profiles').findOne({ userId })\n    ]);\n    \n    const user = {\n      ...authData.rows[0],\n      ...profile\n    };\n    \n    // Cache result\n    await this.redis.setEx(`user:${userId}`, 300, JSON.stringify(user));\n    \n    return user;\n  }\n}\n\n// Migration from SQL to NoSQL (gradual approach)\nclass HybridDataAccess {\n  async getUser(userId) {\n    // Try NoSQL first\n    const noSqlUser = await db.collection('users').findOne({ _id: ObjectId(userId) });\n    if (noSqlUser && noSqlUser.migrated) {\n      return noSqlUser;\n    }\n    \n    // Fallback to SQL\n    const result = await pool.query('SELECT * FROM users WHERE id = $1', [userId]);\n    const sqlUser = result.rows[0];\n    \n    // Lazy migration: copy to NoSQL\n    if (sqlUser) {\n      await db.collection('users').updateOne(\n        { _id: ObjectId(userId) },\n        { $set: { ...sqlUser, migrated: true } },\n        { upsert: true }\n      );\n    }\n    \n    return sqlUser;\n  }\n}\n\n// Performance Comparison\n// SQL: Complex query with JOINs\nconst sqlQuery = `\n  SELECT u.*, COUNT(o.id) as order_count, SUM(o.total) as lifetime_value\n  FROM users u\n  LEFT JOIN orders o ON u.id = o.user_id\n  WHERE u.created_at > '2023-01-01'\n  GROUP BY u.id\n  HAVING COUNT(o.id) > 5\n  ORDER BY lifetime_value DESC\n  LIMIT 100\n`;\n// Execution: 200-500ms with proper indexes\n\n// NoSQL: Pre-aggregated data\nconst noSqlQuery = db.collection('user_analytics').find({\n  createdAt: { $gt: new Date('2023-01-01') },\n  orderCount: { $gt: 5 }\n}).sort({ lifetimeValue: -1 }).limit(100);\n// Execution: 10-50ms (pre-aggregated)\n\n// Trade-off: NoSQL requires denormalization and duplicate data"
    },
    {
      "id": 17,
      "question": "How do you design a distributed task queue system?",
      "answer": "A distributed task queue system manages and executes background jobs across multiple workers, ensuring reliability, scalability, and fault tolerance.\n\nCore Components:\n• Task Producer: Creates tasks and adds to queue\n• Message Broker: Stores and distributes tasks (RabbitMQ, Redis, Kafka)\n• Task Workers: Consume and execute tasks\n• Result Backend: Store task results (Redis, Database)\n• Scheduler: Periodic and delayed tasks\n• Monitor: Track task status and failures\n\nKey Features:\n• Task Priority: High/normal/low priority queues\n• Retry Logic: Automatic retries with exponential backoff\n• Dead Letter Queue: Store failed tasks after max retries\n• Task Timeout: Kill long-running tasks\n• Rate Limiting: Control task execution rate\n• Task Chains: Execute tasks in sequence\n• Task Groups: Execute tasks in parallel\n\nReliability Patterns:\n• At-least-once delivery: Task may execute multiple times\n• At-most-once delivery: Task executes once or not at all\n• Exactly-once delivery: Task executes exactly once (hardest)\n• Idempotent tasks: Safe to execute multiple times\n\nScalability:\n• Horizontal scaling: Add more workers\n• Queue sharding: Distribute across brokers\n• Worker pools: Separate workers for different task types\n• Auto-scaling: Scale based on queue depth\n\nMonitoring:\n• Queue depth: Number of pending tasks\n• Task duration: Execution time metrics\n• Success/failure rates\n• Worker health and capacity\n• Dead letter queue size\n\nPopular Solutions:\n• Celery: Python task queue\n• BullMQ: Node.js with Redis\n• AWS SQS + Lambda: Serverless\n• Sidekiq: Ruby background jobs",
      "explanation": "Distributed task queues separate work from execution using message brokers, enabling asynchronous processing, horizontal scaling, and fault tolerance with features like retries, priorities, and monitoring.",
      "difficulty": "Hard",
      "code": "// BullMQ Task Queue (Node.js with Redis)\nconst { Queue, Worker, QueueScheduler } = require('bullmq');\nconst Redis = require('ioredis');\n\nconst connection = new Redis({\n  host: 'localhost',\n  port: 6379,\n  maxRetriesPerRequest: null\n});\n\n// Create queue\nconst emailQueue = new Queue('email', { connection });\n\n// Producer: Add tasks to queue\nasync function sendWelcomeEmail(userId, email) {\n  const job = await emailQueue.add(\n    'welcome-email',\n    { userId, email },\n    {\n      priority: 1,              // Higher priority\n      attempts: 3,              // Retry 3 times\n      backoff: {\n        type: 'exponential',\n        delay: 2000            // Start with 2s delay\n      },\n      removeOnComplete: 100,    // Keep last 100 completed jobs\n      removeOnFail: false       // Keep failed jobs for debugging\n    }\n  );\n  \n  console.log(`Job ${job.id} added to queue`);\n  return job;\n}\n\nasync function sendBulkEmails(users) {\n  const jobs = users.map(user => ({\n    name: 'bulk-email',\n    data: { userId: user.id, email: user.email },\n    opts: {\n      priority: 5,            // Lower priority than welcome emails\n      attempts: 2\n    }\n  }));\n  \n  await emailQueue.addBulk(jobs);\n}\n\n// Worker: Process tasks\nconst worker = new Worker(\n  'email',\n  async (job) => {\n    console.log(`Processing job ${job.id}: ${job.name}`);\n    \n    try {\n      switch (job.name) {\n        case 'welcome-email':\n          await processWelcomeEmail(job.data);\n          break;\n        case 'bulk-email':\n          await processBulkEmail(job.data);\n          break;\n        default:\n          throw new Error(`Unknown job type: ${job.name}`);\n      }\n      \n      // Update progress\n      await job.updateProgress(100);\n      \n      return { success: true, processedAt: new Date() };\n    } catch (error) {\n      console.error(`Job ${job.id} failed:`, error);\n      throw error; // Will trigger retry\n    }\n  },\n  {\n    connection,\n    concurrency: 5,           // Process 5 jobs concurrently\n    limiter: {\n      max: 10,               // Max 10 jobs\n      duration: 1000         // Per second\n    }\n  }\n);\n\n// Event listeners\nworker.on('completed', (job) => {\n  console.log(`Job ${job.id} completed successfully`);\n});\n\nworker.on('failed', (job, error) => {\n  console.error(`Job ${job.id} failed after ${job.attemptsMade} attempts:`, error);\n  \n  // Move to dead letter queue if max retries exceeded\n  if (job.attemptsMade >= job.opts.attempts) {\n    deadLetterQueue.add('failed-email', {\n      originalJob: job.data,\n      error: error.message,\n      failedAt: new Date()\n    });\n  }\n});\n\nworker.on('progress', (job, progress) => {\n  console.log(`Job ${job.id} progress: ${progress}%`);\n});\n\n// Scheduler for periodic tasks\nconst scheduler = new QueueScheduler('email', { connection });\n\n// Add recurring job\nasync function scheduleReports() {\n  await emailQueue.add(\n    'daily-report',\n    { type: 'daily' },\n    {\n      repeat: {\n        cron: '0 9 * * *',    // Every day at 9 AM\n        tz: 'America/New_York'\n      }\n    }\n  );\n}\n\n// Delayed job\nasync function scheduleReminderEmail(userId, delay) {\n  await emailQueue.add(\n    'reminder',\n    { userId },\n    { delay: delay * 1000 }  // Delay in milliseconds\n  );\n}\n\n// Task chains (execute in sequence)\nconst { FlowProducer } = require('bullmq');\nconst flowProducer = new FlowProducer({ connection });\n\nasync function processNewOrder(orderId) {\n  const flow = await flowProducer.add({\n    name: 'process-order',\n    queueName: 'orders',\n    data: { orderId },\n    children: [\n      {\n        name: 'charge-payment',\n        queueName: 'payments',\n        data: { orderId }\n      },\n      {\n        name: 'update-inventory',\n        queueName: 'inventory',\n        data: { orderId }\n      },\n      {\n        name: 'send-confirmation',\n        queueName: 'email',\n        data: { orderId }\n      }\n    ]\n  });\n  \n  return flow;\n}\n\n// Celery-style Task Queue (Python)\nfrom celery import Celery, chain, group, chord\nfrom celery.schedules import crontab\n\napp = Celery('tasks', broker='redis://localhost:6379/0')\n\napp.conf.update(\n    task_serializer='json',\n    accept_content=['json'],\n    result_serializer='json',\n    timezone='UTC',\n    enable_utc=True,\n    task_track_started=True,\n    task_time_limit=300,        # 5 minute timeout\n    task_soft_time_limit=240,   # Soft limit before hard kill\n    worker_prefetch_multiplier=4,\n    worker_max_tasks_per_child=1000\n)\n\n@app.task(bind=True, max_retries=3, default_retry_delay=60)\ndef send_email(self, user_id, email_type):\n    try:\n        # Send email logic\n        smtp_client.send(\n            to=get_user_email(user_id),\n            subject=get_subject(email_type),\n            body=render_template(email_type)\n        )\n        return {'status': 'sent', 'user_id': user_id}\n    except SMTPException as exc:\n        # Retry with exponential backoff\n        raise self.retry(exc=exc, countdown=2 ** self.request.retries)\n\n@app.task\ndef process_image(image_path):\n    # Resize image\n    img = Image.open(image_path)\n    img.thumbnail((800, 600))\n    img.save(image_path)\n    return image_path\n\n@app.task\ndef generate_thumbnail(image_path):\n    # Create thumbnail\n    img = Image.open(image_path)\n    img.thumbnail((200, 200))\n    thumb_path = image_path.replace('.jpg', '_thumb.jpg')\n    img.save(thumb_path)\n    return thumb_path\n\n# Execute tasks\n\n# Simple task\nresult = send_email.delay(user_id=123, email_type='welcome')\n\n# Task chain (sequential execution)\nworkflow = chain(\n    process_image.s('/path/to/image.jpg'),\n    generate_thumbnail.s()\n)\nworkflow.apply_async()\n\n# Task group (parallel execution)\nparallel = group(\n    send_email.s(user_id=1, email_type='newsletter'),\n    send_email.s(user_id=2, email_type='newsletter'),\n    send_email.s(user_id=3, email_type='newsletter')\n)\nresult = parallel.apply_async()\n\n# Chord (parallel + callback)\nworkflow = chord(\n    [process_image.s(path) for path in image_paths]\n)(\n    finalize_processing.s()  # Called when all tasks complete\n)\n\n# Periodic tasks\napp.conf.beat_schedule = {\n    'send-daily-report': {\n        'task': 'tasks.send_daily_report',\n        'schedule': crontab(hour=9, minute=0),  # 9 AM daily\n    },\n    'cleanup-old-data': {\n        'task': 'tasks.cleanup',\n        'schedule': crontab(hour=2, minute=0, day_of_week=1),  # Monday 2 AM\n    },\n}\n\n# Custom task class for common logic\nclass DatabaseTask(app.Task):\n    _db = None\n    \n    @property\n    def db(self):\n        if self._db is None:\n            self._db = create_db_connection()\n        return self._db\n    \n    def after_return(self, status, retval, task_id, args, kwargs, einfo):\n        if self._db is not None:\n            self._db.close()\n\n@app.task(base=DatabaseTask)\ndef update_user_stats(user_id):\n    stats = calculate_stats(user_id)\n    self.db.execute('UPDATE users SET stats = ? WHERE id = ?', (stats, user_id))\n\n// Monitoring and Management\nconst express = require('express');\nconst { createBullBoard } = require('@bull-board/api');\nconst { BullMQAdapter } = require('@bull-board/api/bullMQAdapter');\nconst { ExpressAdapter } = require('@bull-board/express');\n\nconst serverAdapter = new ExpressAdapter();\nserverAdapter.setBasePath('/admin/queues');\n\ncreateBullBoard({\n  queues: [\n    new BullMQAdapter(emailQueue),\n    new BullMQAdapter(imageQueue),\n    new BullMQAdapter(analyticsQueue)\n  ],\n  serverAdapter\n});\n\nconst app = express();\napp.use('/admin/queues', serverAdapter.getRouter());\n\n// Queue metrics endpoint\napp.get('/metrics/queues', async (req, res) => {\n  const queues = [emailQueue, imageQueue, analyticsQueue];\n  \n  const metrics = await Promise.all(\n    queues.map(async (queue) => {\n      const [waiting, active, completed, failed, delayed] = await Promise.all([\n        queue.getWaitingCount(),\n        queue.getActiveCount(),\n        queue.getCompletedCount(),\n        queue.getFailedCount(),\n        queue.getDelayedCount()\n      ]);\n      \n      return {\n        name: queue.name,\n        waiting,\n        active,\n        completed,\n        failed,\n        delayed,\n        total: waiting + active + delayed\n      };\n    })\n  );\n  \n  res.json({ queues: metrics });\n});\n\napp.listen(3000);\n\n// Auto-scaling based on queue depth\nconst AWS = require('aws-sdk');\nconst autoScaling = new AWS.AutoScaling();\n\nasync function scaleWorkers() {\n  const queueDepth = await emailQueue.getWaitingCount();\n  const activeCount = await emailQueue.getActiveCount();\n  \n  const desiredWorkers = Math.min(\n    Math.max(\n      Math.ceil(queueDepth / 100),  // 1 worker per 100 tasks\n      1                             // Min 1 worker\n    ),\n    20                              // Max 20 workers\n  );\n  \n  await autoScaling.setDesiredCapacity({\n    AutoScalingGroupName: 'email-workers',\n    DesiredCapacity: desiredWorkers\n  }).promise();\n  \n  console.log(`Scaled to ${desiredWorkers} workers (queue: ${queueDepth}, active: ${activeCount})`);\n}\n\nsetInterval(scaleWorkers, 60000); // Check every minute"
    },
    {
      "id": 18,
      "question": "What are WebSockets and when should you use them over HTTP?",
      "answer": "WebSockets provide full-duplex, bidirectional communication between client and server over a single TCP connection, unlike HTTP's request-response model.\n\nWebSocket Characteristics:\n• Persistent connection: Stay open after initial handshake\n• Low latency: No need to establish new connection each time\n• Full-duplex: Both client and server can send messages simultaneously\n• Lower overhead: No HTTP headers on each message\n• Real-time: Instant communication without polling\n\nHTTP vs WebSocket:\n• HTTP: Stateless, request-response, higher latency, better caching\n• WebSocket: Stateful, bidirectional, real-time, no caching\n• HTTP: Good for REST APIs, content delivery\n• WebSocket: Good for chat, live updates, gaming, collaborative editing\n\nWebSocket Flow:\n• Client sends HTTP upgrade request\n• Server accepts and switches protocol\n• Connection upgraded to WebSocket\n• Both parties can send messages anytime\n• Connection stays open until closed\n\nUse Cases:\n• Chat applications: Real-time messaging\n• Live notifications: Push updates to users\n• Collaborative editing: Google Docs-style\n• Gaming: Multiplayer real-time games\n• Live sports scores: Instant updates\n• Trading platforms: Real-time price updates\n• IoT: Device communication\n\nAlternatives:\n• Server-Sent Events (SSE): One-way server to client\n• Long polling: HTTP-based pseudo-real-time\n• HTTP/2 Server Push: Limited server push capability\n\nChallenges:\n• Load balancing: Sticky sessions needed\n• Scaling: Many concurrent connections\n• Reconnection: Handle network failures\n• Security: Same-origin policy, authentication\n• Proxy compatibility: Some proxies block WebSockets",
      "explanation": "WebSockets enable real-time, bidirectional communication with persistent connections and low latency, ideal for chat and live updates, while HTTP suits stateless request-response patterns for APIs and content delivery.",
      "difficulty": "Medium",
      "code": "// WebSocket Server (Node.js with ws library)\nconst WebSocket = require('ws');\nconst http = require('http');\n\nconst server = http.createServer();\nconst wss = new WebSocket.Server({ server });\n\nconst clients = new Map(); // userId -> WebSocket connection\n\nwss.on('connection', (ws, req) => {\n  console.log('Client connected');\n  \n  let userId = null;\n  \n  ws.on('message', async (message) => {\n    try {\n      const data = JSON.parse(message);\n      \n      switch (data.type) {\n        case 'auth':\n          // Authenticate user\n          userId = await authenticateToken(data.token);\n          if (userId) {\n            clients.set(userId, ws);\n            ws.send(JSON.stringify({\n              type: 'auth_success',\n              userId\n            }));\n          } else {\n            ws.close(4001, 'Authentication failed');\n          }\n          break;\n          \n        case 'message':\n          // Send message to specific user\n          const recipientWs = clients.get(data.recipientId);\n          if (recipientWs && recipientWs.readyState === WebSocket.OPEN) {\n            recipientWs.send(JSON.stringify({\n              type: 'new_message',\n              from: userId,\n              message: data.message,\n              timestamp: Date.now()\n            }));\n          }\n          break;\n          \n        case 'typing':\n          // Broadcast typing indicator\n          const typingRecipient = clients.get(data.recipientId);\n          if (typingRecipient && typingRecipient.readyState === WebSocket.OPEN) {\n            typingRecipient.send(JSON.stringify({\n              type: 'user_typing',\n              userId\n            }));\n          }\n          break;\n          \n        case 'ping':\n          ws.send(JSON.stringify({ type: 'pong' }));\n          break;\n      }\n    } catch (error) {\n      console.error('Message handling error:', error);\n      ws.send(JSON.stringify({\n        type: 'error',\n        message: 'Invalid message format'\n      }));\n    }\n  });\n  \n  ws.on('close', (code, reason) => {\n    console.log(`Client disconnected: ${code} ${reason}`);\n    if (userId) {\n      clients.delete(userId);\n      \n      // Notify contacts that user went offline\n      broadcastToContacts(userId, {\n        type: 'user_offline',\n        userId\n      });\n    }\n  });\n  \n  ws.on('error', (error) => {\n    console.error('WebSocket error:', error);\n  });\n  \n  // Heartbeat to detect dead connections\n  ws.isAlive = true;\n  ws.on('pong', () => {\n    ws.isAlive = true;\n  });\n});\n\n// Ping clients every 30 seconds\nsetInterval(() => {\n  wss.clients.forEach((ws) => {\n    if (ws.isAlive === false) {\n      return ws.terminate();\n    }\n    \n    ws.isAlive = false;\n    ws.ping();\n  });\n}, 30000);\n\n// Broadcast to all connected clients\nfunction broadcast(message) {\n  wss.clients.forEach((client) => {\n    if (client.readyState === WebSocket.OPEN) {\n      client.send(JSON.stringify(message));\n    }\n  });\n}\n\n// Broadcast to specific users\nfunction broadcastToUsers(userIds, message) {\n  userIds.forEach(userId => {\n    const ws = clients.get(userId);\n    if (ws && ws.readyState === WebSocket.OPEN) {\n      ws.send(JSON.stringify(message));\n    }\n  });\n}\n\nserver.listen(8080);\n\n// WebSocket Client (Browser)\nclass WebSocketClient {\n  constructor(url, userId, token) {\n    this.url = url;\n    this.userId = userId;\n    this.token = token;\n    this.ws = null;\n    this.reconnectAttempts = 0;\n    this.maxReconnectAttempts = 5;\n    this.reconnectDelay = 1000;\n    this.handlers = new Map();\n  }\n  \n  connect() {\n    this.ws = new WebSocket(this.url);\n    \n    this.ws.onopen = () => {\n      console.log('Connected to server');\n      this.reconnectAttempts = 0;\n      \n      // Authenticate\n      this.send({\n        type: 'auth',\n        token: this.token\n      });\n      \n      // Start heartbeat\n      this.startHeartbeat();\n    };\n    \n    this.ws.onmessage = (event) => {\n      try {\n        const data = JSON.parse(event.data);\n        const handler = this.handlers.get(data.type);\n        if (handler) {\n          handler(data);\n        }\n      } catch (error) {\n        console.error('Message parsing error:', error);\n      }\n    };\n    \n    this.ws.onerror = (error) => {\n      console.error('WebSocket error:', error);\n    };\n    \n    this.ws.onclose = (event) => {\n      console.log('Disconnected from server:', event.code, event.reason);\n      this.stopHeartbeat();\n      this.reconnect();\n    };\n  }\n  \n  reconnect() {\n    if (this.reconnectAttempts >= this.maxReconnectAttempts) {\n      console.error('Max reconnection attempts reached');\n      return;\n    }\n    \n    this.reconnectAttempts++;\n    const delay = this.reconnectDelay * Math.pow(2, this.reconnectAttempts - 1);\n    \n    console.log(`Reconnecting in ${delay}ms (attempt ${this.reconnectAttempts})`);\n    \n    setTimeout(() => {\n      this.connect();\n    }, delay);\n  }\n  \n  send(data) {\n    if (this.ws && this.ws.readyState === WebSocket.OPEN) {\n      this.ws.send(JSON.stringify(data));\n    } else {\n      console.error('WebSocket not connected');\n    }\n  }\n  \n  on(type, handler) {\n    this.handlers.set(type, handler);\n  }\n  \n  startHeartbeat() {\n    this.heartbeatInterval = setInterval(() => {\n      this.send({ type: 'ping' });\n    }, 30000);\n  }\n  \n  stopHeartbeat() {\n    if (this.heartbeatInterval) {\n      clearInterval(this.heartbeatInterval);\n    }\n  }\n  \n  disconnect() {\n    this.stopHeartbeat();\n    if (this.ws) {\n      this.ws.close();\n    }\n  }\n}\n\n// Usage\nconst client = new WebSocketClient('ws://localhost:8080', userId, token);\n\nclient.on('auth_success', (data) => {\n  console.log('Authenticated as', data.userId);\n});\n\nclient.on('new_message', (data) => {\n  console.log('New message from', data.from, ':', data.message);\n  displayMessage(data);\n});\n\nclient.on('user_typing', (data) => {\n  showTypingIndicator(data.userId);\n});\n\nclient.connect();\n\n// Send message\nfunction sendMessage(recipientId, message) {\n  client.send({\n    type: 'message',\n    recipientId,\n    message\n  });\n}\n\n// Comparison: HTTP Polling vs WebSocket\n\n// HTTP Long Polling\nasync function longPoll() {\n  while (true) {\n    try {\n      const response = await fetch('/api/messages/poll', {\n        method: 'GET',\n        headers: { 'Authorization': `Bearer ${token}` }\n      });\n      \n      const messages = await response.json();\n      messages.forEach(displayMessage);\n    } catch (error) {\n      console.error('Polling error:', error);\n      await sleep(5000); // Wait before retry\n    }\n  }\n}\n\n// Problem: High latency, server overhead, wasted bandwidth\n\n// Socket.io (Popular WebSocket Library)\nconst io = require('socket.io');\nconst socketServer = io(server);\n\nsocketServer.on('connection', (socket) => {\n  console.log('User connected:', socket.id);\n  \n  socket.on('join_room', (roomId) => {\n    socket.join(roomId);\n    socket.to(roomId).emit('user_joined', { userId: socket.userId });\n  });\n  \n  socket.on('send_message', (data) => {\n    // Send to specific room\n    socketServer.to(data.roomId).emit('new_message', {\n      from: socket.userId,\n      message: data.message,\n      timestamp: Date.now()\n    });\n  });\n  \n  socket.on('disconnect', () => {\n    console.log('User disconnected:', socket.id);\n  });\n});\n\n// Client\nconst ioClient = require('socket.io-client');\nconst socket = ioClient('http://localhost:8080');\n\nsocket.on('connect', () => {\n  console.log('Connected');\n  socket.emit('join_room', 'room123');\n});\n\nsocket.on('new_message', (data) => {\n  console.log('New message:', data);\n});\n\nsocket.emit('send_message', {\n  roomId: 'room123',\n  message: 'Hello!'\n});\n\n// Load Balancing WebSockets (Redis Adapter)\nconst redisAdapter = require('@socket.io/redis-adapter');\nconst { createClient } = require('redis');\n\nconst pubClient = createClient({ host: 'localhost', port: 6379 });\nconst subClient = pubClient.duplicate();\n\nPromise.all([pubClient.connect(), subClient.connect()]).then(() => {\n  socketServer.adapter(redisAdapter(pubClient, subClient));\n});\n\n// Now multiple Socket.io servers can communicate\n// Messages sent from one server reach clients on other servers\n\n// Nginx Configuration for WebSocket Load Balancing\nupstream websocket_backend {\n    # Use IP hash for sticky sessions\n    ip_hash;\n    \n    server backend1.example.com:8080;\n    server backend2.example.com:8080;\n    server backend3.example.com:8080;\n}\n\nserver {\n    listen 80;\n    server_name ws.example.com;\n    \n    location / {\n        proxy_pass http://websocket_backend;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"Upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        # Timeouts\n        proxy_connect_timeout 7d;\n        proxy_send_timeout 7d;\n        proxy_read_timeout 7d;\n    }\n}"
    },
    {
      "id": 19,
      "question": "How do you implement distributed transactions across microservices?",
      "answer": "Distributed transactions across microservices are challenging because each service has its own database, making traditional ACID transactions impossible.\n\nChallenges:\n• No shared database for atomic commits\n• Network failures between services\n• Partial failures (some services succeed, others fail)\n• Performance overhead of coordination\n• Complexity of rollback across services\n\nSolutions and Patterns:\n\n1. Two-Phase Commit (2PC):\n• Phase 1: Coordinator asks all services to prepare (vote)\n• Phase 2: If all vote yes, commit; otherwise, rollback\n• Problem: Blocking protocol, coordinator is single point of failure\n• Rarely used in microservices due to tight coupling\n\n2. Saga Pattern (Recommended):\n• Sequence of local transactions\n• Each transaction updates own database and publishes event\n• If one fails, execute compensating transactions to undo\n• Two flavors: Choreography and Orchestration\n\n3. Event Sourcing:\n• Store events instead of current state\n• Rebuild state by replaying events\n• Natural fit for eventual consistency\n\n4. Outbox Pattern:\n• Service writes to database and outbox table atomically\n• Separate process publishes outbox events to message broker\n• Guarantees message delivery\n\nSaga Choreography:\n• Services listen to events and trigger next step\n• Decentralized, no orchestrator\n• Harder to track and debug\n\nSaga Orchestration:\n• Central orchestrator coordinates saga\n• Easier to monitor and manage\n• Orchestrator can become bottleneck\n\nWhen to Use:\n• Use Saga for business transactions across services\n• Use eventual consistency when possible\n• Avoid distributed transactions if you can redesign\n• Consider merging services if transactions are common",
      "explanation": "Distributed transactions in microservices use the Saga pattern with compensating actions for failures, avoiding traditional 2PC locks, implemented via choreography (event-driven) or orchestration (coordinator) approaches.",
      "difficulty": "Hard",
      "code": "// Saga Pattern - Orchestration Approach\n// Central orchestrator coordinates the entire saga\n\nconst { Kafka } = require('kafkajs');\nconst kafka = new Kafka({ brokers: ['localhost:9092'] });\nconst producer = kafka.producer();\n\nclass OrderSagaOrchestrator {\n  constructor() {\n    this.sagas = new Map(); // sagaId -> state\n  }\n  \n  async createOrder(orderData) {\n    const sagaId = generateUUID();\n    \n    const saga = {\n      id: sagaId,\n      status: 'started',\n      steps: [\n        { service: 'inventory', action: 'reserve', status: 'pending', data: orderData },\n        { service: 'payment', action: 'charge', status: 'pending', data: orderData },\n        { service: 'shipping', action: 'schedule', status: 'pending', data: orderData }\n      ],\n      currentStep: 0\n    };\n    \n    this.sagas.set(sagaId, saga);\n    \n    try {\n      // Execute saga steps\n      await this.executeNextStep(sagaId);\n    } catch (error) {\n      // Rollback on failure\n      await this.rollback(sagaId);\n      throw error;\n    }\n  }\n  \n  async executeNextStep(sagaId) {\n    const saga = this.sagas.get(sagaId);\n    const step = saga.steps[saga.currentStep];\n    \n    console.log(`Executing step ${saga.currentStep}: ${step.service}.${step.action}`);\n    \n    try {\n      // Send command to service\n      await producer.send({\n        topic: `${step.service}-commands`,\n        messages: [{\n          key: sagaId,\n          value: JSON.stringify({\n            sagaId,\n            action: step.action,\n            data: step.data\n          })\n        }]\n      });\n      \n      // Wait for response (in practice, listen to response topic)\n      step.status = 'completed';\n      saga.currentStep++;\n      \n      // If more steps, continue\n      if (saga.currentStep < saga.steps.length) {\n        await this.executeNextStep(sagaId);\n      } else {\n        saga.status = 'completed';\n        console.log(`Saga ${sagaId} completed successfully`);\n      }\n    } catch (error) {\n      step.status = 'failed';\n      step.error = error.message;\n      saga.status = 'failed';\n      throw error;\n    }\n  }\n  \n  async rollback(sagaId) {\n    const saga = this.sagas.get(sagaId);\n    console.log(`Rolling back saga ${sagaId}`);\n    \n    // Execute compensating transactions in reverse order\n    for (let i = saga.currentStep - 1; i >= 0; i--) {\n      const step = saga.steps[i];\n      \n      if (step.status === 'completed') {\n        const compensatingAction = this.getCompensatingAction(step.action);\n        \n        console.log(`Compensating: ${step.service}.${compensatingAction}`);\n        \n        await producer.send({\n          topic: `${step.service}-commands`,\n          messages: [{\n            key: sagaId,\n            value: JSON.stringify({\n              sagaId,\n              action: compensatingAction,\n              data: step.data\n            })\n          }]\n        });\n      }\n    }\n    \n    saga.status = 'rolled_back';\n  }\n  \n  getCompensatingAction(action) {\n    const compensations = {\n      'reserve': 'release',\n      'charge': 'refund',\n      'schedule': 'cancel'\n    };\n    return compensations[action];\n  }\n}\n\n// Saga Pattern - Choreography Approach\n// Services listen to events and trigger next actions\n\n// Inventory Service\nclass InventoryService {\n  async handleOrderCreated(event) {\n    const { orderId, items } = event;\n    \n    try {\n      // Reserve inventory\n      await this.reserveItems(items);\n      \n      // Publish success event\n      await producer.send({\n        topic: 'order-events',\n        messages: [{\n          key: orderId,\n          value: JSON.stringify({\n            type: 'InventoryReserved',\n            orderId,\n            items\n          })\n        }]\n      });\n    } catch (error) {\n      // Publish failure event\n      await producer.send({\n        topic: 'order-events',\n        messages: [{\n          key: orderId,\n          value: JSON.stringify({\n            type: 'InventoryReservationFailed',\n            orderId,\n            reason: error.message\n          })\n        }]\n      });\n    }\n  }\n  \n  async handlePaymentFailed(event) {\n    // Compensating action: release inventory\n    const { orderId } = event;\n    await this.releaseItems(orderId);\n  }\n}\n\n// Payment Service\nclass PaymentService {\n  async handleInventoryReserved(event) {\n    const { orderId, total } = event;\n    \n    try {\n      // Charge payment\n      await this.chargeCustomer(orderId, total);\n      \n      // Publish success event\n      await producer.send({\n        topic: 'order-events',\n        messages: [{\n          key: orderId,\n          value: JSON.stringify({\n            type: 'PaymentCompleted',\n            orderId,\n            amount: total\n          })\n        }]\n      });\n    } catch (error) {\n      // Publish failure event\n      await producer.send({\n        topic: 'order-events',\n        messages: [{\n          key: orderId,\n          value: JSON.stringify({\n            type: 'PaymentFailed',\n            orderId,\n            reason: error.message\n          })\n        }]\n      });\n    }\n  }\n  \n  async handleShippingFailed(event) {\n    // Compensating action: refund payment\n    const { orderId } = event;\n    await this.refundCustomer(orderId);\n  }\n}\n\n// Outbox Pattern for Reliable Event Publishing\nconst { Client } = require('pg');\n\nclass OrderService {\n  async createOrder(orderData) {\n    const client = new Client();\n    await client.connect();\n    \n    try {\n      await client.query('BEGIN');\n      \n      // 1. Insert order into database\n      const result = await client.query(\n        'INSERT INTO orders (user_id, total, items) VALUES ($1, $2, $3) RETURNING id',\n        [orderData.userId, orderData.total, JSON.stringify(orderData.items)]\n      );\n      \n      const orderId = result.rows[0].id;\n      \n      // 2. Insert event into outbox table (same transaction)\n      await client.query(\n        'INSERT INTO outbox (aggregate_id, event_type, payload) VALUES ($1, $2, $3)',\n        [\n          orderId,\n          'OrderCreated',\n          JSON.stringify({ orderId, ...orderData })\n        ]\n      );\n      \n      await client.query('COMMIT');\n      \n      return { orderId };\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      await client.end();\n    }\n  }\n}\n\n// Outbox Publisher (separate process)\nclass OutboxPublisher {\n  async pollAndPublish() {\n    const client = new Client();\n    await client.connect();\n    \n    try {\n      // Get unpublished events\n      const result = await client.query(\n        'SELECT * FROM outbox WHERE published = false ORDER BY created_at LIMIT 100'\n      );\n      \n      for (const event of result.rows) {\n        try {\n          // Publish to Kafka\n          await producer.send({\n            topic: 'order-events',\n            messages: [{\n              key: event.aggregate_id,\n              value: event.payload\n            }]\n          });\n          \n          // Mark as published\n          await client.query(\n            'UPDATE outbox SET published = true, published_at = NOW() WHERE id = $1',\n            [event.id]\n          );\n        } catch (error) {\n          console.error(`Failed to publish event ${event.id}:`, error);\n        }\n      }\n    } finally {\n      await client.end();\n    }\n  }\n  \n  start() {\n    setInterval(() => this.pollAndPublish(), 1000); // Poll every second\n  }\n}\n\n// Event Sourcing for Distributed Transactions\nclass EventStore {\n  async appendEvent(aggregateId, event) {\n    const client = new Client();\n    await client.connect();\n    \n    try {\n      // Store event\n      await client.query(\n        `INSERT INTO events (aggregate_id, event_type, event_data, version) \n         VALUES ($1, $2, $3, (SELECT COALESCE(MAX(version), 0) + 1 FROM events WHERE aggregate_id = $1))`,\n        [aggregateId, event.type, JSON.stringify(event.data)]\n      );\n      \n      // Publish event to message broker\n      await producer.send({\n        topic: 'events',\n        messages: [{\n          key: aggregateId,\n          value: JSON.stringify(event)\n        }]\n      });\n    } finally {\n      await client.end();\n    }\n  }\n  \n  async getEvents(aggregateId) {\n    const client = new Client();\n    await client.connect();\n    \n    try {\n      const result = await client.query(\n        'SELECT * FROM events WHERE aggregate_id = $1 ORDER BY version',\n        [aggregateId]\n      );\n      \n      return result.rows.map(row => ({\n        type: row.event_type,\n        data: JSON.parse(row.event_data),\n        version: row.version\n      }));\n    } finally {\n      await client.end();\n    }\n  }\n}\n\n// Rebuild aggregate from events\nclass OrderAggregate {\n  constructor(orderId) {\n    this.id = orderId;\n    this.status = 'pending';\n    this.items = [];\n    this.total = 0;\n  }\n  \n  apply(event) {\n    switch (event.type) {\n      case 'OrderCreated':\n        this.items = event.data.items;\n        this.total = event.data.total;\n        this.status = 'created';\n        break;\n      \n      case 'InventoryReserved':\n        this.status = 'inventory_reserved';\n        break;\n      \n      case 'PaymentCompleted':\n        this.status = 'paid';\n        break;\n      \n      case 'OrderShipped':\n        this.status = 'shipped';\n        break;\n      \n      case 'OrderCancelled':\n        this.status = 'cancelled';\n        break;\n    }\n  }\n  \n  static async load(orderId, eventStore) {\n    const aggregate = new OrderAggregate(orderId);\n    const events = await eventStore.getEvents(orderId);\n    \n    events.forEach(event => aggregate.apply(event));\n    \n    return aggregate;\n  }\n}\n\n// Usage\nconst eventStore = new EventStore();\nconst order = await OrderAggregate.load('order-123', eventStore);\nconsole.log(order.status); // 'shipped'"
    },
    {
      "id": 20,
      "question": "What is the Circuit Breaker pattern and how does it work?",
      "answer": "The Circuit Breaker pattern prevents cascading failures in distributed systems by detecting failures and temporarily blocking calls to failing services.\n\nCircuit States:\n• Closed: Normal operation, requests pass through\n• Open: Service is failing, immediately return error without calling\n• Half-Open: Test if service recovered, allow limited requests\n\nState Transitions:\n• Closed → Open: After threshold of failures reached\n• Open → Half-Open: After timeout period expires\n• Half-Open → Closed: If test requests succeed\n• Half-Open → Open: If test requests fail\n\nConfiguration Parameters:\n• Failure threshold: Number/percentage of failures to open circuit\n• Timeout: How long circuit stays open before testing\n• Success threshold: Successful requests needed to close circuit\n• Request volume threshold: Minimum requests before calculating failures\n\nBenefits:\n• Fail fast: Don't wait for timeouts on failing service\n• Prevent cascading failures: Protect upstream services\n• Give failing service time to recover\n• Automatic recovery detection\n• Reduce load on failing service\n\nUse Cases:\n• External API calls\n• Database connections\n• Microservice communication\n• Third-party service integrations\n\nBest Practices:\n• Define meaningful fallbacks\n• Monitor circuit state changes\n• Set appropriate thresholds\n• Combine with retry logic\n• Log failures for debugging\n• Use bulkheads to isolate failures\n\nLibraries:\n• Resilience4j (Java)\n• Hystrix (deprecated, Netflix)\n• Polly (.NET)\n• Opossum (Node.js)\n• PyBreaker (Python)",
      "explanation": "Circuit Breaker prevents cascading failures by monitoring service calls and automatically blocking requests to failing services, allowing recovery time while failing fast to protect system stability.",
      "difficulty": "Medium",
      "code": "// Circuit Breaker Implementation (Node.js)\nclass CircuitBreaker {\n  constructor(options = {}) {\n    this.failureThreshold = options.failureThreshold || 5;\n    this.successThreshold = options.successThreshold || 2;\n    this.timeout = options.timeout || 60000;  // 60 seconds\n    this.requestVolumeThreshold = options.requestVolumeThreshold || 10;\n    \n    this.state = 'CLOSED';\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.requestCount = 0;\n    this.nextAttempt = Date.now();\n  }\n  \n  async call(fn, fallback) {\n    if (this.state === 'OPEN') {\n      if (Date.now() < this.nextAttempt) {\n        // Circuit is open, use fallback\n        console.log('Circuit is OPEN, using fallback');\n        return fallback ? fallback() : Promise.reject(new Error('Circuit breaker is OPEN'));\n      }\n      \n      // Timeout expired, transition to HALF_OPEN\n      this.state = 'HALF_OPEN';\n      console.log('Circuit transitioning to HALF_OPEN');\n    }\n    \n    try {\n      const result = await fn();\n      this.onSuccess();\n      return result;\n    } catch (error) {\n      this.onFailure();\n      \n      if (fallback) {\n        return fallback();\n      }\n      throw error;\n    }\n  }\n  \n  onSuccess() {\n    this.requestCount++;\n    \n    if (this.state === 'HALF_OPEN') {\n      this.successCount++;\n      \n      if (this.successCount >= this.successThreshold) {\n        // Enough successes, close circuit\n        this.close();\n      }\n    } else if (this.state === 'CLOSED') {\n      // Reset failure count on success\n      this.failureCount = 0;\n    }\n  }\n  \n  onFailure() {\n    this.requestCount++;\n    this.failureCount++;\n    \n    if (this.state === 'HALF_OPEN') {\n      // Failed during testing, reopen circuit\n      this.open();\n    } else if (this.state === 'CLOSED') {\n      // Check if we should open circuit\n      if (this.requestCount >= this.requestVolumeThreshold &&\n          this.failureCount >= this.failureThreshold) {\n        this.open();\n      }\n    }\n  }\n  \n  open() {\n    this.state = 'OPEN';\n    this.nextAttempt = Date.now() + this.timeout;\n    console.log(`Circuit OPENED until ${new Date(this.nextAttempt)}`);\n    \n    // Reset counters\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.requestCount = 0;\n  }\n  \n  close() {\n    this.state = 'CLOSED';\n    console.log('Circuit CLOSED');\n    \n    // Reset counters\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.requestCount = 0;\n  }\n  \n  getState() {\n    return {\n      state: this.state,\n      failureCount: this.failureCount,\n      successCount: this.successCount,\n      requestCount: this.requestCount,\n      nextAttempt: this.state === 'OPEN' ? new Date(this.nextAttempt) : null\n    };\n  }\n}\n\n// Usage\nconst axios = require('axios');\n\nconst breaker = new CircuitBreaker({\n  failureThreshold: 5,\n  successThreshold: 2,\n  timeout: 30000,           // 30 seconds\n  requestVolumeThreshold: 10\n});\n\nasync function callExternalAPI(userId) {\n  return await breaker.call(\n    // Main function\n    async () => {\n      const response = await axios.get(\n        `https://api.example.com/users/${userId}`,\n        { timeout: 5000 }\n      );\n      return response.data;\n    },\n    // Fallback function\n    async () => {\n      console.log('Using cached data');\n      return await getCachedUser(userId);\n    }\n  );\n}\n\n// Using Opossum Library (Production-Ready)\nconst CircuitBreaker = require('opossum');\n\nconst options = {\n  timeout: 3000,              // 3 second timeout\n  errorThresholdPercentage: 50,  // Open after 50% failures\n  resetTimeout: 30000,        // Try again after 30 seconds\n  rollingCountTimeout: 10000, // 10 second rolling window\n  rollingCountBuckets: 10,    // Number of buckets for rolling count\n  name: 'externalAPIBreaker'\n};\n\nconst breaker = new CircuitBreaker(callAPI, options);\n\n// Fallback\nbreaker.fallback(() => {\n  return { data: 'fallback response' };\n});\n\n// Event listeners\nbreaker.on('open', () => {\n  console.log('Circuit breaker opened!');\n  alertOps('Circuit breaker opened for externalAPIBreaker');\n});\n\nbreaker.on('halfOpen', () => {\n  console.log('Circuit breaker half-open, testing...');\n});\n\nbreaker.on('close', () => {\n  console.log('Circuit breaker closed, service recovered');\n});\n\nbreaker.on('failure', (error) => {\n  console.error('Request failed:', error.message);\n});\n\nbreaker.on('success', (result) => {\n  console.log('Request succeeded');\n});\n\nbreaker.on('timeout', () => {\n  console.error('Request timed out');\n});\n\nbreaker.on('reject', () => {\n  console.log('Request rejected (circuit open)');\n});\n\n// Use the breaker\nasync function getData() {\n  try {\n    const result = await breaker.fire({ param: 'value' });\n    return result;\n  } catch (error) {\n    console.error('Failed to get data:', error);\n    throw error;\n  }\n}\n\n// With Express middleware\nconst express = require('express');\nconst app = express();\n\napp.get('/api/users/:id', async (req, res) => {\n  try {\n    const user = await breaker.fire(req.params.id);\n    res.json(user);\n  } catch (error) {\n    if (breaker.opened) {\n      res.status(503).json({\n        error: 'Service temporarily unavailable',\n        retryAfter: breaker.remainingTime() / 1000\n      });\n    } else {\n      res.status(500).json({ error: 'Internal server error' });\n    }\n  }\n});\n\n// Health check endpoint\napp.get('/health/breakers', (req, res) => {\n  const stats = breaker.stats;\n  const health = {\n    name: breaker.name,\n    state: breaker.opened ? 'OPEN' : breaker.halfOpen ? 'HALF_OPEN' : 'CLOSED',\n    stats: {\n      failures: stats.failures,\n      successes: stats.successes,\n      rejects: stats.rejects,\n      timeouts: stats.timeouts,\n      failureRate: stats.failures / (stats.fires || 1)\n    }\n  };\n  \n  res.json(health);\n});\n\n// Resilience4j Example (Java/Spring Boot)\n/*\n@Configuration\npublic class CircuitBreakerConfig {\n    \n    @Bean\n    public CircuitBreakerRegistry circuitBreakerRegistry() {\n        CircuitBreakerConfig config = CircuitBreakerConfig.custom()\n            .failureRateThreshold(50.0f)\n            .waitDurationInOpenState(Duration.ofMillis(30000))\n            .slidingWindowType(SlidingWindowType.COUNT_BASED)\n            .slidingWindowSize(10)\n            .minimumNumberOfCalls(5)\n            .permittedNumberOfCallsInHalfOpenState(3)\n            .build();\n        \n        return CircuitBreakerRegistry.of(config);\n    }\n}\n\n@Service\npublic class UserService {\n    \n    private final CircuitBreaker circuitBreaker;\n    private final RestTemplate restTemplate;\n    \n    public UserService(CircuitBreakerRegistry registry) {\n        this.circuitBreaker = registry.circuitBreaker(\"userService\");\n        this.restTemplate = new RestTemplate();\n    }\n    \n    public User getUser(String userId) {\n        return circuitBreaker.executeSupplier(() -> {\n            // Call external service\n            return restTemplate.getForObject(\n                \"https://api.example.com/users/\" + userId,\n                User.class\n            );\n        });\n    }\n    \n    // With fallback\n    public User getUserWithFallback(String userId) {\n        return Try.ofSupplier(\n            circuitBreaker.decorateSupplier(() -> \n                restTemplate.getForObject(\n                    \"https://api.example.com/users/\" + userId,\n                    User.class\n                )\n            )\n        ).recover(throwable -> getCachedUser(userId))\n         .get();\n    }\n}\n*/\n\n// Monitoring Circuit Breakers\nconst prometheus = require('prom-client');\n\nconst circuitStateGauge = new prometheus.Gauge({\n  name: 'circuit_breaker_state',\n  help: 'Circuit breaker state (0=closed, 1=half-open, 2=open)',\n  labelNames: ['name']\n});\n\nconst circuitFailuresCounter = new prometheus.Counter({\n  name: 'circuit_breaker_failures_total',\n  help: 'Total number of failures',\n  labelNames: ['name']\n});\n\nbreaker.on('open', () => {\n  circuitStateGauge.set({ name: breaker.name }, 2);\n});\n\nbreaker.on('halfOpen', () => {\n  circuitStateGauge.set({ name: breaker.name }, 1);\n});\n\nbreaker.on('close', () => {\n  circuitStateGauge.set({ name: breaker.name }, 0);\n});\n\nbreaker.on('failure', () => {\n  circuitFailuresCounter.inc({ name: breaker.name });\n});"
    },
    {
      "id": 21,
      "question": "What is the difference between authentication and authorization?",
      "answer": "Authentication and authorization are two distinct security concepts that work together to control access to systems and resources.\n\nAuthentication (Who are you?):\n• Verifies identity of a user or system\n• Confirms credentials (username/password, token, certificate)\n• Answers: Is this user who they claim to be?\n• Methods: Password, OAuth, JWT, biometrics, MFA\n• Occurs first in the security flow\n• Example: Logging into a system\n\nAuthorization (What can you do?):\n• Determines what authenticated user can access\n• Grants or denies permissions to resources\n• Answers: Is this user allowed to perform this action?\n• Methods: RBAC, ABAC, ACL, policies\n• Occurs after authentication\n• Example: Checking if user can delete a file\n\nAuthentication Methods:\n• Basic Auth: Username and password in header\n• Session-based: Server stores session, cookie sent to client\n• Token-based: JWT, OAuth tokens\n• Multi-factor (MFA): Multiple verification methods\n• Biometric: Fingerprint, face recognition\n• Certificate-based: SSL/TLS certificates\n\nAuthorization Models:\n• Role-Based Access Control (RBAC): Assign permissions to roles\n• Attribute-Based Access Control (ABAC): Policy based on attributes\n• Access Control Lists (ACL): Per-resource permission lists\n• Policy-Based: Complex rules and conditions\n\nJWT (JSON Web Token):\n• Self-contained token with claims\n• Stateless authentication\n• Contains: header, payload, signature\n• Can include authorization info (roles, permissions)\n\nOAuth 2.0:\n• Authorization framework for third-party access\n• Allows apps to access user data without password\n• Uses access tokens and refresh tokens\n• Flows: Authorization Code, Implicit, Client Credentials\n\nBest Practices:\n• Never store passwords in plain text\n• Use HTTPS for all authentication\n• Implement rate limiting on login endpoints\n• Use secure session management\n• Implement principle of least privilege\n• Separate authentication and authorization logic\n• Use standard protocols (OAuth, OpenID Connect)\n• Implement MFA for sensitive operations",
      "explanation": "Authentication verifies user identity (who you are) through credentials, while authorization determines access permissions (what you can do) after identity is confirmed, forming two sequential layers of security control.",
      "difficulty": "Easy",
      "code": "// JWT Authentication (Node.js with Express)\nconst express = require('express');\nconst jwt = require('jsonwebtoken');\nconst bcrypt = require('bcrypt');\n\nconst app = express();\nconst JWT_SECRET = process.env.JWT_SECRET;\nconst JWT_EXPIRY = '1h';\n\napp.use(express.json());\n\n// AUTHENTICATION: Login endpoint\napp.post('/api/login', async (req, res) => {\n  const { email, password } = req.body;\n  \n  try {\n    // Find user\n    const user = await db.query(\n      'SELECT * FROM users WHERE email = $1',\n      [email]\n    );\n    \n    if (!user.rows[0]) {\n      return res.status(401).json({ error: 'Invalid credentials' });\n    }\n    \n    // Verify password (authentication)\n    const validPassword = await bcrypt.compare(\n      password,\n      user.rows[0].password_hash\n    );\n    \n    if (!validPassword) {\n      return res.status(401).json({ error: 'Invalid credentials' });\n    }\n    \n    // Generate JWT token\n    const token = jwt.sign(\n      {\n        userId: user.rows[0].id,\n        email: user.rows[0].email,\n        roles: user.rows[0].roles  // Include authorization info\n      },\n      JWT_SECRET,\n      { expiresIn: JWT_EXPIRY }\n    );\n    \n    res.json({\n      token,\n      user: {\n        id: user.rows[0].id,\n        email: user.rows[0].email,\n        name: user.rows[0].name\n      }\n    });\n  } catch (error) {\n    console.error('Login error:', error);\n    res.status(500).json({ error: 'Internal server error' });\n  }\n});\n\n// AUTHENTICATION: Middleware to verify JWT\nfunction authenticate(req, res, next) {\n  const authHeader = req.headers.authorization;\n  \n  if (!authHeader || !authHeader.startsWith('Bearer ')) {\n    return res.status(401).json({ error: 'No token provided' });\n  }\n  \n  const token = authHeader.substring(7);\n  \n  try {\n    const decoded = jwt.verify(token, JWT_SECRET);\n    req.user = decoded;  // Attach user info to request\n    next();\n  } catch (error) {\n    if (error.name === 'TokenExpiredError') {\n      return res.status(401).json({ error: 'Token expired' });\n    }\n    return res.status(401).json({ error: 'Invalid token' });\n  }\n}\n\n// AUTHORIZATION: Role-based middleware\nfunction authorize(...allowedRoles) {\n  return (req, res, next) => {\n    if (!req.user) {\n      return res.status(401).json({ error: 'Not authenticated' });\n    }\n    \n    const userRoles = req.user.roles || [];\n    const hasPermission = allowedRoles.some(role => \n      userRoles.includes(role)\n    );\n    \n    if (!hasPermission) {\n      return res.status(403).json({ \n        error: 'Forbidden: Insufficient permissions' \n      });\n    }\n    \n    next();\n  };\n}\n\n// Protected routes with authentication and authorization\n\n// Only authenticated users\napp.get('/api/profile', authenticate, async (req, res) => {\n  const user = await db.query(\n    'SELECT id, email, name FROM users WHERE id = $1',\n    [req.user.userId]\n  );\n  \n  res.json(user.rows[0]);\n});\n\n// Only authenticated users with 'admin' role\napp.get('/api/admin/users', \n  authenticate, \n  authorize('admin'), \n  async (req, res) => {\n    const users = await db.query('SELECT id, email, name, roles FROM users');\n    res.json(users.rows);\n  }\n);\n\n// Multiple roles allowed\napp.delete('/api/posts/:id', \n  authenticate,\n  authorize('admin', 'moderator'),\n  async (req, res) => {\n    await db.query('DELETE FROM posts WHERE id = $1', [req.params.id]);\n    res.json({ message: 'Post deleted' });\n  }\n);\n\n// Resource-based authorization\napp.put('/api/posts/:id', authenticate, async (req, res) => {\n  const postId = req.params.id;\n  \n  // Get post\n  const post = await db.query(\n    'SELECT * FROM posts WHERE id = $1',\n    [postId]\n  );\n  \n  if (!post.rows[0]) {\n    return res.status(404).json({ error: 'Post not found' });\n  }\n  \n  // Check authorization: user must own the post or be admin\n  const isOwner = post.rows[0].user_id === req.user.userId;\n  const isAdmin = req.user.roles.includes('admin');\n  \n  if (!isOwner && !isAdmin) {\n    return res.status(403).json({ \n      error: 'You can only edit your own posts' \n    });\n  }\n  \n  // Update post\n  await db.query(\n    'UPDATE posts SET title = $1, content = $2 WHERE id = $3',\n    [req.body.title, req.body.content, postId]\n  );\n  \n  res.json({ message: 'Post updated' });\n});\n\n// OAuth 2.0 Implementation\nconst crypto = require('crypto');\n\n// OAuth Authorization endpoint\napp.get('/oauth/authorize', (req, res) => {\n  const { response_type, client_id, redirect_uri, scope, state } = req.query;\n  \n  // Validate client\n  const client = getClient(client_id);\n  if (!client || client.redirect_uri !== redirect_uri) {\n    return res.status(400).json({ error: 'Invalid client' });\n  }\n  \n  // Show authorization page to user\n  res.render('authorize', {\n    client,\n    scope,\n    state\n  });\n});\n\n// User approves authorization\napp.post('/oauth/authorize/approve', authenticate, async (req, res) => {\n  const { client_id, redirect_uri, scope, state } = req.body;\n  \n  // Generate authorization code\n  const code = crypto.randomBytes(32).toString('hex');\n  \n  // Store authorization code\n  await db.query(\n    'INSERT INTO auth_codes (code, client_id, user_id, scope, expires_at) VALUES ($1, $2, $3, $4, $5)',\n    [code, client_id, req.user.userId, scope, new Date(Date.now() + 600000)] // 10 min expiry\n  );\n  \n  // Redirect to client with code\n  const redirectUrl = new URL(redirect_uri);\n  redirectUrl.searchParams.set('code', code);\n  if (state) redirectUrl.searchParams.set('state', state);\n  \n  res.redirect(redirectUrl.toString());\n});\n\n// OAuth Token endpoint\napp.post('/oauth/token', async (req, res) => {\n  const { grant_type, code, client_id, client_secret, redirect_uri } = req.body;\n  \n  if (grant_type !== 'authorization_code') {\n    return res.status(400).json({ error: 'Unsupported grant type' });\n  }\n  \n  // Verify client credentials\n  const client = await db.query(\n    'SELECT * FROM oauth_clients WHERE id = $1 AND secret = $2',\n    [client_id, client_secret]\n  );\n  \n  if (!client.rows[0]) {\n    return res.status(401).json({ error: 'Invalid client' });\n  }\n  \n  // Verify authorization code\n  const authCode = await db.query(\n    'SELECT * FROM auth_codes WHERE code = $1 AND client_id = $2 AND expires_at > NOW()',\n    [code, client_id]\n  );\n  \n  if (!authCode.rows[0]) {\n    return res.status(400).json({ error: 'Invalid authorization code' });\n  }\n  \n  // Delete used authorization code\n  await db.query('DELETE FROM auth_codes WHERE code = $1', [code]);\n  \n  // Generate access token and refresh token\n  const accessToken = jwt.sign(\n    {\n      userId: authCode.rows[0].user_id,\n      client_id,\n      scope: authCode.rows[0].scope\n    },\n    JWT_SECRET,\n    { expiresIn: '1h' }\n  );\n  \n  const refreshToken = crypto.randomBytes(32).toString('hex');\n  \n  // Store refresh token\n  await db.query(\n    'INSERT INTO refresh_tokens (token, client_id, user_id, scope) VALUES ($1, $2, $3, $4)',\n    [refreshToken, client_id, authCode.rows[0].user_id, authCode.rows[0].scope]\n  );\n  \n  res.json({\n    access_token: accessToken,\n    token_type: 'Bearer',\n    expires_in: 3600,\n    refresh_token: refreshToken,\n    scope: authCode.rows[0].scope\n  });\n});\n\n// Refresh access token\napp.post('/oauth/refresh', async (req, res) => {\n  const { grant_type, refresh_token, client_id, client_secret } = req.body;\n  \n  if (grant_type !== 'refresh_token') {\n    return res.status(400).json({ error: 'Invalid grant type' });\n  }\n  \n  // Verify client\n  const client = await db.query(\n    'SELECT * FROM oauth_clients WHERE id = $1 AND secret = $2',\n    [client_id, client_secret]\n  );\n  \n  if (!client.rows[0]) {\n    return res.status(401).json({ error: 'Invalid client' });\n  }\n  \n  // Verify refresh token\n  const storedToken = await db.query(\n    'SELECT * FROM refresh_tokens WHERE token = $1 AND client_id = $2',\n    [refresh_token, client_id]\n  );\n  \n  if (!storedToken.rows[0]) {\n    return res.status(400).json({ error: 'Invalid refresh token' });\n  }\n  \n  // Generate new access token\n  const accessToken = jwt.sign(\n    {\n      userId: storedToken.rows[0].user_id,\n      client_id,\n      scope: storedToken.rows[0].scope\n    },\n    JWT_SECRET,\n    { expiresIn: '1h' }\n  );\n  \n  res.json({\n    access_token: accessToken,\n    token_type: 'Bearer',\n    expires_in: 3600\n  });\n});\n\n// RBAC (Role-Based Access Control) Implementation\nclass RBACManager {\n  constructor() {\n    this.roles = new Map();\n    this.permissions = new Map();\n  }\n  \n  defineRole(roleName, permissions) {\n    this.roles.set(roleName, new Set(permissions));\n  }\n  \n  definePermission(resource, action) {\n    const permission = `${resource}:${action}`;\n    this.permissions.set(permission, { resource, action });\n    return permission;\n  }\n  \n  grantPermissionToRole(roleName, permission) {\n    if (!this.roles.has(roleName)) {\n      this.roles.set(roleName, new Set());\n    }\n    this.roles.get(roleName).add(permission);\n  }\n  \n  userHasPermission(userRoles, requiredPermission) {\n    return userRoles.some(role => {\n      const rolePermissions = this.roles.get(role);\n      return rolePermissions && rolePermissions.has(requiredPermission);\n    });\n  }\n}\n\n// Setup RBAC\nconst rbac = new RBACManager();\n\n// Define permissions\nconst PERMISSIONS = {\n  READ_USERS: rbac.definePermission('users', 'read'),\n  WRITE_USERS: rbac.definePermission('users', 'write'),\n  DELETE_USERS: rbac.definePermission('users', 'delete'),\n  READ_POSTS: rbac.definePermission('posts', 'read'),\n  WRITE_POSTS: rbac.definePermission('posts', 'write'),\n  DELETE_POSTS: rbac.definePermission('posts', 'delete')\n};\n\n// Define roles\nrbac.defineRole('admin', [\n  PERMISSIONS.READ_USERS,\n  PERMISSIONS.WRITE_USERS,\n  PERMISSIONS.DELETE_USERS,\n  PERMISSIONS.READ_POSTS,\n  PERMISSIONS.WRITE_POSTS,\n  PERMISSIONS.DELETE_POSTS\n]);\n\nrbac.defineRole('moderator', [\n  PERMISSIONS.READ_USERS,\n  PERMISSIONS.READ_POSTS,\n  PERMISSIONS.WRITE_POSTS,\n  PERMISSIONS.DELETE_POSTS\n]);\n\nrbac.defineRole('user', [\n  PERMISSIONS.READ_POSTS,\n  PERMISSIONS.WRITE_POSTS\n]);\n\n// Permission-based middleware\nfunction requirePermission(permission) {\n  return (req, res, next) => {\n    if (!req.user) {\n      return res.status(401).json({ error: 'Not authenticated' });\n    }\n    \n    const hasPermission = rbac.userHasPermission(\n      req.user.roles,\n      permission\n    );\n    \n    if (!hasPermission) {\n      return res.status(403).json({ error: 'Insufficient permissions' });\n    }\n    \n    next();\n  };\n}\n\n// Usage\napp.delete('/api/users/:id',\n  authenticate,\n  requirePermission(PERMISSIONS.DELETE_USERS),\n  async (req, res) => {\n    await db.query('DELETE FROM users WHERE id = $1', [req.params.id]);\n    res.json({ message: 'User deleted' });\n  }\n);\n\n// Multi-Factor Authentication (MFA)\nconst speakeasy = require('speakeasy');\nconst QRCode = require('qrcode');\n\napp.post('/api/mfa/enable', authenticate, async (req, res) => {\n  // Generate secret\n  const secret = speakeasy.generateSecret({\n    name: `MyApp (${req.user.email})`\n  });\n  \n  // Store secret for user (encrypted)\n  await db.query(\n    'UPDATE users SET mfa_secret = $1 WHERE id = $2',\n    [secret.base32, req.user.userId]\n  );\n  \n  // Generate QR code\n  const qrCodeUrl = await QRCode.toDataURL(secret.otpauth_url);\n  \n  res.json({\n    secret: secret.base32,\n    qrCode: qrCodeUrl\n  });\n});\n\napp.post('/api/mfa/verify', authenticate, async (req, res) => {\n  const { token } = req.body;\n  \n  // Get user's MFA secret\n  const user = await db.query(\n    'SELECT mfa_secret FROM users WHERE id = $1',\n    [req.user.userId]\n  );\n  \n  const verified = speakeasy.totp.verify({\n    secret: user.rows[0].mfa_secret,\n    encoding: 'base32',\n    token,\n    window: 2  // Allow 2 time steps tolerance\n  });\n  \n  if (verified) {\n    await db.query(\n      'UPDATE users SET mfa_enabled = true WHERE id = $1',\n      [req.user.userId]\n    );\n    res.json({ message: 'MFA enabled successfully' });\n  } else {\n    res.status(400).json({ error: 'Invalid token' });\n  }\n});\n\napp.listen(3000);"
    },
    {
      "id": 22,
      "question": "What is database normalization and when should you denormalize?",
      "answer": "Database normalization is the process of organizing data to reduce redundancy and improve data integrity by dividing large tables into smaller ones and defining relationships.\n\nNormalization Forms:\n• First Normal Form (1NF): Atomic values, no repeating groups\n• Second Normal Form (2NF): 1NF + no partial dependencies\n• Third Normal Form (3NF): 2NF + no transitive dependencies\n• Boyce-Codd Normal Form (BCNF): Stricter version of 3NF\n• Fourth Normal Form (4NF): No multi-valued dependencies\n• Fifth Normal Form (5NF): No join dependencies\n\nBenefits of Normalization:\n• Eliminates data redundancy\n• Reduces storage space\n• Ensures data consistency\n• Easier to maintain and update\n• Prevents anomalies (insert, update, delete)\n• Better data integrity\n\nDrawbacks of Normalization:\n• More complex queries with multiple JOINs\n• Slower read performance\n• Difficult to understand for complex schemas\n• More tables to manage\n\nWhen to Denormalize:\n• Read-heavy applications with few writes\n• Performance is critical and queries are slow\n• Reporting and analytics databases\n• When JOIN operations are too expensive\n• Caching layer or materialized views\n• NoSQL databases (naturally denormalized)\n• Data warehouses and OLAP systems\n\nDenormalization Techniques:\n• Add redundant columns to avoid JOINs\n• Create materialized views for common queries\n• Store computed/aggregated values\n• Duplicate data across tables\n• Use JSON/JSONB for nested data\n• Create read replicas with denormalized schema\n\nTrade-offs:\n• Normalization: Write-optimized, consistent, complex reads\n• Denormalization: Read-optimized, redundant, simpler queries\n\nBest Practices:\n• Start with normalized design\n• Denormalize only when performance requires\n• Use caching before denormalizing\n• Monitor query performance to identify bottlenecks\n• Consider materialized views for read-heavy queries\n• Document denormalization decisions",
      "explanation": "Normalization organizes data into separate tables with relationships to eliminate redundancy and ensure consistency, while denormalization intentionally adds redundancy to improve read performance in specific scenarios like reporting or read-heavy applications.",
      "difficulty": "Medium",
      "code": "-- NORMALIZATION Examples\n\n-- UNNORMALIZED: Repeating groups, data redundancy\nCREATE TABLE orders_unnormalized (\n    order_id INT PRIMARY KEY,\n    customer_name VARCHAR(100),\n    customer_email VARCHAR(100),\n    customer_phone VARCHAR(20),\n    customer_address VARCHAR(200),\n    product1_name VARCHAR(100),\n    product1_price DECIMAL(10,2),\n    product1_quantity INT,\n    product2_name VARCHAR(100),\n    product2_price DECIMAL(10,2),\n    product2_quantity INT,\n    product3_name VARCHAR(100),\n    product3_price DECIMAL(10,2),\n    product3_quantity INT\n);\n-- Problems: Limited to 3 products, duplicate customer info, update anomalies\n\n-- FIRST NORMAL FORM (1NF): Atomic values, no repeating groups\nCREATE TABLE orders_1nf (\n    order_id INT,\n    customer_name VARCHAR(100),\n    customer_email VARCHAR(100),\n    customer_phone VARCHAR(20),\n    customer_address VARCHAR(200),\n    product_name VARCHAR(100),\n    product_price DECIMAL(10,2),\n    quantity INT,\n    PRIMARY KEY (order_id, product_name)\n);\n-- Each column has atomic values, no repeating groups\n-- Problem: Customer info duplicated, product_price depends on product_name\n\n-- SECOND NORMAL FORM (2NF): 1NF + no partial dependencies\nCREATE TABLE customers (\n    customer_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    phone VARCHAR(20),\n    address VARCHAR(200)\n);\n\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    price DECIMAL(10,2),\n    description TEXT\n);\n\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date TIMESTAMP,\n    total DECIMAL(10,2),\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    price_at_purchase DECIMAL(10,2),  -- Historical price\n    PRIMARY KEY (order_id, product_id),\n    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    FOREIGN KEY (product_id) REFERENCES products(product_id)\n);\n-- No partial dependencies, each table has single purpose\n-- Problem: If customer_address depends on customer_zip_code (transitive dependency)\n\n-- THIRD NORMAL FORM (3NF): 2NF + no transitive dependencies\nCREATE TABLE customers_3nf (\n    customer_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    phone VARCHAR(20),\n    zip_code VARCHAR(10)\n);\n\nCREATE TABLE zip_codes (\n    zip_code VARCHAR(10) PRIMARY KEY,\n    city VARCHAR(100),\n    state VARCHAR(50),\n    country VARCHAR(50)\n);\n-- All non-key attributes depend only on primary key\n\n-- DENORMALIZATION Examples\n\n-- Example 1: Add redundant columns to avoid JOINs\nCREATE TABLE orders_denormalized (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    customer_name VARCHAR(100),      -- Denormalized from customers\n    customer_email VARCHAR(100),     -- Denormalized from customers\n    order_date TIMESTAMP,\n    total DECIMAL(10,2),\n    item_count INT,                  -- Computed value\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n-- Benefit: Fast reads without JOIN\n-- Cost: Must update customer_name/email when customer changes\n\n-- Example 2: Materialized View for complex queries\nCREATE MATERIALIZED VIEW order_summary AS\nSELECT \n    o.order_id,\n    o.order_date,\n    c.customer_id,\n    c.name as customer_name,\n    c.email as customer_email,\n    COUNT(oi.product_id) as item_count,\n    SUM(oi.quantity * oi.price_at_purchase) as total,\n    ARRAY_AGG(p.name) as product_names\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nJOIN products p ON oi.product_id = p.product_id\nGROUP BY o.order_id, o.order_date, c.customer_id, c.name, c.email;\n\n-- Refresh materialized view periodically\nREFRESH MATERIALIZED VIEW CONCURRENTLY order_summary;\n\n-- Example 3: Store computed/aggregated values\nCREATE TABLE products_with_stats (\n    product_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    price DECIMAL(10,2),\n    -- Denormalized statistics\n    total_sold INT DEFAULT 0,\n    revenue_total DECIMAL(15,2) DEFAULT 0,\n    avg_rating DECIMAL(3,2) DEFAULT 0,\n    review_count INT DEFAULT 0,\n    last_purchased_at TIMESTAMP\n);\n\n-- Update stats when order is placed\nCREATE OR REPLACE FUNCTION update_product_stats()\nRETURNS TRIGGER AS $$\nBEGIN\n    UPDATE products_with_stats\n    SET \n        total_sold = total_sold + NEW.quantity,\n        revenue_total = revenue_total + (NEW.quantity * NEW.price_at_purchase),\n        last_purchased_at = NOW()\n    WHERE product_id = NEW.product_id;\n    \n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER order_item_stats_trigger\nAFTER INSERT ON order_items\nFOR EACH ROW\nEXECUTE FUNCTION update_product_stats();\n\n-- Example 4: JSONB for nested data (denormalized)\nCREATE TABLE orders_with_details (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date TIMESTAMP,\n    -- Embedded customer data (denormalized)\n    customer_data JSONB,\n    -- Embedded items (denormalized)\n    items JSONB,\n    total DECIMAL(10,2)\n);\n\n-- Insert with nested data\nINSERT INTO orders_with_details VALUES (\n    1001,\n    501,\n    NOW(),\n    '{\"name\": \"John Doe\", \"email\": \"john@example.com\", \"address\": \"123 Main St\"}',\n    '[\n        {\"product_id\": 1, \"name\": \"Widget\", \"price\": 29.99, \"quantity\": 2},\n        {\"product_id\": 2, \"name\": \"Gadget\", \"price\": 49.99, \"quantity\": 1}\n    ]',\n    109.97\n);\n\n-- Query JSONB data\nSELECT \n    order_id,\n    customer_data->>'name' as customer_name,\n    jsonb_array_length(items) as item_count\nFROM orders_with_details\nWHERE customer_data->>'email' = 'john@example.com';\n\n-- Performance Comparison\n\n-- NORMALIZED: Multiple JOINs (slower read)\nEXPLAIN ANALYZE\nSELECT \n    o.order_id,\n    c.name as customer_name,\n    c.email,\n    COUNT(oi.product_id) as items,\n    SUM(oi.quantity * oi.price_at_purchase) as total\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nJOIN products p ON oi.product_id = p.product_id\nWHERE o.order_date >= '2023-01-01'\nGROUP BY o.order_id, c.name, c.email;\n-- Execution time: ~150ms for 10k orders\n\n-- DENORMALIZED: No JOINs (faster read)\nEXPLAIN ANALYZE\nSELECT \n    order_id,\n    customer_name,\n    customer_email,\n    item_count,\n    total\nFROM orders_denormalized\nWHERE order_date >= '2023-01-01';\n-- Execution time: ~20ms for 10k orders (7x faster)\n\n-- Read Replica with Denormalized Schema\n-- Primary DB: Normalized schema for writes\n-- Read Replica: Denormalized schema populated via ETL\n\n-- Example 5: Data Warehouse (Star Schema - Denormalized)\nCREATE TABLE fact_sales (\n    sale_id BIGINT PRIMARY KEY,\n    date_key INT,\n    customer_key INT,\n    product_key INT,\n    store_key INT,\n    quantity INT,\n    amount DECIMAL(10,2),\n    discount DECIMAL(10,2),\n    tax DECIMAL(10,2),\n    total DECIMAL(10,2)\n);\n\nCREATE TABLE dim_date (\n    date_key INT PRIMARY KEY,\n    date DATE,\n    day_of_week VARCHAR(10),\n    day_of_month INT,\n    month INT,\n    quarter INT,\n    year INT,\n    is_weekend BOOLEAN,\n    is_holiday BOOLEAN\n);\n\nCREATE TABLE dim_customer (\n    customer_key INT PRIMARY KEY,\n    customer_id INT,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    city VARCHAR(100),\n    state VARCHAR(50),\n    country VARCHAR(50),\n    segment VARCHAR(50),\n    lifetime_value DECIMAL(15,2)\n);\n\nCREATE TABLE dim_product (\n    product_key INT PRIMARY KEY,\n    product_id INT,\n    name VARCHAR(100),\n    category VARCHAR(50),\n    subcategory VARCHAR(50),\n    brand VARCHAR(50),\n    price DECIMAL(10,2)\n);\n\n-- Fast analytical queries (no JOINs needed for most queries)\nSELECT \n    d.year,\n    d.quarter,\n    SUM(f.total) as revenue\nFROM fact_sales f\nJOIN dim_date d ON f.date_key = d.date_key\nWHERE d.year = 2023\nGROUP BY d.year, d.quarter;\n\n-- Write Performance Comparison\n\n-- NORMALIZED: Single write to one table\nINSERT INTO customers (customer_id, name, email) \nVALUES (1, 'John Doe', 'john@example.com');\n-- 1 write operation\n\n-- DENORMALIZED: Must update multiple places\nBEGIN;\nINSERT INTO customers (customer_id, name, email) \nVALUES (1, 'John Doe', 'john@example.com');\n\nUPDATE orders_denormalized \nSET customer_name = 'John Doe', customer_email = 'john@example.com'\nWHERE customer_id = 1;\nCOMMIT;\n-- Multiple write operations\n\n-- Hybrid Approach: Caching Layer\n-- Node.js with Redis cache\nconst redis = require('redis');\nconst client = redis.createClient();\n\nasync function getOrderWithDetails(orderId) {\n  // Try cache first\n  const cached = await client.get(`order:${orderId}`);\n  if (cached) {\n    return JSON.parse(cached);\n  }\n  \n  // Cache miss: query normalized database\n  const order = await db.query(`\n    SELECT \n      o.order_id,\n      c.name as customer_name,\n      json_agg(json_build_object(\n        'product_name', p.name,\n        'quantity', oi.quantity,\n        'price', oi.price_at_purchase\n      )) as items\n    FROM orders o\n    JOIN customers c ON o.customer_id = c.customer_id\n    JOIN order_items oi ON o.order_id = oi.order_id\n    JOIN products p ON oi.product_id = p.product_id\n    WHERE o.order_id = $1\n    GROUP BY o.order_id, c.name\n  `, [orderId]);\n  \n  // Cache for 1 hour\n  await client.setEx(\n    `order:${orderId}`,\n    3600,\n    JSON.stringify(order.rows[0])\n  );\n  \n  return order.rows[0];\n}\n-- Benefit: Keep normalized schema, but get fast reads via cache"
    },
    {
      "id": 23,
      "question": "What is the difference between horizontal and vertical partitioning/sharding?",
      "answer": "Partitioning and sharding are techniques to split large datasets into smaller, more manageable pieces to improve performance and scalability.\n\nVertical Partitioning (Column-Based):\n• Split table by columns into multiple tables\n• Separate frequently accessed from rarely accessed columns\n• Example: Split users table into users_core and users_profile\n• Benefit: Reduce I/O, cache frequently used columns separately\n• Use case: Large tables with many columns, some rarely accessed\n\nHorizontal Partitioning (Row-Based):\n• Split table by rows based on a partition key\n• Keep all columns, distribute rows across partitions\n• Example: Partition orders by date (2023_orders, 2024_orders)\n• Benefit: Parallel query processing, easier archival\n• Use case: Time-series data, large tables with clear partition keys\n\nSharding vs Partitioning:\n• Partitioning: Usually within same database server\n• Sharding: Distribute across multiple database servers\n• Sharding is horizontal partitioning across servers\n• Sharding provides better scalability than partitioning\n\nSharding Strategies:\n• Range-based: Partition by value ranges (dates, IDs)\n• Hash-based: Hash partition key to determine shard\n• Geographic: Partition by location (EU data, US data)\n• Directory-based: Lookup table maps keys to shards\n• Consistent hashing: Minimize data movement when adding shards\n\nChallenges:\n• Cross-shard queries are expensive\n• Transactions across shards difficult\n• Rebalancing when adding/removing shards\n• Hotspots: Uneven data distribution\n• Shard key selection is critical\n\nWhen to Use:\n• Vertical: Wide tables with rarely accessed columns\n• Horizontal: Very large tables, time-series data\n• Sharding: When single server can't handle load\n• Partitioning: To improve query performance on single server",
      "explanation": "Vertical partitioning splits tables by columns for I/O optimization, horizontal partitioning splits by rows within a server for query performance, and sharding extends horizontal partitioning across multiple servers for scalability.",
      "difficulty": "Medium",
      "code": "-- VERTICAL PARTITIONING\n\n-- Original wide table\nCREATE TABLE users_original (\n    user_id INT PRIMARY KEY,\n    email VARCHAR(100),\n    username VARCHAR(50),\n    password_hash VARCHAR(255),\n    -- Frequently accessed\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    created_at TIMESTAMP,\n    -- Rarely accessed\n    bio TEXT,\n    preferences JSONB,\n    gdpr_consent BOOLEAN,\n    marketing_consent BOOLEAN,\n    last_login TIMESTAMP,\n    profile_views INT\n);\n-- Problem: Loading user for authentication loads unnecessary data\n\n-- Vertical partitioning: Split by access patterns\nCREATE TABLE users_core (\n    user_id INT PRIMARY KEY,\n    email VARCHAR(100) UNIQUE NOT NULL,\n    username VARCHAR(50) UNIQUE NOT NULL,\n    password_hash VARCHAR(255) NOT NULL,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE users_profile (\n    user_id INT PRIMARY KEY,\n    bio TEXT,\n    avatar_url VARCHAR(255),\n    website VARCHAR(255),\n    location VARCHAR(100),\n    FOREIGN KEY (user_id) REFERENCES users_core(user_id) ON DELETE CASCADE\n);\n\nCREATE TABLE users_preferences (\n    user_id INT PRIMARY KEY,\n    preferences JSONB,\n    gdpr_consent BOOLEAN DEFAULT false,\n    marketing_consent BOOLEAN DEFAULT false,\n    FOREIGN KEY (user_id) REFERENCES users_core(user_id) ON DELETE CASCADE\n);\n\nCREATE TABLE users_stats (\n    user_id INT PRIMARY KEY,\n    last_login TIMESTAMP,\n    login_count INT DEFAULT 0,\n    profile_views INT DEFAULT 0,\n    posts_count INT DEFAULT 0,\n    FOREIGN KEY (user_id) REFERENCES users_core(user_id) ON DELETE CASCADE\n);\n\n-- Fast authentication query (only core data)\nSELECT user_id, email, password_hash \nFROM users_core \nWHERE email = 'user@example.com';\n-- Loads only 3 columns instead of 13\n\n-- Full profile when needed\nSELECT \n    c.*,\n    p.bio,\n    p.avatar_url,\n    s.last_login,\n    s.profile_views\nFROM users_core c\nLEFT JOIN users_profile p ON c.user_id = p.user_id\nLEFT JOIN users_stats s ON c.user_id = s.user_id\nWHERE c.user_id = 123;\n\n-- HORIZONTAL PARTITIONING (PostgreSQL)\n\n-- Partition by range (date)\nCREATE TABLE orders (\n    order_id BIGSERIAL,\n    user_id INT,\n    order_date DATE NOT NULL,\n    total DECIMAL(10,2),\n    status VARCHAR(20)\n) PARTITION BY RANGE (order_date);\n\n-- Create partitions for each year\nCREATE TABLE orders_2022 PARTITION OF orders\n    FOR VALUES FROM ('2022-01-01') TO ('2023-01-01');\n\nCREATE TABLE orders_2023 PARTITION OF orders\n    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');\n\nCREATE TABLE orders_2024 PARTITION OF orders\n    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');\n\nCREATE TABLE orders_default PARTITION OF orders DEFAULT;\n\n-- Queries automatically use correct partition\nSELECT * FROM orders \nWHERE order_date BETWEEN '2023-01-01' AND '2023-12-31';\n-- Only scans orders_2023 partition\n\n-- Easy archival: Detach old partitions\nALTER TABLE orders DETACH PARTITION orders_2022;\n-- Can drop or archive orders_2022 without affecting other data\n\n-- Partition by hash (distribute evenly)\nCREATE TABLE users_partitioned (\n    user_id BIGSERIAL,\n    email VARCHAR(100),\n    username VARCHAR(50),\n    created_at TIMESTAMP\n) PARTITION BY HASH (user_id);\n\n-- Create 4 partitions\nCREATE TABLE users_part_0 PARTITION OF users_partitioned\n    FOR VALUES WITH (MODULUS 4, REMAINDER 0);\n\nCREATE TABLE users_part_1 PARTITION OF users_partitioned\n    FOR VALUES WITH (MODULUS 4, REMAINDER 1);\n\nCREATE TABLE users_part_2 PARTITION OF users_partitioned\n    FOR VALUES WITH (MODULUS 4, REMAINDER 2);\n\nCREATE TABLE users_part_3 PARTITION OF users_partitioned\n    FOR VALUES WITH (MODULUS 4, REMAINDER 3);\n\n-- Data evenly distributed across partitions\nINSERT INTO users_partitioned (email, username) \nVALUES ('user@example.com', 'username');\n-- Automatically goes to correct partition based on hash(user_id)\n\n-- SHARDING (Distributed Across Servers)\n\n// Node.js Sharding Example\nclass ShardManager {\n  constructor(shards) {\n    this.shards = shards; // Array of database connections\n    this.shardCount = shards.length;\n  }\n  \n  // Hash-based sharding\n  getShardForUser(userId) {\n    const shardIndex = userId % this.shardCount;\n    return this.shards[shardIndex];\n  }\n  \n  // Range-based sharding\n  getShardForOrder(orderId) {\n    if (orderId < 1000000) return this.shards[0];\n    if (orderId < 2000000) return this.shards[1];\n    if (orderId < 3000000) return this.shards[2];\n    return this.shards[3];\n  }\n  \n  // Geographic sharding\n  getShardForRegion(region) {\n    const regionMap = {\n      'US': this.shards[0],\n      'EU': this.shards[1],\n      'ASIA': this.shards[2]\n    };\n    return regionMap[region] || this.shards[0];\n  }\n  \n  async getUserById(userId) {\n    const shard = this.getShardForUser(userId);\n    const result = await shard.query(\n      'SELECT * FROM users WHERE user_id = $1',\n      [userId]\n    );\n    return result.rows[0];\n  }\n  \n  async createUser(userData) {\n    const userId = await this.generateUserId();\n    const shard = this.getShardForUser(userId);\n    \n    await shard.query(\n      'INSERT INTO users (user_id, email, username) VALUES ($1, $2, $3)',\n      [userId, userData.email, userData.username]\n    );\n    \n    return userId;\n  }\n  \n  // Cross-shard query (expensive!)\n  async getAllUsers() {\n    const results = await Promise.all(\n      this.shards.map(shard => \n        shard.query('SELECT * FROM users')\n      )\n    );\n    \n    // Merge results from all shards\n    return results.flatMap(result => result.rows);\n  }\n  \n  // Cross-shard transaction (complex!)\n  async transferCredits(fromUserId, toUserId, amount) {\n    const fromShard = this.getShardForUser(fromUserId);\n    const toShard = this.getShardForUser(toUserId);\n    \n    if (fromShard === toShard) {\n      // Same shard: use normal transaction\n      const client = await fromShard.connect();\n      try {\n        await client.query('BEGIN');\n        await client.query(\n          'UPDATE users SET credits = credits - $1 WHERE user_id = $2',\n          [amount, fromUserId]\n        );\n        await client.query(\n          'UPDATE users SET credits = credits + $1 WHERE user_id = $2',\n          [amount, toUserId]\n        );\n        await client.query('COMMIT');\n      } catch (error) {\n        await client.query('ROLLBACK');\n        throw error;\n      } finally {\n        client.release();\n      }\n    } else {\n      // Different shards: use 2-phase commit or Saga pattern\n      throw new Error('Cross-shard transactions not supported');\n    }\n  }\n}\n\n// Initialize with database connections\nconst { Pool } = require('pg');\n\nconst shardManager = new ShardManager([\n  new Pool({ host: 'db-shard-0', database: 'myapp_shard_0' }),\n  new Pool({ host: 'db-shard-1', database: 'myapp_shard_1' }),\n  new Pool({ host: 'db-shard-2', database: 'myapp_shard_2' }),\n  new Pool({ host: 'db-shard-3', database: 'myapp_shard_3' })\n]);\n\n// Consistent Hashing for Sharding\nconst crypto = require('crypto');\n\nclass ConsistentHash {\n  constructor(nodes, virtualNodes = 150) {\n    this.ring = new Map();\n    this.sortedKeys = [];\n    this.virtualNodes = virtualNodes;\n    this.nodes = nodes;\n    \n    nodes.forEach(node => this.addNode(node));\n  }\n  \n  hash(key) {\n    return parseInt(\n      crypto.createHash('md5').update(key).digest('hex').substring(0, 8),\n      16\n    );\n  }\n  \n  addNode(node) {\n    for (let i = 0; i < this.virtualNodes; i++) {\n      const hash = this.hash(`${node.id}:${i}`);\n      this.ring.set(hash, node);\n    }\n    \n    this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n  }\n  \n  removeNode(node) {\n    for (let i = 0; i < this.virtualNodes; i++) {\n      const hash = this.hash(`${node.id}:${i}`);\n      this.ring.delete(hash);\n    }\n    \n    this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n  }\n  \n  getNode(key) {\n    if (this.ring.size === 0) return null;\n    \n    const hash = this.hash(key);\n    \n    // Find first node with hash >= key hash\n    for (const nodeHash of this.sortedKeys) {\n      if (nodeHash >= hash) {\n        return this.ring.get(nodeHash);\n      }\n    }\n    \n    // Wrap around to first node\n    return this.ring.get(this.sortedKeys[0]);\n  }\n}\n\n// Usage\nconst consistentHash = new ConsistentHash([\n  { id: 'shard-0', connection: pool0 },\n  { id: 'shard-1', connection: pool1 },\n  { id: 'shard-2', connection: pool2 }\n]);\n\nfunction getShardForKey(key) {\n  return consistentHash.getNode(key);\n}\n\nconst shard = getShardForKey('user:12345');\nawait shard.connection.query('SELECT * FROM users WHERE user_id = $1', [12345]);\n\n// Adding new shard (minimal data movement)\nconsistentHash.addNode({\n  id: 'shard-3',\n  connection: pool3\n});\n\n// Vitess-style Sharding Configuration\n/*\n# VSchema for keyspace split\nkeyspace:\n  sharded: true\n  vindexes:\n    hash:\n      type: hash\n    user_index:\n      type: lookup_unique\n      owner: users\n      params:\n        table: user_lookup\n        from: email\n        to: user_id\n  tables:\n    users:\n      column_vindexes:\n        - column: user_id\n          name: hash\n        - column: email\n          name: user_index\n    orders:\n      column_vindexes:\n        - column: user_id\n          name: hash\n      \n# Automatically routes queries to correct shard\nSELECT * FROM users WHERE user_id = 12345;\n# Goes to shard based on hash(12345)\n\nSELECT * FROM orders WHERE user_id = 12345;\n# Co-located with user on same shard\n*/"
    },
    {
      "id": 24,
      "question": "What is a Content Delivery Network (CDN) and how does it improve performance?",
      "answer": "A Content Delivery Network (CDN) is a geographically distributed network of servers that cache and deliver content from locations closest to users, reducing latency and improving performance.\n\nHow CDN Works:\n• Content cached on edge servers worldwide\n• DNS routes users to nearest edge server\n• Cache hit: Serve from edge (fast)\n• Cache miss: Fetch from origin, cache, serve\n• Reduce load on origin servers\n\nBenefits:\n• Lower latency: Serve from geographically close servers\n• Reduced bandwidth costs: Less traffic to origin\n• Better availability: Redundancy across multiple servers\n• DDoS protection: Distribute traffic across network\n• Improved SEO: Faster page loads improve rankings\n• Global reach: Serve international users faster\n\nTypes of Content:\n• Static assets: Images, CSS, JavaScript, fonts\n• Video streaming: Adaptive bitrate streaming\n• APIs: Cache API responses at edge\n• Dynamic content: Edge computing, personalization\n\nCDN Features:\n• Caching strategies: TTL, cache headers, purging\n• Compression: Gzip, Brotli\n• Image optimization: Resize, format conversion, WebP\n• SSL/TLS termination: HTTPS at edge\n• DDoS protection: Rate limiting, bot detection\n• Edge computing: Run code at edge (Workers)\n• Analytics: Traffic, performance metrics\n\nCache Strategies:\n• Cache-Control headers: max-age, s-maxage, no-cache\n• Cache purging: Invalidate stale content\n• Cache warming: Pre-populate cache\n• Stale-while-revalidate: Serve stale while updating\n\nPopular CDN Providers:\n• Cloudflare: Global network, edge computing\n• AWS CloudFront: Integrated with AWS\n• Akamai: Enterprise-focused\n• Fastly: Real-time purging, edge compute\n• Azure CDN: Microsoft ecosystem\n\nWhen to Use:\n• Global user base\n• Static asset-heavy sites\n• Video streaming\n• High-traffic websites\n• Need DDoS protection",
      "explanation": "CDNs distribute content across geographically dispersed edge servers, serving users from the nearest location to reduce latency, improve performance, and decrease origin server load.",
      "difficulty": "Easy",
      "code": "// CloudFront CDN Configuration (AWS/Terraform)\nresource \"aws_cloudfront_distribution\" \"main\" {\n  origin {\n    domain_name = aws_s3_bucket.static_assets.bucket_regional_domain_name\n    origin_id   = \"S3-static-assets\"\n    \n    s3_origin_config {\n      origin_access_identity = aws_cloudfront_origin_access_identity.oai.cloudfront_access_identity_path\n    }\n  }\n  \n  # Secondary origin for API\n  origin {\n    domain_name = \"api.example.com\"\n    origin_id   = \"API-origin\"\n    \n    custom_origin_config {\n      http_port              = 80\n      https_port             = 443\n      origin_protocol_policy = \"https-only\"\n      origin_ssl_protocols   = [\"TLSv1.2\"]\n    }\n    \n    custom_header {\n      name  = \"X-Origin-Verify\"\n      value = var.origin_secret\n    }\n  }\n  \n  enabled             = true\n  is_ipv6_enabled     = true\n  default_root_object = \"index.html\"\n  price_class         = \"PriceClass_All\"  # Use all edge locations\n  \n  # Cache behaviors for different content types\n  default_cache_behavior {\n    target_origin_id       = \"S3-static-assets\"\n    viewer_protocol_policy = \"redirect-to-https\"\n    compress               = true\n    \n    allowed_methods = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    cached_methods  = [\"GET\", \"HEAD\"]\n    \n    forwarded_values {\n      query_string = false\n      cookies {\n        forward = \"none\"\n      }\n    }\n    \n    min_ttl     = 0\n    default_ttl = 86400    # 1 day\n    max_ttl     = 31536000 # 1 year\n  }\n  \n  # API cache behavior\n  ordered_cache_behavior {\n    path_pattern           = \"/api/*\"\n    target_origin_id       = \"API-origin\"\n    viewer_protocol_policy = \"https-only\"\n    compress               = true\n    \n    allowed_methods = [\"GET\", \"HEAD\", \"OPTIONS\", \"PUT\", \"POST\", \"PATCH\", \"DELETE\"]\n    cached_methods  = [\"GET\", \"HEAD\"]\n    \n    forwarded_values {\n      query_string = true\n      headers      = [\"Authorization\", \"Accept\", \"Accept-Language\"]\n      \n      cookies {\n        forward = \"all\"\n      }\n    }\n    \n    min_ttl     = 0\n    default_ttl = 300      # 5 minutes\n    max_ttl     = 3600     # 1 hour\n  }\n  \n  # Images with optimization\n  ordered_cache_behavior {\n    path_pattern           = \"/images/*\"\n    target_origin_id       = \"S3-static-assets\"\n    viewer_protocol_policy = \"redirect-to-https\"\n    compress               = true\n    \n    allowed_methods = [\"GET\", \"HEAD\"]\n    cached_methods  = [\"GET\", \"HEAD\"]\n    \n    forwarded_values {\n      query_string = true  # For image transformations\n      cookies {\n        forward = \"none\"\n      }\n    }\n    \n    min_ttl     = 0\n    default_ttl = 2592000  # 30 days\n    max_ttl     = 31536000 # 1 year\n    \n    lambda_function_association {\n      event_type   = \"origin-response\"\n      lambda_arn   = aws_lambda_function.image_optimizer.qualified_arn\n      include_body = false\n    }\n  }\n  \n  # Custom error responses\n  custom_error_response {\n    error_code            = 403\n    response_code         = 404\n    response_page_path    = \"/404.html\"\n    error_caching_min_ttl = 300\n  }\n  \n  custom_error_response {\n    error_code            = 404\n    response_code         = 404\n    response_page_path    = \"/404.html\"\n    error_caching_min_ttl = 300\n  }\n  \n  # SSL certificate\n  viewer_certificate {\n    acm_certificate_arn      = aws_acm_certificate.cert.arn\n    ssl_support_method       = \"sni-only\"\n    minimum_protocol_version = \"TLSv1.2_2021\"\n  }\n  \n  restrictions {\n    geo_restriction {\n      restriction_type = \"none\"\n    }\n  }\n  \n  # Logging\n  logging_config {\n    bucket          = aws_s3_bucket.logs.bucket_domain_name\n    prefix          = \"cloudfront/\"\n    include_cookies = false\n  }\n  \n  tags = {\n    Environment = \"production\"\n  }\n}\n\n// Cache Control Headers (Express.js)\nconst express = require('express');\nconst app = express();\n\n// Static assets with long cache\napp.use('/static', express.static('public', {\n  maxAge: '1y',  // 1 year\n  immutable: true,\n  setHeaders: (res, path) => {\n    if (path.endsWith('.html')) {\n      // HTML: shorter cache, must revalidate\n      res.setHeader('Cache-Control', 'public, max-age=300, must-revalidate');\n    } else if (path.match(/\\.(css|js)$/)) {\n      // Versioned assets: long cache\n      res.setHeader('Cache-Control', 'public, max-age=31536000, immutable');\n    } else if (path.match(/\\.(jpg|jpeg|png|gif|webp|svg)$/)) {\n      // Images: long cache\n      res.setHeader('Cache-Control', 'public, max-age=2592000'); // 30 days\n    }\n  }\n}));\n\n// API with short cache\napp.get('/api/products', (req, res) => {\n  res.set({\n    'Cache-Control': 'public, max-age=300, s-maxage=600', // CDN caches for 10 min\n    'Vary': 'Accept-Encoding, Accept-Language'\n  });\n  \n  res.json({ products: [] });\n});\n\n// Dynamic content with stale-while-revalidate\napp.get('/api/news', (req, res) => {\n  res.set({\n    'Cache-Control': 'public, max-age=60, stale-while-revalidate=300',\n    'Vary': 'Accept-Encoding'\n  });\n  \n  res.json({ news: [] });\n});\n\n// No cache for sensitive data\napp.get('/api/user/profile', authenticate, (req, res) => {\n  res.set({\n    'Cache-Control': 'private, no-cache, no-store, must-revalidate',\n    'Pragma': 'no-cache',\n    'Expires': '0'\n  });\n  \n  res.json({ user: req.user });\n});\n\n// Cloudflare Workers (Edge Computing)\naddEventListener('fetch', event => {\n  event.respondWith(handleRequest(event.request));\n});\n\nasync function handleRequest(request) {\n  const url = new URL(request.url);\n  \n  // Image optimization\n  if (url.pathname.startsWith('/images/')) {\n    return handleImageRequest(request, url);\n  }\n  \n  // API caching with edge logic\n  if (url.pathname.startsWith('/api/')) {\n    return handleAPIRequest(request, url);\n  }\n  \n  // Default: pass through to origin\n  return fetch(request);\n}\n\nasync function handleImageRequest(request, url) {\n  const cache = caches.default;\n  \n  // Check cache first\n  let response = await cache.match(request);\n  if (response) {\n    return response;\n  }\n  \n  // Get dimensions from query params\n  const width = url.searchParams.get('w') || 800;\n  const quality = url.searchParams.get('q') || 85;\n  const format = url.searchParams.get('f') || 'webp';\n  \n  // Fetch original image\n  const originResponse = await fetch(request);\n  \n  // Transform image using Cloudflare Image Resizing\n  response = await fetch(originResponse.url, {\n    cf: {\n      image: {\n        width: parseInt(width),\n        quality: parseInt(quality),\n        format: format\n      }\n    }\n  });\n  \n  // Cache transformed image\n  response = new Response(response.body, response);\n  response.headers.set('Cache-Control', 'public, max-age=2592000'); // 30 days\n  \n  await cache.put(request, response.clone());\n  \n  return response;\n}\n\nasync function handleAPIRequest(request, url) {\n  const cache = caches.default;\n  const cacheKey = new Request(url.toString(), request);\n  \n  // Check cache\n  let response = await cache.match(cacheKey);\n  \n  if (response) {\n    // Cache hit\n    const age = Math.floor((Date.now() - new Date(response.headers.get('Date')).getTime()) / 1000);\n    response = new Response(response.body, response);\n    response.headers.set('X-Cache', 'HIT');\n    response.headers.set('Age', age);\n    return response;\n  }\n  \n  // Cache miss: fetch from origin\n  response = await fetch(request);\n  \n  // Only cache successful responses\n  if (response.ok) {\n    response = new Response(response.body, response);\n    response.headers.set('X-Cache', 'MISS');\n    response.headers.set('Cache-Control', 'public, max-age=300'); // 5 minutes\n    \n    // Store in cache\n    await cache.put(cacheKey, response.clone());\n  }\n  \n  return response;\n}\n\n// Cache Invalidation\nconst AWS = require('aws-sdk');\nconst cloudfront = new AWS.CloudFront();\n\nasync function invalidateCDNCache(paths) {\n  const params = {\n    DistributionId: 'E1234567890ABC',\n    InvalidationBatch: {\n      CallerReference: `invalidation-${Date.now()}`,\n      Paths: {\n        Quantity: paths.length,\n        Items: paths  // ['/images/*', '/api/products']\n      }\n    }\n  };\n  \n  const result = await cloudfront.createInvalidation(params).promise();\n  console.log('Invalidation created:', result.Invalidation.Id);\n  return result;\n}\n\n// Trigger invalidation after deploy\nawait invalidateCDNCache([\n  '/index.html',\n  '/static/js/*',\n  '/static/css/*'\n]);\n\n// Cloudflare API: Purge cache\nconst https = require('https');\n\nfunction purgeCloudflareCache(zone, files) {\n  return new Promise((resolve, reject) => {\n    const options = {\n      hostname: 'api.cloudflare.com',\n      path: `/client/v4/zones/${zone}/purge_cache`,\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${process.env.CLOUDFLARE_API_TOKEN}`,\n        'Content-Type': 'application/json'\n      }\n    };\n    \n    const req = https.request(options, (res) => {\n      let data = '';\n      res.on('data', chunk => data += chunk);\n      res.on('end', () => resolve(JSON.parse(data)));\n    });\n    \n    req.on('error', reject);\n    req.write(JSON.stringify({ files }));\n    req.end();\n  });\n}\n\nawait purgeCloudflareCache(\n  'zone123',\n  [\n    'https://example.com/static/app.js',\n    'https://example.com/api/products'\n  ]\n);\n\n// Performance Metrics\napp.get('/metrics/cdn', async (req, res) => {\n  // Get CloudWatch metrics\n  const cloudwatch = new AWS.CloudWatch();\n  \n  const metrics = await cloudwatch.getMetricStatistics({\n    Namespace: 'AWS/CloudFront',\n    MetricName: 'Requests',\n    Dimensions: [{\n      Name: 'DistributionId',\n      Value: 'E1234567890ABC'\n    }],\n    StartTime: new Date(Date.now() - 3600000), // 1 hour ago\n    EndTime: new Date(),\n    Period: 300,  // 5 minutes\n    Statistics: ['Sum']\n  }).promise();\n  \n  res.json({\n    requests: metrics.Datapoints,\n    cacheHitRate: calculateCacheHitRate(metrics)\n  });\n});"
    },
    {
      "id": 25,
      "question": "What is the bulkhead pattern and how does it prevent cascading failures?",
      "answer": "The Bulkhead pattern isolates system resources into separate pools to prevent failures in one area from cascading to the entire system, inspired by ship compartments that prevent sinking if one floods.\n\nCore Concept:\n• Partition resources (threads, connections, memory)\n• Isolate critical from non-critical operations\n• Limit blast radius of failures\n• Prevent resource exhaustion\n• Maintain partial system availability\n\nTypes of Bulkheads:\n• Thread Pool Isolation: Separate thread pools per service\n• Connection Pool Isolation: Separate DB connections per feature\n• Semaphore Isolation: Limit concurrent executions\n• CPU/Memory Isolation: Containerization, resource quotas\n• Process Isolation: Separate processes for different functions\n\nBenefits:\n• Contain failures to specific components\n• Protect critical paths from non-critical load\n• Prevent thread/connection exhaustion\n• Maintain system stability under partial failure\n• Better resource utilization visibility\n\nImplementation Strategies:\n• Separate thread pools for different APIs\n• Different connection pools for different databases\n• Rate limiting per client/tenant\n• Resource quotas in containers (CPU, memory limits)\n• Process isolation via microservices\n\nUse Cases:\n• Multi-tenant applications: Isolate tenants\n• External API calls: Separate pools for third-party APIs\n• Critical vs non-critical: Isolate reporting from transactions\n• Database operations: Separate read/write pools\n\nCombine With:\n• Circuit Breaker: Fast fail when bulkhead full\n• Retry: Retry with backoff when resource unavailable\n• Timeout: Prevent long-running operations\n• Rate Limiting: Control request rate per bulkhead\n\nDrawbacks:\n• More complex to configure\n• May waste resources if not tuned properly\n• Overhead of managing multiple pools\n• Requires careful capacity planning",
      "explanation": "Bulkhead pattern isolates system resources into separate pools preventing cascading failures by limiting the blast radius, ensuring that failure or resource exhaustion in one area doesn't compromise the entire system.",
      "difficulty": "Medium",
      "code": "// Bulkhead Pattern Implementation (Node.js)\nclass Bulkhead {\n  constructor(name, maxConcurrent) {\n    this.name = name;\n    this.maxConcurrent = maxConcurrent;\n    this.activeCount = 0;\n    this.queue = [];\n    this.stats = {\n      accepted: 0,\n      rejected: 0,\n      queueSize: 0,\n      avgWaitTime: 0\n    };\n  }\n  \n  async execute(fn) {\n    // Check if we can execute immediately\n    if (this.activeCount < this.maxConcurrent) {\n      return await this._execute(fn);\n    }\n    \n    // Queue is full, reject\n    if (this.queue.length >= this.maxConcurrent * 2) {\n      this.stats.rejected++;\n      throw new Error(`Bulkhead ${this.name} is full (rejected)`);\n    }\n    \n    // Add to queue\n    return new Promise((resolve, reject) => {\n      const startWait = Date.now();\n      this.queue.push({ fn, resolve, reject, startWait });\n      this.stats.queueSize = this.queue.length;\n    });\n  }\n  \n  async _execute(fn) {\n    this.activeCount++;\n    this.stats.accepted++;\n    \n    try {\n      const result = await fn();\n      return result;\n    } finally {\n      this.activeCount--;\n      this._processQueue();\n    }\n  }\n  \n  _processQueue() {\n    if (this.queue.length > 0 && this.activeCount < this.maxConcurrent) {\n      const { fn, resolve, reject, startWait } = this.queue.shift();\n      \n      // Update wait time stats\n      const waitTime = Date.now() - startWait;\n      this.stats.avgWaitTime = (this.stats.avgWaitTime + waitTime) / 2;\n      this.stats.queueSize = this.queue.length;\n      \n      this._execute(fn).then(resolve).catch(reject);\n    }\n  }\n  \n  getStats() {\n    return {\n      name: this.name,\n      active: this.activeCount,\n      queued: this.queue.length,\n      maxConcurrent: this.maxConcurrent,\n      utilization: (this.activeCount / this.maxConcurrent) * 100,\n      ...this.stats\n    };\n  }\n}\n\n// Create bulkheads for different services\nconst paymentBulkhead = new Bulkhead('payments', 10);      // 10 concurrent\nconst analyticsBulkhead = new Bulkhead('analytics', 5);     // 5 concurrent\nconst notificationBulkhead = new Bulkhead('notifications', 20); // 20 concurrent\nconst externalAPIBulkhead = new Bulkhead('external-api', 3);   // 3 concurrent\n\n// Usage in Express routes\nconst express = require('express');\nconst app = express();\n\n// Critical operation: protected by bulkhead\napp.post('/api/payment', async (req, res) => {\n  try {\n    const result = await paymentBulkhead.execute(async () => {\n      // Process payment\n      return await processPayment(req.body);\n    });\n    \n    res.json(result);\n  } catch (error) {\n    if (error.message.includes('Bulkhead')) {\n      res.status(503).json({ error: 'Service temporarily overloaded' });\n    } else {\n      res.status(500).json({ error: 'Payment failed' });\n    }\n  }\n});\n\n// Non-critical operation: different bulkhead\napp.post('/api/analytics', async (req, res) => {\n  try {\n    await analyticsBulkhead.execute(async () => {\n      await recordAnalytics(req.body);\n    });\n    \n    res.json({ success: true });\n  } catch (error) {\n    // Don't fail request if analytics fails\n    console.error('Analytics failed:', error);\n    res.json({ success: true, analyticsSkipped: true });\n  }\n});\n\n// External API: isolated bulkhead\napp.get('/api/weather', async (req, res) => {\n  try {\n    const weather = await externalAPIBulkhead.execute(async () => {\n      return await axios.get('https://api.weather.com/current', {\n        timeout: 5000\n      });\n    });\n    \n    res.json(weather.data);\n  } catch (error) {\n    res.status(503).json({ error: 'Weather service unavailable' });\n  }\n});\n\n// Health endpoint with bulkhead stats\napp.get('/health/bulkheads', (req, res) => {\n  res.json([\n    paymentBulkhead.getStats(),\n    analyticsBulkhead.getStats(),\n    notificationBulkhead.getStats(),\n    externalAPIBulkhead.getStats()\n  ]);\n});\n\n// Thread Pool Bulkhead (Java/Resilience4j)\n/*\nimport io.github.resilience4j.bulkhead.Bulkhead;\nimport io.github.resilience4j.bulkhead.BulkheadConfig;\nimport io.github.resilience4j.bulkhead.BulkheadRegistry;\nimport io.github.resilience4j.bulkhead.ThreadPoolBulkhead;\nimport io.github.resilience4j.bulkhead.ThreadPoolBulkheadConfig;\n\n@Configuration\npublic class BulkheadConfiguration {\n    \n    @Bean\n    public BulkheadRegistry bulkheadRegistry() {\n        // Semaphore-based bulkhead\n        BulkheadConfig config = BulkheadConfig.custom()\n            .maxConcurrentCalls(10)\n            .maxWaitDuration(Duration.ofMillis(500))\n            .build();\n        \n        return BulkheadRegistry.of(config);\n    }\n    \n    @Bean\n    public ThreadPoolBulkheadRegistry threadPoolBulkheadRegistry() {\n        // Thread pool bulkhead\n        ThreadPoolBulkheadConfig config = ThreadPoolBulkheadConfig.custom()\n            .maxThreadPoolSize(10)\n            .coreThreadPoolSize(5)\n            .queueCapacity(20)\n            .keepAliveDuration(Duration.ofMillis(1000))\n            .build();\n        \n        return ThreadPoolBulkheadRegistry.of(config);\n    }\n}\n\n@Service\npublic class PaymentService {\n    \n    private final Bulkhead paymentBulkhead;\n    private final ThreadPoolBulkhead externalApiBulkhead;\n    \n    public PaymentService(BulkheadRegistry registry,\n                         ThreadPoolBulkheadRegistry threadPoolRegistry) {\n        this.paymentBulkhead = registry.bulkhead(\"payments\");\n        this.externalApiBulkhead = threadPoolRegistry.bulkhead(\"external-api\");\n    }\n    \n    // Semaphore-based isolation\n    public PaymentResult processPayment(PaymentRequest request) {\n        return Bulkhead.decorateSupplier(paymentBulkhead, () -> {\n            // Process payment logic\n            return paymentProcessor.process(request);\n        }).get();\n    }\n    \n    // Thread pool isolation\n    public CompletableFuture<WeatherData> getWeather(String city) {\n        return ThreadPoolBulkhead.decorateSupplier(externalApiBulkhead, () -> {\n            return weatherClient.getWeather(city);\n        }).get();\n    }\n}\n*/\n\n// Database Connection Pool Bulkheads\nconst { Pool } = require('pg');\n\n// Separate pools for different operations\nconst readPool = new Pool({\n  host: 'db-read-replica',\n  database: 'myapp',\n  max: 20,              // 20 connections for reads\n  idleTimeoutMillis: 30000,\n  connectionTimeoutMillis: 5000\n});\n\nconst writePool = new Pool({\n  host: 'db-primary',\n  database: 'myapp',\n  max: 10,              // 10 connections for writes\n  idleTimeoutMillis: 30000,\n  connectionTimeoutMillis: 5000\n});\n\nconst analyticsPool = new Pool({\n  host: 'db-analytics',\n  database: 'analytics',\n  max: 5,               // 5 connections for analytics\n  idleTimeoutMillis: 60000,\n  connectionTimeoutMillis: 10000\n});\n\n// Usage\nasync function getUsers() {\n  // Use read pool\n  const result = await readPool.query('SELECT * FROM users');\n  return result.rows;\n}\n\nasync function createUser(userData) {\n  // Use write pool\n  const client = await writePool.connect();\n  try {\n    await client.query('BEGIN');\n    const result = await client.query(\n      'INSERT INTO users (email, name) VALUES ($1, $2) RETURNING id',\n      [userData.email, userData.name]\n    );\n    await client.query('COMMIT');\n    return result.rows[0];\n  } catch (error) {\n    await client.query('ROLLBACK');\n    throw error;\n  } finally {\n    client.release();\n  }\n}\n\nasync function generateReport() {\n  // Use analytics pool (doesn't affect main app)\n  const result = await analyticsPool.query(`\n    SELECT date_trunc('day', created_at) as day, COUNT(*) \n    FROM users \n    GROUP BY day\n  `);\n  return result.rows;\n}\n\n// Docker/Kubernetes Resource Limits (Bulkhead via containerization)\n/*\n# docker-compose.yml\nservices:\n  payment-service:\n    image: payment-service:latest\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'      # CPU bulkhead\n          memory: 512M     # Memory bulkhead\n        reservations:\n          cpus: '0.5'\n          memory: 256M\n  \n  analytics-service:\n    image: analytics-service:latest\n    deploy:\n      resources:\n        limits:\n          cpus: '0.5'      # Less critical, fewer resources\n          memory: 256M\n        reservations:\n          cpus: '0.25'\n          memory: 128M\n\n# Kubernetes ResourceQuota\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: payment-quota\n  namespace: payments\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: 10Gi\n    limits.cpu: \"20\"\n    limits.memory: 20Gi\n    pods: \"50\"\n*/\n\n// Multi-tenant Bulkhead\nclass TenantBulkhead {\n  constructor(maxPerTenant, maxGlobal) {\n    this.maxPerTenant = maxPerTenant;\n    this.maxGlobal = maxGlobal;\n    this.tenantCounts = new Map();\n    this.globalCount = 0;\n  }\n  \n  async execute(tenantId, fn) {\n    // Check global limit\n    if (this.globalCount >= this.maxGlobal) {\n      throw new Error('Global capacity exceeded');\n    }\n    \n    // Check tenant limit\n    const tenantCount = this.tenantCounts.get(tenantId) || 0;\n    if (tenantCount >= this.maxPerTenant) {\n      throw new Error(`Tenant ${tenantId} capacity exceeded`);\n    }\n    \n    // Increment counters\n    this.tenantCounts.set(tenantId, tenantCount + 1);\n    this.globalCount++;\n    \n    try {\n      return await fn();\n    } finally {\n      // Decrement counters\n      this.tenantCounts.set(tenantId, this.tenantCounts.get(tenantId) - 1);\n      this.globalCount--;\n    }\n  }\n}\n\nconst tenantBulkhead = new TenantBulkhead(\n  5,   // Max 5 concurrent per tenant\n  50   // Max 50 concurrent globally\n);\n\napp.post('/api/tenant/:tenantId/process', async (req, res) => {\n  const { tenantId } = req.params;\n  \n  try {\n    const result = await tenantBulkhead.execute(tenantId, async () => {\n      return await processTenantRequest(tenantId, req.body);\n    });\n    \n    res.json(result);\n  } catch (error) {\n    if (error.message.includes('capacity')) {\n      res.status(429).json({ error: 'Rate limit exceeded' });\n    } else {\n      res.status(500).json({ error: 'Processing failed' });\n    }\n  }\n});\n\n// Monitoring and Alerting\nconst prometheus = require('prom-client');\n\nconst bulkheadActiveGauge = new prometheus.Gauge({\n  name: 'bulkhead_active_requests',\n  help: 'Number of active requests in bulkhead',\n  labelNames: ['bulkhead']\n});\n\nconst bulkheadRejectedCounter = new prometheus.Counter({\n  name: 'bulkhead_rejected_total',\n  help: 'Total number of rejected requests',\n  labelNames: ['bulkhead']\n});\n\n// Update metrics\nsetInterval(() => {\n  bulkheadActiveGauge.set({ bulkhead: 'payments' }, paymentBulkhead.activeCount);\n  bulkheadActiveGauge.set({ bulkhead: 'analytics' }, analyticsBulkhead.activeCount);\n}, 5000);\n\napp.listen(3000);"
    },
    {
      "id": 26,
      "question": "What is an API Gateway and what problems does it solve?",
      "answer": "An API Gateway acts as a single entry point for all client requests to backend microservices, handling cross-cutting concerns like authentication, rate limiting, and routing.\n\nCore Functions:\n• Request routing to appropriate microservice\n• Protocol translation (REST to gRPC, etc.)\n• Request/response transformation\n• Authentication and authorization\n• Rate limiting and throttling\n• Load balancing\n• Caching responses\n• Logging and monitoring\n• API versioning\n• Request aggregation (composition)\n\nProblems Solved:\n• Eliminates tight coupling between clients and services\n• Centralizes cross-cutting concerns\n• Reduces client complexity\n• Provides single SSL termination point\n• Simplifies client authentication\n• Enables API composition from multiple services\n• Manages API versions and deprecation\n• Provides analytics and monitoring\n\nFeatures:\n• Service Discovery: Find available service instances\n• Circuit Breaking: Handle failing services gracefully\n• Retry Logic: Automatic retries with backoff\n• Request Validation: Validate before reaching services\n• Response Caching: Cache frequent responses\n• API Documentation: Auto-generate docs (Swagger/OpenAPI)\n• WebSocket Support: Bidirectional communication\n• GraphQL Gateway: Aggregate GraphQL APIs\n\nPopular Solutions:\n• Kong: Lua-based, plugin ecosystem\n• AWS API Gateway: Managed service\n• Apigee: Enterprise-focused\n• Azure API Management: Microsoft cloud\n• Express Gateway: Node.js based\n• Tyk: Go-based, open source\n• NGINX: Reverse proxy as gateway\n\nPatterns:\n• Backend for Frontend (BFF): Different gateways per client type\n• Micro Gateway: Lightweight gateways per service\n• Aggregation: Combine multiple service calls\n• Decomposition: Split monolith requests\n\nChallenges:\n• Single point of failure (needs HA)\n• Performance bottleneck\n• Complexity in configuration\n• Versioning and evolution\n• Testing complexity",
      "explanation": "API Gateway provides a unified entry point managing routing, authentication, rate limiting, and protocol translation, decoupling clients from microservices while centralizing cross-cutting concerns.",
      "difficulty": "Medium",
      "code": "// API Gateway with Express (Node.js)\nconst express = require('express');\nconst jwt = require('jsonwebtoken');\nconst rateLimit = require('express-rate-limit');\nconst { createProxyMiddleware } = require('http-proxy-middleware');\nconst redis = require('redis');\n\nconst app = express();\nconst cacheClient = redis.createClient();\n\napp.use(express.json());\n\n// Global rate limiting\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100,                  // 100 requests per window\n  standardHeaders: true,\n  legacyHeaders: false,\n  handler: (req, res) => {\n    res.status(429).json({\n      error: 'Too many requests',\n      retryAfter: req.rateLimit.resetTime\n    });\n  }\n});\n\napp.use(limiter);\n\n// Authentication middleware\nfunction authenticate(req, res, next) {\n  const authHeader = req.headers.authorization;\n  \n  if (!authHeader || !authHeader.startsWith('Bearer ')) {\n    return res.status(401).json({ error: 'No token provided' });\n  }\n  \n  const token = authHeader.substring(7);\n  \n  try {\n    const decoded = jwt.verify(token, process.env.JWT_SECRET);\n    req.user = decoded;\n    next();\n  } catch (error) {\n    return res.status(401).json({ error: 'Invalid token' });\n  }\n}\n\n// Request logging\napp.use((req, res, next) => {\n  const start = Date.now();\n  \n  res.on('finish', () => {\n    const duration = Date.now() - start;\n    console.log({\n      method: req.method,\n      path: req.path,\n      status: res.statusCode,\n      duration: `${duration}ms`,\n      user: req.user?.userId\n    });\n  });\n  \n  next();\n});\n\n// Response caching middleware\nfunction cacheResponse(duration) {\n  return async (req, res, next) => {\n    if (req.method !== 'GET') {\n      return next();\n    }\n    \n    const key = `cache:${req.path}:${JSON.stringify(req.query)}`;\n    \n    try {\n      const cached = await cacheClient.get(key);\n      if (cached) {\n        res.setHeader('X-Cache', 'HIT');\n        return res.json(JSON.parse(cached));\n      }\n    } catch (error) {\n      console.error('Cache error:', error);\n    }\n    \n    // Override res.json to cache response\n    const originalJson = res.json.bind(res);\n    res.json = function(data) {\n      if (res.statusCode === 200) {\n        cacheClient.setEx(key, duration, JSON.stringify(data)).catch(console.error);\n      }\n      res.setHeader('X-Cache', 'MISS');\n      return originalJson(data);\n    };\n    \n    next();\n  };\n}\n\n// Service routing with proxy\nconst services = {\n  users: 'http://users-service:3001',\n  products: 'http://products-service:3002',\n  orders: 'http://orders-service:3003',\n  payments: 'http://payments-service:3004'\n};\n\n// Users service routes\napp.use('/api/users',\n  authenticate,\n  cacheResponse(300), // Cache for 5 minutes\n  createProxyMiddleware({\n    target: services.users,\n    changeOrigin: true,\n    pathRewrite: { '^/api/users': '' },\n    onProxyReq: (proxyReq, req) => {\n      // Forward user info to service\n      proxyReq.setHeader('X-User-Id', req.user.userId);\n      proxyReq.setHeader('X-User-Roles', JSON.stringify(req.user.roles));\n    },\n    onError: (err, req, res) => {\n      console.error('Proxy error:', err);\n      res.status(503).json({ error: 'Service temporarily unavailable' });\n    }\n  })\n);\n\n// Products service routes (public, cached)\napp.use('/api/products',\n  cacheResponse(600), // Cache for 10 minutes\n  createProxyMiddleware({\n    target: services.products,\n    changeOrigin: true,\n    pathRewrite: { '^/api/products': '' }\n  })\n);\n\n// Orders service routes (authenticated, not cached)\napp.use('/api/orders',\n  authenticate,\n  createProxyMiddleware({\n    target: services.orders,\n    changeOrigin: true,\n    pathRewrite: { '^/api/orders': '' },\n    onProxyReq: (proxyReq, req) => {\n      proxyReq.setHeader('X-User-Id', req.user.userId);\n    }\n  })\n);\n\n// Request aggregation (combine multiple services)\napp.get('/api/user/:userId/dashboard', authenticate, async (req, res) => {\n  const { userId } = req.params;\n  \n  // Check authorization\n  if (req.user.userId !== userId && !req.user.roles.includes('admin')) {\n    return res.status(403).json({ error: 'Forbidden' });\n  }\n  \n  try {\n    // Fetch from multiple services in parallel\n    const [userResponse, ordersResponse, paymentsResponse] = await Promise.all([\n      axios.get(`${services.users}/${userId}`, {\n        headers: { 'X-User-Id': req.user.userId }\n      }),\n      axios.get(`${services.orders}/user/${userId}`, {\n        headers: { 'X-User-Id': req.user.userId }\n      }),\n      axios.get(`${services.payments}/user/${userId}`, {\n        headers: { 'X-User-Id': req.user.userId }\n      })\n    ]);\n    \n    // Aggregate response\n    res.json({\n      user: userResponse.data,\n      recentOrders: ordersResponse.data.slice(0, 5),\n      paymentMethods: paymentsResponse.data\n    });\n  } catch (error) {\n    console.error('Dashboard aggregation error:', error);\n    res.status(500).json({ error: 'Failed to load dashboard' });\n  }\n});\n\n// Request transformation\napp.post('/api/v2/orders', authenticate, async (req, res) => {\n  // Transform v2 format to internal format\n  const internalFormat = {\n    userId: req.user.userId,\n    items: req.body.products.map(p => ({\n      productId: p.id,\n      quantity: p.qty,\n      price: p.price\n    })),\n    shipping: {\n      address: req.body.deliveryAddress,\n      method: req.body.shippingMethod\n    }\n  };\n  \n  try {\n    const response = await axios.post(\n      `${services.orders}/create`,\n      internalFormat,\n      { headers: { 'X-User-Id': req.user.userId } }\n    );\n    \n    // Transform response back to v2 format\n    res.json({\n      orderId: response.data.id,\n      status: response.data.status,\n      estimatedDelivery: response.data.estimated_delivery\n    });\n  } catch (error) {\n    res.status(500).json({ error: 'Order creation failed' });\n  }\n});\n\n// Health check aggregation\napp.get('/health', async (req, res) => {\n  const checks = await Promise.allSettled([\n    axios.get(`${services.users}/health`),\n    axios.get(`${services.products}/health`),\n    axios.get(`${services.orders}/health`),\n    axios.get(`${services.payments}/health`)\n  ]);\n  \n  const health = {\n    status: checks.every(c => c.status === 'fulfilled') ? 'healthy' : 'degraded',\n    services: {\n      users: checks[0].status === 'fulfilled' ? 'up' : 'down',\n      products: checks[1].status === 'fulfilled' ? 'up' : 'down',\n      orders: checks[2].status === 'fulfilled' ? 'up' : 'down',\n      payments: checks[3].status === 'fulfilled' ? 'up' : 'down'\n    }\n  };\n  \n  const statusCode = health.status === 'healthy' ? 200 : 503;\n  res.status(statusCode).json(health);\n});\n\napp.listen(8080);\n\n// Kong API Gateway Configuration (kong.yml)\n/*\n_format_version: \"2.1\"\n\nservices:\n  - name: users-service\n    url: http://users-service:3001\n    routes:\n      - name: users-route\n        paths:\n          - /api/users\n    plugins:\n      - name: jwt\n        config:\n          secret_is_base64: false\n      - name: rate-limiting\n        config:\n          minute: 100\n          policy: local\n      - name: response-caching\n        config:\n          ttl: 300\n          cache_control: true\n  \n  - name: products-service\n    url: http://products-service:3002\n    routes:\n      - name: products-route\n        paths:\n          - /api/products\n    plugins:\n      - name: rate-limiting\n        config:\n          minute: 200\n      - name: response-caching\n        config:\n          ttl: 600\n      - name: cors\n        config:\n          origins:\n            - \"*\"\n          methods:\n            - GET\n            - POST\n          headers:\n            - Authorization\n            - Content-Type\n  \n  - name: orders-service\n    url: http://orders-service:3003\n    routes:\n      - name: orders-route\n        paths:\n          - /api/orders\n    plugins:\n      - name: jwt\n      - name: request-transformer\n        config:\n          add:\n            headers:\n              - X-Service-Name:orders\n*/\n\n// AWS API Gateway with Lambda (Terraform)\n/*\nresource \"aws_api_gateway_rest_api\" \"main\" {\n  name        = \"my-api\"\n  description = \"API Gateway for microservices\"\n}\n\nresource \"aws_api_gateway_resource\" \"users\" {\n  rest_api_id = aws_api_gateway_rest_api.main.id\n  parent_id   = aws_api_gateway_rest_api.main.root_resource_id\n  path_part   = \"users\"\n}\n\nresource \"aws_api_gateway_method\" \"users_get\" {\n  rest_api_id   = aws_api_gateway_rest_api.main.id\n  resource_id   = aws_api_gateway_resource.users.id\n  http_method   = \"GET\"\n  authorization = \"CUSTOM\"\n  authorizer_id = aws_api_gateway_authorizer.jwt.id\n}\n\nresource \"aws_api_gateway_integration\" \"users\" {\n  rest_api_id = aws_api_gateway_rest_api.main.id\n  resource_id = aws_api_gateway_resource.users.id\n  http_method = aws_api_gateway_method.users_get.http_method\n  \n  integration_http_method = \"POST\"\n  type                    = \"AWS_PROXY\"\n  uri                     = aws_lambda_function.users.invoke_arn\n}\n\n# JWT Authorizer\nresource \"aws_api_gateway_authorizer\" \"jwt\" {\n  name                   = \"jwt-authorizer\"\n  rest_api_id            = aws_api_gateway_rest_api.main.id\n  authorizer_uri         = aws_lambda_function.authorizer.invoke_arn\n  authorizer_credentials = aws_iam_role.authorizer.arn\n  identity_source        = \"method.request.header.Authorization\"\n  type                   = \"REQUEST\"\n  authorizer_result_ttl_in_seconds = 300\n}\n\n# Usage plan with rate limiting\nresource \"aws_api_gateway_usage_plan\" \"main\" {\n  name = \"standard-plan\"\n  \n  api_stages {\n    api_id = aws_api_gateway_rest_api.main.id\n    stage  = aws_api_gateway_stage.prod.stage_name\n  }\n  \n  quota_settings {\n    limit  = 10000\n    period = \"DAY\"\n  }\n  \n  throttle_settings {\n    burst_limit = 200\n    rate_limit  = 100\n  }\n}\n*/\n\n// GraphQL Gateway (Apollo Federation)\nconst { ApolloServer } = require('apollo-server');\nconst { ApolloGateway, IntrospectAndCompose } = require('@apollo/gateway');\n\nconst gateway = new ApolloGateway({\n  supergraphSdl: new IntrospectAndCompose({\n    subgraphs: [\n      { name: 'users', url: 'http://users-service:4001/graphql' },\n      { name: 'products', url: 'http://products-service:4002/graphql' },\n      { name: 'orders', url: 'http://orders-service:4003/graphql' }\n    ]\n  }),\n  buildService: ({ url }) => {\n    return new RemoteGraphQLDataSource({\n      url,\n      willSendRequest({ request, context }) {\n        // Forward auth token to subgraphs\n        if (context.user) {\n          request.http.headers.set('x-user-id', context.user.id);\n        }\n      }\n    });\n  }\n});\n\nconst server = new ApolloServer({\n  gateway,\n  context: ({ req }) => {\n    const token = req.headers.authorization || '';\n    const user = verifyToken(token);\n    return { user };\n  }\n});\n\nserver.listen({ port: 4000 });\n\n// Backend for Frontend (BFF) Pattern\n// Mobile BFF\napp.get('/mobile/api/home', authenticate, async (req, res) => {\n  // Optimized for mobile: minimal data\n  const [user, recentOrders] = await Promise.all([\n    axios.get(`${services.users}/${req.user.userId}`),\n    axios.get(`${services.orders}/user/${req.user.userId}?limit=3`)\n  ]);\n  \n  res.json({\n    userName: user.data.name,\n    orderCount: recentOrders.data.length\n  });\n});\n\n// Web BFF\napp.get('/web/api/home', authenticate, async (req, res) => {\n  // Rich data for web\n  const [user, orders, products, notifications] = await Promise.all([\n    axios.get(`${services.users}/${req.user.userId}`),\n    axios.get(`${services.orders}/user/${req.user.userId}`),\n    axios.get(`${services.products}/recommended/${req.user.userId}`),\n    axios.get(`${services.notifications}/user/${req.user.userId}`)\n  ]);\n  \n  res.json({\n    user: user.data,\n    orders: orders.data,\n    recommendedProducts: products.data,\n    notifications: notifications.data\n  });\n});"
    },
    {
      "id": 27,
      "question": "What is database indexing and how do different index types work?",
      "answer": "Database indexes are data structures that improve query performance by allowing faster data retrieval, trading additional storage and slower writes for faster reads.\n\nHow Indexes Work:\n• Create auxiliary data structure pointing to table rows\n• Database searches index instead of scanning entire table\n• Similar to book index: find pages without reading everything\n• Maintained automatically on INSERT/UPDATE/DELETE\n\nIndex Types:\n\n1. B-Tree Index (Default):\n• Balanced tree structure\n• Good for range queries and equality\n• Most common type\n• Use for: WHERE, ORDER BY, JOIN conditions\n\n2. Hash Index:\n• Hash table structure\n• Only equality comparisons (=)\n• Very fast lookups\n• Cannot do range queries\n• Use for: Exact match lookups\n\n3. Bitmap Index:\n• Bitmap for each distinct value\n• Good for low-cardinality columns\n• Efficient for data warehouses\n• Use for: Boolean, enum, status columns\n\n4. Full-Text Index:\n• Text search optimization\n• Word tokenization and stemming\n• Relevance scoring\n• Use for: Text search, articles, descriptions\n\n5. Spatial Index (R-Tree):\n• Geographic/geometric data\n• Nearest neighbor searches\n• Use for: Maps, location-based queries\n\n6. Composite (Multi-column) Index:\n• Index on multiple columns\n• Column order matters\n• Use for: Queries filtering on multiple columns\n\n7. Covering Index:\n• Includes all queried columns\n• Avoid table lookup\n• Use for: Frequently queried column sets\n\n8. Partial Index:\n• Index subset of rows\n• Condition-based indexing\n• Use for: Queries with common WHERE clause\n\nWhen to Use Indexes:\n• Columns in WHERE clauses\n• Columns in JOIN conditions\n• Columns in ORDER BY\n• Columns in GROUP BY\n• Foreign key columns\n\nWhen NOT to Use:\n• Small tables (< 1000 rows)\n• Columns with low selectivity\n• Frequently updated tables\n• Columns never in queries\n\nIndex Trade-offs:\n• Faster reads, slower writes\n• Additional storage space\n• Maintenance overhead\n• Diminishing returns with too many indexes",
      "explanation": "Database indexes are auxiliary data structures that speed up queries by enabling quick lookups instead of full table scans, with various types (B-tree, hash, composite) optimized for different query patterns.",
      "difficulty": "Medium",
      "code": "-- B-TREE INDEX (PostgreSQL)\n\n-- Create simple index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Before index (full table scan)\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = 'john@example.com';\n/*\nSeq Scan on users  (cost=0.00..1693.00 rows=1 width=100) (actual time=0.025..15.234 rows=1 loops=1)\n  Filter: (email = 'john@example.com')\n  Rows Removed by Filter: 99999\nPlanning Time: 0.120 ms\nExecution Time: 15.258 ms\n*/\n\n-- After index (index scan)\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = 'john@example.com';\n/*\nIndex Scan using idx_users_email on users  (cost=0.29..8.31 rows=1 width=100) (actual time=0.018..0.020 rows=1 loops=1)\n  Index Cond: (email = 'john@example.com')\nPlanning Time: 0.135 ms\nExecution Time: 0.041 ms  -- 370x faster!\n*/\n\n-- COMPOSITE INDEX (Multi-column)\nCREATE INDEX idx_orders_user_date ON orders(user_id, order_date DESC);\n\n-- Efficient query (uses index)\nSELECT * FROM orders \nWHERE user_id = 123 \nAND order_date > '2023-01-01'\nORDER BY order_date DESC;\n/* Uses idx_orders_user_date */\n\n-- Column order matters!\n-- This query CANNOT use idx_orders_user_date efficiently\nSELECT * FROM orders WHERE order_date > '2023-01-01';\n/* Would need separate index on order_date */\n\n-- Multiple index strategies\nCREATE INDEX idx_orders_user ON orders(user_id);        -- For user lookups\nCREATE INDEX idx_orders_date ON orders(order_date);     -- For date queries\nCREATE INDEX idx_orders_user_date ON orders(user_id, order_date);  -- For combined\n\n-- PARTIAL INDEX (Filtered)\nCREATE INDEX idx_orders_active ON orders(user_id, order_date)\nWHERE status = 'active';\n\n-- Efficient: uses partial index\nSELECT * FROM orders \nWHERE status = 'active' AND user_id = 123;\n\n-- Inefficient: cannot use partial index\nSELECT * FROM orders \nWHERE status = 'completed' AND user_id = 123;\n\n-- Partial index for common patterns\nCREATE INDEX idx_users_premium ON users(created_at)\nWHERE subscription_type = 'premium';\n\nCREATE INDEX idx_products_available ON products(category, price)\nWHERE in_stock = true AND active = true;\n\n-- UNIQUE INDEX (Constraint + Performance)\nCREATE UNIQUE INDEX idx_users_email_unique ON users(email);\n-- Enforces uniqueness AND speeds up lookups\n\n-- Conditional unique index\nCREATE UNIQUE INDEX idx_users_email_active ON users(email)\nWHERE deleted_at IS NULL;\n-- Allows same email if user deleted\n\n-- COVERING INDEX (Include columns)\nCREATE INDEX idx_users_email_covering ON users(email) \nINCLUDE (name, created_at);\n\n-- Index-only scan (no table access needed)\nSELECT name, email, created_at \nFROM users \nWHERE email = 'john@example.com';\n/* All data in index, no heap access */\n\n-- FULL-TEXT SEARCH INDEX\nCREATE INDEX idx_posts_search ON posts \nUSING gin(to_tsvector('english', title || ' ' || content));\n\n-- Full-text search\nSELECT * FROM posts\nWHERE to_tsvector('english', title || ' ' || content) @@ \n      to_tsquery('english', 'database & index');\n\n-- JSONB INDEX (for JSON columns)\nCREATE INDEX idx_users_preferences ON users USING gin(preferences);\n\n-- Query JSON fields\nSELECT * FROM users \nWHERE preferences @> '{\"newsletter\": true}';\n\n-- GIN index for array containment\nCREATE INDEX idx_posts_tags ON posts USING gin(tags);\n\nSELECT * FROM posts WHERE tags @> ARRAY['postgresql', 'indexing'];\n\n-- SPATIAL INDEX (PostGIS)\nCREATE INDEX idx_locations_geom ON locations USING gist(geom);\n\n-- Find nearby locations\nSELECT name, ST_Distance(geom, ST_MakePoint(-73.9857, 40.7484)) as distance\nFROM locations\nWHERE ST_DWithin(geom, ST_MakePoint(-73.9857, 40.7484), 1000)\nORDER BY distance\nLIMIT 10;\n\n-- INDEX STATISTICS AND ANALYSIS\n\n-- View table indexes\nSELECT\n    tablename,\n    indexname,\n    indexdef\nFROM pg_indexes\nWHERE schemaname = 'public'\nORDER BY tablename, indexname;\n\n-- Index usage statistics\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_scan as index_scans,\n    idx_tup_read as tuples_read,\n    idx_tup_fetch as tuples_fetched,\n    pg_size_pretty(pg_relation_size(indexrelid)) as index_size\nFROM pg_stat_user_indexes\nORDER BY idx_scan ASC;\n-- Find unused indexes (idx_scan = 0)\n\n-- Missing indexes query\nSELECT\n    schemaname,\n    tablename,\n    attname,\n    n_distinct,\n    correlation\nFROM pg_stats\nWHERE schemaname = 'public'\nAND n_distinct > 100  -- High cardinality\nORDER BY n_distinct DESC;\n\n-- Table and index sizes\nSELECT\n    tablename,\n    pg_size_pretty(pg_total_relation_size(tablename::regclass)) as total_size,\n    pg_size_pretty(pg_relation_size(tablename::regclass)) as table_size,\n    pg_size_pretty(pg_total_relation_size(tablename::regclass) - \n                   pg_relation_size(tablename::regclass)) as indexes_size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(tablename::regclass) DESC;\n\n-- INDEX MAINTENANCE\n\n-- Reindex (rebuild index)\nREINDEX INDEX idx_users_email;\nREINDEX TABLE users;  -- Rebuild all indexes on table\n\n-- Analyze table (update statistics)\nANALYZE users;\n\n-- Vacuum and analyze (cleanup + update stats)\nVACUUM ANALYZE users;\n\n-- Drop unused index\nDROP INDEX CONCURRENTLY idx_users_old;  -- CONCURRENTLY: no table lock\n\n-- QUERY OPTIMIZATION WITH INDEXES\n\n-- Example: Slow query\nEXPLAIN ANALYZE\nSELECT u.name, COUNT(o.id)\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2023-01-01'\nGROUP BY u.id, u.name\nORDER BY COUNT(o.id) DESC\nLIMIT 10;\n/*\nPlanning time: 0.234 ms\nExecution time: 542.123 ms  -- SLOW!\n*/\n\n-- Add indexes\nCREATE INDEX idx_users_created ON users(created_at);\nCREATE INDEX idx_orders_user ON orders(user_id);\n\n-- Same query after indexes\nEXPLAIN ANALYZE\nSELECT u.name, COUNT(o.id)\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2023-01-01'\nGROUP BY u.id, u.name\nORDER BY COUNT(o.id) DESC\nLIMIT 10;\n/*\nPlanning time: 0.156 ms\nExecution time: 45.234 ms  -- 12x faster!\n*/\n\n-- COMPOUND INDEX EXAMPLES\n\n-- Bad: Separate indexes\nCREATE INDEX idx_orders_user ON orders(user_id);\nCREATE INDEX idx_orders_status ON orders(status);\n\n-- Query may use only one index\nSELECT * FROM orders \nWHERE user_id = 123 AND status = 'pending';\n\n-- Good: Composite index\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n-- Query uses both columns efficiently\n\n-- Index order matters for range queries\nCREATE INDEX idx_orders_date_user ON orders(order_date, user_id);\n\n-- Efficient: uses full index\nSELECT * FROM orders \nWHERE order_date >= '2023-01-01' \nAND user_id = 123;\n\n-- Less efficient: only uses order_date part\nSELECT * FROM orders \nWHERE order_date >= '2023-01-01';\n\n-- Best: Put most selective column first\nCREATE INDEX idx_orders_user_date ON orders(user_id, order_date);\n-- If user_id is more selective than order_date\n\n-- MYSQL SPECIFIC\n\n-- Show indexes\nSHOW INDEXES FROM users;\n\n-- Create index\nCREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_users_name_email ON users(last_name, first_name);\n\n-- Force index usage\nSELECT * FROM users FORCE INDEX (idx_users_email)\nWHERE email = 'john@example.com';\n\n-- Full-text search (MySQL)\nCREATE FULLTEXT INDEX idx_posts_content ON posts(title, body);\n\nSELECT * FROM posts\nWHERE MATCH(title, body) AGAINST ('database indexing' IN NATURAL LANGUAGE MODE);\n\n-- MONGODB INDEXES\n\n// Create index\ndb.users.createIndex({ email: 1 });  // 1 = ascending\ndb.users.createIndex({ email: 1 }, { unique: true });\n\n// Compound index\ndb.orders.createIndex({ userId: 1, orderDate: -1 });  // -1 = descending\n\n// Text index\ndb.posts.createIndex({ title: \"text\", content: \"text\" });\ndb.posts.find({ $text: { $search: \"database indexing\" } });\n\n// Partial index\ndb.orders.createIndex(\n  { userId: 1, orderDate: -1 },\n  { partialFilterExpression: { status: \"active\" } }\n);\n\n// Explain query\ndb.users.find({ email: \"john@example.com\" }).explain(\"executionStats\");\n\n// List indexes\ndb.users.getIndexes();\n\n// Drop index\ndb.users.dropIndex(\"idx_users_email\");\n\n// Index statistics\ndb.users.stats().indexSizes;"
    },
    {
      "id": 28,
      "question": "What is GraphQL and how does it differ from REST?",
      "answer": "GraphQL is a query language and runtime for APIs that allows clients to request exactly the data they need, unlike REST where endpoints return fixed data structures.\n\nGraphQL Core Concepts:\n• Schema: Type system defining available data and operations\n• Queries: Read data (equivalent to GET)\n• Mutations: Modify data (POST/PUT/DELETE)\n• Subscriptions: Real-time data updates via WebSocket\n• Resolvers: Functions that fetch data for each field\n• Single endpoint: All requests to one URL\n\nGraphQL vs REST:\n\nData Fetching:\n• REST: Multiple endpoints, fixed responses, over/under-fetching\n• GraphQL: Single endpoint, request exactly what you need\n\nVersioning:\n• REST: /api/v1, /api/v2 (URL versioning)\n• GraphQL: Schema evolution, deprecate fields\n\nCaching:\n• REST: Built-in HTTP caching\n• GraphQL: Requires custom caching (Apollo, DataLoader)\n\nError Handling:\n• REST: HTTP status codes\n• GraphQL: Always 200, errors in response body\n\nDocumentation:\n• REST: Swagger/OpenAPI\n• GraphQL: Self-documenting schema, GraphiQL\n\nGraphQL Benefits:\n• No over-fetching: Get only requested fields\n• No under-fetching: Get related data in one request\n• Strong typing: Schema validation\n• Introspection: Query schema structure\n• Developer experience: Autocomplete, validation\n• Rapid iteration: Frontend changes without backend\n\nGraphQL Challenges:\n• Complex to implement: Resolvers, N+1 problem\n• Caching difficult: No URL-based caching\n• File uploads awkward: Need multipart form\n• Query complexity: Malicious deep queries\n• Learning curve: New paradigm\n\nWhen to Use GraphQL:\n• Complex, nested data requirements\n• Multiple clients with different needs (web, mobile)\n• Rapid frontend development\n• Need flexible queries\n• GraphQL federation (microservices)\n\nWhen to Use REST:\n• Simple CRUD operations\n• File uploads/downloads\n• Need HTTP caching\n• Simpler implementation\n• Public API with broad usage",
      "explanation": "GraphQL enables precise data fetching through a query language with a single endpoint and strong typing, while REST uses multiple endpoints with fixed responses, each approach suited to different use cases.",
      "difficulty": "Medium",
      "code": "// GraphQL Schema Definition (SDL)\nconst { gql } = require('apollo-server');\n\nconst typeDefs = gql`\n  # User type\n  type User {\n    id: ID!\n    name: String!\n    email: String!\n    posts: [Post!]!\n    followers: [User!]!\n    followerCount: Int!\n  }\n  \n  # Post type\n  type Post {\n    id: ID!\n    title: String!\n    content: String!\n    author: User!\n    comments: [Comment!]!\n    likes: Int!\n    createdAt: String!\n  }\n  \n  # Comment type\n  type Comment {\n    id: ID!\n    content: String!\n    author: User!\n    post: Post!\n    createdAt: String!\n  }\n  \n  # Input types for mutations\n  input CreatePostInput {\n    title: String!\n    content: String!\n  }\n  \n  input UpdatePostInput {\n    title: String\n    content: String\n  }\n  \n  # Queries (read operations)\n  type Query {\n    # Get single user\n    user(id: ID!): User\n    \n    # Get multiple users with pagination\n    users(limit: Int, offset: Int): [User!]!\n    \n    # Search posts\n    searchPosts(query: String!, limit: Int): [Post!]!\n    \n    # Get user's feed\n    feed(userId: ID!, limit: Int): [Post!]!\n  }\n  \n  # Mutations (write operations)\n  type Mutation {\n    createPost(input: CreatePostInput!): Post!\n    updatePost(id: ID!, input: UpdatePostInput!): Post!\n    deletePost(id: ID!): Boolean!\n    \n    followUser(userId: ID!): User!\n    unfollowUser(userId: ID!): User!\n    \n    likePost(postId: ID!): Post!\n  }\n  \n  # Subscriptions (real-time)\n  type Subscription {\n    postCreated: Post!\n    commentAdded(postId: ID!): Comment!\n    userStatusChanged(userId: ID!): User!\n  }\n`;\n\n// Resolvers Implementation\nconst { PubSub } = require('graphql-subscriptions');\nconst pubsub = new PubSub();\n\nconst resolvers = {\n  Query: {\n    user: async (parent, { id }, context) => {\n      return await context.db.query(\n        'SELECT * FROM users WHERE id = $1',\n        [id]\n      );\n    },\n    \n    users: async (parent, { limit = 10, offset = 0 }, context) => {\n      return await context.db.query(\n        'SELECT * FROM users LIMIT $1 OFFSET $2',\n        [limit, offset]\n      );\n    },\n    \n    searchPosts: async (parent, { query, limit = 10 }, context) => {\n      return await context.db.query(\n        `SELECT * FROM posts \n         WHERE to_tsvector('english', title || ' ' || content) @@ plainto_tsquery('english', $1)\n         LIMIT $2`,\n        [query, limit]\n      );\n    },\n    \n    feed: async (parent, { userId, limit = 20 }, context) => {\n      // Get posts from users that userId follows\n      return await context.db.query(\n        `SELECT p.* FROM posts p\n         JOIN follows f ON p.author_id = f.following_id\n         WHERE f.follower_id = $1\n         ORDER BY p.created_at DESC\n         LIMIT $2`,\n        [userId, limit]\n      );\n    }\n  },\n  \n  Mutation: {\n    createPost: async (parent, { input }, context) => {\n      if (!context.user) {\n        throw new Error('Not authenticated');\n      }\n      \n      const result = await context.db.query(\n        'INSERT INTO posts (title, content, author_id) VALUES ($1, $2, $3) RETURNING *',\n        [input.title, input.content, context.user.id]\n      );\n      \n      const post = result.rows[0];\n      \n      // Publish to subscribers\n      pubsub.publish('POST_CREATED', { postCreated: post });\n      \n      return post;\n    },\n    \n    updatePost: async (parent, { id, input }, context) => {\n      if (!context.user) {\n        throw new Error('Not authenticated');\n      }\n      \n      // Check authorization\n      const post = await context.db.query(\n        'SELECT * FROM posts WHERE id = $1',\n        [id]\n      );\n      \n      if (post.rows[0].author_id !== context.user.id) {\n        throw new Error('Not authorized');\n      }\n      \n      const updates = [];\n      const values = [];\n      let paramCount = 1;\n      \n      if (input.title) {\n        updates.push(`title = $${paramCount++}`);\n        values.push(input.title);\n      }\n      if (input.content) {\n        updates.push(`content = $${paramCount++}`);\n        values.push(input.content);\n      }\n      \n      values.push(id);\n      \n      const result = await context.db.query(\n        `UPDATE posts SET ${updates.join(', ')} WHERE id = $${paramCount} RETURNING *`,\n        values\n      );\n      \n      return result.rows[0];\n    },\n    \n    likePost: async (parent, { postId }, context) => {\n      if (!context.user) {\n        throw new Error('Not authenticated');\n      }\n      \n      await context.db.query(\n        'INSERT INTO likes (user_id, post_id) VALUES ($1, $2) ON CONFLICT DO NOTHING',\n        [context.user.id, postId]\n      );\n      \n      const result = await context.db.query(\n        'SELECT * FROM posts WHERE id = $1',\n        [postId]\n      );\n      \n      return result.rows[0];\n    }\n  },\n  \n  Subscription: {\n    postCreated: {\n      subscribe: () => pubsub.asyncIterator(['POST_CREATED'])\n    },\n    \n    commentAdded: {\n      subscribe: (parent, { postId }) => {\n        return pubsub.asyncIterator([`COMMENT_ADDED_${postId}`]);\n      }\n    }\n  },\n  \n  // Field resolvers\n  User: {\n    posts: async (parent, args, context) => {\n      return await context.db.query(\n        'SELECT * FROM posts WHERE author_id = $1',\n        [parent.id]\n      );\n    },\n    \n    followers: async (parent, args, context) => {\n      return await context.db.query(\n        `SELECT u.* FROM users u\n         JOIN follows f ON u.id = f.follower_id\n         WHERE f.following_id = $1`,\n        [parent.id]\n      );\n    },\n    \n    followerCount: async (parent, args, context) => {\n      // Use DataLoader to batch requests\n      return await context.loaders.followerCount.load(parent.id);\n    }\n  },\n  \n  Post: {\n    author: async (parent, args, context) => {\n      // Use DataLoader to avoid N+1 problem\n      return await context.loaders.user.load(parent.author_id);\n    },\n    \n    comments: async (parent, args, context) => {\n      return await context.db.query(\n        'SELECT * FROM comments WHERE post_id = $1 ORDER BY created_at DESC',\n        [parent.id]\n      );\n    },\n    \n    likes: async (parent, args, context) => {\n      // Use DataLoader\n      return await context.loaders.likeCount.load(parent.id);\n    }\n  },\n  \n  Comment: {\n    author: async (parent, args, context) => {\n      return await context.loaders.user.load(parent.author_id);\n    },\n    \n    post: async (parent, args, context) => {\n      return await context.loaders.post.load(parent.post_id);\n    }\n  }\n};\n\n// DataLoader (solve N+1 problem)\nconst DataLoader = require('dataloader');\n\nfunction createLoaders(db) {\n  return {\n    user: new DataLoader(async (userIds) => {\n      const placeholders = userIds.map((_, i) => `$${i + 1}`).join(',');\n      const result = await db.query(\n        `SELECT * FROM users WHERE id IN (${placeholders})`,\n        userIds\n       );\n      \n      // Return in same order as input\n      const userMap = new Map(result.rows.map(u => [u.id, u]));\n      return userIds.map(id => userMap.get(id));\n    }),\n    \n    followerCount: new DataLoader(async (userIds) => {\n      const placeholders = userIds.map((_, i) => `$${i + 1}`).join(',');\n      const result = await db.query(\n        `SELECT following_id, COUNT(*) as count \n         FROM follows \n         WHERE following_id IN (${placeholders})\n         GROUP BY following_id`,\n        userIds\n      );\n      \n      const countMap = new Map(result.rows.map(r => [r.following_id, parseInt(r.count)]));\n      return userIds.map(id => countMap.get(id) || 0);\n    }),\n    \n    likeCount: new DataLoader(async (postIds) => {\n      const placeholders = postIds.map((_, i) => `$${i + 1}`).join(',');\n      const result = await db.query(\n        `SELECT post_id, COUNT(*) as count \n         FROM likes \n         WHERE post_id IN (${placeholders})\n         GROUP BY post_id`,\n        postIds\n      );\n      \n      const countMap = new Map(result.rows.map(r => [r.post_id, parseInt(r.count)]));\n      return postIds.map(id => countMap.get(id) || 0);\n    })\n  };\n}\n\n// Apollo Server Setup\nconst { ApolloServer } = require('apollo-server');\nconst { Pool } = require('pg');\nconst jwt = require('jsonwebtoken');\n\nconst db = new Pool({ connectionString: process.env.DATABASE_URL });\n\nconst server = new ApolloServer({\n  typeDefs,\n  resolvers,\n  context: async ({ req }) => {\n    const token = req.headers.authorization?.replace('Bearer ', '');\n    const user = token ? jwt.verify(token, process.env.JWT_SECRET) : null;\n    \n    return {\n      db,\n      user,\n      loaders: createLoaders(db)\n    };\n  },\n  formatError: (error) => {\n    console.error(error);\n    return {\n      message: error.message,\n      code: error.extensions?.code,\n      path: error.path\n    };\n  }\n});\n\nserver.listen({ port: 4000 });\n\n// GraphQL Client Queries\n\n// Query example (vs REST: GET /users/123, GET /users/123/posts, GET /users/123/followers)\nconst GET_USER_WITH_POSTS = gql`\n  query GetUser($id: ID!) {\n    user(id: $id) {\n      id\n      name\n      email\n      followerCount\n      posts {\n        id\n        title\n        likes\n        createdAt\n      }\n    }\n  }\n`;\n\n// Mutation example (vs REST: POST /posts)\nconst CREATE_POST = gql`\n  mutation CreatePost($input: CreatePostInput!) {\n    createPost(input: $input) {\n      id\n      title\n      content\n      author {\n        id\n        name\n      }\n    }\n  }\n`;\n\n// Subscription example (real-time)\nconst POST_SUBSCRIPTION = gql`\n  subscription OnPostCreated {\n    postCreated {\n      id\n      title\n      author {\n        name\n      }\n    }\n  }\n`;\n\n// Apollo Client\nconst { ApolloClient, InMemoryCache, createHttpLink } = require('@apollo/client');\n\nconst client = new ApolloClient({\n  link: createHttpLink({ uri: 'http://localhost:4000' }),\n  cache: new InMemoryCache()\n});\n\n// Execute query\nconst { data } = await client.query({\n  query: GET_USER_WITH_POSTS,\n  variables: { id: '123' }\n});\n\n// Execute mutation\nconst { data: postData } = await client. mutate({\n  mutation: CREATE_POST,\n  variables: {\n    input: { title: 'GraphQL Tutorial', content: 'Learn GraphQL...' }\n  }\n});\n\n// GraphQL vs REST Comparison\n\n// REST: Multiple requests\n// GET /users/123\n// GET /users/123/posts\n// GET /posts/1/comments\n// GET /posts/2/comments\n// ...\n// Total: 1 + 1 + N requests\n\n// GraphQL: Single request\nconst FEED_QUERY = gql`\n  query GetFeed($userId: ID!) {\n    user(id: $userId) {\n      name\n      posts {\n        title\n        comments {\n          content\n          author {\n            name\n          }\n        }\n      }\n    }\n  }\n`;\n// Total: 1 request"
    },
    {
      "id": 29,
      "question": "What is service mesh and how does it improve microservices communication?",
      "answer": "A service mesh is an infrastructure layer that manages service-to-service communication in microservices architectures, handling traffic management,security, and observability without changing application code.\n\nCore Components:\n\n• Data Plane: Sidecar proxies deployed with each service\n  - Intercept all network traffic\n  - Enforce policies\n  - Collect telemetry\n  - Examples: Envoy, Linkerd2-proxy\n\n• Control Plane: Centralized management\n  - Configure proxies\n  - Distribute policies\n  - Collect metrics\n  - Examples: Istio pilot, Linkerd controller\n\nKey Features:\n\n• Traffic Management:\n  - Load balancing (round-robin, least request)\n  - Service discovery\n  - Routing rules (canary, blue-green)\n  - Timeout and retry policies\n  - Circuit breaking\n  - Rate limiting\n\n• Security:\n  - Mutual TLS (mTLS) encryption\n  - Certificate management\n  - Authentication between services\n  - Authorization policies\n\n• Observability:\n  - Distributed tracing\n  - Metrics collection (latency, errors)\n  - Access logs\n  - Service topology\n\n• Resilience:\n  - Automatic retries\n  - Circuit breakers\n  - Fault injection for testing\n  - Health checking\n\nBenefits:\n\n• Transparency: No application code changes\n• Consistency: Same features across all services\n• Security: Automatic mTLS encryption\n• Observability: Built-in metrics and tracing\n• Traffic control: Advanced routing without code\n• Polyglot: Works with any language\n\nChallenges:\n\n• Complexity: Additional infrastructure\n• Performance overhead: Proxy latency\n• Resource usage: Extra CPU/memory per pod\n• Learning curve: New concepts and tools\n• Debugging: More layers to troubleshoot\n\nPopular Solutions:\n\n• Istio: Feature-rich, complex, uses Envoy\n• Linkerd: Lightweight, easier, purpose-built\n• Consul Connect: HashiCorp, multi-platform\n• AWS App Mesh: Managed service for AWS\n\nWhen to Use:\n\n• Many microservices (>10)\n• Need consistent security/observability\n• Complex traffic requirements\n• Polyglot environment\n• Compliance requirements (mTLS)\n\nWhen NOT to Use:\n\n• Simple architecture (few services)\n• Monolith or small apps\n• Limited resources\n• Team lacks expertise",
      "explanation": "Service mesh provides transparent infrastructure for microservices communication handling traffic management, security (mTLS), and observability through sidecar proxies without modifying application code.",
      "difficulty": "Hard",
      "code": "// ISTIO SERVICE MESH CONFIGURATION\n\n// Virtual Service (Traffic Routing)\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: reviews-route\nspec:\n  hosts:\n  - reviews.example.com\n  http:\n  - match:\n    - headers:\n        user-agent:\n          regex: '.*Mobile.*'\n    route:\n    - destination:\n        host: reviews\n        subset: mobile\n  - route:\n    - destination:\n        host: reviews\n        subset: v2\n      weight: 90\n    - destination:\n        host: reviews\n        subset: v3\n      weight: 10  # Canary: 10% traffic to v3\n\n---\n// Destination Rule (Load Balancing, Circuit Breaking)\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: reviews-destination\nspec:\n  host: reviews\n  trafficPolicy:\n    loadBalancer:\n      simple: LEAST_REQUEST  # Load balancing strategy\n    connectionPool:\n      tcp:\n        maxConnections: 100\n      http:\n        http1MaxPendingRequests: 50\n        http2MaxRequests: 100\n    outlierDetection:  # Circuit breaking\n      consecutive5xxErrors: 5\n      interval: 30s\n      baseEjectionTime: 30s\n      maxEjectionPercent: 50\n  subsets:\n  - name: v2\n    labels:\n      version: v2\n  - name: v3\n    labels:\n      version: v3\n  - name: mobile\n    labels:\n      version: mobile\n\n---\n// Gateway (Ingress Configuration)\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: bookinfo-cert\n    hosts:\n    - \"bookinfo.example.com\"\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n\n---\n// Service Entry (External Service)\napiVersion: networking.istio.io/v1beta1\nkind: ServiceEntry\nmetadata:\n  name: external-api\nspec:\n  hosts:\n  - api.external.com\n  ports:\n  - number: 443\n    name: https\n    protocol: HTTPS\n  location: MESH_EXTERNAL\n  resolution: DNS\n\n---\n// Peer Authentication (mTLS)\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: production\nspec:\n  mtls:\n    mode: STRICT  # Require mTLS for all services\n\n---\n// Authorization Policy (Access Control)\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: reviews-policy\nspec:\n  selector:\n    matchLabels:\n      app: reviews\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/sa/productpage\"]\n    to:\n    - operation:\n        methods: [\"GET\"]\n        paths: [\"/reviews/*\"]\n\n---\n// Rate Limiting \napiVersion: networking.istio.io/v1beta1\nkind: EnvoyFilter\nmetadata:\n  name: rate-limit\nspec:\n  workloadSelector:\n    labels:\n      app: productpage\n  configPatches:\n  - applyTo: HTTP_FILTER\n    match:\n      context: SIDECAR_INBOUND\n      listener:\n        filterChain:\n          filter:\n            name: \"envoy.filters.network.http_connection_manager\"\n    patch:\n      operation: INSERT_BEFORE\n      value:\n        name: envoy.filters.http.local_ratelimit\n        typed_config:\n          \"@type\": type.googleapis.com/udpa.type.v1.TypedStruct\n          type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit\n          value:\n            stat_prefix: http_local_rate_limiter\n            token_bucket:\n              max_tokens: 100\n              tokens_per_fill: 100\n              fill_interval: 60s\n\n---\n// Retry Policy\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: api-retry\nspec:\n  hosts:\n  - api-service\n  http:\n  - route:\n    - destination:\n        host: api-service\n    retries:\n      attempts: 3\n      perTryTimeout: 2s\n      retryOn: 5xx,retriable-4xx,connect-failure,refused-stream\n\n---\n// Timeout Configuration\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: api-timeout\nspec:\n  hosts:\n  - api-service\n  http:\n  - route:\n    - destination:\n        host: api-service\n    timeout: 5s\n\n---\n// Fault Injection (Testing)\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: fault-injection\nspec:\n  hosts:\n  - api-service\n  http:\n  - match:\n    - headers:\n        x-test-fault:\n          exact: \"true\"\n    fault:\n      delay:\n        percentage:\n          value: 50  # 50% of requests\n        fixedDelay: 5s\n      abort:\n        percentage:\n          value: 10  # 10% fail\n        httpStatus: 500\n    route:\n    - destination:\n        host: api-service\n\n// LINKERD SERVICE MESH\n\n// Linkerd automatically injects sidecar proxies\n// Enable per namespace:\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  annotations:\n    linkerd.io/inject: enabled\n\n---\n// Service Profile (Retry and Timeout)\napiVersion: linkerd.io/v1alpha2\nkind: ServiceProfile\nmetadata:\n  name: api-service.production.svc.cluster.local\n  namespace: production\nspec:\n  routes:\n  - name: GET /users/{id}\n    condition:\n      method: GET\n      pathRegex: /users/[^/]+\n    retries:\n      retryableStatusCodes:\n      - 503\n      - 504\n      maxRetries: 3\n      backoff:\n        minBackoff: 100ms\n        maxBackoff: 1s\n        jitter: 0.1\n    timeout: 5s\n  - name: POST /users\n    condition:\n      method: POST\n      pathRegex: /users\n    timeout: 10s\n\n---\n// Traffic Split (Canary)\napiVersion: split.smi-spec.io/v1alpha1\nkind: TrafficSplit\nmetadata:\n  name: api-canary\n  namespace: production\nspec:\n  service: api-service\n  backends:\n  - service: api-service-stable\n    weight: 90\n  - service: api-service-canary\n    weight: 10\n\n// OBSERVABILITY\n\n// Distributed Tracing with Jaeger\n// Istio automatically propagates trace headers\n\n// Application code needs to forward trace headers:\nconst axios = require('axios');\n\napp.get('/api/products', async (req, res) => {\n  // Forward tracing headers\n  const traceHeaders = [\n    'x-request-id',\n    'x-b3-traceid',\n    'x-b3-spanid',\n    'x-b3-parentspanid',\n    'x-b3-sampled',\n    'x-b3-flags',\n    'x-ot-span-context'\n  ];\n  \n  const headers = {};\n  traceHeaders.forEach(header => {\n    if (req.headers[header]) {\n      headers[header] = req.headers[header];\n    }\n  });\n  \n  // Call downstream service\n  const response = await axios.get('http://inventory-service/api/inventory', {\n    headers\n  });\n  \n  res.json(response.data);\n});\n\n// Prometheus Metrics\n// Service mesh automatically exports metrics:\n// - istio_requests_total\n// - istio_request_duration_milliseconds\n// - istio_request_bytes\n// - istio_response_bytes\n\n// Query metrics\nrate(istio_requests_total{destination_service=\"reviews.production.svc.cluster.local\"}[5m])\n\n// p99 latency\nhistogram_quantile(0.99, \n  rate(istio_request_duration_milliseconds_bucket{destination_service=\"reviews.production.svc.cluster.local\"}[5m])\n)\n\n// Error rate\nsum(rate(istio_requests_total{destination_service=\"reviews.production.svc.cluster.local\",response_code=~\"5..\"}[5m])) / \nsum(rate(istio_requests_total{destination_service=\"reviews.production.svc.cluster.local\"}[5m]))\n\n// DEPLOYMENT STRATEGIES\n\n// Blue-Green Deployment\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: api-blue-green\nspec:\n  hosts:\n  - api-service\n  http:\n  - route:\n    - destination:\n        host: api-service\n        subset: green  # Switch to green\n      weight: 100\n    - destination:\n        host: api-service\n        subset: blue\n      weight: 0\n\n// Canary with Prometheus Metrics\napiVersion: flagger.app/v1beta1\nkind: Canary\nmetadata:\n  name: api-canary\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-service\n  progressDeadlineSeconds: 600\n  service:\n    port: 8080\n  analysis:\n    interval: 1m\n    threshold: 5\n    maxWeight: 50\n    stepWeight: 10\n    metrics:\n    - name: request-success-rate\n      thresholdRange:\n        min: 99\n      interval: 1m\n    - name: request-duration\n      thresholdRange:\n        max: 500\n      interval: 1m\n  webhooks:\n  - name: load-test\n    url: http://load-tester/start\n    timeout: 5s\n    metadata:\n      cmd: \"hey -z 1m -q 10 -c 2 http://api-service:8080/\""
    },
    {
      "id": 30,
      "question": "What are SOLID principles and how do they apply to system design?",
      "answer": "SOLID is a set of five object-oriented design principles that promote maintainable, scalable, and testable code, applicable to both class design and system architecture.\n\nThe Five Principles:\n\n1. Single Responsibility Principle (SRP):\n• A class/module should have one reason to change\n• One responsibility per component\n• System Design: Microservices should have single business capability\n• Example: Separate AuthService, PaymentService, NotificationService\n\n2. Open/Closed Principle (OCP):\n• Open for extension, closed for modification\n• Add new functionality without changing existing code\n• System Design: Plugin architecture, strategy pattern, API versioning\n• Example: Payment gateway with multiple providers\n\n3. Liskov Substitution Principle (LSP):\n• Subtypes must be substitutable for base types\n• Interface contracts must be maintained\n• System Design: API compatibility, service contracts\n• Example: Database drivers implementing same interface\n\n4. Interface Segregation Principle (ISP):\n• Clients shouldn't depend on interfaces they don't use\n• Many specific interfaces better than one general\n• System Design: Focused APIs, API segmentation\n• Example: Read API vs Write API, public vs internal APIs\n\n5. Dependency Inversion Principle (DIP):\n• Depend on abstractions, not concretions\n• High-level modules shouldn't depend on low-level\n• System Design: Dependency injection, service abstraction\n• Example: Services depend on interfaces, not implementations\n\nSystem Design Applications:\n\n• Microservices Architecture:\n  - SRP: Each service has single responsibility\n  - OCP: Add services without modifying existing\n  - DIP: Services communicate via abstractions (APIs)\n\n• API Design:\n  - ISP: Focused endpoints, not monolithic APIs\n  - LSP: Backward compatibility\n  - OCP: Versioning strategy\n\n• Database Layer:\n  - DIP: Repository pattern, abstract data access\n  - SRP: Separate read/write operations (CQRS)\n\n• Event-Driven Systems:\n  - OCP: Add event handlers without changing producers\n  - DIP: Depend on event contracts, not implementations\n\nBenefits:\n• Maintainability: Easier to understand and modify\n• Testability: Components can be tested in isolation\n• Flexibility: Easy to extend and adapt\n• Reusability: Components can be reused\n• Scalability: Independent scaling of responsibilities\n\nTrade-offs:\n• Complexity: More classes/services\n• Over-engineering: Can be excessive for simple systems\n• Performance: Abstraction layers add overhead\n• Learning curve: Requires experience to apply correctly",
      "explanation": "SOLID principles guide creating maintainable systems through single responsibilities, extensibility without modification, substitutability, interface segregation, and dependency abstraction, applicable from class design to microservices architecture.",
      "difficulty": "Easy",
      "code": "// SINGLE RESPONSIBILITY PRINCIPLE (SRP)\n\n// BAD: Class has multiple responsibilities\nclass User {\n  constructor(name, email) {\n    this.name = name;\n    this.email = email;\n  }\n  \n  save() {\n    // Database logic\n    db.query('INSERT INTO users ...');\n  }\n  \n  sendEmail(subject, body) {\n    // Email logic\n    smtp.send(...);\n  }\n  \n  generateReport() {\n    // Reporting logic\n    return `Report for ${this.name}`;\n  }\n}\n// Problems: Changes to DB, email, or reporting affect User class\n\n// GOOD: Separate responsibilities\nclass User {\n  constructor(name, email) {\n    this.name = name;\n    this.email = email;\n  }\n  \n  // Only user-related logic\n  getFullProfile() {\n    return { name: this.name, email: this.email };\n  }\n}\n\nclass UserRepository {\n  save(user) {\n    return db.query('INSERT INTO users (name, email) VALUES ($1, $2)', \n      [user.name, user.email]);\n  }\n  \n  findById(id) {\n    return db.query('SELECT * FROM users WHERE id = $1', [id]);\n  }\n}\n\nclass EmailService {\n  sendToUser(user, subject, body) {\n    return smtp.send({\n      to: user.email,\n      subject,\n      body\n    });\n  }\n}\n\nclass UserReportGenerator {\n  generate(user) {\n    return `Report for ${user.name}`;\n  }\n}\n\n// System Design: Microservices with SRP\n// Instead of monolith:\n// - AuthenticationService (only authentication)\n// - UserProfileService (user data management)\n// - NotificationService (sending notifications)\n// - AnalyticsService (reporting and analytics)\n\n// OPEN/CLOSED PRINCIPLE (OCP)\n\n// BAD: Need to modify class to add payment methods\nclass PaymentProcessor {\n  processPayment(amount, method) {\n    if (method === 'credit_card') {\n      // Credit card logic\n    } else if (method === 'paypal') {\n      // PayPal logic\n    } else if (method === 'crypto') {\n      // Crypto logic - NEW, had to modify class\n    }\n  }\n}\n\n// GOOD: Open for extension, closed for modification\nclass PaymentMethod {\n  process(amount) {\n    throw new Error('Must implement process method');\n  }\n}\n\nclass CreditCardPayment extends PaymentMethod {\n  process(amount) {\n    // Credit card logic\n    return this.chargeCreditCard(amount);\n  }\n}\n\nclass PayPalPayment extends PaymentMethod {\n  process(amount) {\n    // PayPal logic\n    return this.chargePayPal(amount);\n  }\n}\n\nclass CryptoPayment extends PaymentMethod {\n  process(amount) {\n    // Crypto logic - NEW class, didn't modify existing\n    return this.chargeCrypto(amount);\n  }\n}\n\nclass PaymentProcessor {\n  constructor() {\n    this.methods = new Map();\n  }\n  \n  registerMethod(name, method) {\n    this.methods.set(name, method);\n  }\n  \n  process(amount, methodName) {\n    const method = this.methods.get(methodName);\n    if (!method) {\n      throw new Error(`Unknown payment method: ${methodName}`);\n    }\n    return method.process(amount);\n  }\n}\n\n// Usage\nconst processor = new PaymentProcessor();\nprocessor.registerMethod('credit_card', new CreditCardPayment());\nprocessor.registerMethod('paypal', new PayPalPayment());\nprocessor.registerMethod('crypto', new CryptoPayment());  // Easy to add\n\n// System Design: Plugin Architecture\nclass NotificationService {\n  constructor() {\n    this.providers = [];\n  }\n  \n  registerProvider(provider) {\n    this.providers.push(provider);  // New providers without modification\n  }\n  \n  async send(user, message) {\n    await Promise.all(\n      this.providers.map(p => p.send(user, message))\n    );\n  }\n}\n\n// LISKOV SUBSTITUTION PRINCIPLE (LSP)\n\n// BAD: Subtype changes behavior unexpectedly\nclass Rectangle {\n  setWidth(width) {\n    this.width = width;\n  }\n  \n  setHeight(height) {\n    this.height = height;\n  }\n  \n  getArea() {\n    return this.width * this.height;\n  }\n}\n\nclass Square extends Rectangle {\n  setWidth(width) {\n    this.width = width;\n    this.height = width;  // Violates LSP!\n  }\n  \n  setHeight(height) {\n    this.width = height;  // Violates LSP!\n    this.height = height;\n  }\n}\n\n// Test breaks:\nfunction testRectangle(rectangle) {\n  rectangle.setWidth(5);\n  rectangle.setHeight(4);\n  console.assert(rectangle.getArea() === 20);  // Fails for Square!\n}\n\n// GOOD: Separate abstractions\nclass Shape {\n  getArea() {\n    throw new Error('Must implement');\n  }\n}\n\nclass Rectangle extends Shape {\n  constructor(width, height) {\n    super();\n    this.width = width;\n    this.height = height;\n  }\n  \n  getArea() {\n    return this.width * this.height;\n  }\n}\n\nclass Square extends Shape {\n  constructor(side) {\n    super();\n    this.side = side;\n  }\n  \n  getArea() {\n    return this.side * this.side;\n  }\n}\n\n// System Design: Service Contracts\ninterface DatabaseDriver {\n  connect(): Promise<Connection>;\n  query(sql: string): Promise<Result>;\n  disconnect(): Promise<void>;\n}\n\n// All implementations must maintain contract\nclass PostgreSQLDriver implements DatabaseDriver {\n  async connect() { /* ... */ }\n  async query(sql) { /* ... */ }\n  async disconnect() { /* ... */ }\n}\n\nclass MySQLDriver implements DatabaseDriver {\n  async connect() { /* ... */ }\n  async query(sql) { /* ... */ }  // Same interface\n  async disconnect() { /* ... */ }\n}\n\n// INTERFACE SEGREGATION PRINCIPLE (ISP)\n\n// BAD: Fat interface\ninterface Worker {\n  work(): void;\n  eat(): void;\n  sleep(): void;\n  getPaid(): void;\n}\n\nclass Robot implements Worker {\n  work() { /* works */ }\n  eat() { /* Robots don't eat - forced to implement */ }\n  sleep() { /* Robots don't sleep - forced to implement */ }\n  getPaid() { /* Robots don't get paid - forced to implement */ }\n}\n\n// GOOD: Segregated interfaces\ninterface Workable {\n  work(): void;\n}\n\ninterface Eatable {\n  eat(): void;\n}\n\ninterface Sleepable {\n  sleep(): void;\n}\n\ninterface Payable {\n  getPaid(): void;\n}\n\nclass Human implements Workable, Eatable, Sleepable, Payable {\n  work() { /* ... */ }\n  eat() { /* ... */ }\n  sleep() { /* ... */ }\n  getPaid() { /* ... */ }\n}\n\nclass Robot implements Workable {\n  work() { /* ... */ }  // Only implements what it needs\n}\n\n// System Design: API Segregation\n// Instead of:\napp.get('/api/everything', ...)  // Returns too much data\n\n// Use focused endpoints:\napp.get('/api/users/:id/profile', ...)  // Profile data only\napp.get('/api/users/:id/orders', ...)   // Orders only\napp.get('/api/users/:id/payments', ...) // Payments only\n\n// DEPENDENCY INVERSION PRINCIPLE (DIP)\n\n// BAD: High-level depends on low-level\nclass MySQLDatabase {\n  query(sql) { /* MySQL-specific */ }\n}\n\nclass UserService {\n  constructor() {\n    this.db = new MySQLDatabase();  // Tight coupling!\n  }\n  \n  getUser(id) {\n    return this.db.query(`SELECT * FROM users WHERE id = ${id}`);\n  }\n}\n\n// GOOD: Depend on abstraction\ninterface Database {\n  query(sql: string): Promise<any>;\n}\n\nclass MySQLDatabase implements Database {\n  query(sql) { /* MySQL implementation */ }\n}\n\nclass PostgreSQLDatabase implements Database {\n  query(sql) { /* PostgreSQL implementation */ }\n}\n\nclass UserService {\n  constructor(private db: Database) {}  // Depends on abstraction\n  \n  getUser(id) {\n    return this.db.query(`SELECT * FROM users WHERE id = ${id}`);\n  }\n}\n\n// Dependency Injection\nconst mysqlDb = new MySQLDatabase();\nconst userService = new UserService(mysqlDb);  // Inject dependency\n\n// Easy to switch:\nconst postgresDb = new PostgreSQLDatabase();\nconst userService2 = new UserService(postgresDb);\n\n// Dependency Injection Container\nclass Container {\n  constructor() {\n    this.services = new Map();\n  }\n  \n  register(name, factory) {\n    this.services.set(name, factory);\n  }\n  \n  resolve(name) {\n    const factory = this.services.get(name);\n    return factory(this);\n  }\n}\n\nconst container = new Container();\n\n// Register services\ncontainer.register('database', () => new PostgreSQLDatabase());\ncontainer.register('userRepository', (c) => new UserRepository(c.resolve('database')));\ncontainer.register('userService', (c) => new UserService(c.resolve('userRepository')));\n\n// Resolve\nconst userService = container.resolve('userService');\n\n// System Design: Microservices with DIP\n// Services depend on contracts (APIs), not implementations\n\ninterface PaymentGateway {\n  charge(amount: number): Promise<PaymentResult>;\n}\n\nclass OrderService {\n  constructor(private paymentGateway: PaymentGateway) {}\n  \n  async createOrder(items) {\n    const total = this.calculateTotal(items);\n    await this.paymentGateway.charge(total);  // Abstraction\n    // ...\n  }\n}\n\n// Can inject different implementations\nconst stripeGateway = new StripePaymentGateway();\nconst orderService = new OrderService(stripeGateway);"
    },
    {
      "id": 31,
      "question": "What is Event Sourcing and how does it differ from traditional CRUD?",
      "answer": "Event Sourcing stores all changes to application state as a sequence of immutable events, rather than storing just the current state in a database.\n\nCore Concepts:\n\n• Events: Immutable facts about what happened\n  - OrderCreated, PaymentProcessed, ItemShipped\n  - Never deleted or modified\n  - Append-only log\n\n• Event Store: Database of events\n  - Sequential storage\n  - Each event has timestamp, type, data\n  - Can rebuild state by replaying events\n\n• Projections: Read models built from events\n  - Materialize current state from events\n  - Multiple projections from same events\n  - Can be rebuilt anytime\n\n• Commands: Intent to change state\n  - CreateOrder, ProcessPayment\n  - Validated before generating events\n\nEvent Sourcing vs CRUD:\n\nCRUD:\n• Store current state only\n• Update/Delete overwrites data\n• Lost history of changes\n• Example: UPDATE users SET name = 'John'\n\nEvent Sourcing:\n• Store all state changes as events\n• Never update/delete, only append\n• Complete audit trail\n• Example: UserNameChanged event stored\n\nBenefits:\n\n• Complete audit trail: Every change recorded\n• Time travel: Replay events to any point\n• Event replay: Rebuild state from scratch\n• Multiple models: Different projections from same events\n• Debugging: See exact sequence of events\n• Analytics: Process historical events\n• Microservices: Events as integration points\n\nChallenges:\n\n• Complexity: More complex than CRUD\n• Learning curve: Different mental model\n• Storage: Events grow infinitely\n• Performance: Rebuilding state can be slow\n• Eventual consistency: Projections may lag\n• Schema evolution: Evolving event formats\n• Querying: Need projections for queries\n\nCQRS (Command Query Responsibility Segregation):\n• Often paired with Event Sourcing\n• Separate write model (events) from read model (projections)\n• Optimize each independently\n• Write: Validate commands, append events\n• Read: Query optimized projections\n\nWhen to Use Event Sourcing:\n\n• Need complete audit trail\n• Complex business logic\n• Temporal queries (state at specific time)\n• Event-driven architecture\n• Multiple read models needed\n• Financial systems, healthcare\n\nWhen NOT to Use:\n\n• Simple CRUD applications\n• Team lacks experience\n• Real-time queries required\n• Storage constraints\n\nPopular Tools:\n\n• Event Store DB: Purpose-built event store\n• Apache Kafka: Distributed event log\n• PostgreSQL: Can use as event store\n• AWS DynamoDB Streams: Event streaming\n• Azure Cosmos DB: Change feed",
      "explanation": "Event Sourcing persists all state changes as immutable events in append-only storage, enabling complete audit trails and time travel, while CRUD stores only current state with destructive updates losing historical context.",
      "difficulty": "Hard",
      "code": "// Event Sourcing Implementation\n\n// Event Types\nclass Event {\n  constructor(aggregateId, type, data, version = 1) {\n    this.id = generateUUID();\n    this.aggregateId = aggregateId;\n    this.type = type;\n    this.data = data;\n    this.version = version;\n    this.timestamp = new Date();\n    this.metadata = {\n      userId: currentUser?.id,\n      correlationId: requestContext.correlationId\n    };\n  }\n}\n\n// Domain Events\nclass OrderCreatedEvent extends Event {\n  constructor(orderId, customerId, items, total) {\n    super(orderId, 'OrderCreated', {\n      customerId,\n      items,\n      total\n    });\n  }\n}\n\nclass OrderPaidEvent extends Event {\n  constructor(orderId, paymentId, amount) {\n    super(orderId, 'OrderPaid', {\n      paymentId,\n      amount\n    });\n  }\n}\n\nclass OrderShippedEvent extends Event {\n  constructor(orderId, trackingNumber) {\n    super(orderId, 'OrderShipped', {\n      trackingNumber\n    });\n  }\n}\n\nclass OrderCancelledEvent extends Event {\n  constructor(orderId, reason) {\n    super(orderId, 'OrderCancelled', {\n      reason\n    });\n  }\n}\n\n// Event Store\nclass EventStore {\n  constructor(db) {\n    this.db = db;\n  }\n  \n  async appendEvent(event) {\n    await this.db.query(\n      `INSERT INTO events (\n        id, aggregate_id, type, data, version, timestamp, metadata\n      ) VALUES ($1, $2, $3, $4, $5, $6, $7)`,\n      [\n        event.id,\n        event.aggregateId,\n        event.type,\n        JSON.stringify(event.data),\n        event.version,\n        event.timestamp,\n        JSON.stringify(event.metadata)\n      ]\n    );\n  }\n  \n  async getEvents(aggregateId, fromVersion = 0) {\n    const result = await this.db.query(\n      `SELECT * FROM events \n       WHERE aggregate_id = $1 AND version > $2 \n       ORDER BY version ASC`,\n      [aggregateId, fromVersion]\n    );\n    \n    return result.rows.map(row => ({\n      id: row.id,\n      aggregateId: row.aggregate_id,\n      type: row.type,\n      data: JSON.parse(row.data),\n      version: row.version,\n      timestamp: row.timestamp,\n      metadata: JSON.parse(row.metadata)\n    }));\n  }\n  \n  async getEventsByType(type, limit = 100) {\n    const result = await this.db.query(\n      `SELECT * FROM events \n       WHERE type = $1 \n       ORDER BY timestamp DESC \n       LIMIT $2`,\n      [type, limit]\n    );\n    \n    return result.rows;\n  }\n  \n  async getEventsAfter(timestamp) {\n    const result = await this.db.query(\n      `SELECT * FROM events \n       WHERE timestamp > $1 \n       ORDER BY timestamp ASC`,\n      [timestamp]\n    );\n    \n    return result.rows;\n  }\n}\n\n// Aggregate Root\nclass OrderAggregate {\n  constructor(orderId) {\n    this.id = orderId;\n    this.version = 0;\n    this.status = 'pending';\n    this.customerId = null;\n    this.items = [];\n    this.total = 0;\n    this.paymentId = null;\n    this.trackingNumber = null;\n    this.uncommittedEvents = [];\n  }\n  \n  // Apply event to update state\n  apply(event) {\n    switch (event.type) {\n      case 'OrderCreated':\n        this.customerId = event.data.customerId;\n        this.items = event.data.items;\n        this.total = event.data.total;\n        this.status = 'created';\n        break;\n      \n      case 'OrderPaid':\n        this.paymentId = event.data.paymentId;\n        this.status = 'paid';\n        break;\n      \n      case 'OrderShipped':\n        this.trackingNumber = event.data.trackingNumber;\n        this.status = 'shipped';\n        break;\n      \n      case 'OrderCancelled':\n        this.status = 'cancelled';\n        break;\n    }\n    \n    this.version = event.version;\n  }\n  \n  // Commands (intent to change state)\n  createOrder(customerId, items) {\n    if (this.version > 0) {\n      throw new Error('Order already exists');\n    }\n    \n    const total = items.reduce((sum, item) => sum + item.price * item.quantity, 0);\n    \n    const event = new OrderCreatedEvent(\n      this.id,\n      customerId,\n      items,\n      total\n    );\n    \n    this.uncommittedEvents.push(event);\n    this.apply(event);\n  }\n  \n  markAsPaid(paymentId, amount) {\n    if (this.status !== 'created') {\n      throw new Error('Order must be in created status');\n    }\n    \n    if (amount !== this.total) {\n      throw new Error('Payment amount mismatch');\n    }\n    \n    const event = new OrderPaidEvent(\n      this.id,\n      paymentId,\n      amount\n    );\n    event.version = this.version + 1;\n    \n    this.uncommittedEvents.push(event);\n    this.apply(event);\n  }\n  \n  ship(trackingNumber) {\n    if (this.status !== 'paid') {\n      throw new Error('Order must be paid before shipping');\n    }\n    \n    const event = new OrderShippedEvent(\n      this.id,\n      trackingNumber\n    );\n    event.version = this.version + 1;\n    \n    this.uncommittedEvents.push(event);\n    this.apply(event);\n  }\n  \n  cancel(reason) {\n    if (this.status === 'shipped') {\n      throw new Error('Cannot cancel shipped order');\n    }\n    \n    const event = new OrderCancelledEvent(\n      this.id,\n      reason\n    );\n    event.version = this.version + 1;\n    \n    this.uncommittedEvents.push(event);\n    this.apply(event);\n  }\n  \n  getUncommittedEvents() {\n    return this.uncommittedEvents;\n  }\n  \n  markEventsAsCommitted() {\n    this.uncommittedEvents = [];\n  }\n  \n  // Load aggregate from events\n  static async load(orderId, eventStore) {\n    const aggregate = new OrderAggregate(orderId);\n    const events = await eventStore.getEvents(orderId);\n    \n    events.forEach(event => aggregate.apply(event));\n    \n    return aggregate;\n  }\n}\n\n// Repository\nclass OrderRepository {\n  constructor(eventStore) {\n    this.eventStore = eventStore;\n  }\n  \n  async save(aggregate) {\n    const events = aggregate.getUncommittedEvents();\n    \n    for (const event of events) {\n      await this.eventStore.appendEvent(event);\n    }\n    \n    aggregate.markEventsAsCommitted();\n  }\n  \n  async getById(orderId) {\n    return await OrderAggregate.load(orderId, this.eventStore);\n  }\n}\n\n// Usage (Command Handler)\nclass OrderCommandHandler {\n  constructor(repository) {\n    this.repository = repository;\n  }\n  \n  async createOrder(orderId, customerId, items) {\n    const order = new OrderAggregate(orderId);\n    order.createOrder(customerId, items);\n    await this.repository.save(order);\n    return order;\n  }\n  \n  async processPayment(orderId, paymentId, amount) {\n    const order = await this.repository.getById(orderId);\n    order.markAsPaid(paymentId, amount);\n    await this.repository.save(order);\n    return order;\n  }\n  \n  async shipOrder(orderId, trackingNumber) {\n    const order = await this.repository.getById(orderId);\n    order.ship(trackingNumber);\n    await this.repository.save(order);\n    return order;\n  }\n}\n\n// Projections (Read Models)\nclass OrderProjection {\n  constructor(db) {\n    this.db = db;\n  }\n  \n  async project(event) {\n    switch (event.type) {\n      case 'OrderCreated':\n        await this.db.query(\n          `INSERT INTO orders_view (\n            id, customer_id, items, total, status, created_at\n          ) VALUES ($1, $2, $3, $4, $5, $6)`,\n          [\n            event.aggregateId,\n            event.data.customerId,\n            JSON.stringify(event.data.items),\n            event.data.total,\n            'created',\n            event.timestamp\n          ]\n        );\n        break;\n      \n      case 'OrderPaid':\n        await this.db.query(\n          `UPDATE orders_view \n           SET status = 'paid', payment_id = $1, updated_at = $2\n           WHERE id = $3`,\n          [event.data.paymentId, event.timestamp, event.aggregateId]\n        );\n        break;\n      \n      case 'OrderShipped':\n        await this.db.query(\n          `UPDATE orders_view \n           SET status = 'shipped', tracking_number = $1, updated_at = $2 \n           WHERE id = $3`,\n          [event.data.trackingNumber, event.timestamp, event.aggregateId]\n        );\n        break;\n      \n      case 'OrderCancelled':\n        await this.db.query(\n          `UPDATE orders_view \n           SET status = 'cancelled', cancelled_reason = $1, updated_at = $2 \n           WHERE id = $3`,\n          [event.data.reason, event.timestamp, event.aggregateId]\n        );\n        break;\n    }\n  }\n  \n  // Query methods\n  async getOrder(orderId) {\n    const result = await this.db.query(\n      'SELECT * FROM orders_view WHERE id = $1',\n      [orderId]\n    );\n    return result.rows[0];\n  }\n  \n  async getOrdersByCustomer(customerId) {\n    const result = await this.db.query(\n      'SELECT * FROM orders_view WHERE customer_id = $1 ORDER BY created_at DESC',\n      [customerId]\n    );\n    return result.rows;\n  }\n}\n\n// Event Processor (Subscribe to events)\nclass EventProcessor {\n  constructor(eventStore, projections) {\n    this.eventStore = eventStore;\n    this.projections = projections;\n    this.lastProcessedTimestamp = new Date(0);\n  }\n  \n  async start() {\n    setInterval(async () => {\n      await this.processNewEvents();\n    }, 1000);  // Poll every second\n  }\n  \n  async processNewEvents() {\n    const events = await this.eventStore.getEventsAfter(\n      this.lastProcessedTimestamp\n    );\n    \n    for (const event of events) {\n      for (const projection of this.projections) {\n        await projection.project(event);\n      }\n      \n      this.lastProcessedTimestamp = event.timestamp;\n    }\n  }\n}\n\n// Snapshots (Performance Optimization)\nclass SnapshotStore {\n  constructor(db) {\n    this.db = db;\n  }\n  \n  async saveSnapshot(aggregateId, version, state) {\n    await this.db.query(\n      `INSERT INTO snapshots (aggregate_id, version, state, created_at)\n       VALUES ($1, $2, $3, NOW())\n       ON CONFLICT (aggregate_id) \n       DO UPDATE SET version = $2, state = $3, created_at = NOW()`,\n      [aggregateId, version, JSON.stringify(state)]\n    );\n  }\n  \n  async getSnapshot(aggregateId) {\n    const result = await this.db.query(\n      'SELECT * FROM snapshots WHERE aggregate_id = $1',\n      [aggregateId]\n    );\n    \n    if (result.rows.length === 0) {\n      return null;\n    }\n    \n    return {\n      version: result.rows[0].version,\n      state: JSON.parse(result.rows[0].state)\n    };\n  }\n}\n\n// Load with snapshot\nstatic async loadWithSnapshot(orderId, eventStore, snapshotStore) {\n  const snapshot = await snapshotStore.getSnapshot(orderId);\n  const aggregate = new OrderAggregate(orderId);\n  \n  if (snapshot) {\n    // Restore from snapshot\n    Object.assign(aggregate, snapshot.state);\n    aggregate.version = snapshot.version;\n    \n    // Apply events after snapshot\n    const events = await eventStore.getEvents(orderId, snapshot.version);\n    events.forEach(event => aggregate.apply(event));\n  } else {\n    // Load from beginning\n    const events = await eventStore.getEvents(orderId);\n    events.forEach(event => aggregate.apply(event));\n  }\n  \n  // Save snapshot every 100 events\n  if (aggregate.version % 100 === 0) {\n    await snapshotStore.saveSnapshot(orderId, aggregate.version, aggregate);\n  }\n  \n  return aggregate;\n}\n\n// Time Travel (Historical State)\nclass OrderHistoryService {\n  constructor(eventStore) {\n    this.eventStore = eventStore;\n  }\n  \n  async getOrderStateAt(orderId, timestamp) {\n    const events = await this.eventStore.getEvents(orderId);\n    const historicalEvents = events.filter(e => e.timestamp <= timestamp);\n    \n    const aggregate = new OrderAggregate(orderId);\n    historicalEvents.forEach(event => aggregate.apply(event));\n    \n    return aggregate;\n  }\n}\n\n// Complete Express API\nconst express = require('express');\nconst app = express();\n\nconst eventStore = new EventStore(db);\nconst repository = new OrderRepository(eventStore);\nconst commandHandler = new OrderCommandHandler(repository);\nconst projection = new OrderProjection(db);\n\n// Start projection processor\nconst processor = new EventProcessor(eventStore, [projection]);\nprocessor.start();\n\n// Commands (Write)\napp.post('/api/orders', async (req, res) => {\n  const { customerId, items } = req.body;\n  const orderId = generateUUID();\n  \n  const order = await commandHandler.createOrder(orderId, customerId, items);\n  res.json({ orderId: order.id });\n});\n\napp.post('/api/orders/:id/pay', async (req, res) => {\n  const { paymentId, amount } = req.body;\n  \n  await commandHandler.processPayment(req.params.id, paymentId, amount);\n  res.json({ success: true });\n});\n\n// Queries (Read from projection)\napp.get('/api/orders/:id', async (req, res) => {\n  const order = await projection.getOrder(req.params.id);\n  res.json(order);\n});\n\napp.get('/api/customers/:id/orders', async (req, res) => {\n  const orders = await projection.getOrdersByCustomer(req.params.id);\n  res.json(orders);\n});\n\n// Time travel query\napp.get('/api/orders/:id/history', async (req, res) => {\n  const { timestamp } = req.query;\n  const historyService = new OrderHistoryService(eventStore);\n  \n  const historicalState = await historyService.getOrderStateAt(\n    req.params.id,\n    new Date(timestamp)\n  );\n  \n  res.json(historicalState);\n});\n\napp.listen(3000);"
    },
    {
      "id": 32,
      "question": "What is the Strangler Fig pattern for migrating legacy systems?",
      "answer": "The Strangler Fig pattern is a gradual migration strategy that incrementally replaces a legacy system by routing new functionality to a new implementation while keeping the old system running.\n\nNamed after strangler fig trees that grow around host trees, eventually replacing them entirely.\n\nCore Strategy:\n\n1. Facade Layer:\n• Create routing layer in front of legacy system\n• Routes requests to legacy or new system\n• Transparent to clients\n\n2. Incremental Migration:\n• Migrate one feature/module at a time\n• New features go to new system\n• Legacy features stay in old system\n• Gradually shift traffic\n\n3. Parallel Operation:\n• Both systems run simultaneously\n• No \"big bang\" cutover\n• Reduced risk\n• Easy rollback\n\n4. Decommission:\n• When all functionality migrated\n• Remove legacy system\n• Remove facade when 100% migrated\n\nMigration Approaches:\n\n• By Feature: Migrate one feature at a time\n• By Route: Migrate specific URLs/endpoints\n• By User Segment: Beta users to new, others to old\n• By Data: Partition data, migrate incrementally\n\nBenefits:\n\n• Low risk: Incremental changes\n• Continuous delivery: Ship during migration\n• Easy rollback: Keep legacy as fallback\n• Team learning: Gradual skill building\n• Prove value: Show ROI incrementally\n• No downtime: Zero-downtime migration\n\nChallenges:\n\n• Data synchronization: Keep data consistent\n• Increased complexity: Two systems to maintain\n• Transactional boundaries: Cross-system transactions\n• Testing overhead: Test both systems\n• Performance: Additional routing layer\n• Duration: Can take months/years\n\nImplementation Patterns:\n\n• Proxy/Gateway: Route based on path/header\n• Feature Toggles: Switch features programmatically\n• Dual Writes: Write to both systems during migration\n• Event Streaming: CDC (Change Data Capture) for sync\n• Read from New, Write to Both: Gradual data migration\n\nWhen to Use:\n\n• Large legacy system\n• Cannot afford downtime\n• High-risk migration\n• Team needs gradual learning\n• Need to prove business value incrementally\n\nWhen NOT to Use:\n\n• Small, simple system\n• Can afford big bang rewrite\n• Legacy system being decommissioned soon\n• No resources for parallel systems",
      "explanation": "Strangler Fig pattern enables gradual legacy system migration by routing requests through a facade that incrementally directs traffic to new implementation while keeping legacy running, reducing risk through parallel operation and iterative replacement.",
      "difficulty": "Medium",
      "code": "// STRANGLER FIG PATTERN IMPLEMENTATION\n\n// 1. API Gateway/Facade Layer\nconst express = require('express');\nconst axios = require('axios');\n\nconst app = express();\napp.use(express.json());\n\n// Configuration: What's migrated\nconst migrationConfig = {\n  features: {\n    '/api/users': { migrated: true, rolloutPercent: 100 },\n    '/api/products': { migrated: true, rolloutPercent: 50 },  // Canary\n    '/api/orders': { migrated: false },\n    '/api/payments': { migrated: false }\n  },\n  userSegments: {\n    beta: true,   // Beta users use new system\n    internal: true\n  }\n};\n\nconst LEGACY_URL = 'http://legacy-system:8080';\nconst NEW_URL = 'http://new-system:3000';\n\n// Routing logic\nfunction shouldUseNewSystem(path, user) {\n  const featureConfig = migrationConfig.features[path];\n  \n  if (!featureConfig) {\n    return false;  // Unknown route -> legacy\n  }\n  \n  if (!featureConfig.migrated) {\n    return false;  // Not migrated yet\n  }\n  \n  // Check user segment\n  if (user?.segment && migrationConfig.userSegments[user.segment]) {\n    return true;  // Beta/internal users\n  }\n  \n  // Gradual rollout (percentage-based)\n  if (featureConfig.rolloutPercent < 100) {\n    const hash = hashUserId(user?.id || getClientIp());\n    return (hash % 100) < featureConfig.rolloutPercent;\n  }\n  \n  return true;  // 100% rollout\n}\n\n// Proxy middleware\napp.use(async (req, res, next) => {\n  const user = req.user;  // From auth middleware\n  const basePath = extractBasePath(req.path);\n  \n  const useNew = shouldUseNewSystem(basePath, user);\n  const targetUrl = useNew ? NEW_URL : LEGACY_URL;\n  \n  console.log({\n    path: req.path,\n    method: req.method,\n    target: useNew ? 'NEW' : 'LEGACY',\n    user: user?.id\n  });\n  \n  try {\n    const response = await axios({\n      method: req.method,\n      url: `${targetUrl}${req.path}`,\n      data: req.body,\n      params: req.query,\n      headers: {\n        ...req.headers,\n        'X-Forwarded-For': req.ip,\n        'X-Routed-To': useNew ? 'new' : 'legacy'\n      },\n      timeout: 30000\n    });\n    \n    // Track metrics\n    trackRequest({\n      path: req.path,\n      target: useNew ? 'NEW' : 'LEGACY',\n      status: response.status,\n      duration: response.duration\n    });\n    \n    res.status(response.status).json(response.data);\n  } catch (error) {\n    console.error('Proxy error:', error);\n    \n    // Fallback to legacy on new system error\n    if (useNew && error.response?.status >= 500) {\n      console.log('Falling back to legacy');\n      return res.redirect(307, `${LEGACY_URL}${req.path}`);\n    }\n    \n    res.status(error.response?.status || 500).json({\n      error: error.message\n    });\n  }\n});\n\napp.listen(8080);\n\n// 2. Feature Toggle Approach\nclass FeatureToggler {\n  constructor() {\n    this.flags = new Map();\n    this.loadFlags();\n  }\n  \n  loadFlags() {\n    // Load from config/database/feature flag service\n    this.flags.set('new-user-service', {\n      enabled: true,\n      rolloutPercent: 100\n    });\n    \n    this.flags.set('new-product-service', {\n      enabled: true,\n      rolloutPercent: 25  // 25% rollout\n    });\n    \n    this.flags.set('new-order-service', {\n      enabled: false  // Not migrated yet\n    });\n  }\n  \n  isEnabled(flagName, context = {}) {\n    const flag = this.flags.get(flagName);\n    \n    if (!flag || !flag.enabled) {\n      return false;\n    }\n    \n    // Percentage-based rollout\n    if (flag.rolloutPercent < 100) {\n      const hash = this.hashContext(context);\n      return (hash % 100) < flag.rolloutPercent;\n    }\n    \n    return true;\n  }\n  \n  hashContext(context) {\n    const key = context.userId || context.sessionId || Math.random();\n    return crc32(key.toString());\n  }\n}\n\nconst toggler = new FeatureToggler();\n\n// Usage in application code\nclass UserService {\n  async getUser(userId) {\n    if (toggler.isEnabled('new-user-service', { userId })) {\n      return await this.newUserService.getUser(userId);\n    } else {\n      return await this.legacyUserService.getUser(userId);\n    }\n  }\n  \n  async createUser(userData) {\n    const user = await this.newUserService.createUser(userData);\n    \n    // Dual write during migration\n    if (!toggler.isEnabled('new-user-service')) {\n      await this.legacyUserService.createUser(userData);\n    }\n    \n    return user;\n  }\n}\n\n// 3. Data Synchronization Patterns\n\n// Pattern A: Dual Write (Write to both systems)\nclass DualWriteOrderService {\n  constructor(legacyRepo, newRepo) {\n    this.legacy = legacyRepo;\n    this.new = newRepo;\n  }\n  \n  async createOrder(orderData) {\n    let legacyOrder, newOrder;\n    \n    try {\n      // Write to new system (primary)\n      newOrder = await this.new.create(orderData);\n      \n      // Write to legacy system (for consistency)\n      legacyOrder = await this.legacy.create({\n        ...orderData,\n        newSystemId: newOrder.id  // Link records\n      });\n      \n      return newOrder;\n    } catch (error) {\n      // Rollback if either fails\n      if (newOrder) await this.new.delete(newOrder.id);\n      if (legacyOrder) await this.legacy.delete(legacyOrder.id);\n      throw error;\n    }\n  }\n  \n  async getOrder(orderId) {\n    // Read from new (primary)\n    const order = await this.new.getById(orderId);\n    \n    if (!order) {\n      // Fallback to legacy\n      return await this.legacy.getById(orderId);\n    }\n    \n    return order;\n  }\n}\n\n// Pattern B: Event-Based Synchronization\nconst { Kafka } = require('kafkajs');\n\nconst kafka = new Kafka({ brokers: ['localhost:9092'] });\nconst producer = kafka.producer();\n\n// Legacy system publishes events\nclass LegacySystemBridge {\n  async onOrderCreated(order) {\n    // Publish to Kafka\n    await producer.send({\n      topic: 'legacy-orders',\n      messages: [{\n        key: order.id,\n        value: JSON.stringify({\n          type: 'OrderCreated',\n          data: order\n        })\n      }]\n    });\n  }\n}\n\n// New system consumes events\nconst consumer = kafka.consumer({ groupId: 'new-system' });\n\nawait consumer.subscribe({ topic: 'legacy-orders' });\n\nawait consumer.run({\n  eachMessage: async ({ message }) => {\n    const event = JSON.parse(message.value.toString());\n    \n    if (event.type === 'OrderCreated') {\n      // Sync to new system\n      await newOrderService.syncFromLegacy(event.data);\n    }\n  }\n});\n\n// Pattern C: Change Data Capture (CDC)\n// Using Debezium to capture database changes\n\n// Debezium connector configuration\nconst debeziumConfig = {\n  'name': 'legacy-db-connector',\n  'config': {\n    'connector.class': 'io.debezium.connector.postgresql.PostgresConnector',\n    'database.hostname': 'legacy-db',\n    'database.port': '5432',\n    'database.user': 'debezium',\n    'database.password': 'dbz',\n    'database.dbname': 'legacy',\n    'database.server.name': 'legacy',\n    'table.include.list': 'public.orders,public.users',\n    'plugin.name': 'pgoutput'\n  }\n};\n\n// Consume CDC events\nclass CDCConsumer {\n  async processCDCEvent(event) {\n    const { op, after, before } = event.payload;\n    \n    switch (op) {\n      case 'c':  // Create\n        await this.syncCreate(after);\n        break;\n      case 'u':  // Update\n        await this.syncUpdate(after, before);\n        break;\n      case 'd':  // Delete\n        await this.syncDelete(before);\n        break;\n    }\n  }\n  \n  async syncCreate(record) {\n    await newSystemDB.query(\n      'INSERT INTO orders (id, legacy_id, ...) VALUES (...)',\n      [record.id, ...]\n    );\n  }\n}\n\n// 4. Gradual Data Migration\nclass DataMigrator {\n  async migrateUsersInBackground() {\n    let offset = 0;\n    const batchSize = 1000;\n    \n    while (true) {\n      // Get batch from legacy\n      const users = await legacyDB.query(\n        'SELECT * FROM users WHERE migrated = false LIMIT $1 OFFSET $2',\n        [batchSize, offset]\n      );\n      \n      if (users.length === 0) break;\n      \n      // Migrate batch\n      for (const user of users) {\n        try {\n          await this.migrateUser(user);\n          \n          // Mark as migrated\n          await legacyDB.query(\n            'UPDATE users SET migrated = true WHERE id = $1',\n            [user.id]\n          );\n        } catch (error) {\n          console.error(`Failed to migrate user ${user.id}:`, error);\n        }\n      }\n      \n      offset += batchSize;\n      \n      // Rate limiting\n      await sleep(1000);\n    }\n  }\n  \n  async migrateUser(legacyUser) {\n    const newUser = this.transformUser(legacyUser);\n    \n    await newSystemDB.query(\n      'INSERT INTO users (...) VALUES (...) ON CONFLICT (legacy_id) DO UPDATE ...',\n      [...]\n    );\n  }\n  \n  transformUser(legacyUser) {\n    return {\n      legacyId: legacyUser.id,\n      email: legacyUser.email_address,  // Field mapping\n      name: `${legacyUser.first_name} ${legacyUser.last_name}`,\n      // Transform data format\n      createdAt: new Date(legacyUser.created_timestamp * 1000)\n    };\n  }\n}\n\n// 5. Monitoring and Metrics\nconst prometheus = require('prom-client');\n\nconst routingCounter = new prometheus.Counter({\n  name: 'strangler_routing_total',\n  help: 'Total requests routed',\n  labelNames: ['target', 'path', 'method']\n});\n\nconst routingDuration = new prometheus.Histogram({\n  name: 'strangler_request_duration_seconds',\n  help: 'Request duration',\n  labelNames: ['target', 'path'],\n  buckets: [0.1, 0.5, 1, 2, 5]\n});\n\nconst fallbackCounter = new prometheus.Counter({\n  name: 'strangler_fallback_total',\n  help: 'Fallback to legacy count',\n  labelNames: ['reason']\n});\n\nfunction trackRequest(data) {\n  routingCounter.inc({\n    target: data.target,\n    path: data.path,\n    method: data.method\n  });\n  \n  routingDuration.observe(\n    { target: data.target, path: data.path },\n    data.duration / 1000\n  );\n}\n\nfunction trackFallback(reason) {\n  fallbackCounter.inc({ reason });\n}\n\n// Grafana Dashboard Query:\n// Percentage of traffic to new system\nsum(rate(strangler_routing_total{target=\"NEW\"}[5m])) /\nsum(rate(strangler_routing_total[5m])) * 100\n\n// Compare error rates\nsum(rate(strangler_routing_total{target=\"NEW\",status=~\"5..\"}[5m])) /\nsum(rate(strangler_routing_total{target=\"NEW\"}[5m]))"
    },
    {
      "id": 33,
      "question": "What is database replication and what are the different replication strategies?",
      "answer": "Database replication is the process of copying and maintaining database data across multiple servers to improve availability, scalability, and performance.\n\nCore Concepts:\n\n• Primary (Master): Source database for write operations\n• Replica (Slave): Copy of database for read operations\n• Replication Lag: Time delay between primary and replica\n• Binlog: Binary log of changes on primary\n\nReplication Types:\n\n1. Master-Slave (Primary-Replica):\n• One primary handles writes\n• Multiple replicas handle reads\n• Asynchronous or synchronous replication\n• Most common pattern\n• Benefits: Read scaling, backup\n• Cons: Single write point, potential lag\n\n2. Master-Master (Multi-Master):\n• Multiple primaries accept writes\n• Bidirectional replication\n• Conflict resolution needed\n• Benefits: High availability, write scaling\n• Cons: Complexity, conflicts\n\n3. Cascading Replication:\n• Replicas replicate from other replicas\n• Reduces load on primary\n• Daisy-chain topology\n• Benefits: Scalability\n• Cons: Increased lag\n\n4. Group Replication:\n• Consensus-based replication\n• Multiple primaries with automatic failover\n• Strong consistency\n• Example: MySQL Group Replication\n\nReplication Methods:\n\n• Statement-Based: Replicate SQL statements\n• Row-Based: Replicate actual data changes\n• Mixed: Combination of both\n\nSynchronous vs Asynchronous:\n\nSynchronous:\n• Wait for replica acknowledgment\n• Strong consistency\n• Higher latency\n• Use for: Critical transactions\n\nAsynchronous:\n• Don't wait for replica\n• Lower latency\n• Potential data loss\n• Eventual consistency\n• Use for: Read scaling\n\nSemi-Synchronous:\n• Wait for at least one replica\n• Balance between sync and async\n• Good compromise\n\nUse Cases:\n\n• Read Scaling: Distribute read queries\n• High Availability: Failover to replica\n• Backup: Use replica for backups\n• Analytics: Separate OLAP from OLTP\n• Geographic Distribution: Replicas near users\n• Disaster Recovery: Cross-region replicas\n\nChallenges:\n\n• Replication Lag: Reads may be stale\n• Failover Complexity: Promote replica to primary\n• Data Conflicts: Multi-master conflicts\n• Network Bandwidth: Large data transfer\n• Monitoring: Track lag and health\n\nFailover Strategies:\n\n• Automatic: Tools detect failure and promote\n• Manual: DBA promotes replica\n• Virtual IP: DNS/Load balancer switches\n\nPopular Solutions:\n\n• MySQL Replication: Built-in master-slave\n• PostgreSQL Streaming: Log shipping\n• MongoDB Replica Sets: Auto-failover\n• AWS RDS: Managed multi-AZ\n• Percona XtraDB Cluster: Multi-master",
      "explanation": "Database replication copies data across multiple servers for scalability and availability, using strategies like master-slave (read scaling), master-master (write scaling), with synchronous or asynchronous methods balancing consistency and performance.",
      "difficulty": "Medium",
      "code": "// MYSQL REPLICATION SETUP\n\n// 1. Primary (Master) Configuration\n// my.cnf on primary server\n/*\n[mysqld]\nserver-id = 1\nlog-bin = /var/log/mysql/mysql-bin.log\nbinlog_format = ROW\nbinlog-do-db = myapp\nexpire_logs_days = 7\nmax_binlog_size = 100M\n*/\n\n// Create replication user\nCREATE USER 'replicator'@'%' IDENTIFIED BY 'strong_password';\nGRANT REPLICATION SLAVE ON *.* TO 'replicator'@'%';\nFLUSH PRIVILEGES;\n\n// Get binary log position\nSHOW MASTER STATUS;\n/*\n+------------------+----------+--------------+------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |\n+------------------+----------+--------------+------------------+\n| mysql-bin.000001 |      154 | myapp        |                  |\n+------------------+----------+--------------+------------------+\n*/\n\n// 2. Replica (Slave) Configuration\n// my.cnf on replica server\n/*\n[mysqld]\nserver-id = 2\nrelay-log = /var/log/mysql/mysql-relay-bin\nlog-bin = /var/log/mysql/mysql-bin.log\nread_only = 1\n*/\n\n// Configure replication\nCHANGE MASTER TO\n  MASTER_HOST='primary-db.example.com',\n  MASTER_USER='replicator',\n  MASTER_PASSWORD='strong_password',\n  MASTER_LOG_FILE='mysql-bin.000001',\n  MASTER_LOG_POS=154;\n\n// Start replication\nSTART SLAVE;\n\n// Check replication status\nSHOW SLAVE STATUS\\G\n/*\nSlave_IO_State: Waiting for master to send event\nSlave_IO_Running: Yes\nSlave_SQL_Running: Yes\nSeconds_Behind_Master: 0\n*/\n\n// 3. Application Code with Read/Write Splitting\nconst { Pool } = require('pg');\n\n// Primary pool (writes)\nconst primaryPool = new Pool({\n  host: 'primary-db.example.com',\n  database: 'myapp',\n  max: 20,\n  idleTimeoutMillis: 30000\n});\n\n// Replica pools (reads)\nconst replicaPools = [\n  new Pool({\n    host: 'replica1-db.example.com',\n    database: 'myapp',\n    max: 30\n  }),\n  new Pool({\n    host: 'replica2-db.example.com',\n    database: 'myapp',\n    max: 30\n  }),\n  new Pool({\n    host: 'replica3-db.example.com',\n    database: 'myapp',\n    max: 30\n  })\n];\n\nlet replicaIndex = 0;\n\nclass DatabaseService {\n  // Write to primary\n  async write(query, params) {\n    return await primaryPool.query(query, params);\n  }\n  \n  // Read from replica (round-robin)\n  async read(query, params) {\n    const replica = replicaPools[replicaIndex];\n    replicaIndex = (replicaIndex + 1) % replicaPools.length;\n    \n    try {\n      return await replica.query(query, params);\n    } catch (error) {\n      // Fallback to primary on replica failure\n      console.error('Replica query failed, falling back to primary');\n      return await primaryPool.query(query, params);\n    }\n  }\n  \n  // Read from primary (when consistency critical)\n  async readFromPrimary(query, params) {\n    return await primaryPool.query(query, params);\n  }\n}\n\nconst db = new DatabaseService();\n\n// Usage\nclass UserRepository {\n  async createUser(userData) {\n    // Write to primary\n    const result = await db.write(\n      'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING *',\n      [userData.name, userData.email]\n    );\n    return result.rows[0];\n  }\n  \n  async getUser(userId) {\n    // Read from replica (eventual consistency OK)\n    const result = await db.read(\n      'SELECT * FROM users WHERE id = $1',\n      [userId]\n    );\n    return result.rows[0];\n  }\n  \n  async getUserForAuth(email) {\n    // Read from primary (need latest password hash)\n    const result = await db.readFromPrimary(\n      'SELECT * FROM users WHERE email = $1',\n      [email]\n    );\n    return result.rows[0];\n  }\n  \n  async updateUser(userId, updates) {\n    // Write to primary\n    await db.write(\n      'UPDATE users SET name = $1, email = $2 WHERE id = $3',\n      [updates.name, updates.email, userId]\n    );\n    \n    // Read-after-write from primary\n    return await db.readFromPrimary(\n      'SELECT * FROM users WHERE id = $1',\n      [userId]\n    );\n  }\n}\n\n// ProxySQL for Automatic Read/Write Split\n/* ProxySQL Configuration\n\nINSERT INTO mysql_servers(hostgroup_id, hostname, port) VALUES\n(0, 'primary-db.example.com', 3306),     -- Write group\n(1, 'replica1-db.example.com', 3306),    -- Read group\n(1, 'replica2-db.example.com', 3306),    -- Read group\n(1, 'replica3-db.example.com', 3306);    -- Read group\n\n-- Routing rules\nINSERT INTO mysql_query_rules(rule_id, match_pattern, destination_hostgroup)\nVALUES\n(1, '^SELECT.*FOR UPDATE', 0),   -- SELECT FOR UPDATE to primary\n(2, '^SELECT', 1),                -- SELECT to replicas\n(3, '.*', 0);                     -- Everything else to primary\n\nLOAD MYSQL QUERY RULES TO RUNTIME;\nSAVE MYSQL QUERY RULES TO DISK;\n*/\n\n// PostgreSQL Streaming Replication\n/* postgresql.conf on primary\n\nwal_level = replica\nmax_wal_senders = 3\nwal_keep_size = 64MB\n*/\n\n/* pg_hba.conf on primary\nhost replication replicator replica-ip/32 md5\n*/\n\n// Create replication slot\nSELECT * FROM pg_create_physical_replication_slot('replica1_slot');\n\n/* recovery.conf on replica\nprimary_conninfo = 'host=primary-db port=5432 user=replicator password=xxx'\nprimary_slot_name = 'replica1_slot'\n*/\n\n// Check replication lag\nSELECT\n  client_addr,\n  state,\n  sent_lsn,\n  write_lsn,\n  flush_lsn,\n  replay_lsn,\n  sync_state,\n  pg_wal_lsn_diff(sent_lsn, replay_lsn) AS lag_bytes\nFROM pg_stat_replication;\n\n// Multi-Master Replication (Percona XtraDB Cluster)\n/* wsrep.cnf\n\n[mysqld]\nwsrep_provider=/usr/lib/libgalera_smm.so\nwsrep_cluster_address=\"gcomm://node1,node2,node3\"\nwsrep_cluster_name=\"my_cluster\"\nwsrep_node_address=\"this_node_ip\"\nwsrep_node_name=\"node1\"\nwsrep_sst_method=xtrabackup-v2\n\n# All nodes are primary\nwsrep_on=ON\n*/\n\n// Application handles conflicts\nclass MultiMasterService {\n  async createUser(userData) {\n    try {\n      await db.query(\n        'INSERT INTO users (id, name, email, created_at) VALUES ($1, $2, $3, $4)',\n        [generateUUID(), userData.name, userData.email, new Date()]\n      );\n    } catch (error) {\n      if (error.code === '23505') {  // Duplicate key\n        // Retry with different ID or handle conflict\n        console.error('Conflict detected:', error);\n        throw new Error('Duplicate entry');\n      }\n      throw error;\n    }\n  }\n}\n\n// MongoDB Replica Set\n/* mongod.conf\nreplication:\n  replSetName: \"myapp-rs\"\n*/\n\n// Initialize replica set\nrs.initiate({\n  _id: \"myapp-rs\",\n  members: [\n    { _id: 0, host: \"mongo1:27017\", priority: 2 },\n    { _id: 1, host: \"mongo2:27017\", priority: 1 },\n    { _id: 2, host: \"mongo3:27017\", priority: 1 }\n  ]\n});\n\n// Check replica set status\nrs.status();\n\n// MongoDB Connection with Replica Set (Node.js)\nconst { MongoClient } = require('mongodb');\n\nconst client = new MongoClient(\n  'mongodb://mongo1:27017,mongo2:27017,mongo3:27017/myapp?replicaSet=myapp-rs',\n  {\n    readPreference: 'secondaryPreferred',  // Read from secondaries\n    w: 'majority',  // Write concern\n    retryWrites: true\n  }\n);\n\n// Force read from primary for critical data\nconst users = await db.collection('users').find({}).readPref('primary').toArray();\n\n// Monitoring Replication Lag\nconst prometheus = require('prom-client');\n\nconst replicationLagGauge = new prometheus.Gauge({\n  name: 'mysql_replication_lag_seconds',\n  help: 'Replication lag in seconds',\n  labelNames: ['replica']\n});\n\nasync function monitorReplicationLag() {\n  for (let i = 0; i < replicaPools.length; i++) {\n    try {\n      const result = await replicaPools[i].query('SHOW SLAVE STATUS');\n      const lagSeconds = result[0].Seconds_Behind_Master || 0;\n      \n      replicationLagGauge.set({ replica: `replica${i + 1}` }, lagSeconds);\n      \n      if (lagSeconds > 60) {\n        console.error(`High replication lag on replica${i + 1}: ${lagSeconds}s`);\n        // Alert\n      }\n    } catch (error) {\n      console.error(`Failed to check replica${i + 1} lag:`, error);\n    }\n  }\n}\n\nsetInterval(monitorReplicationLag, 10000);  // Check every 10s\n\n// Failover Orchestration\nclass FailoverOrchestrator {\n  async detectPrimaryFailure() {\n    try {\n      await primaryPool.query('SELECT 1');\n      return false;  // Primary is healthy\n    } catch (error) {\n      console.error('Primary database is down');\n      return true;\n    }\n  }\n  \n  async promoteReplica() {\n    // Find most up-to-date replica\n    const replicas = await this.getReplicaLagInfo();\n    const best = replicas.sort((a, b) => a.lag - b.lag)[0];\n    \n    console.log(`Promoting replica ${best.host}`);\n    \n    // Stop replication\n    await best.pool.query('STOP SLAVE');\n    \n    // Make it writable\n    await best.pool.query('SET GLOBAL read_only = OFF');\n    \n    // Update application config\n    primaryPool = best.pool;\n    \n    console.log('Failover complete');\n  }\n}"
    },
    {
      "id": 34,
      "question": "What is a service discovery mechanism and why is it important in microservices?",
      "answer": "Service discovery enables services to find and communicate with each other dynamically in a microservices architecture, without hardcoding network locations.\n\nProblem it Solves:\n\n• Dynamic IPs: Services get new IPs on restart/scaling\n• Multiple Instances: Load balance across instances\n• Health Checking: Route only to healthy instances\n• Environment Changes: Different addresses per environment\n• Auto-Scaling: New instances register automatically\n\nService Discovery Patterns:\n\n1. Client-Side Discovery:\n• Client queries service registry\n• Client chooses instance (load balancing)\n• Client makes direct request\n• Examples: Netflix Eureka, Consul\n• Pros: Flexible load balancing, no proxy overhead\n• Cons: Client complexity, language-specific libraries\n\n2. Server-Side Discovery:\n• Client requests through load balancer\n• Load balancer queries registry\n• Load balancer routes request\n• Examples: AWS ELB, Kubernetes Service\n• Pros: Simple client, language-agnostic\n• Cons: Additional hop, load balancer maintenance\n\nService Registry Components:\n\n• Registration: Services register on startup\n• Deregistration: Remove on shutdown\n• Health Checks: Continuous health monitoring\n• Query API: Find service instances\n• Watch/Subscribe: Notify on changes\n\nRegistration Patterns:\n\n• Self-Registration: Service registers itself\n• Third-Party Registration: Sidecar/agent registers\n• Container Orchestrator: Kubernetes auto-registers\n\nPopular Solutions:\n\n• Consul (HashiCorp):\n  - Service registry with health checks\n  - Key-value store\n  - Multi-datacenter support\n  - DNS and HTTP interfaces\n\n• Eureka (Netflix):\n  - REST-based service registry\n  - Client-side load balancing\n  - Java/Spring ecosystem\n\n• etcd:\n  - Distributed key-value store\n  - Used by Kubernetes\n  - Strong consistency\n\n• Zookeeper:\n  - Coordination service\n  - Configuration management\n  - Mature but complex\n\n• Kubernetes Service:\n  - Built-in service discovery\n  - DNS-based\n  - Automatic endpoints\n\nDNS-Based Discovery:\n\n• Service name resolves to IP addresses\n• Round-robin DNS for load balancing\n• Simple, language-agnostic\n• Limited health checking\n• Example: Kubernetes DNS\n\nHealth Checking:\n\n• HTTP endpoint: GET /health returns 200\n• TCP check: Port is reachable\n• Script check: Custom health script\n• TTL check: Service must renew registration\n\nLoad Balancing Strategies:\n\n• Round Robin: Rotate through instances\n• Random: Pick random instance\n• Least Connections: Route to least busy\n• Weighted: Prefer certain instances\n• Zone-Aware: Prefer same availability zone\n\nBenefits:\n\n• Dynamic configuration: No hardcoded addresses\n• Resilience: Auto-remove failed instances\n• Scalability: Auto-discover new instances\n• Flexibility: Change topology without code changes\n• Multi-environment: Same code, different registries\n\nChallenges:\n\n• Consistency: Registry must be accurate\n• Availability: Registry is critical dependency\n• Latency: Additional lookup overhead\n• Complexity: Another system to manage",
      "explanation": "Service discovery enables dynamic location and communication between microservices by maintaining a registry of available instances with health checks, supporting patterns like client-side (direct query) or server-side (load balancer) discovery.",
      "difficulty": "Medium",
      "code": "// CONSUL SERVICE DISCOVERY\n\n// 1. Service Registration with Consul\nconst Consul = require('consul');\n\nconst consul = new Consul({\n  host: 'consul.service.consul',\n  port: 8500,\n  promisify: true\n});\n\nclass ServiceRegistration {\n  constructor(serviceName, port) {\n    this.serviceName = serviceName;\n    this.serviceId = `${serviceName}-${process.pid}`;\n    this.port = port;\n  }\n  \n  async register() {\n    const registration = {\n      name: this.serviceName,\n      id: this.serviceId,\n      address: getLocalIP(),\n      port: this.port,\n      tags: ['api', 'v1', process.env.NODE_ENV],\n      meta: {\n        version: '1.0.0',\n        region: process.env.REGION\n      },\n      check: {\n        http: `http://${getLocalIP()}:${this.port}/health`,\n        interval: '10s',\n        timeout: '5s',\n        deregisterCriticalServiceAfter: '1m'\n      }\n    };\n    \n    await consul.agent.service.register(registration);\n    console.log(`Service ${this.serviceId} registered with Consul`);\n    \n    // Deregister on shutdown\n    process.on('SIGINT', async () => {\n      await this.deregister();\n      process.exit(0);\n    });\n  }\n  \n  async deregister() {\n    await consul.agent.service.deregister(this.serviceId);\n    console.log(`Service ${this.serviceId} deregistered`);\n  }\n}\n\n// Express app with health endpoint\nconst express = require('express');\nconst app = express();\nconst PORT = process.env.PORT || 3000;\n\napp.get('/health', (req, res) => {\n  // Check dependencies\n  const healthy = checkDatabaseConnection() && checkCacheConnection();\n  \n  if (healthy) {\n    res.status(200).json({ status: 'healthy' });\n  } else {\n    res.status(503).json({ status: 'unhealthy' });\n  }\n});\n\napp.get('/api/users', async (req, res) => {\n  const users = await getUsersFromDB();\n  res.json(users);\n});\n\napp.listen(PORT, async () => {\n  console.log(`Server running on port ${PORT}`);\n  \n  // Register with Consul\n  const registration = new ServiceRegistration('user-service', PORT);\n  await registration.register();\n});\n\n// 2. Service Discovery (Client-Side)\nclass ServiceDiscovery {\n  constructor(serviceName) {\n    this.serviceName = serviceName;\n    this.instances = [];\n    this.currentIndex = 0;\n  }\n  \n  async discoverServices() {\n    const result = await consul.health.service({\n      service: this.serviceName,\n      passing: true  // Only healthy instances\n    });\n    \n    this.instances = result.map(entry => ({\n      id: entry.Service.ID,\n      address: entry.Service.Address,\n      port: entry.Service.Port,\n      tags: entry.Service.Tags,\n      meta: entry.Service.Meta\n    }));\n    \n    console.log(`Discovered ${this.instances.length} instances of ${this.serviceName}`);\n  }\n  \n  getInstance() {\n    if (this.instances.length === 0) {\n      throw new Error(`No healthy instances of ${this.serviceName}`);\n    }\n    \n    // Round-robin load balancing\n    const instance = this.instances[this.currentIndex];\n    this.currentIndex = (this.currentIndex + 1) % this.instances.length;\n    \n    return instance;\n  }\n  \n  async startWatching() {\n    // Re-discover every 30 seconds\n    setInterval(async () => {\n      await this.discoverServices();\n    }, 30000);\n    \n    // Initial discovery\n    await this.discoverServices();\n  }\n}\n\n// Usage\nconst axios = require('axios');\n\nclass UserServiceClient {\n  constructor() {\n    this.discovery = new ServiceDiscovery('user-service');\n  }\n  \n  async init() {\n    await this.discovery.startWatching();\n  }\n  \n  async getUser(userId) {\n    const instance = this.discovery.getInstance();\n    const url = `http://${instance.address}:${instance.port}/api/users/${userId}`;\n    \n    try {\n      const response = await axios.get(url, { timeout: 5000 });\n      return response.data;\n    } catch (error) {\n      // Retry with different instance\n      console.error(`Request to ${url} failed, retrying...`);\n      const retryInstance = this.discovery.getInstance();\n      const retryUrl = `http://${retryInstance.address}:${retryInstance.port}/api/users/${userId}`;\n      \n      const response = await axios.get(retryUrl, { timeout: 5000 });\n      return response.data;\n    }\n  }\n}\n\n// KUBERNETES SERVICE DISCOVERY\n\n// Service Definition (YAML)\n/*\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-service\nspec:\n  selector:\n    app: user-service\n  ports:\n  - port: 80\n    targetPort: 3000\n  type: ClusterIP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: user-service\n  template:\n    metadata:\n      labels:\n        app: user-service\n    spec:\n      containers:\n      - name: user-service\n        image: user-service:1.0.0\n        ports:\n        - containerPort: 3000\n        env:\n        - name: NODE_ENV\n          value: \"production\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n*/\n\n// DNS-Based Discovery in Kubernetes\nclass KubernetesServiceClient {\n  async getUser(userId) {\n    // Kubernetes DNS: <service-name>.<namespace>.svc.cluster.local\n    const url = 'http://user-service.default.svc.cluster.local/api/users/' + userId;\n    \n    const response = await axios.get(url);\n    return response.data;\n  }\n  \n  // Cross-namespace call\n  async getOrder(orderId) {\n    const url = 'http://order-service.orders.svc.cluster.local/api/orders/' + orderId;\n    \n    const response = await axios.get(url);\n    return response.data;\n  }\n}\n\n// EUREKA SERVICE DISCOVERY (Spring Cloud)\n\n// Spring Boot Application (Java)\n/*\n@SpringBootApplication\n@EnableEurekaClient\npublic class UserServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(UserServiceApplication.class, args);\n    }\n}\n\n// application.yml\neureka:\n  client:\n    serviceUrl:\n      defaultZone: http://eureka-server:8761/eureka/\n  instance:\n    preferIpAddress: true\n    leaseRenewalIntervalInSeconds: 10\n    instanceId: ${spring.application.name}:${spring.application.instance_id:${random.value}}\n\nspring:\n  application:\n    name: user-service\n\n// Service Client with Ribbon Load Balancer\n@Service\npublic class UserServiceClient {\n    \n    @Autowired\n    private RestTemplate restTemplate;\n    \n    public User getUser(String userId) {\n        // Service name instead of hostname\n        String url = \"http://user-service/api/users/\" + userId;\n        return restTemplate.getForObject(url, User.class);\n    }\n}\n\n@Configuration\npublic class RestTemplateConfig {\n    \n    @Bean\n    @LoadBalanced  // Enable client-side load balancing\n    public RestTemplate restTemplate() {\n        return new RestTemplate();\n    }\n}\n*/\n\n// CUSTOM SERVICE REGISTRY (Simple Implementation)\nclass InMemoryServiceRegistry {\n  constructor() {\n    this.services = new Map();  // serviceName -> [instances]\n    this.healthCheckInterval = 10000;  // 10 seconds\n  }\n  \n  register(serviceName, instance) {\n    if (!this.services.has(serviceName)) {\n      this.services.set(serviceName, []);\n    }\n    \n    const instances = this.services.get(serviceName);\n    \n    // Add or update instance\n    const existingIndex = instances.findIndex(i => i.id === instance.id);\n    if (existingIndex >= 0) {\n      instances[existingIndex] = { ...instance, lastHeartbeat: Date.now() };\n    } else {\n      instances.push({ ...instance, lastHeartbeat: Date.now() });\n    }\n    \n    console.log(`Registered ${instance.id} for service ${serviceName}`);\n  }\n  \n  deregister(serviceName, instanceId) {\n    if (!this.services.has(serviceName)) return;\n    \n    const instances = this.services.get(serviceName);\n    const index = instances.findIndex(i => i.id === instanceId);\n    \n    if (index >= 0) {\n      instances.splice(index, 1);\n      console.log(`Deregistered ${instanceId} from service ${serviceName}`);\n    }\n  }\n  \n  heartbeat(serviceName, instanceId) {\n    if (!this.services.has(serviceName)) return;\n    \n    const instances = this.services.get(serviceName);\n    const instance = instances.find(i => i.id === instanceId);\n    \n    if (instance) {\n      instance.lastHeartbeat = Date.now();\n    }\n  }\n  \n  getInstances(serviceName) {\n    if (!this.services.has(serviceName)) {\n      return [];\n    }\n    \n    // Return only healthy instances (heartbeat within 30s)\n    const now = Date.now();\n    return this.services.get(serviceName)\n      .filter(i => (now - i.lastHeartbeat) < 30000);\n  }\n  \n  startHealthCheck() {\n    setInterval(() => {\n      const now = Date.now();\n      \n      for (const [serviceName, instances] of this.services.entries()) {\n        // Remove stale instances (no heartbeat for > 30s)\n        const healthy = instances.filter(i => (now - i.lastHeartbeat) < 30000);\n        const removed = instances.length - healthy.length;\n        \n        if (removed > 0) {\n          console.log(`Removed ${removed} stale instances of ${serviceName}`);\n          this.services.set(serviceName, healthy);\n        }\n      }\n    }, this.healthCheckInterval);\n  }\n}\n\n// Registry Server\nconst registryApp = express();\nconst registry = new InMemoryServiceRegistry();\n\nregistryApp.use(express.json());\n\nregistryApp.post('/register', (req, res) => {\n  const { serviceName, instance } = req.body;\n  registry.register(serviceName, instance);\n  res.json({ success: true });\n});\n\nregistryApp.delete('/deregister/:serviceName/:instanceId', (req, res) => {\n  registry.deregister(req.params.serviceName, req.params.instanceId);\n  res.json({ success: true });\n});\n\nregistryApp.post('/heartbeat', (req, res) => {\n  const { serviceName, instanceId } = req.body;\n  registry.heartbeat(serviceName, instanceId);\n  res.json({ success: true });\n});\n\nregistryApp.get('/discover/:serviceName', (req, res) => {\n  const instances = registry.getInstances(req.params.serviceName);\n  res.json({ instances });\n});\n\nregistry.startHealthCheck();\nregistryApp.listen(8500);"
    },
    {
      "id": 35,
      "question": "What is the CQRS (Command Query Responsibility Segregation) pattern?",
      "answer": "CQRS separates read and write operations into different models, using commands for updates and queries for reads, optimizing each independently.\n\nCore Concepts:\n\n• Commands: Change state (Create, Update, Delete)\n  - Validated before execution\n  - Generate events\n  - Write to write model\n  - Example: CreateOrderCommand\n\n• Queries: Read state (Get, List, Search)\n  - No side effects\n  - Read from read model\n  - Optimized for queries\n  - Example: GetOrderByIdQuery\n\n• Write Model: Optimized for commands\n  - Normalized schema\n  - Business logic validation\n  - Domain model\n  - Event sourcing often used\n\n• Read Model (Projections): Optimized for queries\n  - Denormalized views\n  - Fast reads\n  - Updated from events\n  - Can have multiple views\n\nTraditional vs CQRS:\n\nTraditional (CRUD):\n• Same model for read/write\n• Single database\n• Normalized schema\n• ORM objects for all operations\n\nCQRS:\n• Separate models\n• Potentially separate databases\n• Write: normalized, Read: denormalized\n• Different optimization strategies\n\nBenefits:\n\n• Independent Scaling: Scale reads separately from writes\n• Optimized Queries: Denormalized read models\n• Simplified Queries: No complex JOINs\n• Flexible Read Models: Multiple views from same data\n• Performance: Read replicas, caching\n• Security: Separate read/write permissions\n• Event Sourcing: Natural fit\n\nChallenges:\n\n• Complexity: More moving parts\n• Eventual Consistency: Read models lag behind writes\n• Learning Curve: Different mental model\n• More Code: Separate models and handlers\n• Synchronization: Keep models in sync\n• Infrastructure: More databases/services\n\nWhen to Use CQRS:\n\n• Complex domains: Business logic in commands\n• Different read/write patterns\n• High read load: Scale reads independently\n• Multiple read models: Different views needed\n• Event Sourcing: Already using events\n• Collaborative domains: Multiple users editing\n• Audit requirements: Track all changes\n\nWhen NOT to Use:\n\n• Simple CRUD applications\n• Low traffic\n• Team lacks experience\n• Start-up/MVP\n• Real-time reads required (no lag tolerance)\n\nCQRS with Event Sourcing:\n\n• Commands create events\n• Events persisted to event store\n• Projections built from events\n• Natural fit, common combination\n• Complete audit trail\n\nCQRS without Event Sourcing:\n\n• Commands update write database\n• Publish events to message broker\n• Projections subscribe to events\n• Simpler than full event sourcing\n\nImplementation Variants:\n\n• Simple CQRS: Same database, different models\n• Two-Database CQRS: Separate read/write databases\n• Event-Driven CQRS: Events for synchronization\n• Full CQRS + ES: Event sourcing for writes\n\nPopular Frameworks:\n\n• Axon Framework: Java CQRS/ES\n• EventFlow: .NET CQRS/ES\n• Nest.js CQRS: Node.js CQRS\n• MediatR: .NET command/query mediator",
      "explanation": "CQRS separates read operations (queries) from write operations (commands) into distinct models, enabling independent optimization and scaling while using events to synchronize denormalized read views from the normalized write model.",
      "difficulty": "Hard",
      "code": "// CQRS IMPLEMENTATION (Node.js)\n\n// 1. Commands (Write Side)\n\n// Command DTOs\nclass CreateOrderCommand {\n  constructor(userId, items, shippingAddress) {\n    this.userId = userId;\n    this.items = items;\n    this.shippingAddress = shippingAddress;\n  }\n}\n\nclass UpdateOrderCommand {\n  constructor(orderId, items) {\n    this.orderId = orderId;\n    this.items = items;\n  }\n}\n\nclass CancelOrderCommand {\n  constructor(orderId, reason) {\n    this.orderId = orderId;\n    this.reason = reason;\n  }\n}\n\n// Command Handlers (Business Logic)\nclass CreateOrderHandler {\n  constructor(orderRepository, eventBus) {\n    this.orderRepository = orderRepository;\n    this.eventBus = eventBus;\n  }\n  \n  async handle(command) {\n    // Validation\n    if (!command.items || command.items.length === 0) {\n      throw new Error('Order must have at least one item');\n    }\n    \n    // Check inventory\n    for (const item of command.items) {\n      const available = await this.checkInventory(item.productId, item.quantity);\n      if (!available) {\n        throw new Error(`Product ${item.productId} not available`);\n      }\n    }\n    \n    // Calculate total\n    const total = command.items.reduce((sum, item) => sum + item.price * item.quantity, 0);\n    \n    // Create aggregate\n    const order = {\n      id: generateUUID(),\n      userId: command.userId,\n      items: command.items,\n      shippingAddress: command.shippingAddress,\n      total,\n      status: 'pending',\n      createdAt: new Date()\n    };\n    \n    // Save to write database\n    await this.orderRepository.save(order);\n    \n    // Publish event for read model\n    await this.eventBus.publish(new OrderCreatedEvent(order));\n    \n    return order.id;\n  }\n  \n  async checkInventory(productId, quantity) {\n    // Call inventory service\n    return true;\n  }\n}\n\n// Write Model Repository\nclass OrderWriteRepository {\n  constructor(db) {\n    this.db = db;  // Write database (normalized)\n  }\n  \n  async save(order) {\n    await this.db.query(\n      `INSERT INTO orders (id, user_id, total, status, shipping_address, created_at)\n       VALUES ($1, $2, $3, $4, $5, $6)`,\n      [order.id, order.userId, order.total, order.status, JSON.stringify(order.shippingAddress), order.createdAt]\n    );\n    \n    // Save order items (normalized)\n    for (const item of order.items) {\n      await this.db.query(\n        `INSERT INTO order_items (order_id, product_id, quantity, price)\n         VALUES ($1, $2, $3, $4)`,\n        [order.id, item.productId, item.quantity, item.price]\n      );\n    }\n  }\n  \n  async findById(orderId) {\n    const orderResult = await this.db.query(\n      'SELECT * FROM orders WHERE id = $1',\n      [orderId]\n    );\n    \n    if (orderResult.rows.length === 0) return null;\n    \n    const itemsResult = await this.db.query(\n      'SELECT * FROM order_items WHERE order_id = $1',\n      [orderId]\n    );\n    \n    return {\n      ...orderResult.rows[0],\n      items: itemsResult.rows\n    };\n  }\n}\n\n// 2. Queries (Read Side)\n\n// Query DTOs\nclass GetOrderByIdQuery {\n  constructor(orderId) {\n    this.orderId = orderId;\n  }\n}\n\nclass GetOrdersByUserQuery {\n  constructor(userId, limit, offset) {\n    this.userId = userId;\n    this.limit = limit;\n    this.offset = offset;\n  }\n}\n\nclass SearchOrdersQuery {\n  constructor(filters, limit, offset) {\n    this.filters = filters;\n    this.limit = limit;\n    this.offset = offset;\n  }\n}\n\n// Query Handlers\nclass GetOrderByIdHandler {\n  constructor(orderReadRepository) {\n    this.orderReadRepository = orderReadRepository;\n  }\n  \n  async handle(query) {\n    return await this.orderReadRepository.findById(query.orderId);\n  }\n}\n\nclass GetOrdersByUserHandler {\n  constructor(orderReadRepository) {\n    this.orderReadRepository = orderReadRepository;\n  }\n  \n  async handle(query) {\n    return await this.orderReadRepository.findByUser(\n      query.userId,\n      query.limit,\n      query.offset\n    );\n  }\n}\n\n// Read Model Repository (Denormalized)\nclass OrderReadRepository {\n  constructor(db) {\n    this.db = db;  // Read database (denormalized views)\n  }\n  \n  async findById(orderId) {\n    const result = await this.db.query(\n      'SELECT * FROM orders_view WHERE id = $1',\n      [orderId]\n    );\n    return result.rows[0];\n  }\n  \n  async findByUser(userId, limit = 10, offset = 0) {\n    const result = await this.db.query(\n      `SELECT * FROM orders_view \n       WHERE user_id = $1 \n       ORDER BY created_at DESC \n       LIMIT $2 OFFSET $3`,\n      [userId, limit, offset]\n    );\n    return result.rows;\n  }\n  \n  async search(filters) {\n    // Complex search on denormalized data\n    let query = 'SELECT * FROM orders_view WHERE 1=1';\n    const params = [];\n    \n    if (filters.status) {\n      params.push(filters.status);\n      query += ` AND status = $${params.length}`;\n    }\n    \n    if (filters.dateFrom) {\n      params.push(filters.dateFrom);\n      query += ` AND created_at >= $${params.length}`;\n    }\n    \n    if (filters.minTotal) {\n      params.push(filters.minTotal);\n      query += ` AND total >= $${params.length}`;\n    }\n    \n    const result = await this.db.query(query, params);\n    return result.rows;\n  }\n}\n\n// 3. Event Bus (Sync Write and Read Models)\n\n// Domain Events\nclass OrderCreatedEvent {\n  constructor(order) {\n    this.type = 'OrderCreated';\n    this.timestamp = new Date();\n    this.data = order;\n  }\n}\n\nclass OrderUpdatedEvent {\n  constructor(order) {\n    this.type = 'OrderUpdated';\n    this.timestamp = new Date();\n    this.data = order;\n  }\n}\n\nclass OrderCancelledEvent {\n  constructor(orderId, reason) {\n    this.type = 'OrderCancelled';\n    this.timestamp = new Date();\n    this.data = { orderId, reason };\n  }\n}\n\n// Event Bus Implementation\nclass EventBus {\n  constructor() {\n    this.handlers = new Map();\n  }\n  \n  subscribe(eventType, handler) {\n    if (!this.handlers.has(eventType)) {\n      this.handlers.set(eventType, []);\n    }\n    this.handlers.get(eventType).push(handler);\n  }\n  \n  async publish(event) {\n    const handlers = this.handlers.get(event.type) || [];\n    \n    for (const handler of handlers) {\n      try {\n        await handler(event);\n      } catch (error) {\n        console.error(`Error handling event ${event.type}:`, error);\n      }\n    }\n  }\n}\n\n// Projection (Build Read Model from Events)\nclass OrderProjection {\n  constructor(readDb) {\n    this.readDb = readDb;\n  }\n  \n  async onOrderCreated(event) {\n    const order = event.data;\n    \n    // Insert into denormalized view\n    await this.readDb.query(\n      `INSERT INTO orders_view (\n        id, user_id, total, status, \n        shipping_address, items, item_count, \n        created_at, user_name, user_email\n      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)`,\n      [\n        order.id,\n        order.userId,\n        order.total,\n        order.status,\n        JSON.stringify(order.shippingAddress),\n        JSON.stringify(order.items),  // Denormalized\n        order.items.length,\n        order.createdAt,\n        await this.getUserName(order.userId),  // Denormalized\n        await this.getUserEmail(order.userId)  // Denormalized\n      ]\n    );\n  }\n  \n  async onOrderUpdated(event) {\n    const order = event.data;\n    \n    await this.readDb.query(\n      `UPDATE orders_view \n       SET status = $1, items = $2, item_count = $3, updated_at = $4\n       WHERE id = $5`,\n      [\n        order.status,\n        JSON.stringify(order.items),\n        order.items.length,\n        new Date(),\n        order.id\n      ]\n    );\n  }\n  \n  async onOrderCancelled(event) {\n    await this.readDb.query(\n      `UPDATE orders_view \n       SET status = 'cancelled', cancelled_reason = $1, updated_at = $2\n       WHERE id = $3`,\n      [event.data.reason, new Date(), event.data.orderId]\n    );\n  }\n  \n  async getUserName(userId) {\n    const result = await this.readDb.query(\n      'SELECT name FROM users WHERE id = $1',\n      [userId]\n    );\n    return result.rows[0]?.name;\n  }\n  \n  async getUserEmail(userId) {\n    const result = await this.readDb.query(\n      'SELECT email FROM users WHERE id = $1',\n      [userId]\n    );\n    return result.rows[0]?.email;\n  }\n}\n\n// 4. Command Bus (Mediator Pattern)\nclass CommandBus {\n  constructor() {\n    this.handlers = new Map();\n  }\n  \n  registerHandler(commandType, handler) {\n    this.handlers.set(commandType, handler);\n  }\n  \n  async execute(command) {\n    const handler = this.handlers.get(command.constructor.name);\n    \n    if (!handler) {\n      throw new Error(`No handler for command ${command.constructor.name}`);\n    }\n    \n    return await handler.handle(command);\n  }\n}\n\n// Query Bus\nclass QueryBus {\n  constructor() {\n    this.handlers = new Map();\n  }\n  \n  registerHandler(queryType, handler) {\n    this.handlers.set(queryType, handler);\n  }\n  \n  async execute(query) {\n    const handler = this.handlers.get(query.constructor.name);\n    \n    if (!handler) {\n      throw new Error(`No handler for query ${query.constructor.name}`);\n    }\n    \n    return await handler.handle(query);\n  }\n}\n\n// 5. Setup and Express API\nconst writeDb = new Pool({ /* write database config */ });\nconst readDb = new Pool({ /* read database config */ });\n\nconst eventBus = new EventBus();\nconst commandBus = new CommandBus();\nconst queryBus = new QueryBus();\n\n// Repositories\nconst orderWriteRepo = new OrderWriteRepository(writeDb);\nconst orderReadRepo = new OrderReadRepository(readDb);\n\n// Handlers\nconst createOrderHandler = new CreateOrderHandler(orderWriteRepo, eventBus);\nconst getOrderHandler = new GetOrderByIdHandler(orderReadRepo);\nconst getUserOrdersHandler = new GetOrdersByUserHandler(orderReadRepo);\n\n// Register handlers\ncommandBus.registerHandler('CreateOrderCommand', createOrderHandler);\nqueryBus.registerHandler('GetOrderByIdQuery', getOrderHandler);\nqueryBus.registerHandler('GetOrdersByUserQuery', getUserOrdersHandler);\n\n// Subscribe projections to events\nconst orderProjection = new OrderProjection(readDb);\neventBus.subscribe('OrderCreated', (event) => orderProjection.onOrderCreated(event));\neventBus.subscribe('OrderUpdated', (event) => orderProjection.onOrderUpdated(event));\neventBus.subscribe('OrderCancelled', (event) => orderProjection.onOrderCancelled(event));\n\n// Express API\nconst app = express();\napp.use(express.json());\n\n// Commands (Write)\napp.post('/api/orders', async (req, res) => {\n  const command = new CreateOrderCommand(\n    req.body.userId,\n    req.body.items,\n    req.body.shippingAddress\n  );\n  \n  const orderId = await commandBus.execute(command);\n  res.status(201).json({ orderId });\n});\n\n// Queries (Read)\napp.get('/api/orders/:id', async (req, res) => {\n  const query = new GetOrderByIdQuery(req.params.id);\n  const order = await queryBus.execute(query);\n  res.json(order);\n});\n\napp.get('/api/users/:userId/orders', async (req, res) => {\n  const query = new GetOrdersByUserQuery(\n    req.params.userId,\n    parseInt(req.query.limit) || 10,\n    parseInt(req.query.offset) || 0\n  );\n  \n  const orders = await queryBus.execute(query);\n  res.json(orders);\n});\n\napp.listen(3000);"
    },
    {
      "id": 36,
      "question": "How do you design a Rate Limiter?",
      "answer": "A rate limiter controls the rate of requests to prevent system overload and abuse.\n\nCore Requirements:\n• Limit requests per user/IP/API key within time window\n• Handle distributed systems (multiple servers)\n• Low latency overhead for legitimate requests\n• Accurate counting and fair throttling\n• Graceful degradation under high load\n\nAlgorithms:\n• Token Bucket: Tokens refill at fixed rate, request consumes token\n• Leaky Bucket: Requests processed at constant rate, queue overflow drops\n• Fixed Window: Count requests in fixed time windows (simple but has boundary issues)\n• Sliding Window Log: Track timestamp of each request (accurate but memory intensive)\n• Sliding Window Counter: Hybrid approach balancing accuracy and efficiency\n\nImplementation Strategies:\n• Application Level: Middleware checks before processing\n• API Gateway: Centralized rate limiting (Kong, AWS API Gateway)\n• Reverse Proxy: Nginx limit_req module\n• Distributed: Redis with atomic operations (INCR with EXPIRE)\n\nResponse Handling:\n• Return 429 Too Many Requests status\n• Include Retry-After header\n• Provide X-RateLimit headers (limit, remaining, reset)\n• Queue or drop excess requests based on requirements",
      "explanation": "Rate limiter protects systems from overload by controlling request frequency using algorithms like token bucket or sliding window, typically implemented with Redis for distributed systems.",
      "difficulty": "Hard",
      "code": "// Token Bucket Rate Limiter with Redis\nconst redis = require('redis');\nconst client = redis.createClient();\n\nclass TokenBucketRateLimiter {\n  constructor(capacity, refillRate) {\n    this.capacity = capacity;        // Max tokens\n    this.refillRate = refillRate;    // Tokens per second\n  }\n\n  async allowRequest(userId) {\n    const key = `rate_limit:${userId}`;\n    const now = Date.now() / 1000;\n    \n    // Lua script for atomic operation\n    const script = `\n      local key = KEYS[1]\n      local capacity = tonumber(ARGV[1])\n      local refill_rate = tonumber(ARGV[2])\n      local now = tonumber(ARGV[3])\n      \n      local bucket = redis.call('HMGET', key, 'tokens', 'last_refill')\n      local tokens = tonumber(bucket[1]) or capacity\n      local last_refill = tonumber(bucket[2]) or now\n      \n      -- Calculate tokens to add\n      local elapsed = now - last_refill\n      local tokens_to_add = elapsed * refill_rate\n      tokens = math.min(capacity, tokens + tokens_to_add)\n      \n      if tokens >= 1 then\n        tokens = tokens - 1\n        redis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)\n        redis.call('EXPIRE', key, 3600)\n        return 1\n      else\n        return 0\n      end\n    `;\n    \n    const allowed = await client.eval(\n      script, 1, key, \n      this.capacity, this.refillRate, now\n    );\n    \n    return allowed === 1;\n  }\n}\n\n// Express Middleware\nconst rateLimiter = new TokenBucketRateLimiter(10, 1); // 10 requests, 1/sec refill\n\napp.use(async (req, res, next) => {\n  const userId = req.user?.id || req.ip;\n  const allowed = await rateLimiter.allowRequest(userId);\n  \n  if (!allowed) {\n    res.set('X-RateLimit-Limit', '10');\n    res.set('X-RateLimit-Remaining', '0');\n    res.set('Retry-After', '1');\n    return res.status(429).json({ error: 'Too many requests' });\n  }\n  \n  next();\n});\n\n// Sliding Window Counter (More efficient)\nclass SlidingWindowRateLimiter {\n  async allowRequest(userId, limit, windowSec) {\n    const key = `rate_limit:${userId}`;\n    const now = Date.now();\n    const windowStart = now - (windowSec * 1000);\n    \n    const pipeline = client.pipeline();\n    \n    // Remove old entries\n    pipeline.zremrangebyscore(key, 0, windowStart);\n    \n    // Count requests in window\n    pipeline.zcard(key);\n    \n    // Add current request\n    pipeline.zadd(key, now, `${now}-${Math.random()}`);\n    \n    // Set expiry\n    pipeline.expire(key, windowSec);\n    \n    const results = await pipeline.exec();\n    const count = results[1][1];\n    \n    return count < limit;\n  }\n}\n\n// API Gateway Pattern (Kong/Nginx style)\nconst apiRateLimits = {\n  '/api/public': { limit: 100, window: 3600 },    // 100/hour\n  '/api/search': { limit: 10, window: 60 },       // 10/min\n  '/api/auth': { limit: 5, window: 300 }          // 5/5min\n};\n\napp.use(async (req, res, next) => {\n  const config = apiRateLimits[req.path];\n  if (!config) return next();\n  \n  const userId = req.user?.id || req.ip;\n  const key = `rate:${req.path}:${userId}`;\n  \n  const count = await client.incr(key);\n  \n  if (count === 1) {\n    await client.expire(key, config.window);\n  }\n  \n  const ttl = await client.ttl(key);\n  \n  res.set('X-RateLimit-Limit', config.limit);\n  res.set('X-RateLimit-Remaining', Math.max(0, config.limit - count));\n  res.set('X-RateLimit-Reset', Date.now() + (ttl * 1000));\n  \n  if (count > config.limit) {\n    res.set('Retry-After', ttl);\n    return res.status(429).json({ \n      error: 'Rate limit exceeded',\n      retryAfter: ttl \n    });\n  }\n  \n  next();\n});"
    },
    {
      "id": 37,
      "question": "Design a Consistent Hashing system",
      "answer": "Consistent hashing minimizes data movement when nodes are added/removed from a distributed system.\n\nCore Concepts:\n• Hash ring: Hash space mapped to a circle (0 to 2^32-1)\n• Node placement: Hash node IDs and place on ring\n• Data placement: Hash keys and assign to next node clockwise\n• Minimal disruption: Only K/n keys move when node added/removed (K=keys, n=nodes)\n\nVirtual Nodes (vnodes):\n• Each physical node has multiple virtual nodes on ring\n• Better load distribution across nodes\n• Reduces impact of hot spots\n• Typical ratio: 100-200 vnodes per physical node\n\nKey Benefits:\n• Horizontal scalability: Easy to add/remove nodes\n• Fault tolerance: Data redistributed only to neighbors\n• Load balancing: Virtual nodes distribute load evenly\n• Reduced coordination: No global rebalancing needed\n\nUse Cases:\n• Distributed caches (Memcached, Redis Cluster)\n• Distributed databases (Cassandra, DynamoDB)\n• CDN content distribution\n• Load balancer node selection\n• Partitioned message queues",
      "explanation": "Consistent hashing maps keys and nodes to a hash ring, minimizing data movement during scaling by only redistributing keys to/from affected nodes using virtual nodes for balanced distribution.",
      "difficulty": "Hard",
      "code": "// Consistent Hashing Implementation\nconst crypto = require('crypto');\n\nclass ConsistentHash {\n  constructor(vnodeCount = 150) {\n    this.vnodeCount = vnodeCount;  // Virtual nodes per physical node\n    this.ring = new Map();          // Hash value -> node mapping\n    this.sortedKeys = [];           // Sorted hash values\n    this.nodes = new Set();         // Physical nodes\n  }\n\n  // Hash function (MD5 for distribution)\n  hash(key) {\n    return parseInt(\n      crypto.createHash('md5')\n        .update(key)\n        .digest('hex')\n        .substring(0, 8),\n      16\n    );\n  }\n\n  // Add physical node with virtual nodes\n  addNode(node) {\n    this.nodes.add(node);\n    \n    // Create virtual nodes\n    for (let i = 0; i < this.vnodeCount; i++) {\n      const vnodeKey = `${node}:vnode${i}`;\n      const hash = this.hash(vnodeKey);\n      this.ring.set(hash, node);\n    }\n    \n    // Keep ring sorted\n    this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n    \n    console.log(`Added node ${node} with ${this.vnodeCount} vnodes`);\n  }\n\n  // Remove node\n  removeNode(node) {\n    this.nodes.delete(node);\n    \n    // Remove all virtual nodes\n    for (let i = 0; i < this.vnodeCount; i++) {\n      const vnodeKey = `${node}:vnode${i}`;\n      const hash = this.hash(vnodeKey);\n      this.ring.delete(hash);\n    }\n    \n    this.sortedKeys = Array.from(this.ring.keys()).sort((a, b) => a - b);\n    \n    console.log(`Removed node ${node}`);\n  }\n\n  // Get node for given key\n  getNode(key) {\n    if (this.ring.size === 0) return null;\n    \n    const hash = this.hash(key);\n    \n    // Binary search for first node >= hash\n    let idx = this.binarySearch(hash);\n    \n    // Wrap around if needed\n    if (idx >= this.sortedKeys.length) {\n      idx = 0;\n    }\n    \n    return this.ring.get(this.sortedKeys[idx]);\n  }\n\n  // Binary search for insertion point\n  binarySearch(hash) {\n    let left = 0;\n    let right = this.sortedKeys.length;\n    \n    while (left < right) {\n      const mid = Math.floor((left + right) / 2);\n      if (this.sortedKeys[mid] < hash) {\n        left = mid + 1;\n      } else {\n        right = mid;\n      }\n    }\n    \n    return left;\n  }\n\n  // Get N replicas for key\n  getNodes(key, n = 3) {\n    if (this.ring.size === 0) return [];\n    \n    const hash = this.hash(key);\n    const nodes = new Set();\n    let idx = this.binarySearch(hash);\n    \n    // Collect N unique physical nodes\n    while (nodes.size < n && nodes.size < this.nodes.size) {\n      if (idx >= this.sortedKeys.length) idx = 0;\n      \n      const node = this.ring.get(this.sortedKeys[idx]);\n      nodes.add(node);\n      idx++;\n    }\n    \n    return Array.from(nodes);\n  }\n\n  // Distribution analysis\n  analyzeDistribution(keyCount = 10000) {\n    const distribution = new Map();\n    this.nodes.forEach(node => distribution.set(node, 0));\n    \n    // Generate random keys\n    for (let i = 0; i < keyCount; i++) {\n      const key = `key${i}`;\n      const node = this.getNode(key);\n      distribution.set(node, distribution.get(node) + 1);\n    }\n    \n    console.log('\\nDistribution Analysis:');\n    const avg = keyCount / this.nodes.size;\n    distribution.forEach((count, node) => {\n      const variance = ((count - avg) / avg * 100).toFixed(2);\n      console.log(`${node}: ${count} keys (${variance}% from average)`);\n    });\n  }\n}\n\n// Usage Example\nconst ch = new ConsistentHash(150);\n\n// Add nodes\nch.addNode('node1:6379');\nch.addNode('node2:6379');\nch.addNode('node3:6379');\n\n// Route keys\nconsole.log('user:1001 ->', ch.getNode('user:1001'));\nconsole.log('user:1002 ->', ch.getNode('user:1002'));\nconsole.log('user:1003 ->', ch.getNode('user:1003'));\n\n// Get replicas\nconsole.log('user:1001 replicas ->', ch.getNodes('user:1001', 3));\n\n// Analyze distribution\nch.analyzeDistribution(10000);\n\n// Simulate node failure\nch.removeNode('node2:6379');\nconsole.log('After node2 removed:');\nconsole.log('user:1001 ->', ch.getNode('user:1001'));\n\n// Distributed Cache with Consistent Hashing\nclass DistributedCache {\n  constructor(cacheNodes) {\n    this.ch = new ConsistentHash();\n    this.caches = new Map();\n    \n    cacheNodes.forEach(node => {\n      this.ch.addNode(node);\n      this.caches.set(node, new Map());\n    });\n  }\n\n  set(key, value) {\n    const node = this.ch.getNode(key);\n    this.caches.get(node).set(key, value);\n    return node;\n  }\n\n  get(key) {\n    const node = this.ch.getNode(key);\n    return this.caches.get(node).get(key);\n  }\n\n  // With replication\n  setReplicated(key, value, replicas = 3) {\n    const nodes = this.ch.getNodes(key, replicas);\n    nodes.forEach(node => {\n      this.caches.get(node).set(key, value);\n    });\n    return nodes;\n  }\n}"
    },
    {
      "id": 38,
      "question": "How do you design a Distributed Lock?",
      "answer": "A distributed lock coordinates access to shared resources across multiple processes/servers.\n\nRequirements:\n• Mutual exclusion: Only one client holds lock at a time\n• Deadlock free: Lock eventually released even if holder crashes\n• Fault tolerance: Lock service remains available despite failures\n• High performance: Low latency for acquire/release operations\n\nImplementation Approaches:\n• Database-based: Use row-level locks or advisory locks (simple but single point of failure)\n• Redis-based: SET with NX and EX options (fast, battle-tested)\n• ZooKeeper: Ephemeral sequential nodes (strong consistency)\n• Etcd: Lease-based locking with automatic expiry\n• Consul: Key-value with session management\n\nRedis Redlock Algorithm:\n• Acquire lock from majority of Redis instances (e.g., 3 of 5)\n• Set locks with same key and random value (ownership proof)\n• Use TTL for automatic release (deadlock prevention)\n• Validate total acquisition time < lock validity time\n• Release by deleting key only if value matches (prevent releasing others' locks)\n\nKey Challenges:\n• Clock skew: TTL expires differently on different machines\n• Process pause: GC pause causes lock release while code thinks it holds lock\n• Network partitions: Split-brain scenarios\n• Lock expiry during work: Fencing tokens to prevent stale operations",
      "explanation": "Distributed locks prevent concurrent access to shared resources in distributed systems using Redis SET NX EX, ZooKeeper ephemeral nodes, or Redlock algorithm for multi-instance safety.",
      "difficulty": "Hard",
      "code": "// Redis-based Distributed Lock\nconst redis = require('redis');\nconst { promisify } = require('util');\nconst crypto = require('crypto');\n\nclass DistributedLock {\n  constructor(redisClient) {\n    this.client = redisClient;\n    this.setAsync = promisify(redisClient.set).bind(redisClient);\n    this.delAsync = promisify(redisClient.del).bind(redisClient);\n    this.evalAsync = promisify(redisClient.eval).bind(redisClient);\n  }\n\n  // Acquire lock with retry\n  async acquire(resource, ttlMs = 30000, retryDelay = 100, retryCount = 50) {\n    const lockKey = `lock:${resource}`;\n    const lockValue = crypto.randomBytes(16).toString('hex'); // Unique identifier\n    const expiry = Math.ceil(ttlMs / 1000);\n    \n    for (let i = 0; i < retryCount; i++) {\n      // SET if not exists with expiry\n      const result = await this.setAsync(\n        lockKey,\n        lockValue,\n        'NX',  // Only set if not exists\n        'EX',  // Set expiry in seconds\n        expiry\n      );\n      \n      if (result === 'OK') {\n        return {\n          key: lockKey,\n          value: lockValue,\n          acquired: true,\n          expiresAt: Date.now() + ttlMs\n        };\n      }\n      \n      // Wait before retry\n      await new Promise(resolve => setTimeout(resolve, retryDelay));\n    }\n    \n    return { acquired: false };\n  }\n\n  // Release lock (only if we own it)\n  async release(lock) {\n    if (!lock.acquired) return false;\n    \n    // Lua script for atomic check-and-delete\n    const script = `\n      if redis.call(\"get\", KEYS[1]) == ARGV[1] then\n        return redis.call(\"del\", KEYS[1])\n      else\n        return 0\n      end\n    `;\n    \n    const result = await this.evalAsync(script, 1, lock.key, lock.value);\n    return result === 1;\n  }\n\n  // Auto-releasing lock with callback\n  async withLock(resource, callback, ttlMs = 30000) {\n    const lock = await this.acquire(resource, ttlMs);\n    \n    if (!lock.acquired) {\n      throw new Error(`Failed to acquire lock: ${resource}`);\n    }\n    \n    try {\n      return await callback();\n    } finally {\n      await this.release(lock);\n    }\n  }\n\n  // Extend lock TTL\n  async extend(lock, additionalMs = 30000) {\n    if (!lock.acquired) return false;\n    \n    const script = `\n      if redis.call(\"get\", KEYS[1]) == ARGV[1] then\n        return redis.call(\"pexpire\", KEYS[1], ARGV[2])\n      else\n        return 0\n      end\n    `;\n    \n    const result = await this.evalAsync(\n      script, 1, \n      lock.key, \n      lock.value, \n      additionalMs\n    );\n    \n    if (result === 1) {\n      lock.expiresAt = Date.now() + additionalMs;\n      return true;\n    }\n    \n    return false;\n  }\n}\n\n// Redlock Algorithm (Multiple Redis Instances)\nclass Redlock {\n  constructor(redisClients) {\n    this.clients = redisClients;\n    this.quorum = Math.floor(clients.length / 2) + 1;\n  }\n\n  async acquire(resource, ttlMs = 30000) {\n    const lockValue = crypto.randomBytes(16).toString('hex');\n    const startTime = Date.now();\n    let lockedInstances = 0;\n    \n    // Try to acquire lock on all instances\n    const promises = this.clients.map(client => \n      this.acquireSingle(client, resource, lockValue, ttlMs)\n        .then(success => success ? 1 : 0)\n        .catch(() => 0)\n    );\n    \n    const results = await Promise.all(promises);\n    lockedInstances = results.reduce((sum, r) => sum + r, 0);\n    \n    const elapsed = Date.now() - startTime;\n    const validityTime = ttlMs - elapsed - 100; // Drift compensation\n    \n    // Check if we got majority and still have time\n    if (lockedInstances >= this.quorum && validityTime > 0) {\n      return {\n        resource,\n        value: lockValue,\n        validity: validityTime,\n        acquired: true\n      };\n    }\n    \n    // Failed to acquire majority, release acquired locks\n    await this.release({ resource, value: lockValue });\n    return { acquired: false };\n  }\n\n  async acquireSingle(client, resource, value, ttlMs) {\n    const result = await client.set(\n      `lock:${resource}`,\n      value,\n      'NX',\n      'PX',\n      ttlMs\n    );\n    return result === 'OK';\n  }\n\n  async release(lock) {\n    const script = `\n      if redis.call(\"get\", KEYS[1]) == ARGV[1] then\n        return redis.call(\"del\", KEYS[1])\n      else\n        return 0\n      end\n    `;\n    \n    const promises = this.clients.map(client =>\n      client.eval(script, 1, `lock:${lock.resource}`, lock.value)\n        .catch(() => 0)\n    );\n    \n    await Promise.all(promises);\n  }\n}\n\n// Usage Examples\nconst client = redis.createClient();\nconst lock = new DistributedLock(client);\n\n// Example 1: Manual lock management\nasync function processOrder(orderId) {\n  const lockResource = `order:${orderId}`;\n  const orderLock = await lock.acquire(lockResource, 30000);\n  \n  if (!orderLock.acquired) {\n    throw new Error('Could not acquire lock');\n  }\n  \n  try {\n    // Critical section\n    const order = await getOrder(orderId);\n    await processPayment(order);\n    await updateInventory(order);\n    await sendConfirmation(order);\n  } finally {\n    await lock.release(orderLock);\n  }\n}\n\n// Example 2: Auto-releasing with callback\nasync function updateUserBalance(userId, amount) {\n  return await lock.withLock(\n    `user:balance:${userId}`,\n    async () => {\n      const user = await getUser(userId);\n      user.balance += amount;\n      await saveUser(user);\n      return user.balance;\n    },\n    10000 // 10 second TTL\n  );\n}\n\n// Example 3: Long-running task with lock extension\nasync function processLargeJob(jobId) {\n  const jobLock = await lock.acquire(`job:${jobId}`, 30000);\n  \n  if (!jobLock.acquired) {\n    throw new Error('Job already processing');\n  }\n  \n  // Extend lock periodically\n  const extender = setInterval(async () => {\n    const extended = await lock.extend(jobLock, 30000);\n    if (!extended) {\n      console.error('Failed to extend lock - stopping job');\n      clearInterval(extender);\n    }\n  }, 20000); // Extend every 20s\n  \n  try {\n    await runJob(jobId);\n  } finally {\n    clearInterval(extender);\n    await lock.release(jobLock);\n  }\n}"
    },
    {
      "id": 39,
      "question": "Design a Notification System (Push notifications, Email, SMS)",
      "answer": "A notification system delivers messages to users across multiple channels reliably and efficiently.\n\nCore Components:\n• Notification Service: Receives notification requests, validates, and routes\n• Channel Handlers: Email (SMTP, SendGrid), SMS (Twilio, SNS), Push (FCM, APNS)\n• Template Engine: Personalized message generation with variables\n• Queue System: Kafka/RabbitMQ for async processing and retry\n• Delivery Tracker: Status tracking (sent, delivered, failed, read)\n• User Preferences: Channel preferences, quiet hours, opt-out management\n\nKey Requirements:\n• Multi-channel support: Email, SMS, push, in-app\n• Priority levels: Critical (immediate), high (fast), normal (batched)\n• Rate limiting: Prevent spam and respect quotas\n• Batching: Group notifications for efficiency\n• Retry logic: Exponential backoff for failures\n• Idempotency: Prevent duplicate notifications\n• Analytics: Delivery rates, open rates, engagement\n\nScalability Patterns:\n• Message Queue: Decouple producers from consumers\n• Worker pools: Scale notification senders independently\n• Template caching: Reduce template compilation overhead\n• Batch processing: Send multiple emails/SMS in bulk\n• Circuit breaker: Fail fast when provider is down\n\nDelivery Guarantees:\n• At-least-once: May send duplicates (use idempotency key)\n• Best-effort: For non-critical notifications\n• Priority queue: Ensure critical notifications bypass regular queue",
      "explanation": "Notification systems deliver multi-channel messages using queues for reliability, templates for personalization, and separate workers for each channel (email/SMS/push) with retry logic and delivery tracking.",
      "difficulty": "Hard",
      "code": "// Notification System Architecture\nconst { Kafka } = require('kafkajs');\nconst AWS = require('aws-sdk');\nconst admin = require('firebase-admin');\nconst nodemailer = require('nodemailer');\n\n// Notification Service (Producer)\nclass NotificationService {\n  constructor() {\n    this.kafka = new Kafka({ clientId: 'notification-service' });\n    this.producer = this.kafka.producer();\n  }\n\n  async sendNotification(notification) {\n    const { userId, type, channel, priority, template, data } = notification;\n    \n    // Validate user preferences\n    const preferences = await this.getUserPreferences(userId);\n    if (!this.shouldSend(preferences, channel, type)) {\n      return { status: 'skipped', reason: 'user_preference' };\n    }\n    \n    // Check quiet hours\n    if (this.isQuietHour(preferences)) {\n      return { status: 'deferred', reason: 'quiet_hours' };\n    }\n    \n    // Generate message from template\n    const message = await this.renderTemplate(template, data);\n    \n    // Determine topic based on priority and channel\n    const topic = `notifications.${channel}.${priority || 'normal'}`;\n    \n    // Send to Kafka\n    await this.producer.send({\n      topic,\n      messages: [{\n        key: userId,\n        value: JSON.stringify({\n          id: generateId(),\n          userId,\n          channel,\n          message,\n          priority,\n          timestamp: Date.now(),\n          retryCount: 0\n        })\n      }]\n    });\n    \n    return { status: 'queued' };\n  }\n\n  async renderTemplate(templateName, data) {\n    // Load and compile template\n    const template = await this.getTemplate(templateName);\n    return template.render(data);\n  }\n\n  shouldSend(preferences, channel, type) {\n    if (!preferences.enabled) return false;\n    if (preferences.blockedChannels?.includes(channel)) return false;\n    if (preferences.mutedTypes?.includes(type)) return false;\n    return true;\n  }\n\n  isQuietHour(preferences) {\n    if (!preferences.quietHours) return false;\n    const hour = new Date().getHours();\n    const { start, end } = preferences.quietHours;\n    return hour >= start || hour < end;\n  }\n}\n\n// Email Worker (Consumer)\nclass EmailWorker {\n  constructor() {\n    this.kafka = new Kafka({ clientId: 'email-worker' });\n    this.consumer = this.kafka.consumer({ groupId: 'email-workers' });\n    this.transporter = nodemailer.createTransport({\n      host: process.env.SMTP_HOST,\n      auth: {\n        user: process.env.SMTP_USER,\n        pass: process.env.SMTP_PASS\n      }\n    });\n    this.maxRetries = 3;\n  }\n\n  async start() {\n    await this.consumer.connect();\n    await this.consumer.subscribe({ \n      topics: ['notifications.email.critical', 'notifications.email.normal']\n    });\n    \n    await this.consumer.run({\n      eachMessage: async ({ topic, partition, message }) => {\n        const notification = JSON.parse(message.value.toString());\n        await this.processEmail(notification);\n      }\n    });\n  }\n\n  async processEmail(notification) {\n    try {\n      const user = await this.getUser(notification.userId);\n      \n      const result = await this.transporter.sendMail({\n        from: 'noreply@example.com',\n        to: user.email,\n        subject: notification.message.subject,\n        html: notification.message.body,\n        headers: {\n          'X-Notification-ID': notification.id\n        }\n      });\n      \n      await this.trackDelivery(notification.id, 'sent', result.messageId);\n      \n    } catch (error) {\n      await this.handleFailure(notification, error);\n    }\n  }\n\n  async handleFailure(notification, error) {\n    notification.retryCount++;\n    \n    if (notification.retryCount < this.maxRetries) {\n      // Exponential backoff\n      const delay = Math.pow(2, notification.retryCount) * 1000;\n      \n      setTimeout(async () => {\n        await this.producer.send({\n          topic: `notifications.email.${notification.priority}`,\n          messages: [{ value: JSON.stringify(notification) }]\n        });\n      }, delay);\n      \n      await this.trackDelivery(notification.id, 'retry', error.message);\n    } else {\n      await this.trackDelivery(notification.id, 'failed', error.message);\n    }\n  }\n}\n\n// SMS Worker\nclass SMSWorker {\n  constructor() {\n    this.sns = new AWS.SNS({ region: 'us-east-1' });\n  }\n\n  async processSMS(notification) {\n    const user = await this.getUser(notification.userId);\n    \n    const params = {\n      Message: notification.message.text,\n      PhoneNumber: user.phone,\n      MessageAttributes: {\n        'AWS.SNS.SMS.SMSType': {\n          DataType: 'String',\n          StringValue: notification.priority === 'critical' ? 'Transactional' : 'Promotional'\n        }\n      }\n    };\n    \n    try {\n      const result = await this.sns.publish(params).promise();\n      await this.trackDelivery(notification.id, 'sent', result.MessageId);\n    } catch (error) {\n      await this.handleFailure(notification, error);\n    }\n  }\n}\n\n// Push Notification Worker (FCM)\nclass PushWorker {\n  constructor() {\n    admin.initializeApp({\n      credential: admin.credential.cert(serviceAccount)\n    });\n  }\n\n  async processPush(notification) {\n    const user = await this.getUser(notification.userId);\n    const devices = await this.getUserDevices(user.id);\n    \n    const message = {\n      notification: {\n        title: notification.message.title,\n        body: notification.message.body\n      },\n      data: notification.message.data || {},\n      tokens: devices.map(d => d.fcmToken)\n    };\n    \n    try {\n      const result = await admin.messaging().sendMulticast(message);\n      \n      // Remove invalid tokens\n      result.responses.forEach((resp, idx) => {\n        if (!resp.success) {\n          const error = resp.error;\n          if (error.code === 'messaging/invalid-registration-token' ||\n              error.code === 'messaging/registration-token-not-registered') {\n            this.removeDeviceToken(devices[idx].fcmToken);\n          }\n        }\n      });\n      \n      await this.trackDelivery(\n        notification.id, \n        'sent', \n        `${result.successCount}/${devices.length} delivered`\n      );\n    } catch (error) {\n      await this.handleFailure(notification, error);\n    }\n  }\n}\n\n// Notification API\nconst express = require('express');\nconst app = express();\nconst notificationService = new NotificationService();\n\napp.post('/api/notifications', async (req, res) => {\n  const { userId, type, channels, template, data } = req.body;\n  \n  // Send to multiple channels\n  const results = await Promise.all(\n    channels.map(channel => \n      notificationService.sendNotification({\n        userId,\n        type,\n        channel,\n        template,\n        data,\n        priority: type === 'alert' ? 'critical' : 'normal'\n      })\n    )\n  );\n  \n  res.json({ results });\n});\n\n// Batch notification endpoint\napp.post('/api/notifications/batch', async (req, res) => {\n  const { userIds, type, channel, template, data } = req.body;\n  \n  // Process in chunks to avoid overwhelming queue\n  const chunkSize = 1000;\n  for (let i = 0; i < userIds.length; i += chunkSize) {\n    const chunk = userIds.slice(i, i + chunkSize);\n    \n    await Promise.all(\n      chunk.map(userId =>\n        notificationService.sendNotification({\n          userId, type, channel, template, data\n        })\n      )\n    );\n  }\n  \n  res.json({ status: 'processing', count: userIds.length });\n});\n\n// User preferences\napp.put('/api/users/:userId/notification-preferences', async (req, res) => {\n  const { userId } = req.params;\n  const preferences = req.body;\n  \n  await saveUserPreferences(userId, preferences);\n  res.json({ status: 'updated' });\n});"
    },
    {
      "id": 40,
      "question": "How would you design a Real-time Leaderboard system?",
      "answer": "A real-time leaderboard displays ranked users/players based on scores with instant updates.\n\nCore Requirements:\n• Real-time updates: Sub-second latency for score changes\n• Global rankings: Top N players worldwide\n• User rank lookup: Find any user's current position\n• Score updates: Handle millions of concurrent updates\n• Time-based: Daily, weekly, monthly, all-time leaderboards\n• Pagination: Fetch leaderboard pages efficiently\n\nData Structure (Redis Sorted Sets):\n• Key: leaderboard:{period} (e.g., leaderboard:daily:2026-02-12)\n• Members: User IDs\n• Scores: Player scores\n• Redis commands: ZADD, ZRANGE, ZREVRANGE, ZRANK, ZSCORE\n\nScalability Strategies:\n• Sharding: Partition by region or game mode\n• Caching: Cache top 100 in memory, refresh periodically\n• Write optimization: Batch score updates, rate limit updates\n• Read optimization: Precompute rankings for common queries\n• Approximate rankings: Use sampling for large leaderboards\n\nAdvanced Features:\n• Friends leaderboard: Show rankings among friends only\n• Percentile ranks: Show user is top 5%, 10%, etc.\n• Historical data: Track rank changes over time\n• Multiple dimensions: Score by different metrics (kills, wins, time)\n• Decay: Reduce old scores to encourage active play\n\nConsistency vs Performance:\n• Eventual consistency: Accept slight delays for better performance\n• Snapshot isolation: Leaderboard reflects recent state, not instant\n• Optimistic updates: Update UI immediately, reconcile later",
      "explanation": "Real-time leaderboards use Redis Sorted Sets for O(log N) rank updates and lookups, with periodic snapshots for top rankings and on-demand calculations for individual user positions.",
      "difficulty": "Hard",
      "code": "// Redis-based Leaderboard Implementation\nconst redis = require('redis');\nconst { promisify } = require('util');\n\nclass Leaderboard {\n  constructor(redisClient) {\n    this.client = redisClient;\n    this.zadd = promisify(redisClient.zadd).bind(redisClient);\n    this.zrevrange = promisify(redisClient.zrevrange).bind(redisClient);\n    this.zrevrank = promisify(redisClient.zrevrank).bind(redisClient);\n    this.zscore = promisify(redisClient.zscore).bind(redisClient);\n    this.zcard = promisify(redisClient.zcard).bind(redisClient);\n    this.zincrby = promisify(redisClient.zincrby).bind(redisClient);\n  }\n\n  // Get leaderboard key for time period\n  getKey(period = 'global') {\n    if (period === 'global') return 'leaderboard:global';\n    \n    const now = new Date();\n    if (period === 'daily') {\n      return `leaderboard:daily:${now.toISOString().split('T')[0]}`;\n    }\n    if (period === 'weekly') {\n      const week = this.getWeekNumber(now);\n      return `leaderboard:weekly:${now.getFullYear()}-W${week}`;\n    }\n    if (period === 'monthly') {\n      return `leaderboard:monthly:${now.getFullYear()}-${now.getMonth() + 1}`;\n    }\n  }\n\n  // Update user score\n  async updateScore(userId, score, period = 'global') {\n    const key = this.getKey(period);\n    await this.zadd(key, score, userId);\n    \n    // Set expiry for time-based leaderboards\n    if (period !== 'global') {\n      await this.client.expire(key, this.getTTL(period));\n    }\n    \n    return this.getUserRank(userId, period);\n  }\n\n  // Increment user score (for score deltas)\n  async incrementScore(userId, delta, period = 'global') {\n    const key = this.getKey(period);\n    const newScore = await this.zincrby(key, delta, userId);\n    return { score: parseFloat(newScore), rank: await this.getUserRank(userId, period) };\n  }\n\n  // Get top N players\n  async getTopPlayers(count = 10, period = 'global', offset = 0) {\n    const key = this.getKey(period);\n    \n    // Get users with scores (WITHSCORES)\n    const results = await this.zrevrange(\n      key, \n      offset, \n      offset + count - 1, \n      'WITHSCORES'\n    );\n    \n    // Parse results [userId, score, userId, score, ...]\n    const leaderboard = [];\n    for (let i = 0; i < results.length; i += 2) {\n      leaderboard.push({\n        rank: offset + (i / 2) + 1,\n        userId: results[i],\n        score: parseFloat(results[i + 1])\n      });\n    }\n    \n    return leaderboard;\n  }\n\n  // Get user's rank (1-indexed)\n  async getUserRank(userId, period = 'global') {\n    const key = this.getKey(period);\n    const rank = await this.zrevrank(key, userId);\n    \n    if (rank === null) return null;\n    return rank + 1; // Convert to 1-indexed\n  }\n\n  // Get user's score and rank\n  async getUserStats(userId, period = 'global') {\n    const key = this.getKey(period);\n    const [score, rank] = await Promise.all([\n      this.zscore(key, userId),\n      this.zrevrank(key, userId)\n    ]);\n    \n    if (score === null) return null;\n    \n    const totalPlayers = await this.zcard(key);\n    \n    return {\n      userId,\n      score: parseFloat(score),\n      rank: rank + 1,\n      totalPlayers,\n      percentile: ((1 - rank / totalPlayers) * 100).toFixed(2)\n    };\n  }\n\n  // Get users around a specific rank\n  async getNeighborhood(userId, period = 'global', radius = 5) {\n    const rank = await this.getUserRank(userId, period);\n    if (!rank) return null;\n    \n    const start = Math.max(0, rank - radius - 1);\n    const end = rank + radius - 1;\n    \n    const leaderboard = await this.getTopPlayers(\n      end - start + 1,\n      period,\n      start\n    );\n    \n    return {\n      user: leaderboard.find(entry => entry.userId === userId),\n      neighbors: leaderboard\n    };\n  }\n\n  // Friends leaderboard\n  async getFriendsLeaderboard(userId, friendIds, period = 'global') {\n    const key = this.getKey(period);\n    const allUsers = [userId, ...friendIds];\n    \n    // Batch get scores for all friends\n    const pipeline = this.client.pipeline();\n    allUsers.forEach(id => {\n      pipeline.zscore(key, id);\n    });\n    const scores = await pipeline.exec();\n    \n    // Build and sort leaderboard\n    const leaderboard = allUsers\n      .map((id, idx) => ({\n        userId: id,\n        score: parseFloat(scores[idx][1]) || 0\n      }))\n      .filter(entry => entry.score > 0)\n      .sort((a, b) => b.score - a.score)\n      .map((entry, idx) => ({\n        ...entry,\n        rank: idx + 1\n      }));\n    \n    return leaderboard;\n  }\n\n  // Batch update scores (for efficiency)\n  async batchUpdateScores(updates, period = 'global') {\n    const key = this.getKey(period);\n    const pipeline = this.client.pipeline();\n    \n    updates.forEach(({ userId, score }) => {\n      pipeline.zadd(key, score, userId);\n    });\n    \n    await pipeline.exec();\n  }\n\n  getTTL(period) {\n    if (period === 'daily') return 86400 * 2;      // 2 days\n    if (period === 'weekly') return 86400 * 14;    // 2 weeks\n    if (period === 'monthly') return 86400 * 60;   // 2 months\n    return 0;\n  }\n\n  getWeekNumber(date) {\n    const d = new Date(date);\n    d.setHours(0, 0, 0, 0);\n    d.setDate(d.getDate() + 4 - (d.getDay() || 7));\n    const yearStart = new Date(d.getFullYear(), 0, 1);\n    return Math.ceil((((d - yearStart) / 86400000) + 1) / 7);\n  }\n}\n\n// API Implementation\nconst express = require('express');\nconst app = express();\nconst leaderboard = new Leaderboard(redis.createClient());\n\n// Update score\napp.post('/api/leaderboard/score', async (req, res) => {\n  const { userId, score, period } = req.body;\n  \n  const rank = await leaderboard.updateScore(userId, score, period);\n  const stats = await leaderboard.getUserStats(userId, period);\n  \n  res.json(stats);\n});\n\n// Get leaderboard\napp.get('/api/leaderboard', async (req, res) => {\n  const { period = 'global', page = 1, pageSize = 10 } = req.query;\n  const offset = (page - 1) * pageSize;\n  \n  const topPlayers = await leaderboard.getTopPlayers(\n    parseInt(pageSize),\n    period,\n    offset\n  );\n  \n  res.json({\n    page: parseInt(page),\n    pageSize: parseInt(pageSize),\n    leaderboard: topPlayers\n  });\n});\n\n// Get user rank with neighborhood\napp.get('/api/leaderboard/user/:userId', async (req, res) => {\n  const { userId } = req.params;\n  const { period = 'global' } = req.query;\n  \n  const [stats, neighborhood] = await Promise.all([\n    leaderboard.getUserStats(userId, period),\n    leaderboard.getNeighborhood(userId, period, 5)\n  ]);\n  \n  res.json({ stats, neighborhood });\n});\n\n// Friends leaderboard\napp.get('/api/leaderboard/friends/:userId', async (req, res) => {\n  const { userId } = req.params;\n  const { period = 'global' } = req.query;\n  \n  const friendIds = await getFriendIds(userId);\n  const friendsBoard = await leaderboard.getFriendsLeaderboard(\n    userId,\n    friendIds,\n    period\n  );\n  \n  res.json(friendsBoard);\n});\n\n// Real-time updates with WebSockets\nconst WebSocket = require('ws');\nconst wss = new WebSocket.Server({ server });\n\nwss.on('connection', (ws, req) => {\n  const userId = req.url.split('userId=')[1];\n  \n  // Subscribe to score updates\n  const interval = setInterval(async () => {\n    const stats = await leaderboard.getUserStats(userId, 'global');\n    ws.send(JSON.stringify({ type: 'rank_update', data: stats }));\n  }, 5000); // Update every 5 seconds\n  \n  ws.on('close', () => clearInterval(interval));\n});"
    },
    {
      "id": 41,
      "question": "Design a URL Shortener (like bit.ly)",
      "answer": "A URL shortener converts long URLs into short, shareable links and redirects users to original URLs.\n\nCore Requirements:\n• Shorten URL: Generate unique short code for long URL\n• Redirect: Lookup short URL and redirect to original (301/302)\n• Custom aliases: Allow users to choose custom short codes\n• Analytics: Track clicks, referrers, geographic data\n• Expiration: Optional TTL for temporary links\n• High availability: 99.9%+ uptime for redirects\n\nShort Code Generation:\n• Base62 encoding: [a-zA-Z0-9] gives 62^n combinations (e.g., 62^7 = 3.5 trillion)\n• Counter-based: Auto-increment ID converted to base62\n• Hash-based: Hash URL and take first N characters (collision handling needed)\n• Random generation: Generate random string, check uniqueness\n\nDatabase Schema:\n• urls table: id, short_code, long_url, user_id, created_at, expires_at, click_count\n• Index on short_code for fast lookups\n• clicks table: id, short_code, timestamp, ip, user_agent, referrer, country\n\nScalability:\n• Read-heavy: 100:1 read-to-write ratio\n• Caching: Redis/Memcached for frequently accessed URLs (TTL-based)\n• CDN: Cache redirects at edge locations\n• Sharding: Partition by short_code range or hash\n• Rate limiting: Prevent abuse and spam\n\nAdvanced Features:\n• QR codes: Generate QR for short URL\n• Link preview: Show preview before redirect\n• Password protection: Require password to access\n• Branded domains: Custom domains for enterprises",
      "explanation": "URL shorteners generate unique short codes using base62 encoding of auto-increment IDs, store mappings in database with caching layer, and redirect users with 301/302 responses while tracking analytics.",
      "difficulty": "Medium",
      "code": "// URL Shortener Implementation\nconst express = require('express');\nconst crypto = require('crypto');\nconst redis = require('redis');\n\n// Base62 encoding for short codes\nconst BASE62 = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ';\n\nclass URLShortener {\n  constructor(db, cache) {\n    this.db = db;        // Database connection\n    this.cache = cache;  // Redis client\n  }\n\n  // Convert number to base62 string\n  toBase62(num) {\n    if (num === 0) return BASE62[0];\n    \n    let result = '';\n    while (num > 0) {\n      result = BASE62[num % 62] + result;\n      num = Math.floor(num / 62);\n    }\n    return result;\n  }\n\n  // Convert base62 string to number\n  fromBase62(str) {\n    let result = 0;\n    for (let i = 0; i < str.length; i++) {\n      result = result * 62 + BASE62.indexOf(str[i]);\n    }\n    return result;\n  }\n\n  // Shorten URL\n  async shortenURL(longUrl, customAlias = null, userId = null, expiresIn = null) {\n    // Validate URL\n    if (!this.isValidURL(longUrl)) {\n      throw new Error('Invalid URL');\n    }\n\n    let shortCode;\n\n    if (customAlias) {\n      // Check if custom alias available\n      const exists = await this.db.query(\n        'SELECT short_code FROM urls WHERE short_code = ?',\n        [customAlias]\n      );\n      \n      if (exists.length > 0) {\n        throw new Error('Alias already taken');\n      }\n      \n      shortCode = customAlias;\n    } else {\n      // Generate short code from auto-increment ID\n      const result = await this.db.query(\n        'INSERT INTO urls (long_url, user_id, created_at) VALUES (?, ?, NOW())',\n        [longUrl, userId]\n      );\n      \n      const id = result.insertId;\n      shortCode = this.toBase62(id);\n      \n      // Update with generated short code\n      await this.db.query(\n        'UPDATE urls SET short_code = ? WHERE id = ?',\n        [shortCode, id]\n      );\n    }\n\n    // Set expiration if provided\n    if (expiresIn) {\n      await this.db.query(\n        'UPDATE urls SET expires_at = DATE_ADD(NOW(), INTERVAL ? SECOND) WHERE short_code = ?',\n        [expiresIn, shortCode]\n      );\n    }\n\n    // Cache the mapping\n    await this.cache.setex(\n      `url:${shortCode}`,\n      3600, // 1 hour TTL\n      longUrl\n    );\n\n    return {\n      shortCode,\n      shortUrl: `${process.env.BASE_URL}/${shortCode}`,\n      longUrl\n    };\n  }\n\n  // Get original URL\n  async getOriginalURL(shortCode) {\n    // Try cache first\n    const cached = await this.cache.get(`url:${shortCode}`);\n    if (cached) {\n      return cached;\n    }\n\n    // Query database\n    const results = await this.db.query(\n      `SELECT long_url, expires_at \n       FROM urls \n       WHERE short_code = ? \n       AND (expires_at IS NULL OR expires_at > NOW())`,\n      [shortCode]\n    );\n\n    if (results.length === 0) {\n      return null;\n    }\n\n    const longUrl = results[0].long_url;\n\n    // Cache for future requests\n    await this.cache.setex(`url:${shortCode}`, 3600, longUrl);\n\n    return longUrl;\n  }\n\n  // Track click\n  async trackClick(shortCode, metadata) {\n    const { ip, userAgent, referrer } = metadata;\n    \n    // Increment counter\n    await this.db.query(\n      'UPDATE urls SET click_count = click_count + 1 WHERE short_code = ?',\n      [shortCode]\n    );\n\n    // Log click details\n    await this.db.query(\n      `INSERT INTO clicks (short_code, ip, user_agent, referrer, country, timestamp)\n       VALUES (?, ?, ?, ?, ?, NOW())`,\n      [shortCode, ip, userAgent, referrer, this.getCountry(ip)]\n    );\n  }\n\n  // Get analytics\n  async getAnalytics(shortCode, userId = null) {\n    // Verify ownership\n    if (userId) {\n      const url = await this.db.query(\n        'SELECT user_id FROM urls WHERE short_code = ?',\n        [shortCode]\n      );\n      \n      if (url.length === 0 || url[0].user_id !== userId) {\n        throw new Error('Unauthorized');\n      }\n    }\n\n    // Get URL info\n    const [urlInfo] = await this.db.query(\n      'SELECT long_url, created_at, click_count FROM urls WHERE short_code = ?',\n      [shortCode]\n    );\n\n    // Get click timeline (last 30 days)\n    const timeline = await this.db.query(\n      `SELECT DATE(timestamp) as date, COUNT(*) as clicks\n       FROM clicks\n       WHERE short_code = ? AND timestamp >= DATE_SUB(NOW(), INTERVAL 30 DAY)\n       GROUP BY DATE(timestamp)\n       ORDER BY date`,\n      [shortCode]\n    );\n\n    // Get referrer stats\n    const referrers = await this.db.query(\n      `SELECT referrer, COUNT(*) as count\n       FROM clicks\n       WHERE short_code = ? AND referrer IS NOT NULL\n       GROUP BY referrer\n       ORDER BY count DESC\n       LIMIT 10`,\n      [shortCode]\n    );\n\n    // Get country distribution\n    const countries = await this.db.query(\n      `SELECT country, COUNT(*) as count\n       FROM clicks\n       WHERE short_code = ?\n       GROUP BY country\n       ORDER BY count DESC\n       LIMIT 10`,\n      [shortCode]\n    );\n\n    return {\n      shortCode,\n      longUrl: urlInfo.long_url,\n      createdAt: urlInfo.created_at,\n      totalClicks: urlInfo.click_count,\n      timeline,\n      topReferrers: referrers,\n      topCountries: countries\n    };\n  }\n\n  isValidURL(url) {\n    try {\n      new URL(url);\n      return true;\n    } catch {\n      return false;\n    }\n  }\n\n  getCountry(ip) {\n    // Use GeoIP library or service\n    // Placeholder implementation\n    return 'US';\n  }\n}\n\n// Express API\nconst app = express();\nconst shortener = new URLShortener(db, redis.createClient());\n\n// Shorten URL\napp.post('/api/shorten', async (req, res) => {\n  try {\n    const { url, customAlias, expiresIn } = req.body;\n    const userId = req.user?.id;\n    \n    const result = await shortener.shortenURL(url, customAlias, userId, expiresIn);\n    \n    res.json(result);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Redirect endpoint\napp.get('/:shortCode', async (req, res) => {\n  const { shortCode } = req.params;\n  \n  const longUrl = await shortener.getOriginalURL(shortCode);\n  \n  if (!longUrl) {\n    return res.status(404).send('URL not found or expired');\n  }\n  \n  // Track click asynchronously\n  shortener.trackClick(shortCode, {\n    ip: req.ip,\n    userAgent: req.headers['user-agent'],\n    referrer: req.headers['referer']\n  }).catch(console.error);\n  \n  // Redirect (302 for temporary, 301 for permanent)\n  res.redirect(302, longUrl);\n});\n\n// Get analytics\napp.get('/api/analytics/:shortCode', async (req, res) => {\n  try {\n    const { shortCode } = req.params;\n    const userId = req.user?.id;\n    \n    const analytics = await shortener.getAnalytics(shortCode, userId);\n    \n    res.json(analytics);\n  } catch (error) {\n    res.status(403).json({ error: error.message });\n  }\n});\n\n// Database Schema\nconst schema = `\nCREATE TABLE urls (\n  id BIGINT PRIMARY KEY AUTO_INCREMENT,\n  short_code VARCHAR(10) UNIQUE NOT NULL,\n  long_url TEXT NOT NULL,\n  user_id BIGINT,\n  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  expires_at TIMESTAMP NULL,\n  click_count BIGINT DEFAULT 0,\n  INDEX idx_short_code (short_code),\n  INDEX idx_user_id (user_id)\n);\n\nCREATE TABLE clicks (\n  id BIGINT PRIMARY KEY AUTO_INCREMENT,\n  short_code VARCHAR(10) NOT NULL,\n  ip VARCHAR(45),\n  user_agent TEXT,\n  referrer TEXT,\n  country VARCHAR(2),\n  timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n  INDEX idx_short_code (short_code),\n  INDEX idx_timestamp (timestamp)\n);\n`;"
    },
    {
      "id": 42,
      "question": "How do you design a Distributed Task Scheduler?",
      "answer": "A distributed task scheduler executes jobs across multiple workers with reliability and scalability.\n\nCore Components:\n• Task Queue: Store pending tasks (Redis, RabbitMQ, Kafka)\n• Worker Nodes: Execute tasks from queue\n• Scheduler: Triggers recurring/scheduled tasks\n• Metadata Store: Task definitions, status, history\n• Coordinator: Assigns tasks to workers, handles failures\n\nTask Types:\n• One-time: Execute once immediately or at specific time\n• Recurring: Cron-like schedules (daily, weekly, custom)\n• Delayed: Execute after delay (e.g., retry after 5 minutes)\n• Dependent: Execute after other tasks complete (DAG)\n\nKey Features:\n• Priority queue: High-priority tasks executed first\n• Retry logic: Automatic retry with exponential backoff\n• Timeout: Kill long-running tasks\n• Concurrency control: Limit concurrent executions per task type\n• At-least-once delivery: Tasks executed at least once (idempotency required)\n• Task chaining: Wire tasks together in workflows\n• Dead letter queue: Store failed tasks for investigation\n\nDistributed Challenges:\n• Leader election: Single scheduler for cron tasks (ZooKeeper, etcd)\n• Worker discovery: Service registry to track available workers\n• Load balancing: Distribute tasks evenly across workers\n• Graceful shutdown: Complete in-progress tasks before shutdown\n• Clock skew: Use logical timestamps for ordering\n\nPopular Solutions:\n• Celery (Python): Redis/RabbitMQ backend\n• Apache Airflow: DAG-based workflows\n• Kubernetes CronJobs: Container-native scheduling\n• AWS SQS + Lambda: Serverless task execution",
      "explanation": "Distributed task schedulers use message queues for task distribution, worker pools for execution, and leader election for cron scheduling, with retry logic and dead letter queues for reliability.",
      "difficulty": "Hard",
      "code": "// Distributed Task Scheduler Implementation\nconst { Queue, Worker } = require('bullmq');\nconst Redis = require('ioredis');\nconst { CronJob } = require('cron');\n\n// Task Scheduler Service\nclass TaskScheduler {\n  constructor(redisConfig) {\n    this.connection = new Redis(redisConfig);\n    this.queues = new Map();\n    this.workers = new Map();\n    this.scheduledJobs = new Map();\n  }\n\n  // Register task queue\n  registerQueue(queueName, options = {}) {\n    const queue = new Queue(queueName, {\n      connection: this.connection,\n      defaultJobOptions: {\n        attempts: 3,\n        backoff: {\n          type: 'exponential',\n          delay: 2000\n        },\n        removeOnComplete: 100,  // Keep last 100 completed\n        removeOnFail: 500,      // Keep last 500 failed\n        ...options\n      }\n    });\n    \n    this.queues.set(queueName, queue);\n    return queue;\n  }\n\n  // Register worker to process tasks\n  registerWorker(queueName, processor, options = {}) {\n    const worker = new Worker(\n      queueName,\n      async (job) => {\n        console.log(`Processing job ${job.id} from ${queueName}`);\n        return await processor(job.data);\n      },\n      {\n        connection: this.connection,\n        concurrency: options.concurrency || 5,\n        ...options\n      }\n    );\n\n    // Event handlers\n    worker.on('completed', (job, result) => {\n      console.log(`Job ${job.id} completed:`, result);\n    });\n\n    worker.on('failed', (job, error) => {\n      console.error(`Job ${job.id} failed:`, error.message);\n      \n      // Move to dead letter queue if max retries exceeded\n      if (job.attemptsMade >= job.opts.attempts) {\n        this.handleDeadLetter(queueName, job, error);\n      }\n    });\n\n    worker.on('error', (error) => {\n      console.error('Worker error:', error);\n    });\n\n    this.workers.set(queueName, worker);\n    return worker;\n  }\n\n  // Add one-time task\n  async addTask(queueName, taskData, options = {}) {\n    const queue = this.queues.get(queueName);\n    if (!queue) {\n      throw new Error(`Queue ${queueName} not registered`);\n    }\n\n    const job = await queue.add(\n      taskData.name || 'task',\n      taskData,\n      {\n        priority: options.priority || 0,\n        delay: options.delay,\n        jobId: options.jobId,  // For deduplication\n        ...options\n      }\n    );\n\n    return job.id;\n  }\n\n  // Schedule recurring task (cron)\n  scheduleRecurring(queueName, cronExpression, taskData, options = {}) {\n    const jobId = `${queueName}:${taskData.name}:${Date.now()}`;\n    \n    const cronJob = new CronJob(\n      cronExpression,\n      async () => {\n        await this.addTask(queueName, taskData, options);\n      },\n      null,\n      true,  // Start immediately\n      options.timezone || 'UTC'\n    );\n\n    this.scheduledJobs.set(jobId, cronJob);\n    \n    return jobId;\n  }\n\n  // Add delayed task\n  async addDelayedTask(queueName, taskData, delayMs) {\n    return await this.addTask(queueName, taskData, { delay: delayMs });\n  }\n\n  // Create task chain (workflow)\n  async createWorkflow(queueName, tasks) {\n    const queue = this.queues.get(queueName);\n    \n    // Create flow (task dependencies)\n    const flow = await queue.addBulk(\n      tasks.map((task, index) => ({\n        name: task.name,\n        data: task.data,\n        opts: {\n          ...task.options,\n          // Link to previous task\n          parent: index > 0 ? {\n            id: tasks[index - 1].id,\n            queue: queueName\n          } : undefined\n        }\n      }))\n    );\n\n    return flow;\n  }\n\n  // Get job status\n  async getJobStatus(queueName, jobId) {\n    const queue = this.queues.get(queueName);\n    const job = await queue.getJob(jobId);\n    \n    if (!job) return null;\n    \n    return {\n      id: job.id,\n      name: job.name,\n      data: job.data,\n      state: await job.getState(),\n      progress: job.progress,\n      attemptsMade: job.attemptsMade,\n      finishedOn: job.finishedOn,\n      processedOn: job.processedOn,\n      failedReason: job.failedReason\n    };\n  }\n\n  // Handle dead letter\n  async handleDeadLetter(queueName, job, error) {\n    const dlqName = `${queueName}:dlq`;\n    \n    if (!this.queues.has(dlqName)) {\n      this.registerQueue(dlqName);\n    }\n    \n    await this.addTask(dlqName, {\n      originalJob: job.toJSON(),\n      error: error.message,\n      failedAt: new Date()\n    });\n  }\n\n  // Graceful shutdown\n  async shutdown() {\n    console.log('Shutting down scheduler...');\n    \n    // Stop all cron jobs\n    this.scheduledJobs.forEach(job => job.stop());\n    \n    // Close all workers\n    await Promise.all(\n      Array.from(this.workers.values()).map(worker => worker.close())\n    );\n    \n    // Close all queues\n    await Promise.all(\n      Array.from(this.queues.values()).map(queue => queue.close())\n    );\n    \n    await this.connection.quit();\n    \n    console.log('Shutdown complete');\n  }\n}\n\n// Example Task Processors\nconst taskProcessors = {\n  // Send email task\n  sendEmail: async (data) => {\n    const { to, subject, body } = data;\n    console.log(`Sending email to ${to}...`);\n    \n    // Simulate email sending\n    await new Promise(resolve => setTimeout(resolve, 1000));\n    \n    return { sent: true, messageId: Math.random().toString(36) };\n  },\n\n  // Process payment\n  processPayment: async (data) => {\n    const { orderId, amount, currency } = data;\n    console.log(`Processing payment: ${amount} ${currency}`);\n    \n    // Call payment gateway\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    return { \n      orderId, \n      transactionId: 'txn_' + Math.random().toString(36),\n      status: 'completed' \n    };\n  },\n\n  // Generate report\n  generateReport: async (data) => {\n    const { reportType, startDate, endDate } = data;\n    console.log(`Generating ${reportType} report...`);\n    \n    // Heavy computation\n    await new Promise(resolve => setTimeout(resolve, 5000));\n    \n    return { \n      reportUrl: `https://example.com/reports/${Date.now()}.pdf`,\n      rowCount: 150000\n    };\n  },\n\n  // Data sync\n  syncData: async (data) => {\n    const { source, destination } = data;\n    console.log(`Syncing from ${source} to ${destination}...`);\n    \n    await new Promise(resolve => setTimeout(resolve, 3000));\n    \n    return { syncedRecords: 10000 };\n  }\n};\n\n// Initialize Scheduler\nconst scheduler = new TaskScheduler({\n  host: 'localhost',\n  port: 6379\n});\n\n// Register queues and workers\nscheduler.registerQueue('emails', { priority: 10 });\nscheduler.registerWorker('emails', taskProcessors.sendEmail, { concurrency: 10 });\n\nscheduler.registerQueue('payments', { priority: 5 });\nscheduler.registerWorker('payments', taskProcessors.processPayment, { concurrency: 5 });\n\nscheduler.registerQueue('reports', { priority: 1 });\nscheduler.registerWorker('reports', taskProcessors.generateReport, { concurrency: 2 });\n\n// API Usage Examples\nconst express = require('express');\nconst app = express();\n\n// Submit task\napp.post('/api/tasks', async (req, res) => {\n  const { queue, taskData, options } = req.body;\n  \n  const jobId = await scheduler.addTask(queue, taskData, options);\n  \n  res.json({ jobId });\n});\n\n// Schedule recurring task\napp.post('/api/tasks/schedule', async (req, res) => {\n  const { queue, cron, taskData } = req.body;\n  \n  const scheduleId = scheduler.scheduleRecurring(queue, cron, taskData);\n  \n  res.json({ scheduleId });\n});\n\n// Get task status\napp.get('/api/tasks/:queue/:jobId', async (req, res) => {\n  const { queue, jobId } = req.params;\n  \n  const status = await scheduler.getJobStatus(queue, jobId);\n  \n  if (!status) {\n    return res.status(404).json({ error: 'Job not found' });\n  }\n  \n  res.json(status);\n});\n\n// Example: Schedule daily report at 2 AM\nscheduler.scheduleRecurring(\n  'reports',\n  '0 2 * * *',  // Every day at 2 AM\n  {\n    name: 'daily-sales-report',\n    reportType: 'sales',\n    startDate: 'yesterday',\n    endDate: 'today'\n  },\n  { timezone: 'America/New_York' }\n);\n\n// Example: Delayed task (retry after 5 minutes)\nscheduler.addDelayedTask(\n  'emails',\n  {\n    name: 'retry-email',\n    to: 'user@example.com',\n    subject: 'Retry notification',\n    body: 'This is a retry'\n  },\n  5 * 60 * 1000  // 5 minutes\n);\n\n// Graceful shutdown\nprocess.on('SIGTERM', async () => {\n  await scheduler.shutdown();\n  process.exit(0);\n});"
    },
    {
      "id": 43,
      "question": "Design a Search Autocomplete System",
      "answer": "An autocomplete system provides real-time query suggestions as users type in search boxes.\n\nCore Requirements:\n• Low latency: < 100ms response time\n• Relevance: Suggest most popular/relevant queries\n• Prefix matching: Match beginning of words\n• Personalization: Consider user history and context\n• Grammar: Handle typos and misspellings\n• Scale: Handle millions of queries per second\n\nData Structures:\n• Trie (Prefix Tree): Efficient for prefix lookups, O(k) where k = query length\n• Each node stores character + frequency/score\n• Top-K suggestions stored at each node for speed\n• Space-optimized: Use hash maps for sparse nodes\n\nRanking Factors:\n• Query frequency: How often users search this\n• Click-through rate: How often selected when suggested\n• Recency: Recent trending queries ranked higher\n• Personalization: User's search history\n• Location: Geographic relevance\n• Time: Time-based trends (e.g., holiday queries)\n\nOptimizations:\n• Caching: Cache popular prefixes in Redis\n• Pre-computation: Pre-calculate top suggestions for prefixes\n• Async updates: Update trie asynchronously from logs\n• Sharding: Partition trie by language or first character\n• CDN: Serve common suggestions from edge\n\nAdvanced Features:\n• Fuzzy matching: Levenshtein distance for typos\n• Multi-word: Suggest completions for phrases\n• Category filtering: Suggest within products, users, etc.\n• Rich results: Include thumbnails, descriptions",
      "explanation": "Autocomplete systems use Trie data structures for fast prefix matching with top-K suggestions cached at nodes, ranked by frequency, recency, and personalization, with Redis caching for popular prefixes.",
      "difficulty": "Hard",
      "code": "// Trie-based Autocomplete System\nclass TrieNode {\n  constructor() {\n    this.children = new Map();     // Character -> TrieNode\n    this.isEndOfWord = false;\n    this.frequency = 0;            // Query frequency\n    this.topSuggestions = [];      // Pre-computed top K suggestions\n  }\n}\n\nclass AutocompleteSystem {\n  constructor(k = 10) {\n    this.root = new TrieNode();\n    this.k = k;  // Number of suggestions to return\n  }\n\n  // Insert query with frequency\n  insert(query, frequency = 1) {\n    let node = this.root;\n    query = query.toLowerCase();\n\n    for (const char of query) {\n      if (!node.children.has(char)) {\n        node.children.set(char, new TrieNode());\n      }\n      node = node.children.get(char);\n    }\n\n    node.isEndOfWord = true;\n    node.frequency += frequency;\n    \n    // Update top suggestions along the path\n    this.updateTopSuggestions(query);\n  }\n\n  // Update top K suggestions for all prefixes of query\n  updateTopSuggestions(query) {\n    let node = this.root;\n    \n    for (let i = 0; i < query.length; i++) {\n      const char = query[i];\n      node = node.children.get(char);\n      \n      // Recompute top K for this node\n      const suggestions = this.getAllSuggestions(node);\n      node.topSuggestions = suggestions\n        .sort((a, b) => b.frequency - a.frequency || a.query.localeCompare(b.query))\n        .slice(0, this.k);\n    }\n  }\n\n  // Get all queries under a node\n  getAllSuggestions(node, prefix = '') {\n    const suggestions = [];\n    \n    if (node.isEndOfWord) {\n      suggestions.push({ query: prefix, frequency: node.frequency });\n    }\n    \n    for (const [char, childNode] of node.children) {\n      suggestions.push(\n        ...this.getAllSuggestions(childNode, prefix + char)\n      );\n    }\n    \n    return suggestions;\n  }\n\n  // Get autocomplete suggestions for prefix\n  search(prefix) {\n    let node = this.root;\n    prefix = prefix.toLowerCase();\n    \n    // Navigate to prefix\n    for (const char of prefix) {\n      if (!node.children.has(char)) {\n        return [];  // Prefix not found\n      }\n      node = node.children.get(char);\n    }\n    \n    // Return pre-computed top K\n    return node.topSuggestions.map(s => s.query);\n  }\n\n  // Search with DFS (without pre-computation)\n  searchDFS(prefix) {\n    let node = this.root;\n    prefix = prefix.toLowerCase();\n    \n    for (const char of prefix) {\n      if (!node.children.has(char)) {\n        return [];\n      }\n      node = node.children.get(char);\n    }\n    \n    const suggestions = this.getAllSuggestions(node, prefix);\n    \n    return suggestions\n      .sort((a, b) => b.frequency - a.frequency)\n      .slice(0, this.k)\n      .map(s => s.query);\n  }\n\n  // Fuzzy search (with typo tolerance)\n  fuzzySearch(prefix, maxDistance = 1) {\n    const suggestions = new Map();\n    this.fuzzySearchHelper(this.root, '', prefix, 0, maxDistance, suggestions);\n    \n    return Array.from(suggestions.entries())\n      .sort((a, b) => b[1] - a[1])\n      .slice(0, this.k)\n      .map(entry => entry[0]);\n  }\n\n  fuzzySearchHelper(node, current, target, index, maxDist, suggestions) {\n    // If we've processed all target characters\n    if (index === target.length) {\n      if (node.isEndOfWord) {\n        suggestions.set(current, node.frequency);\n      }\n      // Continue exploring for insertions at end\n      for (const [char, child] of node.children) {\n        if (maxDist > 0) {\n          this.fuzzySearchHelper(child, current + char, target, index, maxDist - 1, suggestions);\n        }\n      }\n      return;\n    }\n\n    const targetChar = target[index];\n\n    // Exact match\n    if (node.children.has(targetChar)) {\n      this.fuzzySearchHelper(\n        node.children.get(targetChar),\n        current + targetChar,\n        target,\n        index + 1,\n        maxDist,\n        suggestions\n      );\n    }\n\n    if (maxDist > 0) {\n      // Substitution\n      for (const [char, child] of node.children) {\n        if (char !== targetChar) {\n          this.fuzzySearchHelper(\n            child,\n            current + char,\n            target,\n            index + 1,\n            maxDist - 1,\n            suggestions\n          );\n        }\n      }\n\n      // Deletion (skip character in target)\n      this.fuzzySearchHelper(\n        node,\n        current,\n        target,\n        index + 1,\n        maxDist - 1,\n        suggestions\n      );\n\n      // Insertion (add character from trie)\n      for (const [char, child] of node.children) {\n        this.fuzzySearchHelper(\n          child,\n          current + char,\n          target,\n          index,\n          maxDist - 1,\n          suggestions\n        );\n      }\n    }\n  }\n}\n\n// Distributed Autocomplete with Redis\nconst redis = require('redis');\nconst client = redis.createClient();\n\nclass DistributedAutocomplete {\n  constructor() {\n    this.trie = new AutocompleteSystem();\n    this.cachePrefix = 'autocomplete:';\n    this.cacheTTL = 3600; // 1 hour\n  }\n\n  // Search with caching\n  async search(prefix) {\n    const cacheKey = `${this.cachePrefix}${prefix.toLowerCase()}`;\n    \n    // Try cache first\n    const cached = await client.get(cacheKey);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n    \n    // Compute from trie\n    const suggestions = this.trie.search(prefix);\n    \n    // Cache result\n    await client.setex(\n      cacheKey,\n      this.cacheTTL,\n      JSON.stringify(suggestions)\n    );\n    \n    return suggestions;\n  }\n\n  // Batch update from query logs\n  async updateFromLogs(queryLogs) {\n    // queryLogs = [{ query: 'apple', count: 150 }, ...]\n    \n    for (const { query, count } of queryLogs) {\n      this.trie.insert(query, count);\n    }\n    \n    // Invalidate affected caches\n    const keys = await client.keys(`${this.cachePrefix}*`);\n    if (keys.length > 0) {\n      await client.del(...keys);\n    }\n  }\n\n  // Get trending queries\n  async getTrending(limit = 10) {\n    const key = 'trending:queries';\n    const trending = await client.zrevrange(key, 0, limit - 1, 'WITHSCORES');\n    \n    const results = [];\n    for (let i = 0; i < trending.length; i += 2) {\n      results.push({\n        query: trending[i],\n        score: parseFloat(trending[i + 1])\n      });\n    }\n    \n    return results;\n  }\n\n  // Track query\n  async trackQuery(query) {\n    const now = Date.now();\n    const hourKey = `queries:${Math.floor(now / 3600000)}`;\n    \n    // Increment query count for current hour\n    await client.zincrby(hourKey, 1, query.toLowerCase());\n    await client.expire(hourKey, 7200); // 2 hours TTL\n    \n    // Update trending (time-decayed score)\n    const trendingKey = 'trending:queries';\n    const decayFactor = 0.95;\n    const currentScore = await client.zscore(trendingKey, query) || 0;\n    const newScore = currentScore * decayFactor + 1;\n    \n    await client.zadd(trendingKey, newScore, query.toLowerCase());\n  }\n}\n\n// Express API\nconst express = require('express');\nconst app = express();\nconst autocomplete = new DistributedAutocomplete();\n\n// Autocomplete endpoint\napp.get('/api/autocomplete', async (req, res) => {\n  const { q, fuzzy } = req.query;\n  \n  if (!q || q.length < 2) {\n    return res.json([]);\n  }\n  \n  let suggestions;\n  \n  if (fuzzy === 'true') {\n    suggestions = autocomplete.trie.fuzzySearch(q);\n  } else {\n    suggestions = await autocomplete.search(q);\n  }\n  \n  res.json(suggestions);\n});\n\n// Track query\napp.post('/api/track-query', async (req, res) => {\n  const { query } = req.body;\n  \n  await autocomplete.trackQuery(query);\n  \n  res.json({ status: 'tracked' });\n});\n\n// Get trending\napp.get('/api/trending', async (req, res) => {\n  const trending = await autocomplete.getTrending(10);\n  res.json(trending);\n});\n\n// Build index from historical data\nasync function buildIndex() {\n  const queryLogs = [\n    { query: 'apple watch', count: 15000 },\n    { query: 'apple iphone', count: 25000 },\n    { query: 'apple macbook', count: 12000 },\n    { query: 'apple airpods', count: 18000 },\n    { query: 'samsung galaxy', count: 20000 },\n    { query: 'samsung tv', count: 8000 }\n    // ... load from database/logs\n  ];\n  \n  await autocomplete.updateFromLogs(queryLogs);\n  console.log('Index built successfully');\n}\n\nbuildIndex();"
    },
    {
      "id": 44,
      "question": "Design a News Feed System (like Facebook, Twitter)",
      "answer": "A news feed aggregates and displays content from followed users/pages in reverse chronological or ranked order.\n\nCore Requirements:\n• Real-time updates: New posts appear quickly\n• Personalized: Different users see different feeds\n• Scalability: Millions of users, billions of posts\n• Engagement: Show relevant, engaging content first\n• Pagination: Infinite scroll with cursor-based pagination\n\nArchitecture Components:\n• Post Service: Create, store posts\n• Follow Service: Manage user relationships\n• Feed Generator: Create and rank feed\n• Feed Storage: Cache pre-built feeds (Redis)\n• Fanout Service: Distribute posts to followers' feeds\n• Ranking Service: Score posts by relevance\n\nFeed Generation Approaches:\n• Push (Fanout on Write): Pre-build feeds when post created, fast reads, slow writes\n• Pull (Fanout on Read): Build feed on request, slow reads, fast writes\n• Hybrid: Push for active users, pull for inactive/celebrities\n\nFanout Process:\n• User creates post → Post stored in database\n• Get follower list (cached)\n• For each follower: Insert post ID into their feed cache\n• Use message queue for async processing\n• Batch insertions for efficiency\n\nRanking Algorithms:\n• Chronological: Simple, predictable\n• Engagement-based: Likes, comments, shares\n• ML-based: Personalized model per user\n• Time decay: Recent posts scored higher\n• Content type: Video, images ranked higher\n• Social proof: Friends' interactions boost score\n\nOptimizations:\n• Feed cache: Store recent feed in Redis sorted set\n• Celebrity problem: Don't fanout for users with millions of followers, pull instead\n• Hot cache: Keep active users' feeds in memory\n• Lazy loading: Load older posts on demand",
      "explanation": "News feeds use hybrid fanout strategy: push posts to active followers' Redis caches on write, pull from database for inactive users, with ML-based ranking considering engagement, recency, and personalization.",
      "difficulty": "Hard",
      "code": "// News Feed System Implementation\nconst redis = require('redis');\nconst { Kafka } = require('kafkajs');\n\n// Post Service\nclass PostService {\n  constructor(db) {\n    this.db = db;\n  }\n\n  async createPost(userId, content, mediaUrls = []) {\n    const post = {\n      id: this.generateId(),\n      userId,\n      content,\n      mediaUrls,\n      createdAt: Date.now(),\n      likes: 0,\n      comments: 0,\n      shares: 0\n    };\n\n    // Store post\n    await this.db.query(\n      'INSERT INTO posts (id, user_id, content, media_urls, created_at) VALUES (?, ?, ?, ?, ?)',\n      [post.id, userId, content, JSON.stringify(mediaUrls), new Date(post.createdAt)]\n    );\n\n    return post;\n  }\n\n  async getPost(postId) {\n    const results = await this.db.query(\n      'SELECT * FROM posts WHERE id = ?',\n      [postId]\n    );\n    return results[0];\n  }\n\n  async incrementEngagement(postId, type) {\n    // type: 'likes', 'comments', 'shares'\n    await this.db.query(\n      `UPDATE posts SET ${type} = ${type} + 1 WHERE id = ?`,\n      [postId]\n    );\n  }\n\n  generateId() {\n    return `post_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n}\n\n// Follow Service\nclass FollowService {\n  constructor(db, cache) {\n    this.db = db;\n    this.cache = cache;  // Redis client\n  }\n\n  async follow(followerId, followeeId) {\n    await this.db.query(\n      'INSERT INTO follows (follower_id, followee_id, created_at) VALUES (?, ?, NOW())',\n      [followerId, followeeId]\n    );\n\n    // Invalidate cache\n    await this.cache.del(`following:${followerId}`);\n    await this.cache.del(`followers:${followeeId}`);\n  }\n\n  async getFollowers(userId) {\n    const cacheKey = `followers:${userId}`;\n    \n    // Try cache\n    const cached = await this.cache.get(cacheKey);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n\n    // Query database\n    const results = await this.db.query(\n      'SELECT follower_id FROM follows WHERE followee_id = ?',\n      [userId]\n    );\n\n    const followers = results.map(r => r.follower_id);\n\n    // Cache for 1 hour\n    await this.cache.setex(cacheKey, 3600, JSON.stringify(followers));\n\n    return followers;\n  }\n\n  async getFollowing(userId) {\n    const cacheKey = `following:${userId}`;\n    \n    const cached = await this.cache.get(cacheKey);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n\n    const results = await this.db.query(\n      'SELECT followee_id FROM follows WHERE follower_id = ?',\n      [userId]\n    );\n\n    const following = results.map(r => r.followee_id);\n    await this.cache.setex(cacheKey, 3600, JSON.stringify(following));\n\n    return following;\n  }\n\n  async getFollowerCount(userId) {\n    const results = await this.db.query(\n      'SELECT COUNT(*) as count FROM follows WHERE followee_id = ?',\n      [userId]\n    );\n    return results[0].count;\n  }\n}\n\n// Fanout Service\nclass FanoutService {\n  constructor(followService, cache, kafka) {\n    this.followService = followService;\n    this.cache = cache;\n    this.kafka = kafka;\n    this.producer = kafka.producer();\n    this.CELEBRITY_THRESHOLD = 1000000;  // 1M followers\n  }\n\n  async fanoutPost(post) {\n    const followerCount = await this.followService.getFollowerCount(post.userId);\n\n    // Celebrity: don't fanout, use pull model\n    if (followerCount > this.CELEBRITY_THRESHOLD) {\n      console.log(`User ${post.userId} is celebrity, skipping fanout`);\n      return;\n    }\n\n    // Send to Kafka for async processing\n    await this.producer.send({\n      topic: 'post-fanout',\n      messages: [{\n        key: post.userId,\n        value: JSON.stringify(post)\n      }]\n    });\n  }\n\n  // Fanout worker (Kafka consumer)\n  async processFanout(post) {\n    const followers = await this.followService.getFollowers(post.userId);\n\n    // Batch fanout\n    const batchSize = 1000;\n    for (let i = 0; i < followers.length; i += batchSize) {\n      const batch = followers.slice(i, i + batchSize);\n      await this.batchInsertToFeeds(batch, post);\n    }\n\n    console.log(`Fanned out post ${post.id} to ${followers.length} followers`);\n  }\n\n  async batchInsertToFeeds(userIds, post) {\n    const pipeline = this.cache.pipeline();\n\n    userIds.forEach(userId => {\n      const feedKey = `feed:${userId}`;\n      \n      // Add to sorted set (score = timestamp)\n      pipeline.zadd(feedKey, post.createdAt, post.id);\n      \n      // Keep only recent 1000 posts\n      pipeline.zremrangebyrank(feedKey, 0, -1001);\n    });\n\n    await pipeline.exec();\n  }\n}\n\n// Feed Service\nclass FeedService {\n  constructor(postService, followService, cache, db) {\n    this.postService = postService;\n    this.followService = followService;\n    this.cache = cache;\n    this.db = db;\n  }\n\n  // Get user's feed (hybrid approach)\n  async getFeed(userId, limit = 20, cursor = null) {\n    const feedKey = `feed:${userId}`;\n    \n    // Try push feed first (from cache)\n    let postIds = await this.getCachedFeed(feedKey, limit, cursor);\n\n    // If cache miss or celebrity following, pull from database\n    if (postIds.length < limit) {\n      const pulledPosts = await this.pullFeed(userId, limit);\n      postIds = [...postIds, ...pulledPosts.map(p => p.id)].slice(0, limit);\n    }\n\n    // Hydrate posts\n    const posts = await this.hydratePosts(postIds);\n\n    // Rank posts\n    const rankedPosts = await this.rankPosts(userId, posts);\n\n    return {\n      posts: rankedPosts,\n      nextCursor: rankedPosts.length > 0 \n        ? rankedPosts[rankedPosts.length - 1].createdAt \n        : null\n    };\n  }\n\n  async getCachedFeed(feedKey, limit, cursor) {\n    const maxScore = cursor || Date.now();\n    \n    // Get post IDs from sorted set\n    const postIds = await this.cache.zrevrangebyscore(\n      feedKey,\n      maxScore,\n      '-inf',\n      'LIMIT',\n      0,\n      limit\n    );\n\n    return postIds;\n  }\n\n  // Pull feed from database (for cache miss or celebrities)\n  async pullFeed(userId, limit) {\n    const following = await this.followService.getFollowing(userId);\n\n    if (following.length === 0) {\n      return [];\n    }\n\n    // Get recent posts from following\n    const placeholders = following.map(() => '?').join(',');\n    const posts = await this.db.query(\n      `SELECT * FROM posts \n       WHERE user_id IN (${placeholders}) \n       ORDER BY created_at DESC \n       LIMIT ?`,\n      [...following, limit]\n    );\n\n    return posts;\n  }\n\n  async hydratePosts(postIds) {\n    if (postIds.length === 0) return [];\n\n    // Batch fetch posts\n    const placeholders = postIds.map(() => '?').join(',');\n    const posts = await this.db.query(\n      `SELECT p.*, u.name as user_name, u.avatar as user_avatar\n       FROM posts p\n       JOIN users u ON p.user_id = u.id\n       WHERE p.id IN (${placeholders})`,\n      postIds\n    );\n\n    // Maintain order\n    const postMap = new Map(posts.map(p => [p.id, p]));\n    return postIds.map(id => postMap.get(id)).filter(Boolean);\n  }\n\n  // Rank posts by engagement and relevance\n  async rankPosts(userId, posts) {\n    return posts.map(post => {\n      // Simple scoring (in production, use ML model)\n      const ageHours = (Date.now() - post.createdAt) / (1000 * 60 * 60);\n      const engagementScore = post.likes + post.comments * 2 + post.shares * 3;\n      const decayFactor = 1 / (1 + ageHours / 24);  // Decay over days\n      \n      post.score = engagementScore * decayFactor;\n      return post;\n    }).sort((a, b) => b.score - a.score);\n  }\n}\n\n// API Implementation\nconst express = require('express');\nconst app = express();\n\nconst db = createDatabaseConnection();\nconst cache = redis.createClient();\nconst kafka = new Kafka({ clientId: 'feed-service' });\n\nconst postService = new PostService(db);\nconst followService = new FollowService(db, cache);\nconst fanoutService = new FanoutService(followService, cache, kafka);\nconst feedService = new FeedService(postService, followService, cache, db);\n\n// Create post\napp.post('/api/posts', async (req, res) => {\n  const { userId, content, mediaUrls } = req.body;\n  \n  const post = await postService.createPost(userId, content, mediaUrls);\n  \n  // Fanout asynchronously\n  fanoutService.fanoutPost(post).catch(console.error);\n  \n  res.json(post);\n});\n\n// Get feed\napp.get('/api/feed', async (req, res) => {\n  const userId = req.user.id;\n  const { limit = 20, cursor } = req.query;\n  \n  const feed = await feedService.getFeed(\n    userId,\n    parseInt(limit),\n    cursor\n  );\n  \n  res.json(feed);\n});\n\n// Like post\napp.post('/api/posts/:postId/like', async (req, res) => {\n  const { postId } = req.params;\n  \n  await postService.incrementEngagement(postId, 'likes');\n  \n  res.json({ status: 'liked' });\n});"
    },
    {
      "id": 45,
      "question": "How do you design a Video Streaming Platform (like YouTube, Netflix)?",
      "answer": "A video streaming platform delivers video content on-demand with adaptive quality and low latency.\n\nCore Components:\n• Upload Service: Validate, store original video\n• Transcoding Service: Convert to multiple formats/resolutions (H.264, H.265, VP9)\n• Storage: Object storage (S3, GCS) for video files\n• CDN: Distribute content from edge servers globally\n• Streaming Server: HLS/DASH protocols for adaptive streaming\n• Metadata Service: Video info, thumbnails, subtitles\n• Recommendation Engine: Suggest related videos\n• Analytics: View counts, watch time, engagement\n\nVideo Processing Pipeline:\n• Upload: Chunk upload for large files, resume capability\n• Validation: Check format, duration, size limits\n• Transcoding: Generate multiple quality levels (240p, 360p, 480p, 720p, 1080p, 4K)\n• Thumbnail generation: Extract keyframes\n• Subtitle extraction: Generate SRT/VTT files\n• Storage: Upload all variants to object storage\n• CDN: Distribute to edge locations\n\nAdaptive Bitrate Streaming (ABR):\n• Split video into small chunks (2-10 seconds)\n• Encode each chunk at multiple bitrates\n• Manifest file (m3u8 for HLS, mpd for DASH) lists available qualities\n• Player dynamically switches quality based on bandwidth\n• Buffer management: Preload next chunks\n\nKey Challenges:\n• Transcoding cost: Parallel processing, spot instances\n• Storage cost: Compress, archive old/unpopular videos\n• Bandwidth cost: CDN optimization, peer-to-peer for live\n• Cold start: Popular videos cached at edge\n• DRM: Encrypt content, license management\n\nLive Streaming:\n• Ingest: RTMP from encoder\n• Transcode in real-time: Low-latency transcoding\n• Distribute: Push to CDN with minimal delay\n• Latency: 3-30 seconds depending on protocol",
      "explanation": "Video platforms transcode uploads to multiple resolutions, store in object storage, distribute via CDN, and use adaptive bitrate streaming (HLS/DASH) to deliver optimal quality based on viewer's bandwidth.",
      "difficulty": "Hard",
      "code": "// Video Streaming Platform Architecture\nconst AWS = require('aws-sdk');\nconst ffmpeg = require('fluent-ffmpeg');\nconst { Queue, Worker } = require('bullmq');\nconst express = require('express');\n\n// Video Upload Service\nclass VideoUploadService {\n  constructor(s3Client) {\n    this.s3 = s3Client;\n    this.bucket = 'video-platform-raw';\n  }\n\n  // Chunked upload for large files\n  async initiateUpload(videoId, filename, size) {\n    const params = {\n      Bucket: this.bucket,\n      Key: `raw/${videoId}/${filename}`,\n      ContentType: 'video/mp4'\n    };\n\n    const upload = await this.s3.createMultipartUpload(params).promise();\n\n    return {\n      uploadId: upload.UploadId,\n      videoId\n    };\n  }\n\n  async uploadChunk(videoId, filename, uploadId, partNumber, chunk) {\n    const params = {\n      Bucket: this.bucket,\n      Key: `raw/${videoId}/${filename}`,\n      PartNumber: partNumber,\n      UploadId: uploadId,\n      Body: chunk\n    };\n\n    const result = await this.s3.uploadPart(params).promise();\n\n    return {\n      ETag: result.ETag,\n      PartNumber: partNumber\n    };\n  }\n\n  async completeUpload(videoId, filename, uploadId, parts) {\n    const params = {\n      Bucket: this.bucket,\n      Key: `raw/${videoId}/${filename}`,\n      UploadId: uploadId,\n      MultipartUpload: {\n        Parts: parts.sort((a, b) => a.PartNumber - b.PartNumber)\n      }\n    };\n\n    await this.s3.completeMultipartUpload(params).promise();\n\n    return {\n      videoId,\n      status: 'uploaded',\n      location: `s3://${this.bucket}/raw/${videoId}/${filename}`\n    };\n  }\n}\n\n// Video Transcoding Service\nclass TranscodingService {\n  constructor() {\n    this.presets = [\n      { name: '240p', width: 426, height: 240, videoBitrate: '400k', audioBitrate: '64k' },\n      { name: '360p', width: 640, height: 360, videoBitrate: '800k', audioBitrate: '96k' },\n      { name: '480p', width: 854, height: 480, videoBitrate: '1400k', audioBitrate: '128k' },\n      { name: '720p', width: 1280, height: 720, videoBitrate: '2800k', audioBitrate: '128k' },\n      { name: '1080p', width: 1920, height: 1080, videoBitrate: '5000k', audioBitrate: '192k' }\n    ];\n  }\n\n  async transcodeVideo(inputPath, outputPath, preset) {\n    return new Promise((resolve, reject) => {\n      ffmpeg(inputPath)\n        .videoCodec('libx264')  // H.264 codec\n        .audioCodec('aac')\n        .size(`${preset.width}x${preset.height}`)\n        .videoBitrate(preset.videoBitrate)\n        .audioBitrate(preset.audioBitrate)\n        .outputOptions([\n          '-preset veryfast',\n          '-movflags +faststart',  // Enable streaming before download complete\n          '-profile:v main',\n          '-level 4.0'\n        ])\n        .output(outputPath)\n        .on('end', () => resolve(outputPath))\n        .on('error', reject)\n        .run();\n    });\n  }\n\n  // Generate HLS playlist\n  async generateHLS(inputPath, outputDir) {\n    return new Promise((resolve, reject) => {\n      ffmpeg(inputPath)\n        .outputOptions([\n          '-codec: copy',\n          '-start_number 0',\n          '-hls_time 10',           // 10 second chunks\n          '-hls_list_size 0',       // Include all chunks\n          '-f hls'\n        ])\n        .output(`${outputDir}/playlist.m3u8`)\n        .on('end', () => resolve(`${outputDir}/playlist.m3u8`))\n        .on('error', reject)\n        .run();\n    });\n  }\n\n  // Generate thumbnail\n  async generateThumbnail(inputPath, outputPath, timestamp = '00:00:05') {\n    return new Promise((resolve, reject) => {\n      ffmpeg(inputPath)\n        .screenshots({\n          timestamps: [timestamp],\n          filename: 'thumbnail.jpg',\n          folder: outputPath,\n          size: '1280x720'\n        })\n        .on('end', () => resolve(`${outputPath}/thumbnail.jpg`))\n        .on('error', reject);\n    });\n  }\n\n  async processVideo(videoId, inputPath) {\n    const outputs = [];\n\n    // Transcode to all quality levels in parallel\n    const transcodePromises = this.presets.map(async preset => {\n      const outputPath = `/tmp/${videoId}/${preset.name}.mp4`;\n      await this.transcodeVideo(inputPath, outputPath, preset);\n      \n      // Generate HLS for this quality\n      const hlsDir = `/tmp/${videoId}/hls/${preset.name}`;\n      await this.generateHLS(outputPath, hlsDir);\n      \n      return {\n        quality: preset.name,\n        mp4Path: outputPath,\n        hlsPath: `${hlsDir}/playlist.m3u8`\n      };\n    });\n\n    outputs.push(...await Promise.all(transcodePromises));\n\n    // Generate thumbnail\n    const thumbnailPath = await this.generateThumbnail(\n      inputPath,\n      `/tmp/${videoId}`\n    );\n\n    return {\n      videoId,\n      outputs,\n      thumbnail: thumbnailPath\n    };\n  }\n}\n\n// Transcoding Worker\nclass TranscodingWorker {\n  constructor(queue, s3Client) {\n    this.queue = queue;\n    this.s3 = s3Client;\n    this.transcodingService = new TranscodingService();\n    this.outputBucket = 'video-platform-processed';\n  }\n\n  async start() {\n    const worker = new Worker(\n      'video-transcoding',\n      async (job) => {\n        const { videoId, inputLocation } = job.data;\n\n        // Download input from S3\n        const inputPath = `/tmp/${videoId}/input.mp4`;\n        await this.downloadFromS3(inputLocation, inputPath);\n\n        // Process video\n        const result = await this.transcodingService.processVideo(\n          videoId,\n          inputPath\n        );\n\n        // Upload all outputs to S3\n        for (const output of result.outputs) {\n          await this.uploadToS3(\n            output.mp4Path,\n            `${videoId}/${output.quality}/video.mp4`\n          );\n          await this.uploadToS3(\n            output.hlsPath,\n            `${videoId}/${output.quality}/playlist.m3u8`\n          );\n        }\n\n        // Upload thumbnail\n        await this.uploadToS3(\n          result.thumbnail,\n          `${videoId}/thumbnail.jpg`\n        );\n\n        // Generate master playlist (ABR)\n        const masterPlaylist = this.generateMasterPlaylist(result.outputs);\n        await this.s3.putObject({\n          Bucket: this.outputBucket,\n          Key: `${videoId}/master.m3u8`,\n          Body: masterPlaylist,\n          ContentType: 'application/vnd.apple.mpegurl'\n        }).promise();\n\n        // Cleanup temp files\n        await this.cleanup(`/tmp/${videoId}`);\n\n        return { videoId, status: 'completed' };\n      },\n      {\n        connection: redis,\n        concurrency: 2  // Process 2 videos at a time\n      }\n    );\n\n    worker.on('completed', (job) => {\n      console.log(`Video ${job.data.videoId} transcoded successfully`);\n    });\n\n    worker.on('failed', (job, error) => {\n      console.error(`Transcoding failed for ${job.data.videoId}:`, error);\n    });\n  }\n\n  generateMasterPlaylist(outputs) {\n    let playlist = '#EXTM3U\\n#EXT-X-VERSION:3\\n\\n';\n\n    outputs.forEach(output => {\n      const bandwidth = this.getBandwidth(output.quality);\n      const resolution = this.getResolution(output.quality);\n      \n      playlist += `#EXT-X-STREAM-INF:BANDWIDTH=${bandwidth},RESOLUTION=${resolution}\\n`;\n      playlist += `${output.quality}/playlist.m3u8\\n\\n`;\n    });\n\n    return playlist;\n  }\n\n  getBandwidth(quality) {\n    const bandwidths = {\n      '240p': 400000,\n      '360p': 800000,\n      '480p': 1400000,\n      '720p': 2800000,\n      '1080p': 5000000\n    };\n    return bandwidths[quality] || 1000000;\n  }\n\n  getResolution(quality) {\n    const resolutions = {\n      '240p': '426x240',\n      '360p': '640x360',\n      '480p': '854x480',\n      '720p': '1280x720',\n      '1080p': '1920x1080'\n    };\n    return resolutions[quality] || '640x360';\n  }\n\n  async downloadFromS3(location, outputPath) {\n    // Implementation\n  }\n\n  async uploadToS3(filePath, key) {\n    // Implementation\n  }\n\n  async cleanup(dir) {\n    // Remove temp files\n  }\n}\n\n// Express API\nconst app = express();\nconst s3 = new AWS.S3();\nconst uploadService = new VideoUploadService(s3);\nconst transcodingQueue = new Queue('video-transcoding');\n\n// Initiate upload\napp.post('/api/videos/upload/initiate', async (req, res) => {\n  const { filename, size } = req.body;\n  const videoId = generateId();\n\n  const upload = await uploadService.initiateUpload(videoId, filename, size);\n\n  res.json(upload);\n});\n\n// Upload chunk\napp.post('/api/videos/upload/chunk', async (req, res) => {\n  const { videoId, filename, uploadId, partNumber } = req.body;\n  const chunk = req.body.chunk;  // Binary data\n\n  const part = await uploadService.uploadChunk(\n    videoId,\n    filename,\n    uploadId,\n    partNumber,\n    chunk\n  );\n\n  res.json(part);\n});\n\n// Complete upload and start transcoding\napp.post('/api/videos/upload/complete', async (req, res) => {\n  const { videoId, filename, uploadId, parts } = req.body;\n\n  const result = await uploadService.completeUpload(\n    videoId,\n    filename,\n    uploadId,\n    parts\n  );\n\n  // Queue for transcoding\n  await transcodingQueue.add('transcode', {\n    videoId,\n    inputLocation: result.location\n  });\n\n  res.json({ videoId, status: 'processing' });\n});\n\n// Stream video (CDN would cache this)\napp.get('/api/videos/:videoId/stream/:quality/playlist.m3u8', async (req, res) => {\n  const { videoId, quality } = req.params;\n\n  // Fetch from S3 (CDN caches this)\n  const playlist = await s3.getObject({\n    Bucket: 'video-platform-processed',\n    Key: `${videoId}/${quality}/playlist.m3u8`\n  }).promise();\n\n  res.set('Content-Type', 'application/vnd.apple.mpegurl');\n  res.send(playlist.Body);\n});\n\nfunction generateId() {\n  return `vid_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n}"
    },
    {
      "id": 46,
      "question": "Design a Chat Application (like WhatsApp, Slack)",
      "answer": "A chat application enables real-time messaging between users with reliability and low latency.\n\nCore Features:\n• One-on-one messaging: Direct messages between two users\n• Group chat: Multiple participants in conversation\n• Message delivery: Sent, Delivered, Read receipts\n• Online status: Presence indicators\n• Media sharing: Images, videos, documents\n• Message history: Persistent storage and search\n• Push notifications: Notify offline users\n• End-to-end encryption: Security and privacy\n\nArchitecture Components:\n• WebSocket Server: Persistent connections for real-time\n• Message Queue: Kafka/RabbitMQ for message persistence\n• Chat Service: Business logic, message routing\n• Presence Service: Track online/offline status\n• Media Service: Upload, store, serve media files\n• Notification Service: Push notifications\n• Database: Messages, users, groups\n• Cache: Recent messages, active connections\n\nMessage Flow:\n• Sender sends message → WebSocket server\n• Server validates, assigns message ID, timestamp\n• Store in database and message queue\n• For online recipients: Push via WebSocket\n• For offline recipients: Queue for push notification\n• Send delivery receipt back to sender\n• Recipient acknowledges: Send read receipt\n\nScalability Challenges:\n• Connection management: Millions of concurrent WebSocket connections\n• Message delivery: At-least-once guarantees\n• Group messaging: Fanout to all members\n• Message ordering: Same order for all recipients\n• Presence updates: Broadcast status changes efficiently\n\nOptimizations:\n• Connection server sharding: Distribute connections across servers\n• Message batching: Group multiple messages in transit\n• Read receipts batching: Don't send for every message immediately\n• Lazy loading: Load older messages on demand\n• CDN for media: Serve images/videos from edge",
      "explanation": "Chat apps use WebSocket for real-time bidirectional communication, message queues for reliability, separate presence service for status, and sharded connection servers to handle millions of concurrent users.",
      "difficulty": "Hard",
      "code": "// Real-time Chat Application\nconst express = require('express');\nconst http = require('http');\nconst WebSocket = require('ws');\nconst redis = require('redis');\nconst { Kafka } = require('kafkajs');\n\n// Connection Manager\nclass ConnectionManager {\n  constructor() {\n    this.connections = new Map();  // userId -> WebSocket\n    this.userSessions = new Map(); // userId -> Set of connection IDs\n  }\n\n  addConnection(userId, connectionId, ws) {\n    this.connections.set(connectionId, { userId, ws });\n    \n    if (!this.userSessions.has(userId)) {\n      this.userSessions.set(userId, new Set());\n    }\n    this.userSessions.get(userId).add(connectionId);\n\n    console.log(`User ${userId} connected (${connectionId})`);\n  }\n\n  removeConnection(connectionId) {\n    const conn = this.connections.get(connectionId);\n    if (conn) {\n      const { userId } = conn;\n      this.userSessions.get(userId)?.delete(connectionId);\n      \n      if (this.userSessions.get(userId)?.size === 0) {\n        this.userSessions.delete(userId);\n      }\n      \n      this.connections.delete(connectionId);\n      console.log(`Connection ${connectionId} removed`);\n    }\n  }\n\n  isUserOnline(userId) {\n    return this.userSessions.has(userId);\n  }\n\n  getUserConnections(userId) {\n    const connectionIds = this.userSessions.get(userId) || new Set();\n    return Array.from(connectionIds).map(id => this.connections.get(id));\n  }\n\n  sendToUser(userId, message) {\n    const connections = this.getUserConnections(userId);\n    connections.forEach(({ ws }) => {\n      if (ws.readyState === WebSocket.OPEN) {\n        ws.send(JSON.stringify(message));\n      }\n    });\n  }\n}\n\n// Message Service\nclass MessageService {\n  constructor(db, cache, kafka) {\n    this.db = db;\n    this.cache = cache;\n    this.producer = kafka.producer();\n  }\n\n  async sendMessage(senderId, recipientId, content, type = 'text') {\n    // Create message\n    const message = {\n      id: this.generateMessageId(),\n      senderId,\n      recipientId,\n      content,\n      type,\n      timestamp: Date.now(),\n      status: 'sent'\n    };\n\n    // Store in database\n    await this.db.query(\n      'INSERT INTO messages (id, sender_id, recipient_id, content, type, timestamp, status) VALUES (?, ?, ?, ?, ?, ?, ?)',\n      [message.id, senderId, recipientId, content, type, new Date(message.timestamp), message.status]\n    );\n\n    // Cache recent messages\n    const conversationKey = this.getConversationKey(senderId, recipientId);\n    await this.cache.lpush(conversationKey, JSON.stringify(message));\n    await this.cache.ltrim(conversationKey, 0, 99);  // Keep last 100\n    await this.cache.expire(conversationKey, 86400);  // 24 hours\n\n    // Publish to Kafka for delivery\n    await this.producer.send({\n      topic: 'message-delivery',\n      messages: [{\n        key: recipientId,\n        value: JSON.stringify(message)\n      }]\n    });\n\n    return message;\n  }\n\n  async sendGroupMessage(senderId, groupId, content, type = 'text') {\n    const message = {\n      id: this.generateMessageId(),\n      senderId,\n      groupId,\n      content,\n      type,\n      timestamp: Date.now(),\n      status: 'sent'\n    };\n\n    // Store message\n    await this.db.query(\n      'INSERT INTO messages (id, sender_id, group_id, content, type, timestamp) VALUES (?, ?, ?, ?, ?, ?)',\n      [message.id, senderId, groupId, content, type, new Date(message.timestamp)]\n    );\n\n    // Get group members\n    const members = await this.getGroupMembers(groupId);\n\n    // Fanout to all members\n    await Promise.all(\n      members\n        .filter(memberId => memberId !== senderId)\n        .map(memberId =>\n          this.producer.send({\n            topic: 'message-delivery',\n            messages: [{\n              key: memberId,\n              value: JSON.stringify({ ...message, recipientId: memberId })\n            }]\n          })\n        )\n    );\n\n    return message;\n  }\n\n  async getConversation(user1Id, user2Id, limit = 50, before = null) {\n    // Try cache first\n    const conversationKey = this.getConversationKey(user1Id, user2Id);\n    const cached = await this.cache.lrange(conversationKey, 0, limit - 1);\n    \n    if (cached.length >= limit) {\n      return cached.map(m => JSON.parse(m)).reverse();\n    }\n\n    // Fetch from database\n    let query = `\n      SELECT * FROM messages \n      WHERE (sender_id = ? AND recipient_id = ?) \n         OR (sender_id = ? AND recipient_id = ?)\n    `;\n    const params = [user1Id, user2Id, user2Id, user1Id];\n\n    if (before) {\n      query += ' AND timestamp < ?';\n      params.push(new Date(before));\n    }\n\n    query += ' ORDER BY timestamp DESC LIMIT ?';\n    params.push(limit);\n\n    const messages = await this.db.query(query, params);\n    return messages.reverse();\n  }\n\n  async updateMessageStatus(messageId, status) {\n    await this.db.query(\n      'UPDATE messages SET status = ? WHERE id = ?',\n      [status, messageId]\n    );\n  }\n\n  getConversationKey(user1Id, user2Id) {\n    const sorted = [user1Id, user2Id].sort();\n    return `conversation:${sorted[0]}:${sorted[1]}`;\n  }\n\n  generateMessageId() {\n    return `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  async getGroupMembers(groupId) {\n    const results = await this.db.query(\n      'SELECT user_id FROM group_members WHERE group_id = ?',\n      [groupId]\n    );\n    return results.map(r => r.user_id);\n  }\n}\n\n// Presence Service\nclass PresenceService {\n  constructor(cache, pubsub) {\n    this.cache = cache;\n    this.pubsub = pubsub;\n  }\n\n  async setOnline(userId) {\n    await this.cache.setex(`presence:${userId}`, 30, 'online');\n    await this.cache.zadd('presence:heartbeat', Date.now(), userId);\n    \n    // Publish status change\n    await this.pubsub.publish('presence-updates', JSON.stringify({\n      userId,\n      status: 'online',\n      timestamp: Date.now()\n    }));\n  }\n\n  async setOffline(userId) {\n    await this.cache.del(`presence:${userId}`);\n    await this.cache.zrem('presence:heartbeat', userId);\n    \n    // Record last seen\n    await this.cache.set(`last_seen:${userId}`, Date.now());\n    \n    await this.pubsub.publish('presence-updates', JSON.stringify({\n      userId,\n      status: 'offline',\n      timestamp: Date.now()\n    }));\n  }\n\n  async getStatus(userId) {\n    const online = await this.cache.get(`presence:${userId}`);\n    \n    if (online) {\n      return { status: 'online' };\n    }\n    \n    const lastSeen = await this.cache.get(`last_seen:${userId}`);\n    return {\n      status: 'offline',\n      lastSeen: lastSeen ? parseInt(lastSeen) : null\n    };\n  }\n\n  async heartbeat(userId) {\n    await this.setOnline(userId);\n  }\n\n  // Cleanup stale presence\n  async cleanupStale() {\n    const staleThreshold = Date.now() - 60000;  // 1 minute\n    const staleUsers = await this.cache.zrangebyscore(\n      'presence:heartbeat',\n      '-inf',\n      staleThreshold\n    );\n\n    for (const userId of staleUsers) {\n      await this.setOffline(userId);\n    }\n  }\n}\n\n// WebSocket Server\nclass ChatServer {\n  constructor(server, messageService, presenceService) {\n    this.wss = new WebSocket.Server({ server });\n    this.connectionManager = new ConnectionManager();\n    this.messageService = messageService;\n    this.presenceService = presenceService;\n    \n    this.setupWebSocket();\n    this.startHeartbeat();\n  }\n\n  setupWebSocket() {\n    this.wss.on('connection', (ws, req) => {\n      const userId = this.authenticateConnection(req);\n      const connectionId = this.generateConnectionId();\n      \n      if (!userId) {\n        ws.close(4001, 'Authentication required');\n        return;\n      }\n\n      // Add connection\n      this.connectionManager.addConnection(userId, connectionId, ws);\n      this.presenceService.setOnline(userId);\n\n      // Setup message handler\n      ws.on('message', async (data) => {\n        try {\n          const payload = JSON.parse(data);\n          await this.handleMessage(userId, payload, ws);\n        } catch (error) {\n          console.error('Message handling error:', error);\n          ws.send(JSON.stringify({ error: error.message }));\n        }\n      });\n\n      // Handle disconnect\n      ws.on('close', () => {\n        this.connectionManager.removeConnection(connectionId);\n        if (!this.connectionManager.isUserOnline(userId)) {\n          this.presenceService.setOffline(userId);\n        }\n      });\n\n      // Handle errors\n      ws.on('error', (error) => {\n        console.error('WebSocket error:', error);\n      });\n\n      // Send connected confirmation\n      ws.send(JSON.stringify({ type: 'connected', connectionId }));\n    });\n  }\n\n  async handleMessage(userId, payload, ws) {\n    const { type, data } = payload;\n\n    switch (type) {\n      case 'message':\n        await this.handleChatMessage(userId, data);\n        break;\n      \n      case 'group_message':\n        await this.handleGroupMessage(userId, data);\n        break;\n      \n      case 'typing':\n        this.handleTypingIndicator(userId, data);\n        break;\n      \n      case 'read_receipt':\n        await this.handleReadReceipt(userId, data);\n        break;\n      \n      case 'heartbeat':\n        await this.presenceService.heartbeat(userId);\n        ws.send(JSON.stringify({ type: 'heartbeat_ack' }));\n        break;\n      \n      default:\n        throw new Error(`Unknown message type: ${type}`);\n    }\n  }\n\n  async handleChatMessage(senderId, data) {\n    const { recipientId, content, type } = data;\n    \n    const message = await this.messageService.sendMessage(\n      senderId,\n      recipientId,\n      content,\n      type\n    );\n\n    // Send to recipient if online\n    if (this.connectionManager.isUserOnline(recipientId)) {\n      this.connectionManager.sendToUser(recipientId, {\n        type: 'new_message',\n        message\n      });\n      \n      // Update status to delivered\n      await this.messageService.updateMessageStatus(message.id, 'delivered');\n    }\n\n    // Send confirmation to sender\n    this.connectionManager.sendToUser(senderId, {\n      type: 'message_sent',\n      messageId: message.id\n    });\n  }\n\n  async handleGroupMessage(senderId, data) {\n    const { groupId, content, type } = data;\n    \n    const message = await this.messageService.sendGroupMessage(\n      senderId,\n      groupId,\n      content,\n      type\n    );\n\n    // Broadcast to online group members\n    const members = await this.messageService.getGroupMembers(groupId);\n    members\n      .filter(memberId => memberId !== senderId)\n      .forEach(memberId => {\n        if (this.connectionManager.isUserOnline(memberId)) {\n          this.connectionManager.sendToUser(memberId, {\n            type: 'new_message',\n            message\n          });\n        }\n      });\n  }\n\n  handleTypingIndicator(userId, data) {\n    const { recipientId, isTyping } = data;\n    \n    if (this.connectionManager.isUserOnline(recipientId)) {\n      this.connectionManager.sendToUser(recipientId, {\n        type: 'typing',\n        userId,\n        isTyping\n      });\n    }\n  }\n\n  async handleReadReceipt(userId, data) {\n    const { messageId } = data;\n    \n    await this.messageService.updateMessageStatus(messageId, 'read');\n    \n    // Notify sender\n    const message = await this.messageService.db.query(\n      'SELECT sender_id FROM messages WHERE id = ?',\n      [messageId]\n    );\n    \n    if (message.length > 0) {\n      const senderId = message[0].sender_id;\n      if (this.connectionManager.isUserOnline(senderId)) {\n        this.connectionManager.sendToUser(senderId, {\n          type: 'read_receipt',\n          messageId,\n          readBy: userId\n        });\n      }\n    }\n  }\n\n  authenticateConnection(req) {\n    // Extract token from query string or headers\n    const token = new URL(req.url, 'http://localhost').searchParams.get('token');\n    // Verify JWT and return userId\n    // Placeholder: return userId from token\n    return 'user123';\n  }\n\n  generateConnectionId() {\n    return `conn_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  startHeartbeat() {\n    setInterval(() => {\n      this.presenceService.cleanupStale();\n    }, 30000);  // Every 30 seconds\n  }\n}\n\n// Initialize\nconst app = express();\nconst server = http.createServer(app);\nconst db = createDatabaseConnection();\nconst cache = redis.createClient();\nconst pubsub = redis.createClient();\nconst kafka = new Kafka({ clientId: 'chat-service' });\n\nconst messageService = new MessageService(db, cache, kafka);\nconst presenceService = new PresenceService(cache, pubsub);\nconst chatServer = new ChatServer(server, messageService, presenceService);\n\n// REST API\napp.get('/api/messages/:userId', async (req, res) => {\n  const currentUserId = req.user.id;\n  const { userId } = req.params;\n  const { before, limit = 50 } = req.query;\n  \n  const messages = await messageService.getConversation(\n    currentUserId,\n    userId,\n    parseInt(limit),\n    before\n  );\n  \n  res.json(messages);\n});\n\napp.get('/api/presence/:userId', async (req, res) => {\n  const { userId } = req.params;\n  const presence = await presenceService.getStatus(userId);\n  res.json(presence);\n});\n\nserver.listen(3000);"
    },
    {
      "id": 47,
      "question": "Design a Ride Sharing Service (like Uber, Lyft)",
      "answer": "A ride-sharing platform connects riders with nearby drivers for transportation.\n\nCore Features:\n• Real-time matching: Find nearby available drivers\n• GPS tracking: Live location updates for rider and driver\n• ETA calculation: Estimate arrival and trip time\n• Dynamic pricing: Surge pricing during high demand\n• Payment processing: Handle payments and payouts\n• Rating system: Bidirectional reviews\n• Trip history: Past rides and receipts\n• Navigation: Turn-by-turn directions\n\nArchitecture Components:\n• Location Service: Store and query driver locations\n• Matching Service: Pair riders with optimal drivers\n• Pricing Service: Calculate fares based on distance, time, demand\n• Trip Service: Manage trip lifecycle (request, accept, start, complete)\n• Payment Service: Process payments, handle refunds\n• Notification Service: Real-time updates to users\n• Analytics Service: Demand forecasting, driver utilization\n\nLocation Management:\n• Geospatial indexing: QuadTree, Geohash, or Redis GEO\n• Driver location updates: Every 4-5 seconds\n• Efficient queries: Find drivers within radius\n• Historical tracking: Store location trail for safety\n\nMatching Algorithm:\n• Find available drivers within search radius\n• Filter by vehicle type, rating, acceptance rate\n• Calculate ETA for each driver to pickup\n• Score drivers (distance, rating, direction)\n• Send request to top drivers (parallel or sequential)\n• First to accept gets the ride\n\nDynamic Pricing:\n• Monitor supply (available drivers) vs demand (ride requests)\n• Calculate surge multiplier: demand/supply ratio\n• Apply to base fare: base * surge * (distance + time)\n• Real-time adjustments per geographic zone\n\nScale Challenges:\n• High write throughput: Location updates\n• Low latency: Sub-second matching\n• Geographic sharding: Partition by city/region\n• WebSocket connections: Millions of active users",
      "explanation": "Ride-sharing platforms use geospatial indexing (Redis GEO/QuadTree) for finding nearby drivers, real-time matching algorithms considering ETA and ratings, dynamic pricing based on supply-demand, and WebSockets for live updates.",
      "difficulty": "Hard",
      "code": "// Ride Sharing Platform Implementation\nconst redis = require('redis');\nconst express = require('express');\nconst WebSocket = require('ws');\n\n// Location Service using Redis GEO\nclass LocationService {\n  constructor(redisClient) {\n    this.redis = redisClient;\n  }\n\n  // Update driver location\n  async updateDriverLocation(driverId, latitude, longitude) {\n    const key = 'drivers:locations';\n    \n    // Add to geospatial index\n    await this.redis.geoadd(key, longitude, latitude, driverId);\n    \n    // Store detailed location with timestamp\n    await this.redis.setex(\n      `driver:${driverId}:location`,\n      300,  // 5 minute TTL\n      JSON.stringify({\n        latitude,\n        longitude,\n        timestamp: Date.now()\n      })\n    );\n    \n    // Track location history (for safety/disputes)\n    await this.redis.lpush(\n      `driver:${driverId}:trail`,\n      JSON.stringify({ latitude, longitude, timestamp: Date.now() })\n    );\n    await this.redis.ltrim(`driver:${driverId}:trail`, 0, 999);  // Keep last 1000\n  }\n\n  // Find nearby drivers\n  async findNearbyDrivers(latitude, longitude, radiusKm = 5, limit = 20) {\n    const key = 'drivers:locations';\n    \n    // GEORADIUS query\n    const drivers = await this.redis.georadius(\n      key,\n      longitude,\n      latitude,\n      radiusKm,\n      'km',\n      'WITHDIST',\n      'WITHCOORD',\n      'ASC',  // Closest first\n      'COUNT',\n      limit\n    );\n\n    // Format results\n    return drivers.map(driver => ({\n      driverId: driver[0],\n      distance: parseFloat(driver[1]),\n      longitude: driver[2][0],\n      latitude: driver[2][1]\n    }));\n  }\n\n  // Calculate distance between two points (Haversine)\n  calculateDistance(lat1, lon1, lat2, lon2) {\n    const R = 6371; // Earth's radius in km\n    const dLat = this.toRad(lat2 - lat1);\n    const dLon = this.toRad(lon2 - lon1);\n    \n    const a = \n      Math.sin(dLat / 2) * Math.sin(dLat / 2) +\n      Math.cos(this.toRad(lat1)) * Math.cos(this.toRad(lat2)) *\n      Math.sin(dLon / 2) * Math.sin(dLon / 2);\n    \n    const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a));\n    return R * c;\n  }\n\n  toRad(degrees) {\n    return degrees * (Math.PI / 180);\n  }\n}\n\n// Driver Management\nclass DriverService {\n  constructor(db, cache) {\n    this.db = db;\n    this.cache = cache;\n  }\n\n  async setDriverStatus(driverId, status) {\n    // status: 'available', 'busy', 'offline'\n    await this.cache.set(`driver:${driverId}:status`, status);\n    \n    if (status === 'offline') {\n      // Remove from location index\n      await this.cache.zrem('drivers:locations', driverId);\n    }\n  }\n\n  async getDriverStatus(driverId) {\n    return await this.cache.get(`driver:${driverId}:status`) || 'offline';\n  }\n\n  async getDriverInfo(driverId) {\n    const cached = await this.cache.get(`driver:${driverId}:info`);\n    if (cached) return JSON.parse(cached);\n\n    const results = await this.db.query(\n      'SELECT * FROM drivers WHERE id = ?',\n      [driverId]\n    );\n\n    if (results.length === 0) return null;\n\n    const driver = results[0];\n    await this.cache.setex(\n      `driver:${driverId}:info`,\n      3600,\n      JSON.stringify(driver)\n    );\n\n    return driver;\n  }\n\n  async getAvailableDrivers(driverIds) {\n    const pipeline = this.cache.pipeline();\n    driverIds.forEach(id => {\n      pipeline.get(`driver:${id}:status`);\n    });\n    const statuses = await pipeline.exec();\n\n    return driverIds.filter((id, idx) => \n      statuses[idx][1] === 'available'\n    );\n  }\n}\n\n// Matching Service\nclass MatchingService {\n  constructor(locationService, driverService, pricingService) {\n    this.locationService = locationService;\n    this.driverService = driverService;\n    this.pricingService = pricingService;\n  }\n\n  async findDriver(rideRequest) {\n    const { pickupLat, pickupLng, vehicleType, riderRating } = rideRequest;\n\n    // Find nearby drivers\n    let radius = 2;  // Start with 2km\n    let candidates = [];\n    \n    while (candidates.length < 5 && radius <= 10) {\n      const nearby = await this.locationService.findNearbyDrivers(\n        pickupLat,\n        pickupLng,\n        radius\n      );\n\n      // Filter available drivers\n      const driverIds = nearby.map(d => d.driverId);\n      const available = await this.driverService.getAvailableDrivers(driverIds);\n      \n      candidates = nearby.filter(d => available.includes(d.driverId));\n      \n      radius += 2;  // Expand search radius\n    }\n\n    if (candidates.length === 0) {\n      return null;  // No drivers available\n    }\n\n    // Score and rank drivers\n    const scoredDrivers = await Promise.all(\n      candidates.map(async driver => {\n        const info = await this.driverService.getDriverInfo(driver.driverId);\n        const eta = this.calculateETA(driver.distance);\n        \n        // Scoring formula\n        const score = \n          (1 / driver.distance) * 0.5 +  // Closer is better\n          (info.rating / 5) * 0.3 +      // Higher rating is better\n          (info.acceptanceRate / 100) * 0.2;  // Higher acceptance is better\n        \n        return {\n          ...driver,\n          info,\n          eta,\n          score\n        };\n      })\n    );\n\n    // Sort by score descending\n    scoredDrivers.sort((a, b) => b.score - a.score);\n\n    return scoredDrivers[0];\n  }\n\n  calculateETA(distanceKm) {\n    const avgSpeedKmh = 30;  // Average city speed\n    const minutesPerKm = 60 / avgSpeedKmh;\n    return Math.ceil(distanceKm * minutesPerKm);\n  }\n\n  async sendRideRequest(driverId, rideRequest) {\n    const requestId = this.generateRequestId();\n    \n    // Store request with expiration\n    await this.cache.setex(\n      `ride_request:${requestId}`,\n      30,  // 30 second expiry\n      JSON.stringify({\n        ...rideRequest,\n        driverId,\n        requestId,\n        timestamp: Date.now()\n      })\n    );\n\n    // Send via WebSocket to driver\n    this.notifyDriver(driverId, {\n      type: 'ride_request',\n      requestId,\n      rideRequest\n    });\n\n    return requestId;\n  }\n\n  generateRequestId() {\n    return `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  notifyDriver(driverId, message) {\n    // WebSocket notification (implementation below)\n  }\n}\n\n// Pricing Service\nclass PricingService {\n  constructor(db, cache) {\n    this.db = db;\n    this.cache = cache;\n    this.baseFarePerKm = 1.5;\n    this.baseFarePerMin = 0.3;\n    this.baseFare = 2.5;\n  }\n\n  async calculateFare(trip) {\n    const { distanceKm, durationMin, pickupLat, pickupLng } = trip;\n    \n    // Get surge multiplier for location\n    const surgeMultiplier = await this.getSurgeMultiplier(pickupLat, pickupLng);\n    \n    // Calculate base fare\n    const fare = \n      this.baseFare +\n      (distanceKm * this.baseFarePerKm) +\n      (durationMin * this.baseFarePerMin);\n    \n    // Apply surge\n    const finalFare = fare * surgeMultiplier;\n    \n    return {\n      baseFare: fare.toFixed(2),\n      surgeMultiplier,\n      finalFare: finalFare.toFixed(2)\n    };\n  }\n\n  async getSurgeMultiplier(lat, lng) {\n    // Get zone (can use geohash for bucketing)\n    const zone = this.getZone(lat, lng);\n    \n    // Check cache for current surge\n    const cached = await this.cache.get(`surge:${zone}`);\n    if (cached) return parseFloat(cached);\n    \n    // Calculate surge based on supply/demand\n    const surge = await this.calculateSurge(zone);\n    \n    // Cache for 1 minute\n    await this.cache.setex(`surge:${zone}`, 60, surge.toString());\n    \n    return surge;\n  }\n\n  async calculateSurge(zone) {\n    // Get pending rides in zone\n    const demandKey = `demand:${zone}`;\n    const demand = await this.cache.zcard(demandKey) || 1;\n    \n    // Get available drivers in zone\n    const supplyKey = `supply:${zone}`;\n    const supply = await this.cache.zcard(supplyKey) || 1;\n    \n    // Calculate surge ratio\n    const ratio = demand / supply;\n    \n    if (ratio < 1) return 1.0;           // Normal pricing\n    if (ratio < 1.5) return 1.25;        // Low surge\n    if (ratio < 2) return 1.5;           // Medium surge\n    if (ratio < 3) return 2.0;           // High surge\n    return Math.min(3.0, ratio);         // Cap at 3x\n  }\n\n  getZone(lat, lng) {\n    // Simple grid-based zoning (in production, use geohash)\n    const latZone = Math.floor(lat * 100);\n    const lngZone = Math.floor(lng * 100);\n    return `${latZone}_${lngZone}`;\n  }\n\n  async recordDemand(lat, lng, rideRequestId) {\n    const zone = this.getZone(lat, lng);\n    await this.cache.zadd(\n      `demand:${zone}`,\n      Date.now(),\n      rideRequestId\n    );\n    // Cleanup old entries\n    const fiveMinutesAgo = Date.now() - (5 * 60 * 1000);\n    await this.cache.zremrangebyscore(\n      `demand:${zone}`,\n      '-inf',\n      fiveMinutesAgo\n    );\n  }\n}\n\n// Trip Service\nclass TripService {\n  constructor(db) {\n    this.db = db;\n  }\n\n  async createTrip(riderId, driverId, pickup, dropoff) {\n    const trip = {\n      id: this.generateTripId(),\n      riderId,\n      driverId,\n      pickupLat: pickup.lat,\n      pickupLng: pickup.lng,\n      pickupAddress: pickup.address,\n      dropoffLat: dropoff.lat,\n      dropoffLng: dropoff.lng,\n      dropoffAddress: dropoff.address,\n      status: 'accepted',\n      createdAt: new Date()\n    };\n\n    await this.db.query(\n      'INSERT INTO trips (id, rider_id, driver_id, pickup_lat, pickup_lng, pickup_address, dropoff_lat, dropoff_lng, dropoff_address, status, created_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)',\n      Object.values(trip)\n    );\n\n    return trip;\n  }\n\n  async updateTripStatus(tripId, status, data = {}) {\n    const updates = { status, ...data };\n    \n    const fields = Object.keys(updates).map(k => `${k} = ?`).join(', ');\n    const values = [...Object.values(updates), tripId];\n    \n    await this.db.query(\n      `UPDATE trips SET ${fields} WHERE id = ?`,\n      values\n    );\n  }\n\n  generateTripId() {\n    return `trip_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n}\n\n// Express API\nconst app = express();\nconst cache = redis.createClient();\nconst db = createDatabaseConnection();\n\nconst locationService = new LocationService(cache);\nconst driverService = new DriverService(db, cache);\nconst pricingService = new PricingService(db, cache);\nconst matchingService = new MatchingService(locationService, driverService, pricingService);\nconst tripService = new TripService(db);\n\n// Request a ride\napp.post('/api/rides/request', async (req, res) => {\n  const riderId = req.user.id;\n  const { pickup, dropoff, vehicleType } = req.body;\n  \n  // Record demand for surge pricing\n  await pricingService.recordDemand(pickup.lat, pickup.lng, riderId);\n  \n  // Find best driver\n  const driver = await matchingService.findDriver({\n    pickupLat: pickup.lat,\n    pickupLng: pickup.lng,\n    vehicleType\n  });\n  \n  if (!driver) {\n    return res.status(404).json({ error: 'No drivers available' });\n  }\n  \n  // Calculate estimated fare\n  const estimate = await pricingService.calculateFare({\n    distanceKm: 5,  // Placeholder, use routing API in production\n    durationMin: 15,\n    pickupLat: pickup.lat,\n    pickupLng: pickup.lng\n  });\n  \n  // Send request to driver\n  const requestId = await matchingService.sendRideRequest(driver.driverId, {\n    riderId,\n    pickup,\n    dropoff,\n    estimate\n  });\n  \n  res.json({\n    requestId,\n    driver: {\n      id: driver.driverId,\n      name: driver.info.name,\n      rating: driver.info.rating,\n      eta: driver.eta\n    },\n    estimate\n  });\n});\n\n// Update driver location\napp.post('/api/drivers/location', async (req, res) => {\n  const driverId = req.user.id;\n  const { latitude, longitude } = req.body;\n  \n  await locationService.updateDriverLocation(driverId, latitude, longitude);\n  \n  res.json({ status: 'updated' });\n});"
    },
    {
      "id": 48,
      "question": "Design a Payment System",
      "answer": "A payment system processes financial transactions securely and reliably with strong consistency guarantees.\n\nCore Requirements:\n• Transaction integrity: ACID properties, exactly-once processing\n• Security: PCI DSS compliance, encryption, tokenization\n• Idempotency: Prevent duplicate charges from retries\n• Multiple payment methods: Credit cards, wallets, bank transfers\n• Currency support: Multi-currency processing and conversion\n• Reconciliation: Match payments with bank statements\n• Fraud detection: Real-time risk analysis\n• Audit trail: Complete transaction history\n\nArchitecture Components:\n• Payment Gateway: Interface with payment processors (Stripe, PayPal)\n• Transaction Service: Coordinate payment flow\n• Wallet Service: Manage user balances\n• Ledger Service: Double-entry bookkeeping for accuracy\n• Fraud Detection: ML-based risk scoring\n• Notification Service: Payment confirmations\n• Reconciliation Service: Match transactions with settlements\n\nPayment Flow:\n• User initiates payment with amount and payment method\n• Validate: Check balance, limits, fraud risk\n• Reserve: Lock funds (for wallet) or pre-authorize (for cards)\n• Process: Call payment gateway API\n• Update: Record transaction in ledger\n• Notify: Send confirmation to user\n• Settle: Reconcile with actual bank transfer (async)\n\nIdempotency:\n• Client generates idempotency key (UUID)\n• Server checks if request with key already processed\n• If yes: Return cached result\n• If no: Process and cache result with key\n• Prevents double charging from network retries\n\nDouble-Entry Ledger:\n• Every transaction has equal debits and credits\n• Example: User pays $10 → Debit user wallet $10, Credit merchant wallet $10\n• Ensures books always balance\n• Immutable append-only log\n\nConsistency Challenges:\n• Distributed transactions: Use Saga pattern with compensating transactions\n• Eventual consistency: Acceptable for notifications, not for money movement\n• Strong consistency: Use database transactions with SERIALIZABLE isolation",
      "explanation": "Payment systems use idempotency keys for exactly-once processing, double-entry ledgers for accuracy, strong consistency guarantees with database transactions, and fraud detection with ML models for security.",
      "difficulty": "Hard",
      "code": "// Payment System Implementation\nconst express = require('express');\nconst stripe = require('stripe')(process.env.STRIPE_SECRET_KEY);\nconst { v4: uuidv4 } = require('uuid');\n\n// Transaction Service\nclass TransactionService {\n  constructor(db, cache, ledgerService) {\n    this.db = db;\n    this.cache = cache;\n    this.ledger = ledgerService;\n  }\n\n  // Process payment with idempotency\n  async processPayment(paymentRequest, idempotencyKey) {\n    // Check idempotency\n    const cached = await this.checkIdempotency(idempotencyKey);\n    if (cached) {\n      console.log('Returning cached result for idempotency key:', idempotencyKey);\n      return cached;\n    }\n\n    // Start database transaction\n    const connection = await this.db.getConnection();\n    \n    try {\n      await connection.beginTransaction();\n\n      const { userId, amount, currency, paymentMethod, metadata } = paymentRequest;\n\n      // Validate payment\n      await this.validatePayment(userId, amount, currency);\n\n      // Fraud check\n      const riskScore = await this.checkFraud(userId, amount, paymentMethod);\n      if (riskScore > 0.8) {\n        throw new Error('Payment blocked due to high fraud risk');\n      }\n\n      // Create transaction record\n      const transactionId = uuidv4();\n      await connection.query(\n        `INSERT INTO transactions \n         (id, user_id, amount, currency, payment_method, status, risk_score, created_at) \n         VALUES (?, ?, ?, ?, ?, ?, ?, NOW())`,\n        [transactionId, userId, amount, currency, paymentMethod, 'pending', riskScore]\n      );\n\n      // Process with payment gateway\n      let gatewayResponse;\n      \n      if (paymentMethod.type === 'wallet') {\n        gatewayResponse = await this.processWalletPayment(\n          connection,\n          userId,\n          amount,\n          currency\n        );\n      } else if (paymentMethod.type === 'card') {\n        gatewayResponse = await this.processCardPayment(\n          paymentMethod,\n          amount,\n          currency,\n          metadata\n        );\n      } else {\n        throw new Error(`Unsupported payment method: ${paymentMethod.type}`);\n      }\n\n      // Update transaction status\n      await connection.query(\n        'UPDATE transactions SET status = ?, gateway_transaction_id = ? WHERE id = ?',\n        ['completed', gatewayResponse.id, transactionId]\n      );\n\n      // Record in ledger (double-entry bookkeeping)\n      await this.ledger.recordTransaction(connection, {\n        transactionId,\n        entries: [\n          { account: `user:${userId}:wallet`, type: 'debit', amount, currency },\n          { account: `platform:revenue`, type: 'credit', amount, currency }\n        ]\n      });\n\n      // Commit transaction\n      await connection.commit();\n\n      const result = {\n        transactionId,\n        status: 'completed',\n        amount,\n        currency,\n        gatewayTransactionId: gatewayResponse.id\n      };\n\n      // Cache result for idempotency\n      await this.cacheIdempotency(idempotencyKey, result);\n\n      return result;\n\n    } catch (error) {\n      await connection.rollback();\n      console.error('Payment processing error:', error);\n      \n      // Cache error for idempotency\n      const errorResult = {\n        error: error.message,\n        transactionId: null,\n        status: 'failed'\n      };\n      await this.cacheIdempotency(idempotencyKey, errorResult);\n      \n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n\n  async validatePayment(userId, amount, currency) {\n    if (amount <= 0) {\n      throw new Error('Invalid amount');\n    }\n\n    if (!['USD', 'EUR', 'GBP'].includes(currency)) {\n      throw new Error('Unsupported currency');\n    }\n\n    // Check user limits\n    const dailyLimit = await this.getDailyLimit(userId);\n    const todayTotal = await this.getTodayTotal(userId);\n    \n    if (todayTotal + amount > dailyLimit) {\n      throw new Error('Daily limit exceeded');\n    }\n  }\n\n  async checkFraud(userId, amount, paymentMethod) {\n    // Simple rule-based fraud detection\n    // In production: Use ML model\n    let riskScore = 0;\n\n    // High amount\n    if (amount > 1000) riskScore += 0.3;\n\n    // Multiple transactions in short time\n    const recentCount = await this.getRecentTransactionCount(userId, 3600);\n    if (recentCount > 10) riskScore += 0.4;\n\n    // New payment method\n    const isNewMethod = await this.isNewPaymentMethod(userId, paymentMethod);\n    if (isNewMethod) riskScore += 0.2;\n\n    return Math.min(riskScore, 1.0);\n  }\n\n  async processWalletPayment(connection, userId, amount, currency) {\n    // Check balance\n    const balance = await this.getWalletBalance(connection, userId, currency);\n    \n    if (balance < amount) {\n      throw new Error('Insufficient balance');\n    }\n\n    // Deduct from wallet\n    await connection.query(\n      'UPDATE wallets SET balance = balance - ? WHERE user_id = ? AND currency = ?',\n      [amount, userId, currency]\n    );\n\n    return {\n      id: `wallet_${uuidv4()}`,\n      status: 'completed'\n    };\n  }\n\n  async processCardPayment(paymentMethod, amount, currency, metadata) {\n    // Call Stripe API\n    const paymentIntent = await stripe.paymentIntents.create({\n      amount: Math.round(amount * 100),  // Stripe uses cents\n      currency: currency.toLowerCase(),\n      payment_method: paymentMethod.stripePaymentMethodId,\n      confirm: true,\n      metadata\n    });\n\n    if (paymentIntent.status !== 'succeeded') {\n      throw new Error(`Payment failed: ${paymentIntent.status}`);\n    }\n\n    return {\n      id: paymentIntent.id,\n      status: 'completed'\n    };\n  }\n\n  async checkIdempotency(key) {\n    const cached = await this.cache.get(`idempotency:${key}`);\n    return cached ? JSON.parse(cached) : null;\n  }\n\n  async cacheIdempotency(key, result) {\n    await this.cache.setex(\n      `idempotency:${key}`,\n      86400,  // 24 hours\n      JSON.stringify(result)\n    );\n  }\n\n  async getWalletBalance(connection, userId, currency) {\n    const results = await connection.query(\n      'SELECT balance FROM wallets WHERE user_id = ? AND currency = ?',\n      [userId, currency]\n    );\n    return results[0]?.balance || 0;\n  }\n\n  async getDailyLimit(userId) {\n    return 10000;  // Placeholder\n  }\n\n  async getTodayTotal(userId) {\n    const results = await this.db.query(\n      `SELECT COALESCE(SUM(amount), 0) as total \n       FROM transactions \n       WHERE user_id = ? AND DATE(created_at) = CURDATE()`,\n      [userId]\n    );\n    return results[0].total;\n  }\n\n  async getRecentTransactionCount(userId, seconds) {\n    const results = await this.db.query(\n      `SELECT COUNT(*) as count \n       FROM transactions \n       WHERE user_id = ? AND created_at > DATE_SUB(NOW(), INTERVAL ? SECOND)`,\n      [userId, seconds]\n    );\n    return results[0].count;\n  }\n\n  async isNewPaymentMethod(userId, paymentMethod) {\n    const results = await this.db.query(\n      'SELECT id FROM payment_methods WHERE user_id = ? AND fingerprint = ?',\n      [userId, paymentMethod.fingerprint]\n    );\n    return results.length === 0;\n  }\n}\n\n// Ledger Service (Double-Entry Bookkeeping)\nclass LedgerService {\n  async recordTransaction(connection, transaction) {\n    const { transactionId, entries } = transaction;\n\n    // Validate double-entry: debits = credits\n    const debits = entries.filter(e => e.type === 'debit')\n      .reduce((sum, e) => sum + e.amount, 0);\n    const credits = entries.filter(e => e.type === 'credit')\n      .reduce((sum, e) => sum + e.amount, 0);\n\n    if (Math.abs(debits - credits) > 0.01) {\n      throw new Error('Debits and credits must balance');\n    }\n\n    // Insert ledger entries\n    for (const entry of entries) {\n      await connection.query(\n        `INSERT INTO ledger_entries \n         (id, transaction_id, account, type, amount, currency, created_at) \n         VALUES (?, ?, ?, ?, ?, ?, NOW())`,\n        [uuidv4(), transactionId, entry.account, entry.type, entry.amount, entry.currency]\n      );\n    }\n  }\n\n  async getAccountBalance(account, currency = 'USD') {\n    const results = await this.db.query(\n      `SELECT \n         SUM(CASE WHEN type = 'credit' THEN amount ELSE 0 END) -\n         SUM(CASE WHEN type = 'debit' THEN amount ELSE 0 END) as balance\n       FROM ledger_entries\n       WHERE account = ? AND currency = ?`,\n      [account, currency]\n    );\n    return results[0].balance || 0;\n  }\n}\n\n// Refund Service\nclass RefundService {\n  constructor(db, transactionService, ledgerService) {\n    this.db = db;\n    this.transactionService = transactionService;\n    this.ledger = ledgerService;\n  }\n\n  async processRefund(transactionId, amount, reason) {\n    const connection = await this.db.getConnection();\n    \n    try {\n      await connection.beginTransaction();\n\n      // Get original transaction\n      const [transaction] = await connection.query(\n        'SELECT * FROM transactions WHERE id = ?',\n        [transactionId]\n      );\n\n      if (!transaction) {\n        throw new Error('Transaction not found');\n      }\n\n      if (transaction.status !== 'completed') {\n        throw new Error('Can only refund completed transactions');\n      }\n\n      if (amount > transaction.amount) {\n        throw new Error('Refund amount exceeds transaction amount');\n      }\n\n      // Create refund record\n      const refundId = uuidv4();\n      await connection.query(\n        `INSERT INTO refunds \n         (id, transaction_id, amount, currency, reason, status, created_at) \n         VALUES (?, ?, ?, ?, ?, ?, NOW())`,\n        [refundId, transactionId, amount, transaction.currency, reason, 'processing']\n      );\n\n      // Process refund with gateway\n      if (transaction.payment_method === 'card') {\n        await stripe.refunds.create({\n          payment_intent: transaction.gateway_transaction_id,\n          amount: Math.round(amount * 100)\n        });\n      }\n\n      // Update wallet if wallet payment\n      if (transaction.payment_method === 'wallet') {\n        await connection.query(\n          'UPDATE wallets SET balance = balance + ? WHERE user_id = ? AND currency = ?',\n          [amount, transaction.user_id, transaction.currency]\n        );\n      }\n\n      // Record in ledger (reverse entries)\n      await this.ledger.recordTransaction(connection, {\n        transactionId: refundId,\n        entries: [\n          { account: `user:${transaction.user_id}:wallet`, type: 'credit', amount, currency: transaction.currency },\n          { account: `platform:revenue`, type: 'debit', amount, currency: transaction.currency }\n        ]\n      });\n\n      // Update refund status\n      await connection.query(\n        'UPDATE refunds SET status = ? WHERE id = ?',\n        ['completed', refundId]\n      );\n\n      await connection.commit();\n\n      return { refundId, status: 'completed' };\n\n    } catch (error) {\n      await connection.rollback();\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n}\n\n// Express API\nconst app = express();\nconst db = createDatabaseConnection();\nconst cache = createRedisClient();\nconst ledgerService = new LedgerService(db);\nconst transactionService = new TransactionService(db, cache, ledgerService);\nconst refundService = new RefundService(db, transactionService, ledgerService);\n\n// Process payment\napp.post('/api/payments', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { amount, currency, paymentMethod, metadata } = req.body;\n    \n    // Get or generate idempotency key\n    const idempotencyKey = req.headers['idempotency-key'] || uuidv4();\n\n    const result = await transactionService.processPayment(\n      { userId, amount, currency, paymentMethod, metadata },\n      idempotencyKey\n    );\n\n    res.json(result);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Get transaction\napp.get('/api/transactions/:id', async (req, res) => {\n  const { id } = req.params;\n  \n  const results = await db.query(\n    'SELECT * FROM transactions WHERE id = ?',\n    [id]\n  );\n  \n  if (results.length === 0) {\n    return res.status(404).json({ error: 'Transaction not found' });\n  }\n  \n  res.json(results[0]);\n});\n\n// Process refund\napp.post('/api/refunds', async (req, res) => {\n  try {\n    const { transactionId, amount, reason } = req.body;\n    \n    const result = await refundService.processRefund(\n      transactionId,\n      amount,\n      reason\n    );\n    \n    res.json(result);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Get wallet balance\napp.get('/api/wallet/balance', async (req, res) => {\n  const userId = req.user.id;\n  const { currency = 'USD' } = req.query;\n  \n  const balance = await ledgerService.getAccountBalance(\n    `user:${userId}:wallet`,\n    currency\n  );\n  \n  res.json({ balance, currency });\n});"
    },
    {
      "id": 49,
      "question": "Design a Content Delivery Network (CDN)",
      "answer": "A CDN distributes content globally through edge servers to reduce latency and improve availability.\n\nCore Components:\n• Origin Server: Source of truth for content\n• Edge Servers (PoPs): Distributed servers in multiple locations\n• Load Balancer: Route requests to optimal edge server\n• Cache: Store frequently accessed content at edge\n• Purge System: Invalidate stale content\n• Analytics: Traffic patterns, cache hit rates, performance metrics\n\nKey Features:\n• Geographic distribution: Servers in 100+ locations worldwide\n• Intelligent routing: DNS-based or anycast routing to nearest PoP\n• Caching strategy: LRU, TTL-based, cache headers respected\n• Content types: Static files (images, CSS, JS), video streams, API responses\n• SSL/TLS termination: Handle encryption at edge\n• DDoS protection: Absorb attacks at edge, not origin\n• Compression: Gzip, Brotli for faster transfer\n• Image optimization: Resize, format conversion on-the-fly\n\nCDN Request Flow:\n• User requests www.example.com/image.jpg\n• DNS resolves to nearest edge server IP (GEOIP-based)\n• Edge server checks cache for content\n• If HIT: Serve from cache immediately\n• If MISS: Fetch from origin, cache, and serve to user\n• Subsequent requests for same content are cache HITs\n\nCaching Strategies:\n• Cache-Control headers: max-age, no-cache, no-store\n• Stale-while-revalidate: Serve stale content while refreshing\n• Cache purging: API to invalidate specific URLs or patterns\n• Versioning: Use query params or path for cache busting (v=123)\n\nOptimizations:\n• Prefetching: Pre-load popular content to edge\n• Tiered caching: Regional cache layer before origin\n• Compression: Reduce bandwidth with gzip/brotli\n• HTTP/2 Server Push: Push resources before requested\n• Edge computing: Run code at edge for personalization",
      "explanation": "CDNs cache content at geographically distributed edge servers, route users to nearest location via DNS/anycast, respect cache headers for TTL, and invalidate stale content through purge APIs.",
      "difficulty": "Hard",
      "code": "// CDN Edge Server Implementation\nconst express = require('express');\nconst redis = require('redis');\nconst axios = require('axios');\nconst sharp = require('sharp');\nconst crypto = require('crypto');\n\n// Cache Manager\nclass CacheManager {\n  constructor(redisClient) {\n    this.cache = redisClient;\n    this.defaultTTL = 3600;  // 1 hour\n  }\n\n  // Generate cache key\n  getCacheKey(url, params = {}) {\n    const normalized = this.normalizeParams(params);\n    const key = `${url}?${normalized}`;\n    return `cache:${crypto.createHash('md5').update(key).digest('hex')}`;\n  }\n\n  normalizeParams(params) {\n    return Object.keys(params)\n      .sort()\n      .map(key => `${key}=${params[key]}`)\n      .join('&');\n  }\n\n  // Get from cache\n  async get(cacheKey) {\n    const data = await this.cache.get(cacheKey);\n    if (!data) return null;\n\n    const cached = JSON.parse(data);\n    \n    // Check if stale\n    if (cached.expiresAt && Date.now() > cached.expiresAt) {\n      return { ...cached, isStale: true };\n    }\n\n    return cached;\n  }\n\n  // Set in cache\n  async set(cacheKey, content, headers = {}, ttl = null) {\n    const cacheControl = headers['cache-control'];\n    const maxAge = this.parseMaxAge(cacheControl);\n    const finalTTL = ttl || maxAge || this.defaultTTL;\n\n    const cached = {\n      content,\n      headers,\n      cachedAt: Date.now(),\n      expiresAt: Date.now() + (finalTTL * 1000)\n    };\n\n    await this.cache.setex(\n      cacheKey,\n      finalTTL,\n      JSON.stringify(cached)\n    );\n  }\n\n  parseMaxAge(cacheControl) {\n    if (!cacheControl) return null;\n    \n    const match = cacheControl.match(/max-age=(\\d+)/);\n    return match ? parseInt(match[1]) : null;\n  }\n\n  // Check if cacheable\n  isCacheable(method, statusCode, headers) {\n    // Only cache GET requests\n    if (method !== 'GET') return false;\n\n    // Only cache successful responses\n    if (statusCode < 200 || statusCode >= 300) return false;\n\n    // Respect Cache-Control\n    const cacheControl = headers['cache-control'];\n    if (cacheControl && (cacheControl.includes('no-store') || cacheControl.includes('private'))) {\n      return false;\n    }\n\n    return true;\n  }\n\n  // Purge cache\n  async purge(pattern) {\n    const keys = await this.cache.keys(`cache:*${pattern}*`);\n    \n    if (keys.length > 0) {\n      await this.cache.del(...keys);\n    }\n    \n    return keys.length;\n  }\n}\n\n// Origin Fetcher\nclass OriginFetcher {\n  constructor(originUrl) {\n    this.originUrl = originUrl;\n  }\n\n  async fetch(path, headers = {}) {\n    const url = `${this.originUrl}${path}`;\n    \n    console.log(`Fetching from origin: ${url}`);\n    \n    const response = await axios({\n      url,\n      method: 'GET',\n      headers: {\n        ...headers,\n        'X-Forwarded-For': headers['x-real-ip'] || 'unknown'\n      },\n      responseType: 'arraybuffer',\n      validateStatus: () => true  // Don't throw on any status\n    });\n\n    return {\n      content: response.data,\n      status: response.status,\n      headers: response.headers\n    };\n  }\n}\n\n// Image Optimizer\nclass ImageOptimizer {\n  async optimize(buffer, options = {}) {\n    const { width, height, format, quality = 80 } = options;\n\n    let pipeline = sharp(buffer);\n\n    // Resize if dimensions provided\n    if (width || height) {\n      pipeline = pipeline.resize(width, height, {\n        fit: 'inside',\n        withoutEnlargement: true\n      });\n    }\n\n    // Convert format\n    if (format) {\n      if (format === 'webp') {\n        pipeline = pipeline.webp({ quality });\n      } else if (format === 'jpeg' || format === 'jpg') {\n        pipeline = pipeline.jpeg({ quality });\n      } else if (format === 'png') {\n        pipeline = pipeline.png({ quality });\n      }\n    }\n\n    return await pipeline.toBuffer();\n  }\n\n  getContentType(format) {\n    const types = {\n      'jpeg': 'image/jpeg',\n      'jpg': 'image/jpeg',\n      'png': 'image/png',\n      'webp': 'image/webp',\n      'gif': 'image/gif'\n    };\n    return types[format] || 'application/octet-stream';\n  }\n}\n\n// CDN Edge Server\nclass EdgeServer {\n  constructor(originUrl, redisClient) {\n    this.cacheManager = new CacheManager(redisClient);\n    this.originFetcher = new OriginFetcher(originUrl);\n    this.imageOptimizer = new ImageOptimizer();\n  }\n\n  async handle(req, res) {\n    const path = req.path;\n    const method = req.method;\n\n    // Generate cache key\n    const cacheKey = this.cacheManager.getCacheKey(path, req.query);\n\n    // Try cache first\n    const cached = await this.cacheManager.get(cacheKey);\n    \n    if (cached && !cached.isStale) {\n      console.log('Cache HIT:', path);\n      \n      // Set cache headers\n      res.set(cached.headers);\n      res.set('X-Cache', 'HIT');\n      res.set('Age', Math.floor((Date.now() - cached.cachedAt) / 1000));\n      \n      return res.send(Buffer.from(cached.content));\n    }\n\n    // Stale-while-revalidate: Serve stale, refresh in background\n    if (cached && cached.isStale) {\n      console.log('Cache STALE:', path);\n      \n      res.set(cached.headers);\n      res.set('X-Cache', 'STALE');\n      res.send(Buffer.from(cached.content));\n      \n      // Refresh cache in background\n      this.refreshCache(path, cacheKey, req.headers).catch(console.error);\n      \n      return;\n    }\n\n    // Cache MISS: Fetch from origin\n    console.log('Cache MISS:', path);\n    \n    try {\n      let origin = await this.originFetcher.fetch(path, req.headers);\n      let { content, status, headers } = origin;\n\n      // Image optimization if requested\n      if (this.isImage(path) && Object.keys(req.query).length > 0) {\n        content = await this.imageOptimizer.optimize(content, req.query);\n        \n        if (req.query.format) {\n          headers['content-type'] = this.imageOptimizer.getContentType(req.query.format);\n        }\n      }\n\n      // Cache if cacheable\n      if (this.cacheManager.isCacheable(method, status, headers)) {\n        await this.cacheManager.set(cacheKey, content, headers);\n      }\n\n      // Send response\n      res.status(status);\n      res.set(headers);\n      res.set('X-Cache', 'MISS');\n      res.send(Buffer.from(content));\n\n    } catch (error) {\n      console.error('Origin fetch error:', error);\n      res.status(502).json({ error: 'Bad Gateway' });\n    }\n  }\n\n  async refreshCache(path, cacheKey, headers) {\n    try {\n      const { content, headers: originHeaders } = await this.originFetcher.fetch(path, headers);\n      await this.cacheManager.set(cacheKey, content, originHeaders);\n      console.log('Cache refreshed:', path);\n    } catch (error) {\n      console.error('Cache refresh error:', error);\n    }\n  }\n\n  isImage(path) {\n    return /\\.(jpg|jpeg|png|gif|webp)$/i.test(path);\n  }\n}\n\n// CDN Control API\nclass CDNControlAPI {\n  constructor(cacheManager) {\n    this.cacheManager = cacheManager;\n    this.purgeLog = [];\n  }\n\n  setupRoutes(app) {\n    // Purge cache by URL or pattern\n    app.post('/api/cdn/purge', async (req, res) => {\n      const { urls, pattern } = req.body;\n      let purgedCount = 0;\n\n      if (urls && urls.length > 0) {\n        for (const url of urls) {\n          const cacheKey = this.cacheManager.getCacheKey(url);\n          await this.cacheManager.cache.del(cacheKey);\n          purgedCount++;\n        }\n      }\n\n      if (pattern) {\n        purgedCount += await this.cacheManager.purge(pattern);\n      }\n\n      this.purgeLog.push({\n        timestamp: Date.now(),\n        urls,\n        pattern,\n        count: purgedCount\n      });\n\n      res.json({\n        status: 'success',\n        purgedCount\n      });\n    });\n\n    // Get cache stats\n    app.get('/api/cdn/stats', async (req, res) => {\n      const info = await this.cacheManager.cache.info();\n      \n      res.json({\n        uptime: process.uptime(),\n        cacheInfo: this.parseCacheInfo(info),\n        recentPurges: this.purgeLog.slice(-10)\n      });\n    });\n  }\n\n  parseCacheInfo(info) {\n    const lines = info.split('\\r\\n');\n    const stats = {};\n    \n    lines.forEach(line => {\n      const [key, value] = line.split(':');\n      if (key && value) stats[key] = value;\n    });\n    \n    return {\n      usedMemory: stats.used_memory_human,\n      totalKeys: stats.db0?.split(',')[0]?.split('=')[1],\n      hitRate: stats.keyspace_hits / (stats.keyspace_hits + stats.keyspace_misses) * 100\n    };\n  }\n}\n\n// Initialize CDN Edge Server\nconst app = express();\napp.use(express.json());\n\nconst redisClient = redis.createClient({\n  host: process.env.REDIS_HOST || 'localhost',\n  port: process.env.REDIS_PORT || 6379\n});\n\nconst edgeServer = new EdgeServer(\n  process.env.ORIGIN_URL || 'https://origin.example.com',\n  redisClient\n);\n\nconst cdnAPI = new CDNControlAPI(edgeServer.cacheManager);\ncdnAPI.setupRoutes(app);\n\n// Main CDN proxy handler\napp.get('*', async (req, res) => {\n  await edgeServer.handle(req, res);\n});\n\n// Health check\napp.get('/_health', (req, res) => {\n  res.json({ status: 'healthy', timestamp: Date.now() });\n});\n\nconst PORT = process.env.PORT || 8080;\napp.listen(PORT, () => {\n  console.log(`CDN Edge Server running on port ${PORT}`);\n});\n\n// Usage Examples:\n// 1. Request image with optimization:\n//    GET /images/photo.jpg?width=800&format=webp\n//\n// 2. Purge cache:\n//    POST /api/cdn/purge\n//    Body: { \"urls\": [\"/images/photo.jpg\"] }\n//    Or: { \"pattern\": \"images\" }\n//\n// 3. Get stats:\n//   GET /api/cdn/stats"
    },
    {
      "id": 50,
      "question": "Design a Distributed File Storage System (like Dropbox, Google Drive)",
      "answer": "A distributed file storage system allows users to upload, store, sync, and share files across devices.\n\nCore Features:\n• File upload/download: Chunked uploads for large files\n• File synchronization: Keep files in sync across devices\n• File sharing: Share files/folders with permissions\n• Version control: Track file versions, restore previous versions\n• Search: Find files by name, content, metadata\n• Collaboration: Real-time collaborative editing\n• Offline access: Local cache with sync when online\n• Storage quotas: Per-user storage limits\n\nArchitecture Components:\n• Client Application: Desktop/mobile apps with local cache\n• Upload Service: Handle file uploads with chunking\n• Metadata Service: File names, paths, versions, permissions\n• Block Storage: Distributed storage (S3, HDFS) for file chunks\n• Sync Service: Detect changes and sync across devices\n• Notification Service: Notify clients of file changes\n• Search Service: Index and search file metadata/content\n\nFile Upload Flow:\n• Client splits file into chunks (4MB each)\n• Calculate hash for each chunk (deduplication)\n• Check if chunks already exist (hash-based)\n• Upload only new chunks to storage\n• Update metadata service with file info\n• Notify other devices to sync\n\nChunking Benefits:\n• Resume interrupted uploads\n• Parallel upload of chunks for speed\n• Deduplication: Same chunk stored once\n• Incremental sync: Only changed chunks uploaded\n• Bandwidth optimization\n\nSync Algorithm:\n• Client maintains local file tree with metadata\n• Periodically poll or listen (WebSocket) for server changes\n• Compare local vs server modification times\n• Download changes from server\n• Upload local changes to server\n• Conflict resolution: Last-write-wins or merge\n\nScalability:\n• Metadata sharding: Partition by user ID or file hash\n• Block storage: Replicate chunks across nodes (3x replication)\n• CDN: Serve frequently accessed files from edge\n• Caching: Cache metadata in Redis\n• Rate limiting: Prevent abuse",
      "explanation": "File storage systems chunk files for efficient upload/sync, use hash-based deduplication to save space, store blocks in distributed storage (S3), maintain metadata separately, and sync changes across devices using delta detection.",
      "difficulty": "Hard",
      "code": "// Distributed File Storage System\nconst express = require('express');\nconst multer = require('multer');\nconst AWS = require('aws-sdk');\nconst crypto = require('crypto');\nconst { v4: uuidv4 } = require('uuid');\n\n// Chunk Manager\nclass ChunkManager {\n  constructor(s3Client) {\n    this.s3 = s3Client;\n    this.bucket = 'file-storage-chunks';\n    this.chunkSize = 4 * 1024 * 1024;  // 4MB\n  }\n\n  // Split file into chunks\n  splitIntoChunks(buffer) {\n    const chunks = [];\n    let offset = 0;\n\n    while (offset < buffer.length) {\n      const chunk = buffer.slice(offset, offset + this.chunkSize);\n      const hash = this.calculateHash(chunk);\n      \n      chunks.push({\n        data: chunk,\n        hash,\n        size: chunk.length,\n        offset\n      });\n\n      offset += this.chunkSize;\n    }\n\n    return chunks;\n  }\n\n  calculateHash(buffer) {\n    return crypto.createHash('sha256').update(buffer).digest('hex');\n  }\n\n  // Check if chunk exists (deduplication)\n  async chunkExists(hash) {\n    try {\n      await this.s3.headObject({\n        Bucket: this.bucket,\n        Key: `chunks/${hash}`\n      }).promise();\n      return true;\n    } catch (error) {\n      if (error.code === 'NotFound') {\n        return false;\n      }\n      throw error;\n    }\n  }\n\n  // Upload chunk\n  async uploadChunk(chunk) {\n    const { hash, data } = chunk;\n\n    // Skip if already exists (deduplication)\n    if (await this.chunkExists(hash)) {\n      console.log(`Chunk ${hash} already exists, skipping upload`);\n      return { hash, action: 'skipped' };\n    }\n\n    await this.s3.putObject({\n      Bucket: this.bucket,\n      Key: `chunks/${hash}`,\n      Body: data\n    }).promise();\n\n    return { hash, action: 'uploaded' };\n  }\n\n  // Download chunk\n  async downloadChunk(hash) {\n    const result = await this.s3.getObject({\n      Bucket: this.bucket,\n      Key: `chunks/${hash}`\n    }).promise();\n\n    return result.Body;\n  }\n\n  // Assemble file from chunks\n  async assembleFile(chunkHashes) {\n    const chunks = await Promise.all(\n      chunkHashes.map(hash => this.downloadChunk(hash))\n    );\n\n    return Buffer.concat(chunks);\n  }\n}\n\n// Metadata Service\nclass MetadataService {\n  constructor(db) {\n    this.db = db;\n  }\n\n  async createFile(userId, fileData) {\n    const fileId = uuidv4();\n    const now = new Date();\n\n    await this.db.query(\n      `INSERT INTO files \n       (id, user_id, name, path, size, mime_type, chunks, version, created_at, modified_at) \n       VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`,\n      [\n        fileId,\n        userId,\n        fileData.name,\n        fileData.path,\n        fileData.size,\n        fileData.mimeType,\n        JSON.stringify(fileData.chunks),\n        1,\n        now,\n        now\n      ]\n    );\n\n    return {\n      id: fileId,\n      ...fileData,\n      version: 1,\n      createdAt: now,\n      modifiedAt: now\n    };\n  }\n\n  async updateFile(fileId, userId, updates) {\n    const connection = await this.db.getConnection();\n\n    try {\n      await connection.beginTransaction();\n\n      // Get current file\n      const [currentFile] = await connection.query(\n        'SELECT * FROM files WHERE id = ? AND user_id = ? FOR UPDATE',\n        [fileId, userId]\n      );\n\n      if (!currentFile) {\n        throw new Error('File not found');\n      }\n\n      // Archive current version\n      await connection.query(\n        `INSERT INTO file_versions \n         (file_id, version, chunks, size, modified_at) \n         VALUES (?, ?, ?, ?, ?)`,\n        [\n          fileId,\n          currentFile.version,\n          currentFile.chunks,\n          currentFile.size,\n          currentFile.modified_at\n        ]\n      );\n\n      // Update file\n      const newVersion = currentFile.version + 1;\n      await connection.query(\n        `UPDATE files \n         SET chunks = ?, size = ?, version = ?, modified_at = NOW() \n         WHERE id = ?`,\n        [JSON.stringify(updates.chunks), updates.size, newVersion, fileId]\n      );\n\n      await connection.commit();\n\n      return { fileId, version: newVersion };\n\n    } catch (error) {\n      await connection.rollback();\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n\n  async getFile(fileId, userId) {\n    const results = await this.db.query(\n      'SELECT * FROM files WHERE id = ? AND user_id = ?',\n      [fileId, userId]\n    );\n\n    if (results.length === 0) return null;\n\n    const file = results[0];\n    file.chunks = JSON.parse(file.chunks);\n    return file;\n  }\n\n  async listFiles(userId, path = '/') {\n    const results = await this.db.query(\n      'SELECT * FROM files WHERE user_id = ? AND path = ? ORDER BY name',\n      [userId, path]\n    );\n\n    return results.map(file => ({\n      ...file,\n      chunks: JSON.parse(file.chunks)\n    }));\n  }\n\n  async deleteFile(fileId, userId) {\n    await this.db.query(\n      'UPDATE files SET deleted_at = NOW() WHERE id = ? AND user_id = ?',\n      [fileId, userId]\n    );\n  }\n\n  async getFileVersions(fileId, userId) {\n    // Verify ownership\n    const [file] = await this.db.query(\n      'SELECT id FROM files WHERE id = ? AND user_id = ?',\n      [fileId, userId]\n    );\n\n    if (!file) throw new Error('File not found');\n\n    const versions = await this.db.query(\n      'SELECT * FROM file_versions WHERE file_id = ? ORDER BY version DESC',\n      [fileId]\n    );\n\n    return versions.map(v => ({\n      ...v,\n      chunks: JSON.parse(v.chunks)\n    }));\n  }\n}\n\n// File Upload Service\nclass FileUploadService {\n  constructor(chunkManager, metadataService) {\n    this.chunkManager = chunkManager;\n    this.metadata = metadataService;\n  }\n\n  async uploadFile(userId, file, path = '/') {\n    const buffer = file.buffer;\n\n    // Split into chunks\n    const chunks = this.chunkManager.splitIntoChunks(buffer);\n\n    // Upload chunks (with deduplication)\n    const uploadResults = await Promise.all(\n      chunks.map(chunk => this.chunkManager.uploadChunk(chunk))\n    );\n\n    const chunkHashes = uploadResults.map(r => r.hash);\n\n    // Create metadata\n    const fileMetadata = await this.metadata.createFile(userId, {\n      name: file.originalname,\n      path,\n      size: buffer.length,\n      mimeType: file.mimetype,\n      chunks: chunkHashes\n    });\n\n    const dedupCount = uploadResults.filter(r => r.action === 'skipped').length;\n\n    return {\n      file: fileMetadata,\n      stats: {\n        totalChunks: chunks.length,\n        newChunks: chunks.length - dedupCount,\n        dedupChunks: dedupCount,\n        savedBytes: dedupCount * this.chunkManager.chunkSize\n      }\n    };\n  }\n\n  async downloadFile(userId, fileId) {\n    // Get file metadata\n    const file = await this.metadata.getFile(fileId, userId);\n\n    if (!file) {\n      throw new Error('File not found');\n    }\n\n    // Assemble file from chunks\n    const fileBuffer = await this.chunkManager.assembleFile(file.chunks);\n\n    return {\n      buffer: fileBuffer,\n      name: file.name,\n      mimeType: file.mime_type\n    };\n  }\n}\n\n// Sync Service\nclass SyncService {\n  constructor(metadataService, notificationService) {\n    this.metadata = metadataService;\n    this.notifications = notificationService;\n  }\n\n  async getChangesSince(userId, lastSyncTime) {\n    const results = await this.metadata.db.query(\n      `SELECT * FROM files \n       WHERE user_id = ? AND modified_at > ? AND deleted_at IS NULL`,\n      [userId, new Date(lastSyncTime)]\n    );\n\n    const deleted = await this.metadata.db.query(\n      `SELECT id, path, name FROM files \n       WHERE user_id = ? AND deleted_at > ?`,\n      [userId, new Date(lastSyncTime)]\n    );\n\n    return {\n      updated: results.map(f => ({ ...f, chunks: JSON.parse(f.chunks) })),\n      deleted\n    };\n  }\n\n  async notifyFileChange(userId, fileId, action) {\n    // Notify all user's devices via WebSocket/Push\n    await this.notifications.send(userId, {\n      type: 'file_change',\n      fileId,\n      action,  // 'created', 'updated', 'deleted'\n      timestamp: Date.now()\n    });\n  }\n}\n\n// Express API\nconst app = express();\napp.use(express.json());\n\nconst upload = multer({ storage: multer.memoryStorage() });\n\nconst s3 = new AWS.S3();\nconst db = createDatabaseConnection();\n\nconst chunkManager = new ChunkManager(s3);\nconst metadataService = new MetadataService(db);\nconst uploadService = new FileUploadService(chunkManager, metadataService);\nconst syncService = new SyncService(metadataService, notificationService);\n\n// Upload file\napp.post('/api/files/upload', upload.single('file'), async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { path = '/' } = req.body;\n    \n    const result = await uploadService.uploadFile(userId, req.file, path);\n    \n    // Notify other devices\n    await syncService.notifyFileChange(userId, result.file.id, 'created');\n    \n    res.json(result);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Download file\napp.get('/api/files/:fileId/download', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { fileId } = req.params;\n    \n    const file = await uploadService.downloadFile(userId, fileId);\n    \n    res.set('Content-Type', file.mimeType);\n    res.set('Content-Disposition', `attachment; filename=\"${file.name}\"`);\n    res.send(file.buffer);\n  } catch (error) {\n    res.status(404).json({ error: error.message });\n  }\n});\n\n// List files\napp.get('/api/files', async (req, res) => {\n  const userId = req.user.id;\n  const { path = '/' } = req.query;\n  \n  const files = await metadataService.listFiles(userId, path);\n  res.json(files);\n});\n\n// Get changes for sync\napp.get('/api/sync/changes', async (req, res) => {\n  const userId = req.user.id;\n  const { since } = req.query;\n  \n  const changes = await syncService.getChangesSince(\n    userId,\n    parseInt(since)\n  );\n  \n  res.json(changes);\n});\n\n// Delete file\napp.delete('/api/files/:fileId', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { fileId } = req.params;\n    \n    await metadataService.deleteFile(fileId, userId);\n    await syncService.notifyFileChange(userId, fileId, 'deleted');\n    \n    res.json({ status: 'deleted' });\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Get file versions\napp.get('/api/files/:fileId/versions', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { fileId } = req.params;\n    \n    const versions = await metadataService.getFileVersions(fileId, userId);\n    res.json(versions);\n  } catch (error) {\n    res.status(404).json({ error: error.message });\n  }\n});"
    },
    {
      "id": 51,
      "question": "Design a Hotel Booking System",
      "answer": "A hotel booking system manages room inventory, reservations, and prevents double-booking with strong consistency.\n\nCore Features:\n• Search and filter: Find available rooms by location, dates, amenities, price\n• Real-time availability: Check room availability for date ranges\n• Booking management: Create, view, modify, cancel reservations\n• Payment processing: Secure payment with refund support\n• Inventory management: Room types, pricing, availability\n• Double-booking prevention: Pessimistic locking or optimistic concurrency\n• Dynamic pricing: Adjust rates based on demand, season, events\n• Review system: Guest reviews and ratings\n\nArchitecture Components:\n• Search Service: Index hotels and rooms for quick queries\n• Inventory Service: Track room availability\n• Booking Service: Create and manage reservations\n• Payment Service: Process payments with hold and capture\n• Pricing Service: Calculate rates based on demand\n• Notification Service: Send confirmations and reminders\n\nDouble-Booking Prevention:\n• Database-level locking: SELECT FOR UPDATE locks room record\n• Optimistic locking: Use version numbers, retry on conflict\n• Distributed lock: Redis lock per room-date combination\n• Two-phase commit: Reserve → Confirm within timeout\n\nBooking Flow:\n• User searches hotels for dates (check-in, check-out)\n• System queries available rooms aggregated by hotel\n• User selects room and initiates booking\n• Lock room for date range (pessimistic or optimistic)\n• Process payment (authorization hold)\n• Create reservation record\n• Release lock, capture payment\n• Send confirmation email\n\nAvailability Logic:\n• Room has total_count inventory\n• For each date, track booked_count\n• A room is available if booked_count < total_count\n• Query: return rooms where ALL dates in range have available capacity\n\nChallenges:\n• High concurrency: Many users booking same room simultaneously\n• Consistency: No double bookings under any circumstance\n• Performance: Fast availability checks across thousands of hotels",
      "explanation": "Hotel booking systems use pessimistic locking (SELECT FOR UPDATE) to prevent double-booking, track per-day inventory counts, implement two-phase booking (reserve→confirm), and use dynamic pricing based on demand.",
      "difficulty": "Hard",
      "code": "// Hotel Booking System Implementation\nconst express = require('express');\nconst mysql = require('mysql2/promise');\nconst redis = require('redis');\n\n// Inventory Service\nclass InventoryService {\n  constructor(db, cache) {\n    this.db = db;\n    this.cache = cache;\n  }\n\n  // Check availability with date range\n  async checkAvailability(roomTypeId, checkIn, checkOut) {\n    // Get all dates in range\n    const dates = this.getDateRange(checkIn, checkOut);\n\n    // Query inventory for each date\n    const placeholders = dates.map(() => '?').join(',');\n    const query = `\n      SELECT date, booked_count, total_count\n      FROM room_inventory\n      WHERE room_type_id = ? AND date IN (${placeholders})\n    `;\n\n    const results = await this.db.query(query, [roomTypeId, ...dates]);\n\n    // Check if all dates have availability\n    if (results[0].length !== dates.length) {\n      return { available: false, reason: 'Inventory not found for all dates' };\n    }\n\n    const unavailable = results[0].find(\n      row => row.booked_count >= row.total_count\n    );\n\n    if (unavailable) {\n      return {\n        available: false,\n        reason: 'Fully booked',\n        date: unavailable.date\n      };\n    }\n\n    return { available: true, availableRooms: results[0][0].total_count - results[0][0].booked_count };\n  }\n\n  // Search available hotels\n  async searchHotels(searchParams) {\n    const { location, checkIn, checkOut, guests, minPrice, maxPrice } = searchParams;\n\n    // Get all dates in range\n    const dates = this.getDateRange(checkIn, checkOut);\n\n    // Complex query to find hotels with available rooms\n    const query = `\n      SELECT DISTINCT\n        h.id, h.name, h.location, h.rating, h.image_url,\n        rt.id as room_type_id, rt.name as room_name,\n        rt.capacity, rt.base_price,\n        MIN(rt.total_count - COALESCE(ri.booked_count, 0)) as available_rooms\n      FROM hotels h\n      JOIN room_types rt ON h.id = rt.hotel_id\n      LEFT JOIN room_inventory ri ON rt.id = ri.room_type_id\n      WHERE h.location LIKE ?\n        AND rt.capacity >= ?\n        AND rt.base_price BETWEEN ? AND ?\n        AND ri.date IN (${dates.map(() => '?').join(',')})\n      GROUP BY h.id, rt.id\n      HAVING COUNT(DISTINCT ri.date) = ? \n         AND MIN(rt.total_count - COALESCE(ri.booked_count, 0)) > 0\n      ORDER BY h.rating DESC, rt.base_price ASC\n      LIMIT 50\n    `;\n\n    const results = await this.db.query(query, [\n      `%${location}%`,\n      guests,\n      minPrice || 0,\n      maxPrice || 999999,\n      ...dates,\n      dates.length\n    ]);\n\n    return results[0];\n  }\n\n  getDateRange(checkIn, checkOut) {\n    const dates = [];\n    const current = new Date(checkIn);\n    const end = new Date(checkOut);\n\n    while (current < end) {\n      dates.push(current.toISOString().split('T')[0]);\n      current.setDate(current.getDate() + 1);\n    }\n\n    return dates;\n  }\n\n  // Initialize inventory for date range\n  async initializeInventory(roomTypeId, startDate, endDate, totalCount) {\n    const dates = this.getDateRange(startDate, endDate);\n\n    const values = dates.map(date => [roomTypeId, date, totalCount, 0]);\n\n    await this.db.query(\n      `INSERT INTO room_inventory (room_type_id, date, total_count, booked_count) \n       VALUES ? \n       ON DUPLICATE KEY UPDATE total_count = VALUES(total_count)`,\n      [values]\n    );\n  }\n}\n\n// Booking Service with Double-Booking Prevention\nclass BookingService {\n  constructor(db, inventoryService, paymentService) {\n    this.db = db;\n    this.inventory = inventoryService;\n    this.payment = paymentService;\n  }\n\n  // Create booking with pessimistic locking\n  async createBooking(bookingRequest) {\n    const connection = await this.db.getConnection();\n\n    try {\n      await connection.beginTransaction();\n\n      const { userId, roomTypeId, checkIn, checkOut, guestCount, paymentMethod } = bookingRequest;\n\n      // Get date range\n      const dates = this.inventory.getDateRange(checkIn, checkOut);\n\n      // Lock inventory rows for update (pessimistic locking)\n      const placeholders = dates.map(() => '?').join(',');\n      const lockQuery = `\n        SELECT date, booked_count, total_count\n        FROM room_inventory\n        WHERE room_type_id = ? AND date IN (${placeholders})\n        FOR UPDATE\n      `;\n\n      const [inventoryRows] = await connection.query(lockQuery, [roomTypeId, ...dates]);\n\n      // Verify availability under lock\n      if (inventoryRows.length !== dates.length) {\n        throw new Error('Inventory not initialized for all dates');\n      }\n\n      const fullyBooked = inventoryRows.find(row => row.booked_count >= row.total_count);\n      if (fullyBooked) {\n        throw new Error(`Room fully booked on ${fullyBooked.date}`);\n      }\n\n      // Calculate total price\n      const [roomType] = await connection.query(\n        'SELECT base_price, hotel_id FROM room_types WHERE id = ?',\n        [roomTypeId]\n      );\n\n      const nightsCount = dates.length;\n      const totalPrice = roomType[0].base_price * nightsCount;\n\n      // Process payment authorization\n      const paymentAuth = await this.payment.authorize(\n        userId,\n        totalPrice,\n        paymentMethod\n      );\n\n      // Create booking record\n      const bookingId = this.generateBookingId();\n      await connection.query(\n        `INSERT INTO bookings \n         (id, user_id, room_type_id, hotel_id, check_in, check_out, guest_count, \n          total_price, payment_auth_id, status, created_at) \n         VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 'confirmed', NOW())`,\n        [\n          bookingId,\n          userId,\n          roomTypeId,\n          roomType[0].hotel_id,\n          checkIn,\n          checkOut,\n          guestCount,\n          totalPrice,\n          paymentAuth.id\n        ]\n      );\n\n      // Update inventory: increment booked_count for each date\n      const updateQuery = `\n        UPDATE room_inventory \n        SET booked_count = booked_count + 1 \n        WHERE room_type_id = ? AND date IN (${placeholders})\n      `;\n      await connection.query(updateQuery, [roomTypeId, ...dates]);\n\n      // Capture payment\n      await this.payment.capture(paymentAuth.id);\n\n      // Commit transaction\n      await connection.commit();\n\n      return {\n        bookingId,\n        totalPrice,\n        nightsCount,\n        status: 'confirmed'\n      };\n\n    } catch (error) {\n      await connection.rollback();\n      console.error('Booking error:', error);\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n\n  // Alternative: Optimistic Locking Approach\n  async createBookingOptimistic(bookingRequest) {\n    const maxRetries = 3;\n    let attempt = 0;\n\n    while (attempt < maxRetries) {\n      try {\n        return await this.attemptBooking(bookingRequest);\n      } catch (error) {\n        if (error.message.includes('version mismatch') && attempt < maxRetries - 1) {\n          attempt++;\n          await this.sleep(100 * attempt);  // Exponential backoff\n          continue;\n        }\n        throw error;\n      }\n    }\n  }\n\n  async attemptBooking(bookingRequest) {\n    const connection = await this.db.getConnection();\n\n    try {\n      await connection.beginTransaction();\n\n      const { roomTypeId, checkIn, checkOut } = bookingRequest;\n      const dates = this.inventory.getDateRange(checkIn, checkOut);\n\n      // Read current version and inventory\n      const [inventoryRows] = await connection.query(\n        `SELECT date, booked_count, total_count, version \n         FROM room_inventory \n         WHERE room_type_id = ? AND date IN (${dates.map(() => '?').join(',')})`,\n        [roomTypeId, ...dates]\n      );\n\n      // Check availability\n      const fullyBooked = inventoryRows.find(row => row.booked_count >= row.total_count);\n      if (fullyBooked) {\n        throw new Error('Room fully booked');\n      }\n\n      // ... (payment and booking creation)\n\n      // Update with version check (optimistic locking)\n      for (const row of inventoryRows) {\n        const [result] = await connection.query(\n          `UPDATE room_inventory \n           SET booked_count = booked_count + 1, version = version + 1 \n           WHERE room_type_id = ? AND date = ? AND version = ?`,\n          [roomTypeId, row.date, row.version]\n        );\n\n        if (result.affectedRows === 0) {\n          throw new Error('version mismatch');  // Retry\n        }\n      }\n\n      await connection.commit();\n      return { bookingId: this.generateBookingId() };\n\n    } catch (error) {\n      await connection.rollback();\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n\n  // Cancel booking\n  async cancelBooking(bookingId, userId) {\n    const connection = await this.db.getConnection();\n\n    try {\n      await connection.beginTransaction();\n\n      // Get booking details\n      const [booking] = await connection.query(\n        'SELECT * FROM bookings WHERE id = ? AND user_id = ? FOR UPDATE',\n        [bookingId, userId]\n      );\n\n      if (!booking[0]) {\n        throw new Error('Booking not found');\n      }\n\n      if (booking[0].status === 'cancelled') {\n        throw new Error('Booking already cancelled');\n      }\n\n      // Check cancellation policy\n      const daysTilCheckIn = this.getDaysDifference(\n        new Date(),\n        new Date(booking[0].check_in)\n      );\n\n      let refundAmount = booking[0].total_price;\n      if (daysTilCheckIn < 7) {\n        refundAmount *= 0.5;  // 50% refund if less than 7 days\n      }\n\n      // Update booking status\n      await connection.query(\n        'UPDATE bookings SET status = ?, cancelled_at = NOW() WHERE id = ?',\n        ['cancelled', bookingId]\n      );\n\n      // Release inventory\n      const dates = this.inventory.getDateRange(\n        booking[0].check_in,\n        booking[0].check_out\n      );\n\n      await connection.query(\n        `UPDATE room_inventory \n         SET booked_count = booked_count - 1 \n         WHERE room_type_id = ? AND date IN (${dates.map(() => '?').join(',')})`,\n        [booking[0].room_type_id, ...dates]\n      );\n\n      // Process refund\n      await this.payment.refund(\n        booking[0].payment_auth_id,\n        refundAmount\n      );\n\n      await connection.commit();\n\n      return { status: 'cancelled', refundAmount };\n\n    } catch (error) {\n      await connection.rollback();\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n\n  generateBookingId() {\n    return `BK${Date.now()}${Math.random().toString(36).substr(2, 6).toUpperCase()}`;\n  }\n\n  getDaysDifference(date1, date2) {\n    const diff = date2 - date1;\n    return Math.ceil(diff / (1000 * 60 * 60 * 24));\n  }\n\n  sleep(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n}\n\n// Dynamic Pricing Service\nclass PricingService {\n  constructor(db) {\n    this.db = db;\n  }\n\n  async calculatePrice(roomTypeId, checkIn, checkOut) {\n    const [roomType] = await this.db.query(\n      'SELECT base_price FROM room_types WHERE id = ?',\n      [roomTypeId]\n    );\n\n    const basePrice = roomType[0].base_price;\n    const dates = this.getDateRange(checkIn, checkOut);\n\n    let totalPrice = 0;\n\n    for (const date of dates) {\n      // Check for seasonal pricing, events, demand\n      const multiplier = await this.getDemandMultiplier(roomTypeId, date);\n      totalPrice += basePrice * multiplier;\n    }\n\n    return totalPrice;\n  }\n\n  async getDemandMultiplier(roomTypeId, date) {\n    // Get occupancy rate for the date\n    const [inventory] = await this.db.query(\n      'SELECT booked_count, total_count FROM room_inventory WHERE room_type_id = ? AND date = ?',\n      [roomTypeId, date]\n    );\n\n    if (!inventory[0]) return 1.0;\n\n    const occupancyRate = inventory[0].booked_count / inventory[0].total_count;\n\n    // Dynamic pricing based on occupancy\n    if (occupancyRate > 0.9) return 1.5;     // 90%+ booked: 1.5x price\n    if (occupancyRate > 0.75) return 1.25;   // 75-90%: 1.25x\n    if (occupancyRate > 0.5) return 1.1;     // 50-75%: 1.1x\n    if (occupancyRate < 0.25) return 0.85;   // <25%: 15% discount\n\n    return 1.0;  // Normal pricing\n  }\n\n  getDateRange(checkIn, checkOut) {\n    const dates = [];\n    const current = new Date(checkIn);\n    const end = new Date(checkOut);\n\n    while (current < end) {\n      dates.push(current.toISOString().split('T')[0]);\n      current.setDate(current.getDate() + 1);\n    }\n\n    return dates;\n  }\n}\n\n// Express API\nconst app = express();\napp.use(express.json());\n\nconst db = mysql.createPool({\n  host: 'localhost',\n  user: 'root',\n  database: 'hotel_booking',\n  waitForConnections: true,\n  connectionLimit: 10\n});\n\nconst cache = redis.createClient();\nconst inventoryService = new InventoryService(db, cache);\nconst paymentService = new PaymentService();\nconst bookingService = new BookingService(db, inventoryService, paymentService);\nconst pricingService = new PricingService(db);\n\n// Search hotels\napp.get('/api/hotels/search', async (req, res) => {\n  try {\n    const { location, checkIn, checkOut, guests, minPrice, maxPrice } = req.query;\n\n    const hotels = await inventoryService.searchHotels({\n      location,\n      checkIn,\n      checkOut,\n      guests: parseInt(guests),\n      minPrice: parseFloat(minPrice),\n      maxPrice: parseFloat(maxPrice)\n    });\n\n    res.json(hotels);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Check availability\napp.get('/api/rooms/:roomTypeId/availability', async (req, res) => {\n  try {\n    const { roomTypeId } = req.params;\n    const { checkIn, checkOut } = req.query;\n\n    const availability = await inventoryService.checkAvailability(\n      roomTypeId,\n      checkIn,\n      checkOut\n    );\n\n    res.json(availability);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Create booking\napp.post('/api/bookings', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { roomTypeId, checkIn, checkOut, guestCount, paymentMethod } = req.body;\n\n    const booking = await bookingService.createBooking({\n      userId,\n      roomTypeId,\n      checkIn,\n      checkOut,\n      guestCount,\n      paymentMethod\n    });\n\n    res.json(booking);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Cancel booking\napp.post('/api/bookings/:bookingId/cancel', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { bookingId } = req.params;\n\n    const result = await bookingService.cancelBooking(bookingId, userId);\n    res.json(result);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Get booking\napp.get('/api/bookings/:bookingId', async (req, res) => {\n  const userId = req.user.id;\n  const { bookingId } = req.params;\n\n  const [booking] = await db.query(\n    'SELECT * FROM bookings WHERE id = ? AND user_id = ?',\n    [bookingId, userId]\n  );\n\n  if (!booking[0]) {\n    return res.status(404).json({ error: 'Booking not found' });\n  }\n\n  res.json(booking[0]);\n});\n\napp.listen(3000);"
    },
    {
      "id": 52,
      "question": "Design an E-commerce Platform",
      "answer": "An e-commerce platform enables buying and selling products online with inventory, orders, and payments.\n\nCore Features:\n• Product catalog: Browse products with categories, filters, search\n• Shopping cart: Add/remove items, apply coupons\n• Inventory management: Track stock levels, prevent overselling\n• Order processing: Create orders, payment, fulfillment\n• Payment gateway: Multiple payment methods\n• Order tracking: Real-time status updates\n• Review and ratings: User-generated content\n• Recommendations: Personalized product suggestions\n\nArchitecture Components:\n• Product Service: Manage product catalog\n• Cart Service: Shopping cart management\n• Inventory Service: Stock tracking with reservations\n• Order Service: Order lifecycle management\n• Payment Service: Payment processing\n• Search Service: Elasticsearch for product search\n• Recommendation Engine: Collaborative filtering, ML-based\n• Notification Service: Order updates via email/SMS\n\nInventory Management:\n• Track available_quantity per product\n• Reserve items when added to cart (soft reservation, expires)\n• Deduct quantity when order confirmed (hard reservation)\n• Handle concurrent purchases with locking or optimistic concurrency\n• Support backorders and pre-orders\n\nOrder Flow:\n• User adds items to cart\n• Cart validates inventory availability\n• User proceeds to checkout\n• Reserve inventory for items\n• Process payment\n• Create order with status 'confirmed'\n• Deduct inventory\n• Trigger fulfillment workflow\n• Send confirmation\n\nStock Management Strategies:\n• Pessimistic locking: Lock inventory row during checkout\n• Optimistic locking: Version-based updates, retry on conflict\n• Distributed lock: Redis lock per product SKU\n• Pre-allocate inventory: Reserve batch for high-demand items\n\nChallen ges:\n• Flash sales: Massive concurrent requests for limited stock\n• Cart abandonment: Release reserved inventory after timeout\n• Distributed inventory: Multiple warehouses with different stock\n• Price consistency: Handle price changes during checkout",
      "explanation": "E-commerce platforms use inventory reservations (soft→hard on payment), optimistic/pessimistic locking to prevent overselling, search engines (Elasticsearch) for products, and order state machines for lifecycle management.",
      "difficulty": "Hard",
      "code": "// E-commerce Platform Implementation\nconst express = require('express');\nconst redis = require('redis');\n\n// Product Service\nclass ProductService {\n  constructor(db, searchEngine) {\n    this.db = db;\n    this.search = searchEngine;\n  }\n\n  async searchProducts(query, filters = {}) {\n    const { category, minPrice, maxPrice, sortBy = 'relevance' } = filters;\n\n    // Elasticsearch query\n    const searchQuery = {\n      bool: {\n        must: [{\n          multi_match: {\n            query,\n            fields: ['name^3', 'description', 'brand', 'tags']\n          }\n        }],\n        filter: []\n      }\n    };\n\n    if (category) {\n      searchQuery.bool.filter.push({ term: { category } });\n    }\n\n    if (minPrice || maxPrice) {\n      searchQuery.bool.filter.push({\n        range: {\n          price: {\n            gte: minPrice || 0,\n            lte: maxPrice || 999999\n          }\n        }\n      });\n    }\n\n    const sortOrder = sortBy === 'price_low' ? [{ price: 'asc' }] :\n                      sortBy === 'price_high' ? [{ price: 'desc' }] :\n                      sortBy === 'rating' ? [{ rating: 'desc' }] :\n                      ['_score'];  // relevance\n\n    const results = await this.search.search({\n      index: 'products',\n      body: {\n        query: searchQuery,\n        sort: sortOrder,\n        size: 50\n      }\n    });\n\n    return results.hits.hits.map(hit => hit._source);\n  }\n\n  async getProduct(productId) {\n    const [product] = await this.db.query(\n      'SELECT * FROM products WHERE id = ?',\n      [productId]\n    );\n\n    return product[0];\n  }\n}\n\n// Cart Service\nclass CartService {\n  constructor(cache, inventoryService) {\n    this.cache = cache;\n    this.inventory = inventoryService;\n    this.cartTTL = 3600;  // 1 hour\n  }\n\n  async addToCart(userId, productId, quantity) {\n    // Check inventory\n    const available = await this.inventory.checkAvailability(productId, quantity);\n    \n    if (!available) {\n      throw new Error('Insufficient stock');\n    }\n\n    // Get cart\n    const cart = await this.getCart(userId);\n\n    // Add or update item\n    const existingItem = cart.items.find(item => item.productId === productId);\n    \n    if (existingItem) {\n      existingItem.quantity += quantity;\n    } else {\n      cart.items.push({ productId, quantity });\n    }\n\n    // Soft reserve inventory\n    await this.inventory.reserveStock(productId, quantity, `cart:${userId}`);\n\n    // Save cart\n    await this.saveCart(userId, cart);\n\n    return cart;\n  }\n\n  async removeFromCart(userId, productId) {\n    const cart = await this.getCart(userId);\n\n    const itemIndex = cart.items.findIndex(item => item.productId === productId);\n    \n    if (itemIndex === -1) {\n      throw new Error('Item not in cart');\n    }\n\n    const item = cart.items[itemIndex];\n    \n    // Release reservation\n    await this.inventory.releaseReservation(`cart:${userId}`, productId, item.quantity);\n\n    cart.items.splice(itemIndex, 1);\n    await this.saveCart(userId, cart);\n\n    return cart;\n  }\n\n  async getCart(userId) {\n    const cached = await this.cache.get(`cart:${userId}`);\n    \n    if (cached) {\n      return JSON.parse(cached);\n    }\n\n    return { items: [], userId };\n  }\n\n  async saveCart(userId, cart) {\n    await this.cache.setex(\n      `cart:${userId}`,\n      this.cartTTL,\n      JSON.stringify(cart)\n    );\n  }\n\n  async clearCart(userId) {\n    // Release all reservations\n    const cart = await this.getCart(userId);\n    \n    for (const item of cart.items) {\n      await this.inventory.releaseReservation(\n        `cart:${userId}`,\n        item.productId,\n        item.quantity\n      );\n    }\n\n    await this.cache.del(`cart:${userId}`);\n  }\n}\n\n// Inventory Service with Reservation System\nclass InventoryService {\n  constructor(db, cache) {\n    this.db = db;\n    this.cache = cache;\n    this.reservationTTL = 900;  // 15 minutes\n  }\n\n  async checkAvailability(productId, quantity) {\n    const [product] = await this.db.query(\n      'SELECT available_quantity FROM inventory WHERE product_id = ?',\n      [productId]\n    );\n\n    if (!product[0]) return false;\n\n    return product[0].available_quantity >= quantity;\n  }\n\n  // Soft reservation (for cart)\n  async reserveStock(productId, quantity, reservationId) {\n    const key = `reservation:${reservationId}:${productId}`;\n    \n    await this.cache.setex(\n      key,\n      this.reservationTTL,\n      quantity.toString()\n    );\n\n    // Track total reserved\n    await this.cache.hincrby(\n      `reserved:${productId}`,\n      reservationId,\n      quantity\n    );\n  }\n\n  // Release reservation\n  async releaseReservation(reservationId, productId, quantity) {\n    await this.cache.del(`reservation:${reservationId}:${productId}`);\n    await this.cache.hincrby(\n      `reserved:${productId}`,\n      reservationId,\n      -quantity\n    );\n  }\n\n  // Hard deduction (when order confirmed)\n  async deductStock(productId, quantity, orderId) {\n    const connection = await this.db.getConnection();\n\n    try {\n      await connection.beginTransaction();\n\n      // Lock inventory row\n      const [inventory] = await connection.query(\n        'SELECT available_quantity FROM inventory WHERE product_id = ? FOR UPDATE',\n        [productId]\n      );\n\n      if (!inventory[0]) {\n        throw new Error('Product not found');\n      }\n\n      if (inventory[0].available_quantity < quantity) {\n        throw new Error('Insufficient stock');\n      }\n\n      // Deduct stock\n      await connection.query(\n        'UPDATE inventory SET available_quantity = available_quantity - ? WHERE product_id = ?',\n        [quantity, productId]\n      );\n\n      // Log stock movement\n      await connection.query(\n        'INSERT INTO inventory_log (product_id, quantity_change, reason, reference_id, created_at) VALUES (?, ?, ?, ?, NOW())',\n        [productId, -quantity, 'order', orderId]\n      );\n\n      await connection.commit();\n\n    } catch (error) {\n      await connection.rollback();\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n\n  async restoreStock(productId, quantity, orderId) {\n    await this.db.query(\n      'UPDATE inventory SET available_quantity = available_quantity + ? WHERE product_id = ?',\n      [quantity, productId]\n    );\n\n    await this.db.query(\n      'INSERT INTO inventory_log (product_id, quantity_change, reason, reference_id, created_at) VALUES (?, ?, ?, ?, NOW())',\n      [productId, quantity, 'order_cancelled', orderId]\n    );\n  }\n}\n\n// Order Service\nclass OrderService {\n  constructor(db, inventoryService, paymentService, cartService) {\n    this.db = db;\n    this.inventory = inventoryService;\n    this.payment = paymentService;\n    this.cart = cartService;\n  }\n\n  async createOrder(userId, shippingAddress, paymentMethod) {\n    const connection = await this.db.getConnection();\n\n    try {\n      await connection.beginTransaction();\n\n      // Get cart\n      const cart = await this.cart.getCart(userId);\n      \n      if (cart.items.length === 0) {\n        throw new Error('Cart is empty');\n      }\n\n      // Get product details and calculate total\n      let totalAmount = 0;\n      const orderItems = [];\n\n      for (const item of cart.items) {\n        const [product] = await connection.query(\n          'SELECT id, name, price FROM products WHERE id = ?',\n          [item.productId]\n        );\n\n        if (!product[0]) {\n          throw new Error(`Product ${item.productId} not found`);\n        }\n\n        const itemTotal = product[0].price * item.quantity;\n        totalAmount += itemTotal;\n\n        orderItems.push({\n          productId: product[0].id,\n          name: product[0].name,\n          price: product[0].price,\n          quantity: item.quantity,\n          subtotal: itemTotal\n        });\n      }\n\n      // Deduct inventory for all items\n      for (const item of cart.items) {\n        await this.inventory.deductStock(\n          item.productId,\n          item.quantity,\n          'pending'  // Will update with actual order ID\n        );\n      }\n\n      // Process payment\n      const paymentResult = await this.payment.charge(\n        userId,\n        totalAmount,\n        paymentMethod\n      );\n\n      // Create order\n      const orderId = this.generateOrderId();\n      await connection.query(\n        `INSERT INTO orders \n         (id, user_id, total_amount, shipping_address, payment_id, status, created_at) \n         VALUES (?, ?, ?, ?, ?, 'confirmed', NOW())`,\n        [orderId, userId, totalAmount, JSON.stringify(shippingAddress), paymentResult.id]\n      );\n\n      // Create order items\n      for (const item of orderItems) {\n        await connection.query(\n          'INSERT INTO order_items (order_id, product_id, name, price, quantity, subtotal) VALUES (?, ?, ?, ?, ?, ?)',\n          [orderId, item.productId, item.name, item.price, item.quantity, item.subtotal]\n        );\n      }\n\n      // Clear cart and reservations\n      await this.cart.clearCart(userId);\n\n      await connection.commit();\n\n      return {\n        orderId,\n        totalAmount,\n        items: orderItems,\n        status: 'confirmed'\n      };\n\n    } catch (error) {\n      await connection.rollback();\n      console.error('Order creation error:', error);\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n\n  async cancelOrder(orderId, userId) {\n    const connection = await this.db.getConnection();\n\n    try {\n      await connection.beginTransaction();\n\n      // Get order\n      const [[order]] = await connection.query(\n        'SELECT * FROM orders WHERE id = ? AND user_id = ? FOR UPDATE',\n        [orderId, userId]\n      );\n\n      if (!order) {\n        throw new Error('Order not found');\n      }\n\n      if (order.status !== 'confirmed') {\n        throw new Error('Order cannot be cancelled');\n      }\n\n      // Get order items\n      const [items] = await connection.query(\n        'SELECT product_id, quantity FROM order_items WHERE order_id = ?',\n        [orderId]\n      );\n\n      // Restore inventory\n      for (const item of items) {\n        await this.inventory.restoreStock(\n          item.product_id,\n          item.quantity,\n          orderId\n        );\n      }\n\n      // Refund payment\n      await this.payment.refund(order.payment_id, order.total_amount);\n\n      // Update order status\n      await connection.query(\n        'UPDATE orders SET status = ?, cancelled_at = NOW() WHERE id = ?',\n        ['cancelled', orderId]\n      );\n\n      await connection.commit();\n\n      return { status: 'cancelled', refundAmount: order.total_amount };\n\n    } catch (error) {\n      await connection.rollback();\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n\n  generateOrderId() {\n    return `ORD${Date.now()}${Math.random().toString(36).substr(2, 6).toUpperCase()}`;\n  }\n}\n\n// Express API\nconst app = express();\napp.use(express.json());\n\nconst db = createDatabaseConnection();\nconst cache = redis.createClient();\nconst searchEngine = createElasticsearchClient();\n\nconst inventoryService = new InventoryService(db, cache);\nconst productService = new ProductService(db, searchEngine);\nconst cartService = new CartService(cache, inventoryService);\nconst paymentService = new PaymentService();\nconst orderService = new OrderService(db, inventoryService, paymentService, cartService);\n\n// Search products\napp.get('/api/products/search', async (req, res) => {\n  const { q, category, minPrice, maxPrice, sortBy } = req.query;\n  \n  const products = await productService.searchProducts(q, {\n    category,\n    minPrice: parseFloat(minPrice),\n    maxPrice: parseFloat(maxPrice),\n    sortBy\n  });\n  \n  res.json(products);\n});\n\n// Add to cart\napp.post('/api/cart/items', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { productId, quantity } = req.body;\n    \n    const cart = await cartService.addToCart(userId, productId, quantity);\n    res.json(cart);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Get cart\napp.get('/api/cart', async (req, res) => {\n  const userId = req.user.id;\n  const cart = await cartService.getCart(userId);\n  res.json(cart);\n});\n\n//Create order\napp.post('/api/orders', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { shippingAddress, paymentMethod } = req.body;\n    \n    const order = await orderService.createOrder(\n      userId,\n      shippingAddress,\n      paymentMethod\n    );\n    \n    res.json(order);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Cancel order\napp.post('/api/orders/:orderId/cancel', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { orderId } = req.params;\n    \n    const result = await orderService.cancelOrder(orderId, userId);\n    res.json(result);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\napp.listen(3000);"
    },
    {
      "id": 53,
      "question": "Design a Stock Trading Platform",
      "answer": "A stock trading platform enables buying and selling of stocks with real-time order matching and execution.\n\nCore Features:\n• Order placement: Market orders, limit orders, stop-loss orders\n• Order matching: Match buy and sell orders\n• Portfolio management: Track holdings and P&L\n• Real-time quotes: Live stock prices\n• Order book: Visualize buy/sell orders at different price levels\n• Trade history: Past trades and transactions\n• Risk management: Position limits, margin requirements\n• Market data: Charts, indicators, news feeds\n\nArchitecture Components:\n• Trading Engine: Core matching engine for orders\n• Order Management System (OMS): Validate and route orders\n• Market Data Service: Real-time price feeds\n• Portfolio Service: Track positions and balances\n• Risk Management: Pre-trade and post-trade checks\n• Settlement Service: Clear and settle trades\n• WebSocket Server: Push real-time updates to clients\n\nOrder Matching Algorithm (Price-Time Priority):\n• Maintain two order books: Buy orders (bids), Sell orders (asks)\n• Bids sorted by price DESC (highest first), then time ASC\n• Asks sorted by price ASC (lowest first), then time ASC\n• When new order arrives:\n  - If market order: Match immediately with best available price\n  - If limit order: Try to match with opposite orders\n  - If partial match: Keep remainder in order book\n  - If no match: Add to order book\n• Best bid and best ask form the \"spread\"\n\nOrder Types:\n• Market Order: Execute immediately at current market price\n• Limit Order: Execute only at specified price or better\n• Stop-Loss Order: Trigger market order when  price hits stop price\n• Stop-Limit Order: Trigger limit order when price hits stop price\n• Good-Till-Cancelled (GTC): Remains until executed or cancelled\n• Day Order: Expires at market close\n\nChallenges:\n• Ultra-low latency: Microsecond-level order processing\n• High throughput: Handle millions of orders per second\n• Fairness: Price-time priority strictly enforced\n• Atomicity: Order matching must be atomic\n• Data consistency: All clients see same order book state",
      "explanation": "Stock trading platforms use in-memory order books sorted by price-time priority, matching engines that atomically match buy/sell orders, real-time WebSocket updates for quotes, and risk checks before order execution.",
      "difficulty": "Hard",
      "code": "// Stock Trading Platform Implementation\nconst WebSocket = require('ws');\nconst express = require('express');\nconst EventEmitter = require('events');\n\n// Order Book (Priority Queue-based)\nclass OrderBook {\n  constructor(symbol) {\n    this.symbol = symbol;\n    this.bids = [];  // Buy orders: sorted by price DESC, time ASC\n    this.asks = [];  // Sell orders: sorted by price ASC, time ASC\n    this.trades = [];\n    this.lastPrice = null;\n  }\n\n  // Add order to book\n  addOrder(order) {\n    const queue = order.side === 'buy' ? this.bids : this.asks;\n    \n    // Insert in sorted order\n    let inserted = false;\n    for (let i = 0; i < queue.length; i++) {\n      if (this.shouldInsertBefore(order, queue[i])) {\n        queue.splice(i, 0, order);\n        inserted = true;\n        break;\n      }\n    }\n    \n    if (!inserted) {\n      queue.push(order);\n    }\n  }\n\n  shouldInsertBefore(newOrder, existingOrder) {\n    if (newOrder.side === 'buy') {\n      // Bids: Higher price first, then earlier time\n      if (newOrder.price > existingOrder.price) return true;\n      if (newOrder.price < existingOrder.price) return false;\n      return newOrder.timestamp < existingOrder.timestamp;\n    } else {\n      // Asks: Lower price first, then earlier time\n      if (newOrder.price < existingOrder.price) return true;\n      if (newOrder.price > existingOrder.price) return false;\n      return newOrder.timestamp < existingOrder.timestamp;\n    }\n  }\n\n  // Remove order from book\n  removeOrder(orderId, side) {\n    const queue = side === 'buy' ? this.bids : this.asks;\n    const index = queue.findIndex(o => o.id === orderId);\n    \n    if (index !== -1) {\n      queue.splice(index, 1);\n      return true;\n    }\n    \n    return false;\n  }\n\n  // Get best bid (highest buy price)\n  getBestBid() {\n    return this.bids.length > 0 ? this.bids[0] : null;\n  }\n\n  // Get best ask (lowest sell price)\n  getBestAsk() {\n    return this.asks.length > 0 ? this.asks[0] : null;\n  }\n\n  // Get spread\n  getSpread() {\n    const bestBid = this.getBestBid();\n    const bestAsk = this.getBestAsk();\n    \n    if (!bestBid || !bestAsk) return null;\n    \n    return {\n      bid: bestBid.price,\n      ask: bestAsk.price,\n      spread: bestAsk.price - bestBid.price\n    };\n  }\n\n  // Get order book depth (aggregated by price level)\n  getDepth(levels = 10) {\n    const aggregateBids = this.aggregateOrders(this.bids.slice(0, levels * 5));\n    const aggregateAsks = this.aggregateOrders(this.asks.slice(0, levels * 5));\n    \n    return {\n      bids: aggregateBids.slice(0, levels),\n      asks: aggregateAsks.slice(0, levels)\n    };\n  }\n\n  aggregateOrders(orders) {\n    const levels = new Map();\n    \n    orders.forEach(order => {\n      const existing = levels.get(order.price) || { price: order.price, quantity: 0, orders: 0 };\n      existing.quantity += order.quantity;\n      existing.orders += 1;\n      levels.set(order.price, existing);\n    });\n    \n    return Array.from(levels.values());\n  }\n}\n\n// Matching Engine\nclass MatchingEngine extends EventEmitter {\n  constructor() {\n    super();\n    this.orderBooks = new Map();  // symbol -> OrderBook\n    this.orders = new Map();      // orderId -> Order\n  }\n\n  getOrderBook(symbol) {\n    if (!this.orderBooks.has(symbol)) {\n      this.orderBooks.set(symbol, new OrderBook(symbol));\n    }\n    return this.orderBooks.get(symbol);\n  }\n\n  // Submit new order\n  async submitOrder(order) {\n    const orderBook = this.getOrderBook(order.symbol);\n    \n    // Store order\n    this.orders.set(order.id, order);\n    \n    // Try to match\n    if (order.type === 'market') {\n      await this.matchMarketOrder(order, orderBook);\n    } else if (order.type === 'limit') {\n      await this.matchLimitOrder(order, orderBook);\n    }\n    \n    // If order not fully filled, add to book\n    if (order.quantity > 0 && order.type === 'limit') {\n      orderBook.addOrder(order);\n      this.emit('order_placed', order);\n    }\n    \n    // Emit order book update\n    this.emit('orderbook_update', {\n      symbol: order.symbol,\n      depth: orderBook.getDepth()\n    });\n    \n    return order;\n  }\n\n  async matchMarketOrder(order, orderBook) {\n    const oppositeQueue = order.side === 'buy' ? orderBook.asks : orderBook.bids;\n    \n    while (order.quantity > 0 && oppositeQueue.length > 0) {\n      const matchOrder = oppositeQueue[0];\n      \n      // Execute trade\n      const tradeQuantity = Math.min(order.quantity, matchOrder.quantity);\n      const tradePrice = matchOrder.price;  // Market order takes matching price\n      \n      await this.executeTrade(order, matchOrder, tradeQuantity, tradePrice, orderBook);\n      \n      // Update quantities\n      order.quantity -= tradeQuantity;\n      matchOrder.quantity -= tradeQuantity;\n      \n      // Remove filled order\n      if (matchOrder.quantity === 0) {\n        oppositeQueue.shift();\n        this.emit('order_filled', matchOrder);\n      }\n    }\n    \n    if (order.quantity > 0) {\n      // Market order not fully filled - reject remainder\n      this.emit('order_rejected', {\n        orderId: order.id,\n        reason: 'Insufficient liquidity'\n      });\n    }\n  }\n\n  async matchLimitOrder(order, orderBook) {\n    const oppositeQueue = order.side === 'buy' ? orderBook.asks : orderBook.bids;\n    \n    while (order.quantity > 0 && oppositeQueue.length > 0) {\n      const matchOrder = oppositeQueue[0];\n      \n      // Check if prices match\n      const canMatch = order.side === 'buy' \n        ? order.price >= matchOrder.price  // Buy limit must be >= ask\n        : order.price <= matchOrder.price; // Sell limit must be <= bid\n      \n      if (!canMatch) break;\n      \n      // Execute trade\n      const tradeQuantity = Math.min(order.quantity, matchOrder.quantity);\n      const tradePrice = matchOrder.price;  // Price-time priority: match order gets its price\n      \n      await this.executeTrade(order, matchOrder, tradeQuantity, tradePrice, orderBook);\n      \n      // Update quantities\n      order.quantity -= tradeQuantity;\n      matchOrder.quantity -= tradeQuantity;\n      \n      // Remove filled order\n      if (matchOrder.quantity === 0) {\n        oppositeQueue.shift();\n        this.emit('order_filled', matchOrder);\n      }\n    }\n    \n    if (order.quantity === 0) {\n      this.emit('order_filled', order);\n    }\n  }\n\n  async executeTrade(buyOrder, sellOrder, quantity, price, orderBook) {\n    const trade = {\n      id: this.generateTradeId(),\n      symbol: buyOrder.symbol,\n      buyOrderId: buyOrder.id,\n      sellOrderId: sellOrder.id,\n      quantity,\n      price,\n      timestamp: Date.now(),\n      buyUserId: buyOrder.userId,\n      sellUserId: sellOrder.userId\n    };\n    \n    orderBook.trades.push(trade);\n    orderBook.lastPrice = price;\n    \n    // Emit trade event\n    this.emit('trade', trade);\n    \n    return trade;\n  }\n\n  // Cancel order\n  cancelOrder(orderId, userId) {\n    const order = this.orders.get(orderId);\n    \n    if (!order) {\n      throw new Error('Order not found');\n    }\n    \n    if (order.userId !== userId) {\n      throw new Error('Unauthorized');\n    }\n    \n    if (order.quantity === 0) {\n      throw new Error('Order already filled');\n    }\n    \n    const orderBook = this.getOrderBook(order.symbol);\n    const removed = orderBook.removeOrder(orderId, order.side);\n    \n    if (removed) {\n      order.status = 'cancelled';\n      this.emit('order_cancelled', order);\n    }\n    \n    return order;\n  }\n\n  generateTradeId() {\n    return `T${Date.now()}${Math.random().toString(36).substr(2, 6)}`;\n  }\n}\n\n// Portfolio Service\nclass PortfolioService {\n  constructor(db) {\n    this.db = db;\n  }\n\n  async getPortfolio(userId) {\n    const [positions] = await this.db.query(\n      'SELECT * FROM positions WHERE user_id = ?',\n      [userId]\n    );\n    \n    const [balance] = await this.db.query(\n      'SELECT balance FROM accounts WHERE user_id = ?',\n      [userId]\n    );\n    \n    return {\n      positions,\n      cash: balance[0]?.balance || 0\n    };\n  }\n\n  async updatePosition(userId, symbol, quantityChange, pricePerShare) {\n    const connection = await this.db.getConnection();\n    \n    try {\n      await connection.beginTransaction();\n      \n      // Update position\n      const [existing] = await connection.query(\n        'SELECT * FROM positions WHERE user_id = ? AND symbol = ? FOR UPDATE',\n        [userId, symbol]\n      );\n      \n      if (existing[0]) {\n        const newQuantity = existing[0].quantity + quantityChange;\n        \n        if (newQuantity === 0) {\n          // Close position\n          await connection.query(\n            'DELETE FROM positions WHERE user_id = ? AND symbol = ?',\n            [userId, symbol]\n          );\n        } else {\n          // Update position\n          const newAvgPrice = ((existing[0].avg_price * existing[0].quantity) + \n                              (pricePerShare * quantityChange)) / newQuantity;\n          \n          await connection.query(\n            'UPDATE positions SET quantity = ?, avg_price = ? WHERE user_id = ? AND symbol = ?',\n            [newQuantity, newAvgPrice, userId, symbol]\n          );\n        }\n      } else if (quantityChange > 0) {\n        // Create new position\n        await connection.query(\n          'INSERT INTO positions (user_id, symbol, quantity, avg_price) VALUES (?, ?, ?, ?)',\n          [userId, symbol, quantityChange, pricePerShare]\n        );\n      }\n      \n      // Update cash balance\n      const cashChange = -quantityChange * pricePerShare;\n      await connection.query(\n        'UPDATE accounts SET balance = balance + ? WHERE user_id = ?',\n        [cashChange, userId]\n      );\n      \n      await connection.commit();\n      \n    } catch (error) {\n      await connection.rollback();\n      throw error;\n    } finally {\n      connection.release();\n    }\n  }\n}\n\n// Trading API\nclass TradingAPI {\n  constructor(matchingEngine, portfolioService) {\n    this.matchingEngine = matchingEngine;\n    this.portfolio = portfolioService;\n    \n    // Listen to trades and update portfolios\n    this.matchingEngine.on('trade', trade => this.handleTrade(trade));\n  }\n\n  async submitOrder(userId, orderRequest) {\n    const { symbol, side, type, quantity, price } = orderRequest;\n    \n    // Validate order\n    this.validateOrder(orderRequest);\n    \n    // Pre-trade risk check\n    await this.checkRisk(userId, side, quantity, price);\n    \n    // Create order\n    const order = {\n      id: this.generateOrderId(),\n      userId,\n      symbol,\n      side,\n      type,\n      quantity,\n      price: price || null,\n      timestamp: Date.now(),\n      status: 'pending'\n    };\n    \n    // Submit to matching engine\n    return await this.matchingEngine.submitOrder(order);\n  }\n\n  validateOrder(order) {\n    if (!['buy', 'sell'].includes(order.side)) {\n      throw new Error('Invalid side');\n    }\n    \n    if (!['market', 'limit'].includes(order.type)) {\n      throw new Error('Invalid order type');\n    }\n    \n    if (order.quantity <= 0) {\n      throw new Error('Invalid quantity');\n    }\n    \n    if (order.type === 'limit' && (!order.price || order.price <= 0)) {\n      throw new Error('Limit order requires valid price');\n    }\n  }\n\n  async checkRisk(userId, side, quantity, price) {\n    const portfolio = await this.portfolio.getPortfolio(userId);\n    \n    if (side === 'buy') {\n      const estimatedCost = quantity * (price || 1000);  // Conservative estimate for market orders\n      \n      if (portfolio.cash < estimatedCost) {\n        throw new Error('Insufficient funds');\n      }\n    } else {\n      // For sell orders, check position\n      // (simplified - real system would check unsettled trades)\n    }\n  }\n\n  async handleTrade(trade) {\n    // Update buyer's position\n    await this.portfolio.updatePosition(\n      trade.buyUserId,\n      trade.symbol,\n      trade.quantity,\n      trade.price\n    );\n    \n    // Update seller's position\n    await this.portfolio.updatePosition(\n      trade.sellUserId,\n      trade.symbol,\n      -trade.quantity,\n      trade.price\n    );\n  }\n\n  generateOrderId() {\n    return `O${Date.now()}${Math.random().toString(36).substr(2, 6)}`;\n  }\n}\n\n// WebSocket Server for Real-time Updates\nclass TradingWebSocketServer {\n  constructor(server, matchingEngine) {\n    this.wss = new WebSocket.Server({ server });\n    this.clients = new Map();  // userId -> WebSocket\n    this.subscriptions = new Map();  // symbol -> Set of userIds\n    \n    this.setupWebSocket();\n    this.setupMatchingEngineListeners(matchingEngine);\n  }\n\n  setupWebSocket() {\n    this.wss.on('connection', (ws, req) => {\n      const userId = this.authenticateConnection(req);\n      \n      if (!userId) {\n        ws.close(4001, 'Authentication required');\n        return;\n      }\n      \n      this.clients.set(userId, ws);\n      \n      ws.on('message', (data) => {\n        const message = JSON.parse(data);\n        this.handleClientMessage(userId, message);\n      });\n      \n      ws.on('close', () => {\n        this.clients.delete(userId);\n      });\n    });\n  }\n\n  setupMatchingEngineListeners(engine) {\n    engine.on('trade', trade => {\n      this.broadcast(trade.symbol, {\n        type: 'trade',\n        data: trade\n      });\n    });\n    \n    engine.on('orderbook_update', update => {\n      this.broadcast(update.symbol, {\n        type: 'orderbook',\n        data: update.depth\n      });\n    });\n  }\n\n  handleClientMessage(userId, message) {\n    if (message.type === 'subscribe') {\n      this.subscribe(userId, message.symbol);\n    } else if (message.type === 'unsubscribe') {\n      this.unsubscribe(userId, message.symbol);\n    }\n  }\n\n  subscribe(userId, symbol) {\n    if (!this.subscriptions.has(symbol)) {\n      this.subscriptions.set(symbol, new Set());\n    }\n    this.subscriptions.get(symbol).add(userId);\n  }\n\n  unsubscribe(userId, symbol) {\n    this.subscriptions.get(symbol)?.delete(userId);\n  }\n\n  broadcast(symbol, message) {\n    const subscribers = this.subscriptions.get(symbol) || new Set();\n    \n    subscribers.forEach(userId => {\n      const ws = this.clients.get(userId);\n      if (ws && ws.readyState === WebSocket.OPEN) {\n        ws.send(JSON.stringify(message));\n      }\n    });\n  }\n\n  authenticateConnection(req) {\n    // Extract token and verify\n    return 'user123';  // Placeholder\n  }\n}\n\n// Initialize\nconst app = express();\nconst server = require('http').createServer(app);\nconst db = createDatabaseConnection();\n\nconst matchingEngine = new MatchingEngine();\nconst portfolioService = new PortfolioService(db);\nconst tradingAPI = new TradingAPI(matchingEngine, portfolioService);\nconst wsServer = new TradingWebSocketServer(server, matchingEngine);\n\napp.use(express.json());\n\n// Submit order\napp.post('/api/orders', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const order = await tradingAPI.submitOrder(userId, req.body);\n    res.json(order);\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Get order book\napp.get('/api/orderbook/:symbol', (req, res) => {\n  const { symbol } = req.params;\n  const orderBook = matchingEngine.getOrderBook(symbol);\n  res.json(orderBook.getDepth());\n});\n\nserver.listen(3000);"
    },
    {
      "id": 54,
      "question": "Design a Recommendation System",
      "answer": "A recommendation system suggests relevant items to users based on their behavior and preferences.\n\nCore Approaches:\n• Collaborative Filtering: Recommend based on similar users\n  - User-based: Find users with similar tastes\n  - Item-based: Find items similar to what user liked\n• Content-Based Filtering: Recommend based on item features\n  - Analyze item attributes (genre, tags, description)\n  - Match to user's historical preferences\n• Hybrid: Combine multiple approaches for better results\n• Deep Learning: Neural networks (embeddings, autoencoders)\n\nArchitecture Components:\n• Interaction Service: Track user-item interactions (views, clicks, purchases)\n• Feature Store: Store user and item features\n• Model Training Pipeline: Offline batch training\n• Inference Service: Real-time recommendation generation\n• Ranking Service: Re-rank recommendations by business rules\n• A/B Testing Framework: Test different models\n• Feedback Loop: Collect interaction data for retraining\n\nCollaborative Filtering (Matrix Factorization):\n• Build user-item interaction matrix (sparse)\n• Decompose into user matrix U and item matrix V\n• User i's rating for item j ≈ U[i] · V[j]\n• Learn embeddings with gradient descent\n• Recommend items with highest predicted scores\n\nContent-Based Filtering:\n• Extract item features: TF-IDF of description, category embeddings\n• Build user profile: Weighted average of liked items\n• Compute similarity (cosine) between user profile and items\n• Recommend most similar items\n\nHybrid Approach:\n• Weighted combination: α * CF_score + (1-α) * CB_score\n• Feature augmentation: Use CF predictions as CB features\n• Cascade: CF first, CB for cold-start items\n• Meta-model: Learn to combine multiple models\n\nCold Start Problem:\n• New users: No history → Use trending items, demographic data\n• New items: No interactions → Use content features, side information\n• Explore-exploit: Balance recommendations (known good) vs exploration (new items)\n\nEvaluation Metrics:\n• Precision@K: Of top K recommendations, how many are relevant\n• Recall@K: Of all relevant items, how many in top K\n• NDCG: Normalized Discounted Cumulative Gain (rank matters)\n• CTR: Click-through rate (online metric)",
      "explanation": "Recommendation systems use collaborative filtering (user/item similarities via matrix factorization), content-based filtering (item features matching user profile), hybrid models combining both, and handle cold-start with content features and exploration strategies.",
      "difficulty": "Hard",
      "code": "// Recommendation System Implementation\nconst express = require('express');\nconst tf = require('@tensorflow/tfjs-node');\n\n// Collaborative Filtering (Matrix Factorization)\nclass CollaborativeFilter {\n  constructor(embeddingDim = 50) {\n    this.embeddingDim = embeddingDim;\n    this.userEmbeddings = new Map();\n    this.itemEmbeddings = new Map();\n  }\n\n  // Load pre-trained embeddings\n  async loadEmbeddings(userEmbeddings, itemEmbeddings) {\n    this.userEmbeddings = new Map(Object.entries(userEmbeddings));\n    this.itemEmbeddings = new Map(Object.entries(itemEmbeddings));\n  }\n\n  // Predict score for user-item pair\n  predict(userId, itemId) {\n    const userEmb = this.userEmbeddings.get(userId);\n    const itemEmb = this.itemEmbeddings.get(itemId);\n    \n    if (!userEmb || !itemEmb) return 0;\n    \n    // Dot product\n    return this.dotProduct(userEmb, itemEmb);\n  }\n\n  // Generate recommendations for user\n  recommend(userId, items, topK = 10, excludeIds = []) {\n    const scores = items\n      .filter(item => !excludeIds.includes(item.id))\n      .map(item => ({\n        itemId: item.id,\n        score: this.predict(userId, item.id),\n        ...item\n      }))\n      .sort((a, b) => b.score - a.score)\n      .slice(0, topK);\n    \n    return scores;\n  }\n\n  // Find similar users\n  findSimilarUsers(userId, allUserIds, topK = 10) {\n    const userEmb = this.userEmbeddings.get(userId);\n    \n    if (!userEmb) return [];\n    \n    const similarities = allUserIds\n      .filter(id => id !== userId)\n      .map(id => ({\n        userId: id,\n        similarity: this.cosineSimilarity(userEmb, this.userEmbeddings.get(id))\n      }))\n      .sort((a, b) => b.similarity - a.similarity)\n      .slice(0, topK);\n    \n    return similarities;\n  }\n\n  // Find similar items\n  findSimilarItems(itemId, allItemIds, topK = 10) {\n    const itemEmb = this.itemEmbeddings.get(itemId);\n    \n    if (!itemEmb) return [];\n    \n    const similarities = allItemIds\n      .filter(id => id !== itemId)\n      .map(id => ({\n        itemId: id,\n        similarity: this.cosineSimilarity(itemEmb, this.itemEmbeddings.get(id))\n      }))\n      .sort((a, b) => b.similarity - a.similarity)\n      .slice(0, topK);\n    \n    return similarities;\n  }\n\n  dotProduct(a, b) {\n    return a.reduce((sum, val, i) => sum + val * b[i], 0);\n  }\n\n  cosineSimilarity(a, b) {\n    const dot = this.dotProduct(a, b);\n    const normA = Math.sqrt(this.dotProduct(a, a));\n    const normB = Math.sqrt(this.dotProduct(b, b));\n    return dot / (normA * normB);\n  }\n}\n\n// Content-Based Filtering\nclass ContentBasedFilter {\n  constructor() {\n    this.userProfiles = new Map();  // userId -> feature vector\n    this.itemFeatures = new Map();  // itemId -> feature vector\n  }\n\n  // Build user profile from interaction history\n  buildUserProfile(userId, interactions) {\n    // interactions: [{ itemId, rating/weight }]\n    const weightedFeatures = new Array(this.featureDim).fill(0);\n    let totalWeight = 0;\n    \n    interactions.forEach(interaction => {\n      const itemFeatures = this.itemFeatures.get(interaction.itemId);\n      if (itemFeatures) {\n        const weight = interaction.rating || 1;\n        itemFeatures.forEach((val, idx) => {\n          weightedFeatures[idx] += val * weight;\n        });\n        totalWeight += weight;\n      }\n    });\n    \n    // Normalize\n    const userProfile = weightedFeatures.map(val => val / totalWeight);\n    this.userProfiles.set(userId, userProfile);\n    \n    return userProfile;\n  }\n\n  // Extract item features (TF-IDF, category embeddings, etc.)\n  extractItemFeatures(item) {\n    const features = [];\n    \n    // Category one-hot encoding\n    const categories = ['Electronics', 'Books', 'Clothing', 'Home', 'Toys'];\n    categories.forEach(cat => {\n      features.push(item.category === cat ? 1 : 0);\n    });\n    \n    // Price bucket\n    const priceBucket = Math.min(Math.floor(item.price / 50), 4);\n    for (let i = 0; i < 5; i++) {\n      features.push(i === priceBucket ? 1 : 0);\n    }\n    \n    // Rating\n    features.push(item.rating / 5);\n    \n    // TF-IDF of description (simplified)\n    const descWords = item.description.toLowerCase().split(' ');\n    const keywords = ['premium', 'quality', 'affordable', 'new', 'best'];\n    keywords.forEach(keyword => {\n      features.push(descWords.includes(keyword) ? 1 : 0);\n    });\n    \n    this.featureDim = features.length;\n    this.itemFeatures.set(item.id, features);\n    \n    return features;\n  }\n\n  // Recommend items based on user profile\n  recommend(userId, items, topK = 10, excludeIds = []) {\n    const userProfile = this.userProfiles.get(userId);\n    \n    if (!userProfile) {\n      // Cold start: return trending items\n      return items.slice(0, topK);\n    }\n    \n    const scores = items\n      .filter(item => !excludeIds.includes(item.id))\n      .map(item => {\n        const itemFeatures = this.itemFeatures.get(item.id) || \n                             this.extractItemFeatures(item);\n        \n        const similarity = this.cosineSimilarity(userProfile, itemFeatures);\n        \n        return {\n          itemId: item.id,\n          score: similarity,\n          ...item\n        };\n      })\n      .sort((a, b) => b.score - a.score)\n      .slice(0, topK);\n    \n    return scores;\n  }\n\n  cosineSimilarity(a, b) {\n    const dot = a.reduce((sum, val, i) => sum + val * b[i], 0);\n    const normA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));\n    const normB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));\n    return dot / (normA * normB);\n  }\n}\n\n// Hybrid Recommendation System\nclass HybridRecommender {\n  constructor(cfFilter, cbFilter, alpha = 0.6) {\n    this.cf = cfFilter;\n    this.cb = cbFilter;\n    this.alpha = alpha;  // Weight for CF vs CB\n  }\n\n  recommend(userId, items, topK = 10, excludeIds = []) {\n    // Get CF recommendations\n    const cfRecs = this.cf.recommend(userId, items, topK * 2, excludeIds);\n    \n    // Get CB recommendations\n    const cbRecs = this.cb.recommend(userId, items, topK * 2, excludeIds);\n    \n    // Combine scores\n    const combinedScores = new Map();\n    \n    cfRecs.forEach(rec => {\n      combinedScores.set(rec.itemId, {\n        ...rec,\n        cfScore: rec.score,\n        cbScore: 0,\n        finalScore: rec.score * this.alpha\n      });\n    });\n    \n    cbRecs.forEach(rec => {\n      if (combinedScores.has(rec.itemId)) {\n        const existing = combinedScores.get(rec.itemId);\n        existing.cbScore = rec.score;\n        existing.finalScore += rec.score * (1 - this.alpha);\n      } else {\n        combinedScores.set(rec.itemId, {\n          ...rec,\n          cfScore: 0,\n          cbScore: rec.score,\n          finalScore: rec.score * (1 - this.alpha)\n        });\n      }\n    });\n    \n    // Sort by final score\n    return Array.from(combinedScores.values())\n      .sort((a, b) => b.finalScore - a.finalScore)\n      .slice(0, topK);\n  }\n}\n\n// Interaction Tracker\nclass InteractionService {\n  constructor(db, cache) {\n    this.db = db;\n    this.cache = cache;\n  }\n\n  async trackInteraction(userId, itemId, interactionType, metadata = {}) {\n    // interactionType: 'view', 'click', 'purchase', 'rating'\n    await this.db.query(\n      'INSERT INTO interactions (user_id, item_id, interaction_type, metadata, created_at) VALUES (?, ?, ?, ?, NOW())',\n      [userId, itemId, interactionType, JSON.stringify(metadata)]\n    );\n    \n    // Update real-time cache\n    await this.cache.zadd(\n      `user:${userId}:recent`,\n      Date.now(),\n      itemId\n    );\n    await this.cache.expire(`user:${userId}:recent`, 86400);  // 24 hours\n    \n    // Increment item popularity\n    await this.cache.zincrby('trending:items', 1, itemId);\n  }\n\n  async getUserInteractions(userId, limit = 100) {\n    const results = await this.db.query(\n      'SELECT * FROM interactions WHERE user_id = ? ORDER BY created_at DESC LIMIT ?',\n      [userId, limit]\n    );\n    \n    return results[0];\n  }\n\n  async getTrendingItems(topK = 20) {\n    // Get top items from sorted set\n    const trending = await this.cache.zrevrange(\n      'trending:items',\n      0,\n      topK - 1,\n      'WITHSCORES'\n    );\n    \n    return trending;\n  }\n}\n\n// Recommendation API\nclass RecommendationAPI {\n  constructor(recommender, interactionService, db) {\n    this.recommender = recommender;\n    this.interactions = interactionService;\n    this.db = db;\n  }\n\n  async getRecommendations(userId, context = {}) {\n    const { page = 'home', topK = 10 } = context;\n    \n    // Get user's interaction history\n    const interactions = await this.interactions.getUserInteractions(userId);\n    const viewedItemIds = interactions.map(i => i.item_id);\n    \n    // Get candidate items (simplified - in production, use efficient retrieval)\n    const [items] = await this.db.query(\n      'SELECT * FROM items WHERE id NOT IN (?) LIMIT 1000',\n      [viewedItemIds.length > 0 ? viewedItemIds : [0]]\n    );\n    \n    // Generate recommendations\n    let recommendations;\n    \n    if (interactions.length < 5) {\n      // Cold start: Use trending items\n      recommendations = await this.coldStartRecommendations(topK);\n    } else {\n      // Personalized recommendations\n      recommendations = this.recommender.recommend(\n        userId,\n        items,\n        topK,\n        viewedItemIds\n      );\n    }\n    \n    // Apply business rules (diversity, freshness, etc.)\n    recommendations = this.applyBusinessRules(recommendations, context);\n    \n    // Log recommendations for A/B testing\n    await this.logRecommendations(userId, recommendations, context);\n    \n    return recommendations;\n  }\n\n  async coldStartRecommendations(topK) {\n    const trending = await this.interactions.getTrendingItems(topK);\n    \n    const itemIds = trending.filter((_, i) => i % 2 === 0);  // Every other element is ID\n    \n    const [items] = await this.db.query(\n      'SELECT * FROM items WHERE id IN (?)',\n      [itemIds]\n    );\n    \n    return items;\n  }\n\n  applyBusinessRules(recommendations, context) {\n    // Diversity: Don't recommend too many items from same category\n    const categoryCount = new Map();\n    const diversified = [];\n    \n    for (const rec of recommendations) {\n      const count = categoryCount.get(rec.category) || 0;\n      if (count < 3) {  // Max 3 per category\n        diversified.push(rec);\n        categoryCount.set(rec.category, count + 1);\n      }\n    }\n    \n    // Freshness: Boost new items\n    diversified.forEach(rec => {\n      const ageInDays = (Date.now() - new Date(rec.created_at)) / (1000 * 60 * 60 * 24);\n      if (ageInDays < 7) {\n        rec.finalScore *= 1.2;  // 20% boost for new items\n      }\n    });\n    \n    return diversified.sort((a, b) => b.finalScore - a.finalScore);\n  }\n\n  async logRecommendations(userId, recommendations, context) {\n    await this.db.query(\n      'INSERT INTO recommendation_logs (user_id, item_ids, context, created_at) VALUES (?, ?, ?, NOW())',\n      [userId, JSON.stringify(recommendations.map(r => r.itemId)), JSON.stringify(context)]\n    );\n  }\n\n  async getSimilarItems(itemId, topK = 10) {\n    // Use CF similar items\n    const [allItems] = await this.db.query('SELECT id FROM items');\n    const itemIds = allItems.map(i => i.id);\n    \n    const similar = this.recommender.cf.findSimilarItems(itemId, itemIds, topK);\n    \n    // Fetch full item data\n    const [items] = await this.db.query(\n      'SELECT * FROM items WHERE id IN (?)',\n      [similar.map(s => s.itemId)]\n    );\n    \n    return items;\n  }\n}\n\n// Express API\nconst app = express();\napp.use(express.json());\n\nconst db = createDatabaseConnection();\nconst cache = createRedisClient();\n\nconst cfFilter = new CollaborativeFilter();\nconst cbFilter = new ContentBasedFilter();\nconst hybridRecommender = new HybridRecommender(cfFilter, cbFilter);\nconst interactionService = new InteractionService(db, cache);\nconst recommendationAPI = new RecommendationAPI(hybridRecommender, interactionService, db);\n\n// Get personalized recommendations\napp.get('/api/recommendations', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { page, topK } = req.query;\n    \n    const recommendations = await recommendationAPI.getRecommendations(\n      userId,\n      { page, topK: parseInt(topK) || 10 }\n    );\n    \n    res.json(recommendations);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Track interaction\napp.post('/api/interactions', async (req, res) => {\n  try {\n    const userId = req.user.id;\n    const { itemId, interactionType, metadata } = req.body;\n    \n    await interactionService.trackInteraction(\n      userId,\n      itemId,\n      interactionType,\n      metadata\n    );\n    \n    res.json({ status: 'tracked' });\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Get similar items\napp.get('/api/items/:itemId/similar', async (req, res) => {\n  try {\n    const { itemId } = req.params;\n    const { topK } = req.query;\n    \n    const similar = await recommendationAPI.getSimilarItems(\n      itemId,\n      parseInt(topK) || 10\n    );\n    \n    res.json(similar);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.listen(3000);"
    },
    {
      "id": 55,
      "question": "Design a Job Scheduler / Background Task System",
      "answer": "A job scheduler executes background tasks reliably with scheduling, retries, and monitoring.\n\nCore Features:\n• Task scheduling: Execute at specific time or interval\n• Priority queues: High/medium/low priority tasks\n• Retry mechanism: Exponential backoff for failed tasks\n• Concurrency control: Limit parallel execution\n• Cron-based scheduling: Recurring tasks with cron expressions\n• Distributed execution: Multiple worker nodes\n• Dead letter queue: Failed tasks after max retries\n• Monitoring: Task status, execution time, failure rates\n\nArchitecture Components:\n• Job Queue: Message queue (Redis, RabbitMQ, Kafka)\n• Scheduler Service: Add tasks to queue at scheduled time\n• Worker Nodes: Poll queue and execute tasks\n• Task Registry: Registered task handlers\n• Coordinator: Distribute jobs across workers\n• Metrics Service: Track execution metrics\n• Admin UI: Monitor and manage jobs\n\nScheduling Approaches:\n• Immediate: Execute as soon as worker available\n• Delayed: Execute after specific delay (e.g., 5 minutes)\n• Scheduled: Execute at exact time (e.g., 2024-01-01 10:00)\n• Recurring: Execute periodically (cron)\n• Dependent: Execute after another job completes (DAG)\n\nTask Lifecycle:\n• PENDING: Task created, waiting to be picked\n• RUNNING: Worker executing task\n• COMPLETED: Task finished successfully\n• FAILED: Task failed, will retry if retries remain\n• DEAD: Task failed after max retries → Dead letter queue\n\nRetry Strategy:\n• Exponential backoff: delay = base_delay * (2 ^ attempt)\n• Max retries: E.g., 3 attempts\n• Jitter: Add randomness to prevent thundering herd\n• Idempotency: Tasks must be safe to retry\n\nDistributed Challenges:\n• Task deduplication: Prevent same task executed twice\n• Load balancing: Distribute tasks evenly across workers\n• Worker health: Detect and handle worker failures\n• Task timeout: Kill long-running tasks\n• Exactly-once semantics: Difficult, usually at-least-once\n\nPopular Solutions:\n• Celery (Python): Distributed task queue\n• BullMQ (Node.js): Redis-based job queue\n• Apache Airflow: DAG-based workflow scheduler\n• Kubernetes CronJobs: Container-based job scheduling",
      "explanation": "Job schedulers use message queues for task distribution, worker pools for execution, retry with exponential backoff for failures, cron expressions for recurring tasks, and dead letter queues for terminally failed jobs.",
      "difficulty": "Hard",
      "code": "// Background Job Scheduler Implementation\nconst { Queue, Worker, QueueScheduler } = require('bullmq');\nconst Redis = require('ioredis');\nconst cron = require('node-cron');\n\n// Task Registry\nclass TaskRegistry {\n  constructor() {\n    this.handlers = new Map();\n  }\n\n  register(taskName, handler) {\n    if (this.handlers.has(taskName)) {\n      throw new Error(`Task ${taskName} already registered`);\n    }\n    this.handlers.set(taskName, handler);\n  }\n\n  get(taskName) {\n    const handler = this.handlers.get(taskName);\n    if (!handler) {\n      throw new Error(`Task ${taskName} not found`);\n    }\n    return handler;\n  }\n\n  has(taskName) {\n    return this.handlers.has(taskName);\n  }\n\n  list() {\n    return Array.from(this.handlers.keys());\n  }\n}\n\n// Job Scheduler\nclass JobScheduler {\n  constructor(redisConnection) {\n    this.connection = redisConnection;\n    this.queues = new Map();\n    this.registry = new TaskRegistry();\n    this.cronJobs = new Map();\n    \n    // Create queues for different priorities\n    this.priorities = ['high', 'default', 'low'];\n    this.priorities.forEach(priority => {\n      this.queues.set(priority, new Queue(priority, {\n        connection: this.connection\n      }));\n      \n      // Queue scheduler for delayed/scheduled jobs\n      new QueueScheduler(priority, {\n        connection: this.connection\n      });\n    });\n  }\n\n  // Register task handler\n  registerTask(taskName, handler) {\n    this.registry.register(taskName, handler);\n  }\n\n  // Add job to queue\n  async addJob(taskName, data, options = {}) {\n    const {\n      priority = 'default',\n      delay = 0,\n      attempts = 3,\n      backoff = { type: 'exponential', delay: 1000 },\n      jobId = null\n    } = options;\n\n    if (!this.registry.has(taskName)) {\n      throw new Error(`Task ${taskName} not registered`);\n    }\n\n    const queue = this.queues.get(priority);\n\n    const job = await queue.add(\n      taskName,\n      data,\n      {\n        delay,\n        attempts,\n        backoff,\n        jobId,  // For deduplication\n        removeOnComplete: 100,  // Keep last 100 completed jobs\n        removeOnFail: 1000      // Keep last 1000 failed jobs\n      }\n    );\n\n    console.log(`Job ${job.id} added to ${priority} queue:`, taskName);\n    return job;\n  }\n\n  // Schedule job at specific time\n  async scheduleJob(taskName, data, scheduledTime, options = {}) {\n    const now = Date.now();\n    const scheduledMs = new Date(scheduledTime).getTime();\n    const delay = Math.max(0, scheduledMs - now);\n\n    return await this.addJob(taskName, data, {\n      ...options,\n      delay\n    });\n  }\n\n  // Add recurring job with cron expression\n  addRecurringJob(taskName, data, cronExpression, options = {}) {\n    if (!cron.validate(cronExpression)) {\n      throw new Error('Invalid cron expression');\n    }\n\n    const job = cron.schedule(cronExpression, async () => {\n      console.log(`Executing recurring job: ${taskName}`);\n      await this.addJob(taskName, data, options);\n    });\n\n    this.cronJobs.set(`${taskName}:${cronExpression}`, job);\n\n    return job;\n  }\n\n  // Cancel recurring job\n  cancelRecurringJob(taskName, cronExpression) {\n    const key = `${taskName}:${cronExpression}`;\n    const job = this.cronJobs.get(key);\n    \n    if (job) {\n      job.stop();\n      this.cronJobs.delete(key);\n      return true;\n    }\n    \n    return false;\n  }\n\n  // Start worker\n  startWorker(priority = 'default', concurrency = 5) {\n    const queue = this.queues.get(priority);\n\n    const worker = new Worker(\n      priority,\n      async (job) => {\n        console.log(`Processing job ${job.id}: ${job.name}`);\n        const startTime = Date.now();\n\n        try {\n          // Get task handler\n          const handler = this.registry.get(job.name);\n\n          // Execute task\n          const result = await handler(job.data);\n\n          const duration = Date.now() - startTime;\n          console.log(`Job ${job.id} completed in ${duration}ms`);\n\n          return result;\n\n        } catch (error) {\n          const duration = Date.now() - startTime;\n          console.error(`Job ${job.id} failed after ${duration}ms:`, error.message);\n          throw error;  // Will trigger retry\n        }\n      },\n      {\n        connection: this.connection,\n        concurrency  // Number of parallel jobs\n      }\n    );\n\n    // Event listeners\n    worker.on('completed', (job, result) => {\n      console.log(`✓ Job ${job.id} completed`);\n    });\n\n    worker.on('failed', (job, error) => {\n      console.error(`✗ Job ${job.id} failed:`, error.message);\n      console.log(`  Attempts: ${job.attemptsMade}/${job.opts.attempts}`);\n    });\n\n    worker.on('error', (error) => {\n      console.error('Worker error:', error);\n    });\n\n    return worker;\n  }\n\n  // Get job status\n  async getJobStatus(jobId, priority = 'default') {\n    const queue = this.queues.get(priority);\n    const job = await queue.getJob(jobId);\n\n    if (!job) {\n      return null;\n    }\n\n    const state = await job.getState();\n\n    return {\n      id: job.id,\n      name: job.name,\n      state,\n      progress: job.progress,\n      attemptsMade: job.attemptsMade,\n      failedReason: job.failedReason,\n      finishedOn: job.finishedOn,\n      processedOn: job.processedOn,\n      returnvalue: job.returnvalue\n    };\n  }\n\n  // Get queue stats\n  async getQueueStats(priority = 'default') {\n    const queue = this.queues.get(priority);\n\n    const [waiting, active, completed, failed, delayed] = await Promise.all([\n      queue.getWaitingCount(),\n      queue.getActiveCount(),\n      queue.getCompletedCount(),\n      queue.getFailedCount(),\n      queue.getDelayedCount()\n    ]);\n\n    return {\n      priority,\n      waiting,\n      active,\n      completed,\n      failed,\n      delayed,\n      total: waiting + active + completed + failed + delayed\n    };\n  }\n\n  // Retry failed job\n  async retryJob(jobId, priority = 'default') {\n    const queue = this.queues.get(priority);\n    const job = await queue.getJob(jobId);\n\n    if (!job) {\n      throw new Error('Job not found');\n    }\n\n    await job.retry();\n    console.log(`Job ${jobId} queued for retry`);\n  }\n\n  // Move to dead letter queue\n  async moveToDeadLetter(jobId, priority = 'default') {\n    const queue = this.queues.get(priority);\n    const job = await queue.getJob(jobId);\n\n    if (!job) {\n      throw new Error('Job not found');\n    }\n\n    await job.moveToFailed(new Error('Moved to dead letter queue'), true);\n  }\n\n  // Clean old jobs\n  async cleanQueue(priority = 'default', grace = 86400000) {\n    // grace: milliseconds to keep completed/failed jobs (default 24h)\n    const queue = this.queues.get(priority);\n\n    const cleaned = await queue.clean(grace, 1000, 'completed');\n    console.log(`Cleaned ${cleaned.length} completed jobs from ${priority} queue`);\n\n    const failedCleaned = await queue.clean(grace * 7, 1000, 'failed');\n    console.log(`Cleaned ${failedCleaned.length} failed jobs from ${priority} queue`);\n  }\n}\n\n// Example Task Handlers\nclass EmailTask {\n  static async handle(data) {\n    const { to, subject, body } = data;\n    console.log(`Sending email to ${to}: ${subject}`);\n    \n    // Simulate email sending\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    \n    // Simulate random failures for testing retries\n    if (Math.random() < 0.2) {\n      throw new Error('Email service unavailable');\n    }\n    \n    console.log(`Email sent successfully to ${to}`);\n    return { status: 'sent', to, timestamp: Date.now() };\n  }\n}\n\nclass DataProcessingTask {\n  static async handle(data) {\n    const { fileId, operation } = data;\n    console.log(`Processing file ${fileId}: ${operation}`);\n    \n    // Simulate heavy processing\n    await new Promise(resolve => setTimeout(resolve, 5000));\n    \n    return { status: 'processed', fileId, records: 1000 };\n  }\n}\n\nclass ReportGenerationTask {\n  static async handle(data) {\n    const { reportType, filters } = data;\n    console.log(`Generating ${reportType} report`);\n    \n    // Simulate report generation\n    await new Promise(resolve => setTimeout(resolve, 10000));\n    \n    return {\n      status: 'generated',\n      reportType,\n      url: `https://example.com/reports/${Date.now()}.pdf`\n    };\n  }\n}\n\n// Express API\nconst express = require('express');\nconst app = express();\napp.use(express.json());\n\nconst redis = new Redis({\n  host: 'localhost',\n  port: 6379,\n  maxRetriesPerRequest: null\n});\n\nconst scheduler = new JobScheduler(redis);\n\n// Register task handlers\nscheduler.registerTask('send-email', EmailTask.handle);\nscheduler.registerTask('process-data', DataProcessingTask.handle);\nscheduler.registerTask('generate-report', ReportGenerationTask.handle);\n\n// Start workers\nscheduler.startWorker('high', 10);     // 10 concurrent jobs\nscheduler.startWorker('default', 5);   // 5 concurrent jobs\nscheduler.startWorker('low', 2);       // 2 concurrent jobs\n\n// Add recurring jobs\nscheduler.addRecurringJob(\n  'generate-report',\n  { reportType: 'daily-summary', filters: {} },\n  '0 2 * * *',  // Every day at 2 AM\n  { priority: 'high' }\n);\n\n// API Endpoints\n// Submit job\napp.post('/api/jobs', async (req, res) => {\n  try {\n    const { taskName, data, priority, delay } = req.body;\n    \n    const job = await scheduler.addJob(taskName, data, {\n      priority,\n      delay\n    });\n    \n    res.json({\n      jobId: job.id,\n      status: 'queued'\n    });\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Schedule job\napp.post('/api/jobs/schedule', async (req, res) => {\n  try {\n    const { taskName, data, scheduledTime, priority } = req.body;\n    \n    const job = await scheduler.scheduleJob(\n      taskName,\n      data,\n      scheduledTime,\n      { priority }\n    );\n    \n    res.json({\n      jobId: job.id,\n      scheduledTime,\n      status: 'scheduled'\n    });\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\n// Get job status\napp.get('/api/jobs/:jobId', async (req, res) => {\n  const { jobId } = req.params;\n  const { priority = 'default' } = req.query;\n  \n  const status = await scheduler.getJobStatus(jobId, priority);\n  \n  if (!status) {\n    return res.status(404).json({ error: 'Job not found' });\n  }\n  \n  res.json(status);\n});\n\n// Get queue stats\napp.get('/api/queues/:priority/stats', async (req, res) => {\n  const { priority } = req.params;\n  const stats = await scheduler.getQueueStats(priority);\n  res.json(stats);\n});\n\n// Retry job\napp.post('/api/jobs/:jobId/retry', async (req, res) => {\n  try {\n    const { jobId } = req.params;\n    const { priority = 'default' } = req.body;\n    \n    await scheduler.retryJob(jobId, priority);\n    res.json({ status: 'retrying' });\n  } catch (error) {\n    res.status(400).json({ error: error.message });\n  }\n});\n\napp.listen(3000, () => {\n  console.log('Job scheduler API running on port 3000');\n});\n\n// Graceful shutdown\nprocess.on('SIGTERM', async () => {\n  console.log('Shutting down...');\n  await redis.quit();\n  process.exit(0);\n});"
    },
    {
      "id": 56,
      "question": "How do you design for Multi-Region Deployment and Global Load Balancing?",
      "answer": "Multi-region deployment distributes applications across multiple geographic locations for availability and performance.\n\nKey Strategies:\n• Geographic distribution: Deploy in regions close to users (US-East, EU-West, Asia-Pacific)\n• Active-active: All regions handle live traffic simultaneously\n• Active-passive: Primary region serves traffic, secondary is standby for disaster recovery\n• DNS-based routing: Route users to nearest/healthiest region\n• Cross-region data replication: Keep data synchronized across regions\n• Regional failover: Automatically switch regions during outages\n\nBenefits:\n• Low latency: Users connect to nearest region\n• High availability: No single point of failure\n• Disaster recovery: Survive regional outages\n• Compliance: Data residency requirements (GDPR)\n• Scalability: Distribute load  globally\n\nChallenges:\n• Data consistency: Managing eventual consistency across regions\n• Latency: Cross-region database writes are slow\n• Cost: Running multiple regions is expensive\n• Complexity: More components to manage",
      "explanation": "Multi-region architectures use DNS routing to direct users to nearest region, replicate data across regions with eventual consistency, implement regional failover for availability, and use CDN for static assets.",
      "difficulty": "Hard",
      "code": "// Multi-Region Architecture Example\n// Setup:\n// - 3 regions: US-East, EU-West, Asia-Pacific\n// - Each region: App servers, Database (read replicas), Cache\n// - Global: Route53 for DNS, DynamoDB Global Tables, CloudFront CDN\n\n// Route53 Geo DNS Configuration (simplified)\nconst route53Config = {\n  hostedZone: 'example.com',\n  records: [\n    {\n      name: 'api.example.com',\n      type: 'A',\n      geolocation: { continentCode: 'NA' },\n      target: 'us-east-lb.example.com',  // ELB in US-East\n      healthCheck: 'us-east-health'\n    },\n    {\n      name: 'api.example.com',\n      type: 'A',\n      geolocation: { continentCode: 'EU' },\n      target: 'eu-west-lb.example.com',\n      healthCheck: 'eu-west-health'\n    },\n    {\n      name: 'api.example.com',\n      type: 'A',\n      geolocation: { continentCode: 'AS' },\n      target: 'asia-lb.example.com',\n      healthCheck: 'asia-health'\n    },\n    {\n      name: 'api.example.com',\n      type: 'A',\n      geolocation: { default: true },\n      target: 'us-east-lb.example.com'  // Default fallback\n    }\n  ]\n};\n\n// Database Replication Strategy\n// Primary: US-East (write master)\n// Replicas: EU-West, Asia-Pacific (read replicas with async replication)\n\nclass MultiRegionDatabase {\n  constructor(regions) {\n    this.primary = regions.find(r => r.isPrimary);\n    this.replicas = regions.filter(r => !r.isPrimary);\n  }\n\n  async write(data) {\n    // Always write to primary\n    await this.primary.db.query('INSERT INTO ...', data);\n    // Async replication to replicas (handled by DB)\n  }\n\n  async read(preferredRegion) {\n    // Read from nearest replica\n    const region = this.replicas.find(r => r.name === preferredRegion) || this.primary;\n    return await region.db.query('SELECT ...');\n  }\n}"
    },
    {
      "id": 57,
      "question": "What is Database Partitioning and when should you use it?",
      "answer": "Database partitioning divides a large table into smaller, manageable pieces called partitions.\n\nTypes of Partitioning:\n• Horizontal (Sharding): Split rows across partitions based on partition key\n• Vertical: Split columns into separate tables\n• Range: Partition by value range (e.g., dates: 2020, 2021, 2022)\n• List: Partition by discrete values (e.g., regions: US, EU, Asia)\n• Hash: Partition by hash of key for even distribution\n\nBenefits:\n• Performance: Queries scan only relevant partitions\n• Scalability: Distribute partitions across servers\n• Maintenance: Archive/delete old partitions easily\n• Parallelism: Parallel query execution across partitions\n\nWhen to Use:\n• Tables with hundreds of millions/billions of rows\n• Time-series data: Partition by date for easy archival\n• Multi-tenant: Partition by tenant_id for isolation\n• Hot data: Frequently accessed data in faster storage\n\nChallenges:\n• Query complexity: Cross-partition queries are slow\n• Partition key selection: Poor key causes skew\n• Rebalancing: Moving data between partitions is costly",
      "explanation": "Partition large tables by range/hash/list to improve query performance (pruning), enable parallel processing, facilitate data archival, and distribute data across servers.",
      "difficulty": "Medium",
      "code": "-- PostgreSQL Table Partitioning Example\n\n-- Range Partitioning by Date\nCREATE TABLE orders (\n  id BIGSERIAL,\n  user_id BIGINT,\n  amount DECIMAL(10,2),\n  created_at TIMESTAMP\n) PARTITION BY RANGE (created_at);\n\n-- Create partitions\nCREATE TABLE orders_2023 PARTITION OF orders\n  FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');\n\nCREATE TABLE orders_2024 PARTITION OF orders\n  FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');\n\n-- Query uses partition pruning\nSELECT * FROM orders\nWHERE created_at >= '2024-01-01' AND created_at < '2024-02-01';\n-- Only scans orders_2024 partition\n\n-- Hash Partitioning for Even Distribution\nCREATE TABLE users (\n  id BIGSERIAL PRIMARY KEY,\n  email VARCHAR(255),\n  created_at TIMESTAMP\n) PARTITION BY HASH (id);\n\nCREATE TABLE users_p0 PARTITION OF users\n  FOR VALUES WITH (MODULUS 4, REMAINDER 0);\n\nCREATE TABLE users_p1 PARTITION OF users\n  FOR VALUES WITH (MODULUS 4, REMAINDER 1);\n\nCREATE TABLE users_p2 PARTITION OF users\n  FOR VALUES WITH (MODULUS 4, REMAINDER 2);\n\nCREATE TABLE users_p3 PARTITION OF users\n  FOR VALUES WITH (MODULUS 4, REMAINDER 3);"
    },
    {
      "id": 58,
      "question": "How do you implement Chaos Engineering for resilience testing?",
      "answer": "Chaos Engineering intentionally introduces failures to test system resilience and uncover weaknesses.\n\nCore Principles:\n• Hypothesis: Define steady-state behavior (e.g., 99.9% success rate)\n• Experiment: Inject controlled failures\n• Observe: Measure impact on steady state\n• Learn: Fix discovered issues\n• Automate: Run experiments continuously\n\nCommon Experiments:\n• Service failures: Kill random instances\n• Network issues: Inject latency, packet loss, DNS failures\n• Resource exhaustion: Fill disk, consume CPU/memory\n• Time drift: Adjust system clocks\n• Dependency failures: Simulate downstream service outages\n• Data corruption: Modify data in unexpected ways\n\nTools:\n• Chaos Monkey (Netflix): Randomly terminates instances\n• Chaos Mesh: Kubernetes chaos experiments\n• Gremlin: Chaos engineering platform\n• Litmus: Cloud-native chaos engineering\n\nBest Practices:\n• Start small: Begin with non-production environments\n• Gradual rollout: Increase blast radius carefully\n• Have rollback: Ability to stop experiment immediately\n• Monitor closely: Watch all metrics during experiments\n• Run during business hours: Team available to respond",
      "explanation": "Chaos engineering proactively injects failures (instance termination, latency, resource exhaustion) in production to validate resilience, uncover hidden issues, and build confidence in system reliability.",
      "difficulty": "Hard",
      "code": "// Chaos Engineering with Chaos Mesh (Kubernetes)\n// Example: Inject network latency to a service\n\napiVersion: chaos-mesh.org/v1alpha1\nkind: NetworkChaos\nmetadata:\n  name: network-delay-experiment\nspec:\n  action: delay\n  mode: one\n  selector:\n    namespaces:\n      - production\n    labelSelectors:\n      app: payment-service\n  delay:\n    latency: \"500ms\"\n    correlation: \"50\"\n    jitter: \"100ms\"\n  duration: \"10m\"\n  scheduler:\n    cron: \"@every 2h\"\n\n---\n// Programmatic Chaos Injection (Node.js)\nclass ChaosInjector {\n  constructor(config) {\n    this.failureRate = config.failureRate || 0.01;  // 1%\n    this.latencyMs = config.latencyMs || 1000;\n  }\n\n  async injectChaos(fn) {\n    if (Math.random() < this.failureRate) {\n      throw new Error('Chaos: Simulated failure');\n    }\n\n    if (Math.random() < this.failureRate * 2) {\n      await new Promise(r => setTimeout(r, this.latencyMs));\n    }\n\n    return await fn();\n  }\n}\n\n// Usage in service\nconst chaos = new ChaosInjector({ failureRate: 0.05 });\n\napp.get('/api/orders', async (req, res) => {\n  try {\n    const orders = await chaos.injectChaos(\n      () => orderService.getOrders(req.user.id)\n    );\n    res.json(orders);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});"
    },
    {
      "id": 59,
      "question": "Design an Analytics and Metrics Collection System",
      "answer": "An analytics system collects, processes, and visualizes metrics for monitoring and insights.\n\nData Types:\n• Metrics: Numerical time-series (CPU, latency, request count)\n• Logs: Text-based event records\n• Traces: Distributed request traces\n• Events: Discrete occurrences (user actions, errors)\n\nArchitecture:\n• Collection: Agents/SDKs collect data from sources\n• Ingestion: Message queue buffers incoming data (Kafka)\n• Processing: Stream processing for real-time (Flink, Spark)\n• Storage: Time-series DB (InfluxDB, TimescaleDB) for metrics, data warehouse (Snowflake, BigQuery) for analytics\n• Visualization: Dashboards (Grafana, Tableau)\n• Alerting: Trigger alerts on anomalies\n\nMetrics Collection Patterns:\n• Push: Agents push metrics to collector\n• Pull: Collector scrapes metrics from endpoints (Prometheus)\n• Sampling: Sample subset of data for high-volume streams\n• Batching: Group metrics before sending to reduce overhead\n\nAggregations:\n• Counter: Incrementing value (total requests)\n• Gauge: Current value (memory usage)\n• Histogram: Distribution of values (latency percentiles)\n• Summary: Pre-computed percentiles\n\nStorage Optimization:\n• Downsampling: Reduce resolution for old data\n• Retention policies: Delete old data automatically\n• Compression: Time-series compression\n• Partitioning: Partition by time for fast queries",
      "explanation": "Analytics systems use agents to collect metrics, Kafka for ingestion, stream processors for real-time analysis, time-series databases for storage, and Grafana dashboards for visualization.",
      "difficulty": "Hard",
      "code": "// Metrics Collection with Prometheus Client\nconst client = require('prom-client');\nconst express = require('express');\n\n// Create metrics registry\nconst register = new client.Registry();\n\n// Counter: Total requests\nconst httpRequestsTotal = new client.Counter({\n  name: 'http_requests_total',\n  help: 'Total number of HTTP requests',\n  labelNames: ['method', 'route', 'status'],\n  registers: [register]\n});\n\n// Histogram: Request duration\nconst httpRequestDuration = new client.Histogram({\n  name: 'http_request_duration_seconds',\n  help: 'HTTP request duration in seconds',\n  labelNames: ['method', 'route', 'status'],\n  buckets: [0.1, 0.5, 1, 2, 5],\n  registers: [register]\n});\n\n// Gauge: Active connections\nconst activeConnections = new client.Gauge({\n  name: 'active_connections',\n  help: 'Number of active connections',\n  registers: [register]\n});\n\n// Middleware to track metrics\nfunction metricsMiddleware(req, res, next) {\n  const start = Date.now();\n  activeConnections.inc();\n\n  res.on('finish', () => {\n    const duration = (Date.now() - start) / 1000;\n    const labels = {\n      method: req.method,\n      route: req.route?.path || req.path,\n      status: res.statusCode\n    };\n\n    httpRequestsTotal.inc(labels);\n    httpRequestDuration.observe(labels, duration);\n    activeConnections.dec();\n  });\n\n  next();\n}\n\nconst app = express();\napp.use(metricsMiddleware);\n\n// Prometheus scrape endpoint\napp.get('/metrics', async (req, res) => {\n  res.set('Content-Type', register.contentType);\n  res.end(await register.metrics());\n});\n\n// Custom business metrics\nconst orderTotal = new client.Counter({\n  name: 'orders_total',\n  help: 'Total orders',\n  labelNames: ['status'],\n  registers: [register]\n});\n\napp.post('/api/orders', async (req, res) => {\n  try {\n    const order = await createOrder(req.body);\n    orderTotal.inc({ status: 'success' });\n    res.json(order);\n  } catch (error) {\n    orderTotal.inc({ status: 'failed' });\n    res.status(500).json({ error: error.message });\n  }\n});"
    },
    {
      "id": 60,
      "question": "How do you implement Feature Flags for controlled feature rollouts?",
      "answer": "Feature flags (feature toggles) allow enabling/disabling features without deploying new code.\n\nUse Cases:\n• A/B testing: Show different features to different user groups\n• Gradual rollout: Enable feature for 10%, then 50%, then 100%\n• Kill switch: Quickly disable problematic features\n• Beta testing: Enable for select users\n• Development: Hide incomplete features in production\n\nTypes of Flags:\n• Release flags: Control feature rollout (temporary)\n• Ops flags: Circuit breakers, performance tuning (long-lived)\n• Experiment flags: A/B tests (temporary)\n• Permission flags: Access control (long-lived)\n\nImplementation:\n• Flag service: Centralized service stores flag state\n• Client SDKs: Fetch flags and evaluate locally\n• Targeting rules: Enable for specific users, regions, plans\n• Caching: Cache flag state to reduce latency\n• Real-time updates: WebSocket/SSE for instant flag changes\n\nRollout Strategies:\n• Percentage: Enable for X% of users\n• User targeting: Enable for specific user IDs\n• Group targeting: Enable for beta testers, employees\n• Geographic: Enable for specific regions\n• Gradual: Increase percentage over time automatically\n\nBest Practices:\n• Clean up: Remove old flags after full rollout\n• Monitoring: Track flag evaluation metrics\n• Default values: Fail safe (usually disabled)\n• Versioning: Flag changes are versioned\n• Documentation: Document flag purpose and cleanup date",
      "explanation": "Feature flags use a centralized service to control feature availability, support percentage-based rollouts, enable/disable via admin UI, cache flag state for performance, and allow instant rollback without deployment.",
      "difficulty": "Medium",
      "code": "// Feature Flag System Implementation\nclass FeatureFlagService {\n  constructor(cache, db) {\n    this.cache = cache;\n    this.db = db;\n    this.cachePrefix = 'flag:';\n    this.cacheTTL = 300;  // 5 minutes\n  }\n\n  // Evaluate flag for user\n  async isEnabled(flagKey, context) {\n    // context: { userId, email, plan, region, ... }\n    const flag = await this.getFlag(flagKey);\n\n    if (!flag || !flag.enabled) {\n      return false;\n    }\n\n    // Check targeting rules\n    return this.evaluateTargeting(flag, context);\n  }\n\n  async getFlag(flagKey) {\n    // Try cache\n    const cached = await this.cache.get(`${this.cachePrefix}${flagKey}`);\n    if (cached) return JSON.parse(cached);\n\n    //  Fetch from DB\n    const [flag] = await this.db.query(\n      'SELECT * FROM feature_flags WHERE key = ?',\n      [flagKey]\n    );\n\n    if (flag[0]) {\n      await this.cache.setex(\n        `${this.cachePrefix}${flagKey}`,\n        this.cacheTTL,\n        JSON.stringify(flag[0])\n      );\n    }\n\n    return flag[0];\n  }\n\n  evaluateTargeting(flag, context) {\n    const { targeting } = flag;\n\n    // User targeting\n    if (targeting.userIds && targeting.userIds.includes(context.userId)) {\n      return true;\n    }\n\n    // Region targeting\n    if (targeting.regions && !targeting.regions.includes(context.region)) {\n      return false;\n    }\n\n    // Plan targeting\n    if (targeting.plans && !targeting.plans.includes(context.plan)) {\n      return false;\n    }\n\n    // Percentage rollout\n    if (targeting.percentage !== undefined) {\n      const hash = this.hashUserId(context.userId);\n      const bucket = hash % 100;\n      return bucket < targeting.percentage;\n    }\n\n    return true;\n  }\n\n  hashUserId(userId) {\n    let hash = 0;\n    const str = userId.toString();\n    for (let i = 0; i < str.length; i++) {\n      hash = ((hash << 5) - hash) + str.charCodeAt(i);\n      hash = hash & hash;\n    }\n    return Math.abs(hash);\n  }\n\n  // Admin: Create/update flag\n  async setFlag(flagKey, config) {\n    const { enabled, targeting, description } = config;\n\n    await this.db.query(\n      `INSERT INTO feature_flags (key, enabled, targeting, description, updated_at)\n       VALUES (?, ?, ?, ?, NOW())\n       ON DUPLICATE KEY UPDATE\n         enabled = VALUES(enabled),\n         targeting = VALUES(targeting),\n         updated_at = NOW()`,\n      [flagKey, enabled, JSON.stringify(targeting), description]\n    );\n\n    // Invalidate cache\n    await this.cache.del(`${this.cachePrefix}${flagKey}`);\n  }\n}\n\n// Usage in application\nconst flagService = new FeatureFlagService(cache, db);\n\napp.get('/api/dashboard', async (req, res) => {\n  const userId = req.user.id;\n\n  // Check feature flag\n  const newDashboardEnabled = await flagService.isEnabled(\n    'new-dashboard',\n    {\n      userId,\n      plan: req.user.plan,\n      region: req.user.region\n    }\n  );\n\n  if (newDashboardEnabled) {\n    res.json({ dashboard: 'new', features: [...] });\n  } else {\n    res.json({ dashboard: 'legacy', features: [...] });\n  }\n});\n\n// Admin API\napp.post('/admin/flags/:flagKey', async (req, res) => {\n  const { flagKey } = req.params;\n  const { enabled, targeting, description } = req.body;\n\n  await flagService.setFlag(flagKey, {\n    enabled,\n    targeting,  // { percentage: 50, regions: ['US'], plans: ['pro'] }\n    description\n  });\n\n  res.json({ status: 'updated' });\n});"
    },
    {
      "id": 61,
      "question": "What is Database Indexing Strategy and how do you optimize it?",
      "answer": "Database indexes improve query performance by creating data structures for fast lookups.\n\nIndex Types:\n• B-Tree Index: Default, good for range queries\n• Hash Index: Fast equality lookups, no range support\n• Full-text Index: Text search\n• Composite Index: Multiple columns\n• Covering Index: Includes all queried columns (no table lookup needed)\n\nWhen to Index:\n• Columns in WHERE clauses\n• Columns in JOIN conditions\n• Columns in ORDER BY\n• Foreign keys\n• Columns with high cardinality (many unique values)\n\nWhen NOT to Index:\n• Small tables (sequential scan is fast)\n• Columns with low cardinality (gender: M/F)\n• Frequently updated columns (index maintenance overhead)\n• Columns rarely queried\n\nOptimization:\n• Use EXPLAIN to analyze query plans\n• Index selectivity: More unique values = better\n• Leftmost prefix rule: Composite index (a,b,c) supports (a), (a,b), (a,b,c)\n• Avoid function calls on indexed columns\n• Regular maintenance: REINDEX, ANALYZE",
      "explanation": "Indexes use B-trees for fast lookups, benefit WHERE/JOIN/ORDER BY clauses, have maintenance overhead, require selectivity analysis, and need regular rebuilding.",
      "difficulty": "Medium",
      "code": "-- Index optimization examples\n\n-- Single column index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Composite index (order matters!)\nCREATE INDEX idx_orders_user_date ON orders(user_id, created_at);\n-- Supports:\n--   WHERE user_id = 123\n--   WHERE user_id = 123 AND created_at > '2024-01-01'\n-- Does NOT support:\n--   WHERE created_at > '2024-01-01' (user_id not in WHERE)\n\n-- Covering index (includes all SELECT columns)\nCREATE INDEX idx_orders_covering \nON orders(user_id, created_at) \nINCLUDE (amount, status);\n-- No table lookup needed for:\nSELECT amount, status FROM orders \nWHERE user_id = 123 ORDER BY created_at;\n\n-- Partial index (index subset of rows)\nCREATE INDEX idx_orders_pending \nON orders(created_at) \nWHERE status = 'pending';\n\n-- Check index usage\nEXPLAIN ANALYZE\nSELECT * FROM orders WHERE user_id = 123;\n-- Look for \"Index Scan\" vs \"Seq Scan\""
    },
    {
      "id": 62,
      "question": "How do you implement API Versioning?",
      "answer": "API versioning manages changes to APIs while maintaining backward compatibility.\n\nVersioning Strategies:\n• URL Path: /api/v1/users, /api/v2/users\n• Query Parameter: /api/users?version=1\n• Header: Accept: application/vnd.api+json; version=1\n• Content Negotiation: Accept: application/vnd.myapi.v1+json\n\nBest Practices:\n• Semantic versioning: Major.Minor.Patch (2.1.3)\n• Major version: Breaking changes\n• Minor version: New features, backward compatible\n• Patch version: Bug fixes\n• Deprecation policy: Announce deprecation, grace period, sunset date\n• Documentation: Clearly document changes per version\n\nMigration Strategies:\n• Parallel running: Run v1 and v2 simultaneously\n• Adapter pattern: Convert v1 requests to v2 internally\n• Gradual migration: Route percentage of traffic to v2\n• Client opt-in: Clients choose when to upgrade\n\nWhen to Create New Version:\n• Removing fields from response\n• Changing field types\n• Changing URL structure\n• Changing authentication mechanism\n• NOT for adding optional fields (backward compatible)",
      "explanation": "Version APIs through URL paths (v1, v2), maintain multiple versions simultaneously, use adapters for backward compatibility, deprecate gracefully with sunset dates.",
      "difficulty": "Easy",
      "code": "// API Versioning with Express\nconst express = require('express');\nconst app = express();\n\n// Version 1 API\nconst v1Router = express.Router();\nv1Router.get('/users/:id', (req, res) => {\n  // V1 response format\n  res.json({\n    id: 123,\n    name: 'John Doe',\n    email: 'john@example.com'\n  });\n});\n\n// Version 2 API (different structure)\nconst v2Router = express.Router();\nv2Router.get('/users/:id', (req, res) => {\n  // V2 response format\n  res.json({\n    user: {\n      id: 123,\n      fullName: 'John Doe',  // Renamed field\n      contactInfo: {\n        email: 'john@example.com',\n        phone: '+1234567890'  // Added field\n      }\n    },\n    meta: {\n      version: '2.0',\n      timestamp: new Date().toISOString()\n    }\n  });\n});\n\napp.use('/api/v1', v1Router);\napp.use('/api/v2', v2Router);\n\n// Deprecation warning middleware\nfunction deprecationMiddleware(req, res, next) {\n  if (req.baseUrl.includes('/v1')) {\n    res.set('X-API-Warn', 'API v1 is deprecated. Migrate to v2 by 2025-01-01');\n    res.set('X-API-Sunset', '2025-01-01');\n  }\n  next();\n}\n\napp.use(deprecationMiddleware);"
    },
    {
      "id": 63,
      "question": "Design a Search Engine with Autocomplete and Ranking",
      "answer": "A search engine indexes documents and ranks results by relevance.\n\nCore Components:\n• Crawler: Fetch and parse documents\n• Indexer: Build inverted index (word → document IDs)\n• Query Processor: Parse search queries\n• Ranker: Score and rank results\n• Autocomplete: Suggest completions as user types\n\nInverted Index:\n• Map: term → list of (doc_id, position, frequency)\n• Allows fast lookup: Which documents contain \"database\"?\n• TF-IDF scoring: Term frequency × inverse document frequency\n\nRanking Factors:\n• Relevance: TF-IDF, BM25 algorithm\n• Popularity: Click-through rate, backlinks (PageRank)\n• Freshness: Boost recent documents\n• User context: Location, history, personalization\n• Query understanding: Synonyms, spelling correction\n\nAutocomplete:\n• Trie data structure: Prefix → completions\n• Top-K queries: Pre-compute popular completions\n• Personalization: User's search history\n• Ranking: Frequency, recency, CTR\n\nOptimizations:\n• Caching: Cache popular queries\n• Query rewriting: Expand synonyms\n• Fuzzy matching: Handle typos (Levenshtein distance)\n• Faceted search: Filter by category, date, etc.",
      "explanation": "Search engines build inverted indexes for fast term lookup, use TF-IDF/BM25 for relevance scoring, implement tries for autocomplete, and rank by multiple factors (relevance, popularity, freshness).",
      "difficulty": "Hard",
      "code": "// Search Engine with Elasticsearch\nconst { Client } = require('@elastic/elasticsearch');\nconst client = new Client({ node: 'http://localhost:9200' });\n\n// Create index with mappings\nasync function createIndex() {\n  await client.indices.create({\n    index: 'documents',\n    body: {\n      settings: {\n        analysis: {\n          analyzer: {\n            autocomplete: {\n              tokenizer: 'autocomplete',\n              filter: ['lowercase']\n            }\n          },\n          tokenizer: {\n            autocomplete: {\n              type: 'edge_ngram',\n              min_gram: 2,\n              max_gram: 10,\n              token_chars: ['letter']\n            }\n          }\n        }\n      },\n      mappings: {\n        properties: {\n          title: {\n            type: 'text',\n            analyzer: 'standard',\n            fields: {\n              autocomplete: {\n                type: 'text',\n                analyzer: 'autocomplete'\n              }\n            }\n          },\n          content: { type: 'text' },\n          url: { type: 'keyword' },\n          published_date: { type: 'date' },\n          views: { type: 'integer' }\n        }\n      }\n    }\n  });\n}\n\n// Index document\nasync function indexDocument(doc) {\n  await client.index({\n    index: 'documents',\n    id: doc.id,\n    body: doc\n  });\n}\n\n// Search with ranking\nasync function search(query, options = {}) {\n  const { from = 0, size = 10, filters = {} } = options;\n\n  const response = await client.search({\n    index: 'documents',\n    body: {\n      from,\n      size,\n      query: {\n        bool: {\n          must: {\n            multi_match: {\n              query,\n              fields: ['title^3', 'content'],  // Boost title\n              fuzziness: 'AUTO'  // Handle typos\n            }\n          },\n          filter: Object.entries(filters).map(([field, value]) => ({\n            term: { [field]: value }\n          }))\n        }\n      },\n      highlight: {\n        fields: {\n          title: {},\n          content: { fragment_size: 150 }\n        }\n      },\n      sort: [\n        '_score',  // Relevance\n        { views: 'desc' },  // Popularity\n        { published_date: 'desc' }  // Freshness\n      ]\n    }\n  });\n\n  return response.hits.hits.map(hit => ({\n    id: hit._id,\n    score: hit._score,\n    ...hit._source,\n    highlight: hit.highlight\n  }));\n}\n\n// Autocomplete\nasync function autocomplete(prefix) {\n  const response = await client.search({\n    index: 'documents',\n    body: {\n      size: 5,\n      query: {\n        match: {\n          'title.autocomplete': prefix\n        }\n      },\n      _source: ['title']\n    }\n  });\n\n  return response.hits.hits.map(hit => hit._source.title);\n}"
    },
    {
      "id": 64,
      "question": "How do you implement OAuth 2.0 and Single Sign-On (SSO)?",
      "answer": "OAuth 2.0 is authorization framework allowing third-party access without sharing credentials.\n\nOAuth 2.0 Roles:\n• Resource Owner: User who owns data\n• Client: Application requesting access\n• Authorization Server: Issues tokens (e.g., Auth0, Okta)\n• Resource Server: API protecting resources\n\nOAuth 2.0 Flows:\n• Authorization Code: Server-side apps (most secure)\n• Implicit: Client-side JS apps (deprecated, use PKCE)\n• Client Credentials: Server-to-server\n• Password: Direct username/password (legacy)\n• PKCE: Extension for public clients (mobile, SPA)\n\nAuthorization Code Flow:\n1. Client redirects user to authorization server\n2. User authenticates and approves\n3. Authorization server redirects back with code\n4. Client exchanges code for access token\n5. Client uses access token to call API\n\nTokens:\n• Access Token: Short-lived (15min-1hr), used to access API\n• Refresh Token: Long-lived (days/months), used to get new access token\n• ID Token (OpenID Connect): Contains user info (JWT)\n\nSSO (Single Sign-On):\n• User logs in once, accesses multiple apps\n• Protocols: SAML, OAuth 2.0 + OpenID Connect\n• Identity Provider (IdP): Central authentication (Okta, Azure AD)\n• Service Provider (SP): Applications trusting IdP",
      "explanation": "OAuth 2.0 uses authorization code flow for secure token exchange, separates access tokens (API access) from refresh tokens (long-term), and SSO enables one login across multiple apps via centralized IdP.",
      "difficulty": "Medium",
      "code": "// OAuth 2.0 Authorization Code Flow (Express)\nconst express = require('express');\nconst axios = require('axios');\nconst app = express();\n\nconst OAUTH_CONFIG = {\n  clientId: process.env.OAUTH_CLIENT_ID,\n  clientSecret: process.env.OAUTH_CLIENT_SECRET,\n  redirectUri: 'http://localhost:3000/callback',\n  authorizationUrl: 'https://oauth.provider.com/authorize',\n  tokenUrl: 'https://oauth.provider.com/token',\n  scope: 'read:user read:email'\n};\n\n// Step 1: Redirect to authorization server\napp.get('/login', (req, res) => {\n  const authUrl = `${OAUTH_CONFIG.authorizationUrl}?` +\n    `client_id=${OAUTH_CONFIG.clientId}&` +\n    `redirect_uri=${encodeURIComponent(OAUTH_CONFIG.redirectUri)}&` +\n    `response_type=code&` +\n    `scope=${encodeURIComponent(OAUTH_CONFIG.scope)}&` +\n    `state=${generateState()}`;\n  \n  res.redirect(authUrl);\n});\n\n// Step 2: Handle callback with authorization code\napp.get('/callback', async (req, res) => {\n  const { code, state } = req.query;\n  \n  // Verify state (CSRF protection)\n  if (!verifyState(state)) {\n    return res.status(400).send('Invalid state');\n  }\n  \n  try {\n    // Step 3: Exchange code for tokens\n    const tokenResponse = await axios.post(\n      OAUTH_CONFIG.tokenUrl,\n      new URLSearchParams({\n        grant_type: 'authorization_code',\n        code,\n        redirect_uri: OAUTH_CONFIG.redirectUri,\n        client_id: OAUTH_CONFIG.clientId,\n        client_secret: OAUTH_CONFIG.clientSecret\n      }),\n      { headers: { 'Content-Type': 'application/x-www-form-urlencoded' } }\n    );\n    \n    const { access_token, refresh_token, expires_in } = tokenResponse.data;\n    \n    // Store tokens (in session/database)\n    req.session.accessToken = access_token;\n    req.session.refreshToken = refresh_token;\n    \n    res.redirect('/dashboard');\n    \n  } catch (error) {\n    res.status(500).send('Token exchange failed');\n  }\n});\n\n// Use access token to call API\napp.get('/api/user/profile', async (req, res) => {\n  const accessToken = req.session.accessToken;\n  \n  try {\n    const response = await axios.get(\n      'https://api.provider.com/user',\n      { headers: { Authorization: `Bearer ${accessToken}` } }\n    );\n    \n    res.json(response.data);\n    \n  } catch (error) {\n    if (error.response.status === 401) {\n      // Token expired, refresh it\n      const newToken = await refreshAccessToken(req.session.refreshToken);\n      req.session.accessToken = newToken;\n      return res.redirect('/api/user/profile');\n    }\n    res.status(500).send('API call failed');\n  }\n});\n\n// Refresh access token\nasync function refreshAccessToken(refreshToken) {\n  const response = await axios.post(\n    OAUTH_CONFIG.tokenUrl,\n    new URLSearchParams({\n      grant_type: 'refresh_token',\n      refresh_token: refreshToken,\n      client_id: OAUTH_CONFIG.clientId,\n      client_secret: OAUTH_CONFIG.clientSecret\n    })\n  );\n  \n  return response.data.access_token;\n}"
    },
    {
      "id": 65,
      "question": "What is the Saga Pattern for Distributed Transactions?",
      "answer": "The Saga pattern manages distributed transactions across microservices without distributed locks.\n\nProblem:\n• Distributed transactions (2PC) are slow and lock resources\n• Microservices have independent databases\n• Need transactional guarantees across services\n\nSaga Approach:\n• Break transaction into sequence of local transactions\n• Each step commits immediately\n• If step fails, execute compensating transactions to undo\n• Eventual consistency instead of strong consistency\n\nSaga Types:\n• Choreography: Services publish events, others listen and react\n• Orchestration: Central coordinator directs saga flow\n\nExample: Order Placement Saga\n1. Order Service: Create order (status: pending)\n2. Payment Service: Reserve payment\n3. Inventory Service: Reserve stock\n4. Shipping Service: Schedule delivery\n5. Order Service: Mark order confirmed\n\nFailure: If step 3 fails:\n1. Inventory Service: Publish \"reservation failed\" event\n2. Payment Service: Release payment (compensating)\n3. Order Service: Cancel order (compensating)\n\nBest Practices:\n• Idempotent operations: Safe to retry\n• Compensating transactions: Must always succeed\n• Saga log: Track saga state for recovery\n• Timeouts: Handle slow/stuck steps",
      "explanation": "Sagas break distributed transactions into local transactions with compensating actions for rollback, use choreography (events) or orchestration (coordinator), trade consistency for availability.",
      "difficulty": "Hard",
      "code": "// Saga Pattern with Orchestration\nclass OrderSaga {\n  constructor(services) {\n    this.orderService = services.order;\n    this.paymentService = services.payment;\n    this.inventoryService = services.inventory;\n    this.shippingService = services.shipping;\n    this.sagaLog = [];\n  }\n\n  async execute(orderData) {\n    const sagaId = this.generateSagaId();\n    \n    try {\n      // Step 1: Create order\n      this.log(sagaId, 'create_order', 'started');\n      const order = await this.orderService.createOrder(orderData);\n      this.log(sagaId, 'create_order', 'completed', { orderId: order.id });\n      \n      // Step 2: Reserve payment\n      this.log(sagaId, 'reserve_payment', 'started');\n      const payment = await this.paymentService.reserve(\n        order.userId,\n        order.totalAmount\n      );\n      this.log(sagaId, 'reserve_payment', 'completed', { paymentId: payment.id });\n      \n      // Step 3: Reserve inventory\n      this.log(sagaId, 'reserve_inventory', 'started');\n      await this.inventoryService.reserve(order.items);\n      this.log(sagaId, 'reserve_inventory', 'completed');\n      \n      // Step 4: Schedule shipping\n      this.log(sagaId, 'schedule_shipping', 'started');\n      const shipping = await this.shippingService.schedule(order);\n      this.log(sagaId, 'schedule_shipping', 'completed', { shippingId: shipping.id });\n      \n      // Step 5: Confirm order\n      await this.orderService.confirm(order.id);\n      this.log(sagaId, 'saga_completed', 'success');\n      \n      return { success: true, orderId: order.id };\n      \n    } catch (error) {\n      console.error('Saga failed:', error);\n      this.log(sagaId, 'saga_failed', 'error', { error: error.message });\n      \n      // Execute compensating transactions\n      await this.compensate(sagaId);\n      \n      return { success: false, error: error.message };\n    }\n  }\n\n  async compensate(sagaId) {\n    console.log(`Starting compensation for saga ${sagaId}`);\n    \n    // Get completed steps from log\n    const completedSteps = this.sagaLog\n      .filter(entry => entry.sagaId === sagaId && entry.status === 'completed')\n      .reverse();  // Compensate in reverse order\n    \n    for (const step of completedSteps) {\n      try {\n        switch (step.action) {\n          case 'create_order':\n            await this.orderService.cancel(step.data.orderId);\n            console.log('Compensated: order cancelled');\n            break;\n          \n          case 'reserve_payment':\n            await this.paymentService.release(step.data.paymentId);\n            console.log('Compensated: payment released');\n            break;\n          \n          case 'reserve_inventory':\n            await this.inventoryService.release(step.data.orderId);\n            console.log('Compensated: inventory released');\n            break;\n          \n          case 'schedule_shipping':\n            await this.shippingService.cancel(step.data.shippingId);\n            console.log('Compensated: shipping cancelled');\n            break;\n        }\n      } catch (error) {\n        // Compensating transaction failed - this should be retried\n        console.error('Compensation failed:', error);\n        // In production: send to dead letter queue for manual intervention\n      }\n    }\n  }\n\n  log(sagaId, action, status, data = {}) {\n    this.sagaLog.push({\n      sagaId,\n      action,\n      status,\n      data,\n      timestamp: Date.now()\n    });\n  }\n\n  generateSagaId() {\n    return `saga_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n}\n\n// Usage\nconst saga = new OrderSaga({\n  order: orderService,\n  payment: paymentService,\n  inventory: inventoryService,\n  shipping: shippingService\n});\n\nconst result = await saga.execute({\n  userId: 123,\n  items: [{ productId: 456, quantity: 2 }],\n  totalAmount: 99.99\n});"
    },
    {
      "id": 66,
      "question": "Design a Social Media Feed with Ranking Algorithm",
      "answer": "A social media feed aggregates posts from connections and ranks them by relevance.\n\nFeed Generation Approaches:\n• Fan-out on Write (Push): When user posts, push to all followers' feeds\n  - Pros: Fast read, pre-computed\n  - Cons: Slow write, storage overhead for users with many followers\n• Fan-out on Read (Pull): Fetch posts from followings when user requests feed\n  - Pros: Fast write, less storage\n  - Cons: Slow read, must aggregate on-the-fly\n• Hybrid: Fan-out for normal users, pull for celebrities\n\nRanking Factors:\n• Recency: Recent posts rank higher\n• Engagement: Likes, comments, shares\n• Affinity: How often user interacts with author\n• Content type: Videos may rank higher than text\n• Dwell time: How long user views post\n\nRanking Algorithm:\n• Score = w1×recency + w2×engagement + w3×affinity + w4×content_type\n• Machine learning: Train model on user behavior\n• A/B testing: Experiment with different weights\n\nOptimizations:\n• Pre-compute feed: Generate hourly in background\n• Pagination: Load top 20, fetch more on scroll\n• Real-time updates: WebSocket for new posts\n• Caching: Cache user's feed, invalidate on new post\n• Database: Use time-series DB or sorted sets (Redis)",
      "explanation": "Social feeds use fan-out strategies (push to followers or pull on read), rank by recency/engagement/affinity scores, optimize with background pre-computation and caching.",
      "difficulty": "Hard",
      "code": "// Social Media Feed System\nconst Redis = require('ioredis');\nconst redis = new Redis();\n\nclass FeedService {\n  constructor(db, redis) {\n    this.db = db;\n    this.redis = redis;\n  }\n\n  // Create post with fan-out\n  async createPost(userId, content) {\n    // Save post to database\n    const post = await this.db.posts.create({\n      userId,\n      content,\n      createdAt: new Date(),\n      likes: 0,\n      comments: 0\n    });\n\n    // Get user's followers\n    const followers = await this.db.followers.find({ followingId: userId });\n\n    // Decision: Fan-out or not?\n    if (followers.length < 10000) {\n      // Normal user: Fan-out on write\n      await this.fanOutPost(post, followers);\n    } else {\n      // Celebrity: Mark for pull on read\n      await this.redis.set(`celebrity_post:${post.id}`, JSON.stringify(post));\n    }\n\n    return post;\n  }\n\n  // Fan-out: Push post to followers' feeds\n  async fanOutPost(post, followers) {\n    const pipeline = this.redis.pipeline();\n\n    for (const follower of followers) {\n      const feedKey = `feed:${follower.followerId}`;\n      \n      // Add to Redis sorted set (score = timestamp)\n      pipeline.zadd(\n        feedKey,\n        post.createdAt.getTime(),\n        JSON.stringify(post)\n      );\n\n      // Keep only recent 1000 posts\n      pipeline.zremrangebyrank(feedKey, 0, -1001);\n    }\n\n    await pipeline.exec();\n  }\n\n  // Get user's feed\n  async getFeed(userId, options = {}) {\n    const { page = 0, limit = 20 } = options;\n    const offset = page * limit;\n\n    // Check cache first\n    const cacheKey = `feed:${userId}:page:${page}`;\n    const cached = await this.redis.get(cacheKey);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n\n    // Get posts from pre-computed feed\n    const feedKey = `feed:${userId}`;\n    const posts = await this.redis.zrevrange(\n      feedKey,\n      offset,\n      offset + limit - 1,\n      'WITHSCORES'\n    );\n\n    // Get celebrity posts (pull on read)\n    const followings = await this.db.followers.find({ followerId: userId });\n    const celebrityPosts = await this.getCelebrityPosts(followings, limit);\n\n    // Merge and rank\n    const allPosts = [...posts, ...celebrityPosts];\n    const rankedPosts = await this.rankPosts(userId, allPosts);\n\n    // Cache for 5 minutes\n    await this.redis.setex(cacheKey, 300, JSON.stringify(rankedPosts));\n\n    return rankedPosts;\n  }\n\n  // Get posts from celebrities (pull on read)\n  async getCelebrityPosts(followings, limit) {\n    const celebrities = followings.filter(f => f.isCelebrity);\n    const posts = [];\n\n    for (const celeb of celebrities) {\n      const recentPosts = await this.db.posts.find(\n        { userId: celeb.followingId },\n        { limit: 10, sort: { createdAt: -1 } }\n      );\n      posts.push(...recentPosts);\n    }\n\n    return posts.slice(0, limit);\n  }\n\n  // Rank posts by relevance\n  async rankPosts(userId, posts) {\n    const userInteractions = await this.getUserInteractions(userId);\n\n    const rankedPosts = posts.map(post => {\n      const score = this.calculateScore(post, userInteractions);\n      return { ...post, score };\n    });\n\n    return rankedPosts\n      .sort((a, b) => b.score - a.score)\n      .slice(0, 20);\n  }\n\n  calculateScore(post, userInteractions) {\n    const now = Date.now();\n    const ageInHours = (now - post.createdAt) / (1000 * 60 * 60);\n\n    // Recency: Decay exponentially (half-life = 24 hours)\n    const recencyScore = Math.exp(-0.693 * ageInHours / 24);\n\n    // Engagement: Normalized by post age\n    const engagementScore = (post.likes + post.comments * 2) / (ageInHours + 1);\n\n    // Affinity: How often user interacts with author\n    const affinity = userInteractions[post.userId] || 0;\n    const affinityScore = Math.log(affinity + 1);\n\n    // Content type: Videos rank higher\n    const contentTypeScore = post.type === 'video' ? 1.5 : 1.0;\n\n    // Weighted sum\n    return (\n      0.3 * recencyScore +\n      0.4 * engagementScore +\n      0.2 * affinityScore +\n      0.1 * contentTypeScore\n    );\n  }\n\n  async getUserInteractions(userId) {\n    const interactions = await this.db.interactions.aggregate([\n      { $match: { userId } },\n      { $group: { _id: '$authorId', count: { $sum: 1 } } }\n    ]);\n\n    return Object.fromEntries(\n      interactions.map(i => [i._id, i.count])\n    );\n  }\n}\n\n// Background job: Pre-compute feeds\nasync function precomputeFeeds() {\n  const users = await db.users.find({ active: true });\n\n  for (const user of users) {\n    await feedService.getFeed(user.id, { page: 0 });\n  }\n}\n\n// Run every hour\nsetInterval(precomputeFeeds, 60 * 60 * 1000);"
    },
    {
      "id": 67,
      "question": "How do you design a Distributed Cache with Cache Invalidation?",
      "answer": "A distributed cache stores data across multiple nodes for scalability and fault tolerance.\n\nCache Strategies:\n• Cache-Aside: App checks cache, loads from DB on miss\n• Write-Through: Write to cache and DB synchronously\n• Write-Behind: Write to cache, async write to DB\n• Refresh-Ahead: Pre-fetch data before expiration\n\nCache Invalidation Strategies:\n• TTL (Time to Live): Auto-expire after N seconds\n• Event-based: Invalidate on DB update\n• Write-invalidate: Delete cache on write\n• Write-update: Update cache on write\n\nInvalidation Patterns:\n• Active expiration: Background job checks TTL\n• Passive expiration: Check TTL on access\n• Versioning: Increment version on update\n• Tags/Groups: Invalidate related items\n\nDistributed Challenges:\n• Consistency: Cache may be stale\n• Cache stampede: Many requests fetch same data simultaneously\n• Hot keys: Popular keys overload single node\n• Cold start: Empty cache after restart\n\nSolutions:\n• Consistent hashing: Distribute keys evenly\n• Replication: Replicate hot keys\n• Probabilistic expiration: Randomize TTL to prevent stampede\n• Warming: Pre-populate cache on startup",
      "explanation": "Distributed caches use TTL, event-based, or write-based invalidation; prevent cache stampede with probabilistic expiration; use consistent hashing for distribution; replicate hot keys for load distribution.",
      "difficulty": "Medium",
      "code": "// Distributed Cache with Redis and Invalidation\nconst Redis = require('ioredis');\nconst redis = new Redis.Cluster([/* nodes */]);\n\nclass DistributedCache {\n  constructor(redis, db) {\n    this.redis = redis;\n    this.db = db;\n  }\n\n  // Cache-aside pattern with TTL\n  async get(key) {\n    // Try cache first\n    const cached = await this.redis.get(key);\n    if (cached) {\n      return JSON.parse(cached);\n    }\n\n    // Cache miss: Load from database\n    const data = await this.db.findById(key);\n    if (!data) return null;\n\n    // Store in cache with TTL (1 hour)\n    await this.set(key, data, 3600);\n    return data;\n  }\n\n  // Set with probabilistic expiration (prevent stampede)\n  async set(key, value, ttl) {\n    // Add random jitter: ±10% to prevent simultaneous expiration\n    const jitter = ttl * 0.1 * (Math.random() * 2 - 1);\n    const actualTTL = Math.floor(ttl + jitter);\n\n    await this.redis.setex(\n      key,\n      actualTTL,\n      JSON.stringify({\n        data: value,\n        version: Date.now()  // Version for optimistic locking\n      })\n    );\n  }\n\n  // Invalidate single key\n  async invalidate(key) {\n    await this.redis.del(key);\n  }\n\n  // Invalidate by pattern (e.g., \"user:*\")\n  async invalidatePattern(pattern) {\n    const keys = await this.redis.keys(pattern);\n    if (keys.length > 0) {\n      await this.redis.del(...keys);\n    }\n  }\n\n  // Invalidate by tags (group invalidation)\n  async setWithTags(key, value, ttl, tags = []) {\n    await this.set(key, value, ttl);\n\n    // Store key in tag sets\n    const pipeline = this.redis.pipeline();\n    for (const tag of tags) {\n      pipeline.sadd(`tag:${tag}`, key);\n      pipeline.expire(`tag:${tag}`, ttl);\n    }\n    await pipeline.exec();\n  }\n\n  async invalidateByTag(tag) {\n    const keys = await this.redis.smembers(`tag:${tag}`);\n    if (keys.length > 0) {\n      await this.redis.del(...keys);\n      await this.redis.del(`tag:${tag}`);\n    }\n  }\n}\n\n// Event-based invalidation\nclass CacheInvalidator {\n  constructor(cache, eventBus) {\n    this.cache = cache;\n    this.eventBus = eventBus;\n    this.setupListeners();\n  }\n\n  setupListeners() {\n    // Invalidate on user update\n    this.eventBus.on('user.updated', async (userId) => {\n      await this.cache.invalidate(`user:${userId}`);\n      await this.cache.invalidateByTag(`user:${userId}`);\n    });\n\n    // Invalidate related caches\n    this.eventBus.on('post.created', async (post) => {\n      await this.cache.invalidateByTag(`user:${post.userId}:posts`);\n      await this.cache.invalidate(`feed:${post.userId}`);\n    });\n  }\n}\n\n// Cache warming on startup\nasync function warmCache(cache, db) {\n  console.log('Warming cache...');\n  \n  // Pre-load popular users\n  const popularUsers = await db.users.find(\n    { followers: { $gt: 10000 } },\n    { limit: 100 }\n  );\n  \n  for (const user of popularUsers) {\n    await cache.set(`user:${user.id}`, user, 3600);\n  }\n  \n  console.log('Cache warmed');\n}"
    },
    {
      "id": 68,
      "question": "What is the Circuit Breaker Pattern and how do you implement it?",
      "answer": "Circuit breaker prevents cascading failures by stopping requests to failing services.\n\nStates:\n• Closed: Normal operation, requests pass through\n• Open: Service failing, reject requests immediately (fail fast)\n• Half-Open: Test if service recovered, allow limited requests\n\nState Transitions:\n• Closed → Open: After N failures in time window\n• Open → Half-Open: After timeout period\n• Half-Open → Closed: If test requests succeed\n• Half-Open → Open: If test requests fail\n\nConfiguration:\n• Failure threshold: Number of failures to open circuit (e.g., 5)\n• Timeout: How long to wait before half-open (e.g., 30s)\n• Success threshold: Successes needed to close (e.g., 2)\n• Rolling window: Time window to count failures (e.g., 10s)\n\nBenefits:\n• Fail fast: Don't waste resources on doomed requests\n• Give time to recover: Stop overwhelming failing service\n• Graceful degradation: Return cached/default response\n• Monitoring: Track circuit state for alerting\n\nFallback Strategies:\n• Return cached data\n• Return default value\n• Return error message\n• Retry with backoff",
      "explanation": "Circuit breaker has closed/open/half-open states, opens after threshold failures, prevents cascading failures by failing fast, allows service recovery time, implements fallback strategies.",
      "difficulty": "Medium",
      "code": "// Circuit Breaker Implementation\nclass CircuitBreaker {\n  constructor(options = {}) {\n    this.failureThreshold = options.failureThreshold || 5;\n    this.timeout = options.timeout || 30000;  // 30 seconds\n    this.successThreshold = options.successThreshold || 2;\n    this.windowSize = options.windowSize || 10000;  // 10 seconds\n    \n    this.state = 'CLOSED';\n    this.failures = [];\n    this.successes = 0;\n    this.nextAttempt = null;\n  }\n\n  async execute(fn, fallback) {\n    if (this.state === 'OPEN') {\n      if (Date.now() < this.nextAttempt) {\n        // Circuit is open, fail fast\n        console.log('Circuit OPEN, using fallback');\n        return fallback ? fallback() : Promise.reject(new Error('Circuit breaker is open'));\n      } else {\n        // Timeout elapsed, try half-open\n        this.state = 'HALF_OPEN';\n        this.successes = 0;\n        console.log('Circuit entering HALF_OPEN state');\n      }\n    }\n\n    try {\n      const result = await fn();\n      this.onSuccess();\n      return result;\n    } catch (error) {\n      this.onFailure();\n      return fallback ? fallback() : Promise.reject(error);\n    }\n  }\n\n  onSuccess() {\n    if (this.state === 'HALF_OPEN') {\n      this.successes++;\n      if (this.successes >= this.successThreshold) {\n        // Service recovered\n        this.state = 'CLOSED';\n        this.failures = [];\n        console.log('Circuit CLOSED (service recovered)');\n      }\n    } else if (this.state === 'CLOSED') {\n      // Remove old failures outside window\n      this.cleanupFailures();\n    }\n  }\n\n  onFailure() {\n    this.failures.push(Date.now());\n    this.cleanupFailures();\n\n    if (this.state === 'HALF_OPEN') {\n      // Failed during test, go back to open\n      this.openCircuit();\n    } else if (this.failures.length >= this.failureThreshold) {\n      // Too many failures, open circuit\n      this.openCircuit();\n    }\n  }\n\n  openCircuit() {\n    this.state = 'OPEN';\n    this.nextAttempt = Date.now() + this.timeout;\n    console.log(`Circuit OPEN, will retry at ${new Date(this.nextAttempt)}`);\n  }\n\n  cleanupFailures() {\n    const cutoff = Date.now() - this.windowSize;\n    this.failures = this.failures.filter(time => time > cutoff);\n  }\n\n  getState() {\n    return {\n      state: this.state,\n      failures: this.failures.length,\n      nextAttempt: this.nextAttempt\n    };\n  }\n}\n\n// Usage with API client\nconst breaker = new CircuitBreaker({\n  failureThreshold: 5,\n  timeout: 30000,\n  successThreshold: 2,\n  windowSize: 10000\n});\n\nasync function callExternalAPI(userId) {\n  return breaker.execute(\n    // Main function\n    async () => {\n      const response = await fetch(`https://api.example.com/users/${userId}`);\n      if (!response.ok) throw new Error('API error');\n      return response.json();\n    },\n    // Fallback\n    async () => {\n      // Return cached data or default\n      const cached = await cache.get(`user:${userId}`);\n      if (cached) return cached;\n      return { id: userId, name: 'Unknown' };\n    }\n  );\n}\n\n// Monitor circuit breaker state\nsetInterval(() => {\n  const state = breaker.getState();\n  console.log('Circuit breaker state:', state);\n}, 5000);"
    },
    {
      "id": 69,
      "question": "Design a Geographically Distributed Database with Conflict Resolution",
      "answer": "Geo-distributed databases replicate data across regions for low latency and availability.\n\nReplication Strategies:\n• Master-Replica: One write master, multiple read replicas\n• Multi-Master: Multiple regions accept writes\n• Leaderless: All nodes equal (Cassandra, DynamoDB)\n\nConsistency Models:\n• Strong Consistency: All replicas agree before response (slow)\n• Eventual Consistency: Replicas converge over time (fast)\n• Causal Consistency: Causally related ops ordered\n• Session Consistency: Consistent within user session\n\nConflict Resolution (Multi-Master):\n• Last Write Wins (LWW): Use timestamp, higher wins\n  - Problem: Clock skew, lost updates\n• Version Vectors: Track versions per replica\n• Conflict-Free Replicated Data Types (CRDTs): Math guarantees convergence\n• Application-level: Custom merge logic\n\nCRDT Examples:\n• G-Counter: Grow-only counter (sum all replicas)\n• PN-Counter: Positive-negative counter (increments - decrements)\n• LWW-Register: Last-write-wins with timestamp\n• OR-Set: Observed-remove set (add/remove with unique IDs)\n\nDistributed Transactions:\n• 2PC (Two-Phase Commit): Coordinator + participants (slow, blocking)\n• Paxos/Raft: Consensus algorithms\n• Calvin: Deterministic ordering",
      "explanation": "Geo-distributed databases use multi-master replication for multi-region writes, resolve conflicts with LWW/CRDTs/application logic, trade consistency for availability, use vector clocks for causality.",
      "difficulty": "Hard",
      "code": "// Conflict Resolution with CRDTs\n\n// PN-Counter (Positive-Negative Counter)\nclass PNCounter {\n  constructor(replicaId) {\n    this.replicaId = replicaId;\n    this.increments = {};  // replicaId -> count\n    this.decrements = {};  // replicaId -> count\n  }\n\n  increment(amount = 1) {\n    this.increments[this.replicaId] = \n      (this.increments[this.replicaId] || 0) + amount;\n  }\n\n  decrement(amount = 1) {\n    this.decrements[this.replicaId] = \n      (this.decrements[this.replicaId] || 0) + amount;\n  }\n\n  value() {\n    const totalInc = Object.values(this.increments).reduce((a, b) => a + b, 0);\n    const totalDec = Object.values(this.decrements).reduce((a, b) => a + b, 0);\n    return totalInc - totalDec;\n  }\n\n  merge(other) {\n    // Merge increments (take max from each replica)\n    for (const [replica, count] of Object.entries(other.increments)) {\n      this.increments[replica] = Math.max(\n        this.increments[replica] || 0,\n        count\n      );\n    }\n\n    // Merge decrements\n    for (const [replica, count] of Object.entries(other.decrements)) {\n      this.decrements[replica] = Math.max(\n        this.decrements[replica] || 0,\n        count\n      );\n    }\n  }\n}\n\n// Last Write Wins with Vector Clocks\nclass VectorClock {\n  constructor(replicaId) {\n    this.replicaId = replicaId;\n    this.clock = { [replicaId]: 0 };\n  }\n\n  increment() {\n    this.clock[this.replicaId]++;\n  }\n\n  update(other) {\n    for (const [replica, time] of Object.entries(other.clock)) {\n      this.clock[replica] = Math.max(\n        this.clock[replica] || 0,\n        time\n      );\n    }\n    this.clock[this.replicaId]++;\n  }\n\n  // Compare vector clocks\n  compare(other) {\n    let greater = false;\n    let less = false;\n\n    const allReplicas = new Set([\n      ...Object.keys(this.clock),\n      ...Object.keys(other.clock)\n    ]);\n\n    for (const replica of allReplicas) {\n      const mine = this.clock[replica] || 0;\n      const theirs = other.clock[replica] || 0;\n\n      if (mine > theirs) greater = true;\n      if (mine < theirs) less = true;\n    }\n\n    if (greater && !less) return 1;   // This is newer\n    if (less && !greater) return -1;  // Other is newer\n    if (!greater && !less) return 0;  // Equal\n    return null;  // Concurrent (conflict!)\n  }\n}\n\n// Versioned Document with Conflict Detection\nclass Document {\n  constructor(id, data, replicaId) {\n    this.id = id;\n    this.data = data;\n    this.vector = new VectorClock(replicaId);\n  }\n\n  update(newData, replicaId) {\n    this.data = { ...this.data, ...newData };\n    this.vector.increment();\n  }\n\n  merge(otherDoc) {\n    const comparison = this.vector.compare(otherDoc.vector);\n\n    if (comparison === 1) {\n      // This is newer, keep it\n      return this;\n    } else if (comparison === -1) {\n      // Other is newer, take it\n      return otherDoc;\n    } else if (comparison === 0) {\n      // Same version, no conflict\n      return this;\n    } else {\n      // Concurrent writes - CONFLICT!\n      return this.resolveConflict(otherDoc);\n    }\n  }\n\n  resolveConflict(otherDoc) {\n    console.log('Conflict detected, resolving...');\n\n    // Strategy 1: Last write wins (use timestamp)\n    const thisTime = Math.max(...Object.values(this.vector.clock));\n    const otherTime = Math.max(...Object.values(otherDoc.vector.clock));\n    \n    if (thisTime > otherTime) {\n      this.vector.update(otherDoc.vector);\n      return this;\n    } else {\n      otherDoc.vector.update(this.vector);\n      return otherDoc;\n    }\n\n    // Strategy 2: Merge data (application-specific)\n    // const merged = { ...this.data, ...otherDoc.data };\n    // this.data = merged;\n    // this.vector.update(otherDoc.vector);\n    // return this;\n  }\n}\n\n// Example usage\nconst doc1 = new Document('user:123', { name: 'Alice', age: 30 }, 'replica1');\nconst doc2 = new Document('user:123', { name: 'Alice', age: 31 }, 'replica2');\n\n// Concurrent updates\ndoc1.update({ age: 32 }, 'replica1');\ndoc2.update({ city: 'NYC' }, 'replica2');\n\n// Replicas sync - detect conflict\nconst resolved = doc1.merge(doc2);\nconsole.log('Resolved:', resolved.data);"
    },
    {
      "id": 70,
      "question": "How do you implement Real-Time Collaborative Editing (like Google Docs)?",
      "answer": "Real-time collaborative editing allows multiple users to edit the same document simultaneously.\n\nChallenges:\n• Concurrent edits: Users edit simultaneously\n• Network latency: Edits arrive out of order\n• Conflict resolution: Same position edited by multiple users\n• Consistency: All users see same final state\n\nOperational Transformation (OT):\n• Transform operations based on concurrent ops\n• Example: User A inserts at pos 5, User B deletes at pos 3\n  - Transform A's op: Insert at pos 4 (adjusted)\n• Used by Google Docs, Office 365\n\nCRDT Approach (Conflict-Free Replicated Data Types):\n• Each character has unique ID and position\n• Operations commutative: Order doesn't matter\n• Used by: Atom Teletype, IPFS\n\nArchitecture:\n• Client: Text editor (Quill, CodeMirror)\n• Server: Operation gateway (WebSocket)\n• Persistence: Store operations + snapshots\n• Presence: Show user cursors and selections\n\nOptimizations:\n• Local echo: Apply edits immediately, replay if conflict\n• Batching: Group operations (e.g., typing \"hello\" as one op)\n• Snapshots: Periodically save full state\n• Compaction: Merge old operations\n• Cursors: Broadcast cursor positions",
      "explanation": "Collaborative editing uses Operational Transformation or CRDTs to resolve concurrent edits, broadcasts operations via WebSocket, maintains consistency with transformed operations, shows presence with user cursors.",
      "difficulty": "Hard",
      "code": "// Collaborative Editing with OT (Operational Transformation)\n\nclass Operation {\n  constructor(type, position, content, userId) {\n    this.type = type;  // 'insert' or 'delete'\n    this.position = position;\n    this.content = content;\n    this.userId = userId;\n    this.version = 0;\n  }\n}\n\nclass OTEngine {\n  // Transform operation A against operation B\n  static transform(opA, opB) {\n    if (opA.type === 'insert' && opB.type === 'insert') {\n      if (opA.position < opB.position) {\n        // A before B, B position shifts right\n        return {\n          opA: opA,\n          opB: new Operation('insert', opB.position + opA.content.length, opB.content, opB.userId)\n        };\n      } else if (opA.position > opB.position) {\n        // B before A, A position shifts right\n        return {\n          opA: new Operation('insert', opA.position + opB.content.length, opA.content, opA.userId),\n          opB: opB\n        };\n      } else {\n        // Same position, tie-break by userId\n        if (opA.userId < opB.userId) {\n          return {\n            opA: opA,\n            opB: new Operation('insert', opB.position + opA.content.length, opB.content, opB.userId)\n          };\n        } else {\n          return {\n            opA: new Operation('insert', opA.position + opB.content.length, opA.content, opA.userId),\n            opB: opB\n          };\n        }\n      }\n    }\n\n    if (opA.type === 'delete' && opB.type === 'insert') {\n      if (opA.position < opB.position) {\n        return {\n          opA: opA,\n          opB: new Operation('insert', opB.position - opA.content.length, opB.content, opB.userId)\n        };\n      } else {\n        return {\n          opA: new Operation('delete', opA.position + opB.content.length, opA.content, opA.userId),\n          opB: opB\n        };\n      }\n    }\n\n    if (opA.type === 'insert' && opB.type === 'delete') {\n      if (opA.position <= opB.position) {\n        return {\n          opA: opA,\n          opB: new Operation('delete', opB.position + opA.content.length, opB.content, opB.userId)\n        };\n      } else {\n        return {\n          opA: new Operation('insert', opA.position - opB.content.length, opA.content, opA.userId),\n          opB: opB\n        };\n      }\n    }\n\n    if (opA.type === 'delete' && opB.type === 'delete') {\n      // Complex: overlapping deletes\n      return { opA, opB };  // Simplified\n    }\n  }\n}\n\nclass CollaborativeDocument {\n  constructor(id) {\n    this.id = id;\n    this.content = '';\n    this.version = 0;\n    this.history = [];  // All operations\n  }\n\n  apply(operation) {\n    if (operation.type === 'insert') {\n      this.content = \n        this.content.slice(0, operation.position) +\n        operation.content +\n        this.content.slice(operation.position);\n    } else if (operation.type === 'delete') {\n      this.content = \n        this.content.slice(0, operation.position) +\n        this.content.slice(operation.position + operation.content.length);\n    }\n    \n    operation.version = this.version++;\n    this.history.push(operation);\n  }\n}\n\n// Server: WebSocket handler\nconst WebSocket = require('ws');\nconst wss = new WebSocket.Server({ port: 8080 });\n\nconst documents = new Map();\nconst clients = new Map();\n\nwss.on('connection', (ws) => {\n  let userId, docId;\n\n  ws.on('message', (data) => {\n    const msg = JSON.parse(data);\n\n    if (msg.type === 'join') {\n      userId = msg.userId;\n      docId = msg.docId;\n\n      if (!documents.has(docId)) {\n        documents.set(docId, new CollaborativeDocument(docId));\n      }\n\n      clients.set(userId, { ws, docId });\n\n      // Send current document state\n      ws.send(JSON.stringify({\n        type: 'init',\n        content: documents.get(docId).content,\n        version: documents.get(docId).version\n      }));\n    }\n\n    if (msg.type === 'operation') {\n      const doc = documents.get(docId);\n      const operation = new Operation(\n        msg.operation.type,\n        msg.operation.position,\n        msg.operation.content,\n        userId\n      );\n\n      // Apply operation\n      doc.apply(operation);\n\n      // Broadcast to all clients (except sender)\n      for (const [clientId, client] of clients.entries()) {\n        if (client.docId === docId && clientId !== userId) {\n          client.ws.send(JSON.stringify({\n            type: 'operation',\n            operation\n          }));\n        }\n      }\n    }\n  });\n\n  ws.on('close', () => {\n    clients.delete(userId);\n  });\n});"
    },
    {
      "id": 71,
      "question": "Design a Distributed Logging and Tracing System",
      "answer": "Distributed logging collects logs from multiple services for centralized analysis.\n\nLogging Architecture:\n• Log sources: Applications, containers, servers\n• Shippers: Agents collecting logs (Fluentd, Filebeat)\n• Aggregator: Collects from shippers (Logstash, Fluentd)\n• Storage: Elasticsearch, S3, BigQuery\n• Visualization: Kibana, Grafana\n\nLog Structure:\n• timestamp: ISO 8601 format\n• level: DEBUG, INFO, WARN, ERROR, FATAL\n• message: Human-readable description\n• traceId: Correlate across services\n• spanId: Unique operation ID\n• service: Service name\n• metadata: userId, requestId, etc.\n\nDistributed Tracing:\n• Trace: End-to-end request journey\n• Span: Single operation within trace\n• Context propagation: Pass traceId/spanId in headers\n• Tools: Jaeger, Zipkin, OpenTelemetry\n\nBest Practices:\n• Structured logging: JSON format\n• Correlation IDs: Track requests across services\n• Sampling: Trace subset of requests (1-10%)\n• Retention: Hot storage (7 days), cold storage (90 days)\n• Alerting: Monitor error rates, latency",
      "explanation": "Distributed logging uses agents to ship logs to centralized storage (Elasticsearch), structured JSON format with traceId correlation, distributed tracing with spans tracking operations, sampling to reduce volume.",
      "difficulty": "Medium",
      "code": "// Distributed Logging with Winston\nconst winston = require('winston');\nconst { ElasticsearchTransport } = require('winston-elasticsearch');\nconst { context } = require('./tracing');\n\n// Logger configuration\nconst logger = winston.createLogger({\n  level: 'info',\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.errors({ stack: true }),\n    winston.format.json()\n  ),\n  defaultMeta: {\n    service: process.env.SERVICE_NAME || 'api',\n    environment: process.env.NODE_ENV || 'development',\n    hostname: require('os').hostname()\n  },\n  transports: [\n    // Console output\n    new winston.transports.Console({\n      format: winston.format.combine(\n        winston.format.colorize(),\n        winston.format.simple()\n      )\n    }),\n    \n    // Elasticsearch\n    new ElasticsearchTransport({\n      level: 'info',\n      clientOpts: { node: 'http://localhost:9200' },\n      index: 'logs'\n    }),\n    \n    // File (for local dev)\n    new winston.transports.File({ \n      filename: 'logs/error.log', \n      level: 'error' \n    })\n  ]\n});\n\n// Add trace context to logs\nfunction enrichWithTrace(log) {\n  const traceContext = context.getActiveContext();\n  if (traceContext) {\n    log.traceId = traceContext.traceId;\n    log.spanId = traceContext.spanId;\n  }\n  return log;\n}\n\n// Wrapper for structured logging\nclass Logger {\n  info(message, meta = {}) {\n    logger.info(message, enrichWithTrace(meta));\n  }\n\n  error(message, error, meta = {}) {\n    logger.error(message, enrichWithTrace({\n      ...meta,\n      error: {\n        message: error.message,\n        stack: error.stack,\n        name: error.name\n      }\n    }));\n  }\n\n  warn(message, meta = {}) {\n    logger.warn(message, enrichWithTrace(meta));\n  }\n}\n\nmodule.exports = new Logger();\n\n// Distributed Tracing with OpenTelemetry\nconst { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');\nconst { JaegerExporter } = require('@opentelemetry/exporter-jaeger');\nconst { Resource } = require('@opentelemetry/resources');\nconst { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');\nconst { registerInstrumentations } = require('@opentelemetry/instrumentation');\nconst { HttpInstrumentation } = require('@opentelemetry/instrumentation-http');\nconst { ExpressInstrumentation } = require('@opentelemetry/instrumentation-express');\n\n// Initialize tracer\nconst provider = new NodeTracerProvider({\n  resource: new Resource({\n    [SemanticResourceAttributes.SERVICE_NAME]: 'api-service'\n  })\n});\n\n// Export to Jaeger\nprovider.addSpanProcessor(\n  new BatchSpanProcessor(\n    new JaegerExporter({\n      endpoint: 'http://localhost:14268/api/traces'\n    })\n  )\n);\n\nprovider.register();\n\n// Auto-instrument HTTP and Express\nregisterInstrumentations({\n  instrumentations: [\n    new HttpInstrumentation(),\n    new ExpressInstrumentation()\n  ]\n});\n\nconst tracer = provider.getTracer('api-service');\n\n// Manual span creation\nasync function processOrder(orderId) {\n  const span = tracer.startSpan('processOrder', {\n    attributes: { orderId }\n  });\n\n  try {\n    // Child span\n    const paymentSpan = tracer.startSpan('processPayment', {\n      parent: span\n    });\n    await processPayment(orderId);\n    paymentSpan.end();\n\n    // Another child span\n    const inventorySpan = tracer.startSpan('updateInventory', {\n      parent: span\n    });\n    await updateInventory(orderId);\n    inventorySpan.end();\n\n    span.setStatus({ code: SpanStatusCode.OK });\n  } catch (error) {\n    span.recordException(error);\n    span.setStatus({ \n      code: SpanStatusCode.ERROR, \n      message: error.message \n    });\n    throw error;\n  } finally {\n    span.end();\n  }\n}\n\n// Express middleware for trace context\nfunction traceMiddleware(req, res, next) {\n  const span = tracer.startSpan('http_request', {\n    attributes: {\n      'http.method': req.method,\n      'http.url': req.url,\n      'http.user_agent': req.headers['user-agent']\n    }\n  });\n\n  // Propagate trace context\n  const traceId = span.spanContext().traceId;\n  const spanId = span.spanContext().spanId;\n  \n  req.traceId = traceId;\n  req.spanId = spanId;\n  \n  res.setHeader('X-Trace-Id', traceId);\n\n  res.on('finish', () => {\n    span.setAttribute('http.status_code', res.statusCode);\n    span.setStatus({ \n      code: res.statusCode < 400 ? SpanStatusCode.OK : SpanStatusCode.ERROR \n    });\n    span.end();\n  });\n\n  next();\n}"
    },
    {
      "id": 72,
      "question": "How do you implement API Rate Limiting and Throttling?",
      "answer": "Rate limiting prevents abuse by restricting request frequency per user/IP.\n\nAlgorithms:\n• Token Bucket: Tokens refill at constant rate, consume on request\n• Leaky Bucket: Requests queued, processed at constant rate\n• Fixed Window: Count requests per time window (e.g., 100/hour)\n• Sliding Window: Rolling window, more accurate than fixed\n• Sliding Log: Store timestamp of each request\n\nRateLimiting Strategies:\n• Per user: Based on API key or userId\n• Per IP: Based on client IP address\n• Per endpoint: Different limits per route\n• Tier-based: Free (100/day), Pro (1000/day), Enterprise (unlimited)\n\nImplementation Approaches:\n• In-memory: Fast, but doesn't scale across servers\n• Redis: Distributed, shared state\n• API Gateway: Centralized (Kong, AWS API Gateway)\n• Middleware: Application-level (express-rate-limit)\n\nResponse Headers:\n• X-RateLimit-Limit: Max requests allowed\n• X-RateLimit-Remaining: Requests left\n• X-RateLimit-Reset: When limit resets (Unix timestamp)\n• Retry-After: Seconds to wait before retry\n\nThrottling vs Rate Limiting:\n• Rate Limiting: Hard limit, reject after threshold\n• Throttling: Slow down requests, don't reject",
      "explanation": "Rate limiting uses token bucket or sliding window algorithms, implemented with Redis for distributed enforcement, returns 429 status with Retry-After header, supports tiered limits per user/IP.",
      "difficulty": "Medium",
      "code": "// Token Bucket Rate Limiter with Redis\nconst Redis = require('ioredis');\nconst redis = new Redis();\n\nclass TokenBucketRateLimiter {\n  constructor(options) {\n    this.capacity = options.capacity;  // Max tokens\n    this.refillRate = options.refillRate;  // Tokens per second\n    this.ttl = Math.ceil(this.capacity / this.refillRate);  // Bucket TTL\n  }\n\n  async consume(key, tokens = 1) {\n    const now = Date.now() / 1000;\n    const bucketKey = `ratelimit:${key}`;\n\n    // Lua script for atomic operation\n    const script = `\n      local capacity = tonumber(ARGV[1])\n      local refillRate = tonumber(ARGV[2])\n      local tokens = tonumber(ARGV[3])\n      local now = tonumber(ARGV[4])\n      local ttl = tonumber(ARGV[5])\n      \n      local bucket = redis.call('HMGET', KEYS[1], 'tokens', 'lastRefill')\n      local currentTokens = tonumber(bucket[1]) or capacity\n      local lastRefill = tonumber(bucket[2]) or now\n      \n      -- Refill tokens based on time elapsed\n      local elapsedTime = now - lastRefill\n      local tokensToAdd = elapsedTime * refillRate\n      currentTokens = math.min(capacity, currentTokens + tokensToAdd)\n      \n      -- Try to consume\n      if currentTokens >= tokens then\n        currentTokens = currentTokens - tokens\n        redis.call('HMSET', KEYS[1], 'tokens', currentTokens, 'lastRefill', now)\n        redis.call('EXPIRE', KEYS[1], ttl)\n        return {1, currentTokens}  -- Success\n      else\n        return {0, currentTokens}  -- Rate limited\n      end\n    `;\n\n    const [allowed, remaining] = await redis.eval(\n      script,\n      1,\n      bucketKey,\n      this.capacity,\n      this.refillRate,\n      tokens,\n      now,\n      this.ttl\n    );\n\n    return {\n      allowed: allowed === 1,\n      remaining: Math.floor(remaining),\n      reset: now + (this.capacity - remaining) / this.refillRate\n    };\n  }\n}\n\n// Sliding Window Rate Limiter\nclass SlidingWindowRateLimiter {\n  constructor(options) {\n    this.limit = options.limit;  // Max requests\n    this.window = options.window;  // Window size in seconds\n  }\n\n  async consume(key) {\n    const now = Date.now();\n    const windowStart = now - this.window * 1000;\n    const rateLimitKey = `ratelimit:${key}`;\n\n    // Remove old entries\n    await redis.zremrangebyscore(rateLimitKey, 0, windowStart);\n\n    // Count requests in window\n    const count = await redis.zcard(rateLimitKey);\n\n    if (count < this.limit) {\n      // Add current request\n      await redis.zadd(rateLimitKey, now, `${now}-${Math.random()}`);\n      await redis.expire(rateLimitKey, this.window);\n\n      return {\n        allowed: true,\n        remaining: this.limit - count - 1,\n        reset: Math.ceil((now + this.window * 1000) / 1000)\n      };\n    } else {\n      // Get oldest request time\n      const oldest = await redis.zrange(rateLimitKey, 0, 0, 'WITHSCORES');\n      const resetTime = parseInt(oldest[1]) + this.window * 1000;\n\n      return {\n        allowed: false,\n        remaining: 0,\n        reset: Math.ceil(resetTime / 1000),\n        retryAfter: Math.ceil((resetTime - now) / 1000)\n      };\n    }\n  }\n}\n\n// Express Middleware\nfunction rateLimitMiddleware(limiter) {\n  return async (req, res, next) => {\n    // Identify user (API key, userId, or IP)\n    const key = req.user?.id || req.ip;\n\n    try {\n      const result = await limiter.consume(key);\n\n      // Set headers\n      res.setHeader('X-RateLimit-Limit', limiter.limit || limiter.capacity);\n      res.setHeader('X-RateLimit-Remaining', result.remaining);\n      res.setHeader('X-RateLimit-Reset', result.reset);\n\n      if (result.allowed) {\n        next();\n      } else {\n        res.setHeader('Retry-After', result.retryAfter || 60);\n        res.status(429).json({\n          error: 'Too Many Requests',\n          message: 'Rate limit exceeded',\n          retryAfter: result.retryAfter || 60\n        });\n      }\n    } catch (error) {\n      // Fail open: Allow request if rate limiter fails\n      console.error('Rate limiter error:', error);\n      next();\n    }\n  };\n}\n\n// Usage\nconst limiter = new TokenBucketRateLimiter({\n  capacity: 100,  // 100 tokens\n  refillRate: 10  // 10 tokens/sec = 36000/hour\n});\n\napp.use('/api', rateLimitMiddleware(limiter));\n\n// Tiered rate limiting\nconst tierLimits = {\n  free: new SlidingWindowRateLimiter({ limit: 100, window: 3600 }),\n  pro: new SlidingWindowRateLimiter({ limit: 1000, window: 3600 }),\n  enterprise: null  // No limit\n};\n\nfunction tieredRateLimitMiddleware(req, res, next) {\n  const tier = req.user?.tier || 'free';\n  const limiter = tierLimits[tier];\n\n  if (!limiter) {\n    return next();  // No limit for enterprise\n  }\n\n  return rateLimitMiddleware(limiter)(req, res, next);\n}"
    },
    {
      "id": 73,
      "question": "Design a Content Delivery Network (CDN) from scratch",
      "answer": "A CDN distributes content across edge servers globally for low latency.\n\nArchitecture:\n• Origin Server: Original content source\n• Edge Servers (PoPs): Distributed globally, cache content\n• DNS: Route users to nearest edge server\n• CDN Controller: Manages cache, routing, purging\n\nHow CDN Works:\n1. User requests content (e.g., image.jpg)\n2. DNS resolves to nearest edge server (geo-based)\n3. Edge checks cache:\n   - Hit: Return cached content\n   - Miss: Fetch from origin, cache, return\n4. Set Cache-Control headers (TTL)\n\nCaching Strategies:\n• Static content: Images, CSS, JS (long TTL: 1 year)\n• Dynamic content: API responses, HTML (short TTL: 5 min)\n• Personalized content: User-specific (no cache or private)\n• Cache key: URL + query params + headers (Vary)\n\nCache Invalidation:\n• TTL expiration: Auto-expire after time\n• Purge: Manual invalidation by URL/pattern\n• Versioning: Append version to URL (?v=123)\n• Surrogate keys: Tag-based invalidation\n\nOptimizations:\n• Compression: gzip, Brotli\n• Image optimization: WebP, auto-resize\n• HTTP/2: Multiplexing, server push\n• TLS termination: Decrypt at edge\n• DDoS protection: Rate limiting, WAF",
      "explanation": "CDN uses geo-DNS to route to nearest edge server, caches static content with long TTL, fetches from origin on cache miss, supports purge/versioning for invalidation, optimizes with compression/image processing.",
      "difficulty": "Hard",
      "code": "// CDN Edge Server Implementation\nconst express = require('express');\nconst axios = require('axios');\nconst sharp = require('sharp');\nconst crypto = require('crypto');\n\nclass CDNEdgeServer {\n  constructor(originUrl, region) {\n    this.originUrl = originUrl;\n    this.region = region;\n    this.cache = new Map();\n    this.stats = { hits: 0, misses: 0 };\n  }\n\n  // Generate cache key\n  getCacheKey(url, headers) {\n    const varyHeaders = ['accept', 'accept-encoding'];\n    const varyValues = varyHeaders\n      .map(h => `${h}:${headers[h] || ''}`)\n      .join('|');\n    \n    return crypto\n      .createHash('md5')\n      .update(`${url}|${varyValues}`)\n      .digest('hex');\n  }\n\n  // Fetch from origin\n  async fetchFromOrigin(url) {\n    console.log(`[${this.region}] Cache MISS: ${url}`);\n    this.stats.misses++;\n\n    const response = await axios.get(`${this.originUrl}${url}`, {\n      responseType: 'arraybuffer'\n    });\n\n    return {\n      data: response.data,\n      contentType: response.headers['content-type'],\n      cacheControl: response.headers['cache-control'],\n      etag: response.headers['etag']\n    };\n  }\n\n  // Get content (cache or origin)\n  async getContent(url, headers) {\n    const cacheKey = this.getCacheKey(url, headers);\n\n    // Check cache\n    const cached = this.cache.get(cacheKey);\n    if (cached && !this.isExpired(cached)) {\n      console.log(`[${this.region}] Cache HIT: ${url}`);\n      this.stats.hits++;\n      return cached.content;\n    }\n\n    // Fetch from origin\n    const content = await this.fetchFromOrigin(url);\n\n    // Parse Cache-Control\n    const maxAge = this.parseCacheControl(content.cacheControl);\n    \n    // Store in cache\n    if (maxAge > 0) {\n      this.cache.set(cacheKey, {\n        content,\n        expiresAt: Date.now() + maxAge * 1000\n      });\n    }\n\n    return content;\n  }\n\n  // Optimize image\n  async optimizeImage(buffer, query) {\n    const { width, height, format = 'webp', quality = 80 } = query;\n\n    let pipeline = sharp(buffer);\n\n    if (width || height) {\n      pipeline = pipeline.resize({\n        width: width ? parseInt(width) : undefined,\n        height: height ? parseInt(height) : undefined,\n        fit: 'inside'\n      });\n    }\n\n    if (format === 'webp') {\n      pipeline = pipeline.webp({ quality: parseInt(quality) });\n    } else if (format === 'jpeg') {\n      pipeline = pipeline.jpeg({ quality: parseInt(quality) });\n    }\n\n    return pipeline.toBuffer();\n  }\n\n  parseCacheControl(header) {\n    if (!header) return 0;\n    const match = header.match(/max-age=(\\d+)/);\n    return match ? parseInt(match[1]) : 0;\n  }\n\n  isExpired(cached) {\n    return Date.now() > cached.expiresAt;\n  }\n\n  // Purge cache by URL pattern\n  purge(pattern) {\n    let count = 0;\n    for (const [key, value] of this.cache.entries()) {\n      if (value.content.url?.includes(pattern)) {\n        this.cache.delete(key);\n        count++;\n      }\n    }\n    console.log(`Purged ${count} entries matching \"${pattern}\"`);\n  }\n\n  getStats() {\n    const total = this.stats.hits + this.stats.misses;\n    const hitRate = total > 0 ? (this.stats.hits / total * 100).toFixed(2) : 0;\n    \n    return {\n      hits: this.stats.hits,\n      misses: this.stats.misses,\n      hitRate: `${hitRate}%`,\n      cacheSize: this.cache.size\n    };\n  }\n}\n\n// Express app\nconst app = express();\nconst cdn = new CDNEdgeServer('https://origin.example.com', 'us-east');\n\n// CDN endpoint\napp.get('*', async (req, res) => {\n  try {\n    const content = await cdn.getContent(req.path, req.headers);\n\n    // Image optimization\n    let responseData = content.data;\n    if (content.contentType?.startsWith('image/') && Object.keys(req.query).length > 0) {\n      responseData = await cdn.optimizeImage(content.data, req.query);\n    }\n\n    // Set headers\n    res.set('Content-Type', content.contentType);\n    res.set('Cache-Control', content.cacheControl || 'public, max-age=3600');\n    res.set('X-Cache', cdn.cache.has(cdn.getCacheKey(req.path, req.headers)) ? 'HIT' : 'MISS');\n    res.set('X-CDN-Region', cdn.region);\n    \n    res.send(responseData);\n  } catch (error) {\n    console.error('CDN error:', error);\n    res.status(502).send('Bad Gateway');\n  }\n});\n\n// Purge endpoint\napp.post('/purge', express.json(), (req, res) => {\n  const { pattern } = req.body;\n  cdn.purge(pattern);\n  res.json({ message: 'Purged', pattern });\n});\n\n// Stats endpoint\napp.get('/stats', (req, res) => {\n  res.json(cdn.getStats());\n});\n\napp.listen(8080, () => {\n  console.log(`CDN edge server (${cdn.region}) listening on :8080`);\n});"
    },
    {
      "id": 74,
      "question": "How do you implement Blue-Green Deployment and Canary Releases?",
      "answer": "Blue-green deployment and canary releases are strategies for zero-downtime deployments.\n\nBlue-Green Deployment:\n• Two identical environments: Blue (current), Green (new)\n• Deploy new version to Green\n• Test Green thoroughly\n• Switch traffic from Blue to Green (instant)\n• Keep Blue as rollback option\n• Benefits: Instant rollback, full testing before switch\n• Drawbacks: Requires double infrastructure, database migrations tricky\n\nCanary Release:\n• Deploy new version to small subset of servers (e.g., 5%)\n• Monitor metrics: Error rate, latency, CPU\n• Gradually increase traffic: 5% → 25% → 50% → 100%\n• Auto-rollback if metrics degrade\n• Benefits: Risk mitigation, real user testing\n• Drawbacks: Complex routing, requires monitoring\n\nImplementation:\n• Load balancer: Route traffic based on weight\n• Feature flags: Enable for subset of users\n• Kubernetes: Rolling update with maxSurge/maxUnavailable\n• Service mesh: Istio, Linkerd for traffic splitting\n\nMonitoring During Deployment:\n• Error rate: Should not increase\n• Latency (p50, p95, p99): Should remain stable\n• Traffic distribution: Verify routing percentages\n• Business metrics: Orders, signups, etc.",
      "explanation": "Blue-green switches traffic instantly between two environments, canary gradually shifts traffic to new version while monitoring metrics, load balancers/service mesh handle routing, auto-rollback on degradation.",
      "difficulty": "Medium",
      "code": "// Canary Deployment with Traffic Shifting\n\n// Kubernetes Deployment YAML\nconst blueDeployment = `\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-blue\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: myapp\n      version: blue\n  template:\n    metadata:\n      labels:\n        app: myapp\n        version: blue\n    spec:\n      containers:\n      - name: app\n        image: myapp:v1.0\n        ports:\n        - containerPort: 8080\n`;\n\nconst greenDeployment = `\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-green\nspec:\n  replicas: 1  # Start with 10% capacity\n  selector:\n    matchLabels:\n      app: myapp\n      version: green\n  template:\n    metadata:\n      labels:\n        app: myapp\n        version: green\n    spec:\n      containers:\n      - name: app\n        image: myapp:v2.0\n        ports:\n        - containerPort: 8080\n`;\n\n// Istio VirtualService for traffic splitting\nconst istioVirtualService = `\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: myapp\nspec:\n  hosts:\n  - myapp.example.com\n  http:\n  - match:\n    - headers:\n        x-canary:\n          exact: \"true\"  # Force canary for testing\n    route:\n    - destination:\n        host: myapp\n        subset: green\n      weight: 100\n  - route:\n    - destination:\n        host: myapp\n        subset: blue\n      weight: 95  # 95% to stable\n    - destination:\n        host: myapp\n        subset: green\n      weight: 5   # 5% to canary\n`;\n\n// Canary Deployment Controller\nclass CanaryController {\n  constructor(metrics, kubectl) {\n    this.metrics = metrics;\n    this.kubectl = kubectl;\n    this.stages = [5, 25, 50, 100];  // Percentage stages\n    this.currentStage = 0;\n  }\n\n  async deploy() {\n    console.log('Starting canary deployment...');\n\n    // Deploy green (canary) version\n    await this.kubectl.apply('app-green.yaml');\n    await this.waitForReady('app-green');\n\n    // Gradually shift traffic\n    for (const percentage of this.stages) {\n      console.log(`Shifting ${percentage}% traffic to canary...`);\n      \n      await this.updateTrafficSplit(percentage);\n      \n      // Wait for metrics to stabilize\n      await this.sleep(300000);  // 5 minutes\n      \n      // Check metrics\n      const healthy = await this.checkMetrics();\n      \n      if (!healthy) {\n        console.error('Canary unhealthy, rolling back!');\n        await this.rollback();\n        return false;\n      }\n      \n      this.currentStage++;\n    }\n\n    // Canary successful, promote to blue\n    console.log('Canary successful, promoting...');\n    await this.promote();\n    return true;\n  }\n\n  async updateTrafficSplit(percentage) {\n    const blueWeight = 100 - percentage;\n    const greenWeight = percentage;\n\n    // Update Istio VirtualService or load balancer\n    await this.kubectl.patch('virtualservice', 'myapp', {\n      spec: {\n        http: [{\n          route: [\n            { destination: { subset: 'blue' }, weight: blueWeight },\n            { destination: { subset: 'green' }, weight: greenWeight }\n          ]\n        }]\n      }\n    });\n  }\n\n  async checkMetrics() {\n    // Get metrics for blue and green\n    const blueMetrics = await this.metrics.get('blue');\n    const greenMetrics = await this.metrics.get('green');\n\n    // Compare error rates\n    if (greenMetrics.errorRate > blueMetrics.errorRate * 1.5) {\n      console.error(`Error rate increased: ${greenMetrics.errorRate}`);\n      return false;\n    }\n\n    // Compare latency (p95)\n    if (greenMetrics.latencyP95 > blueMetrics.latencyP95 * 1.2) {\n      console.error(`Latency increased: ${greenMetrics.latencyP95}ms`);\n      return false;\n    }\n\n    // Check for anomalies\n    if (greenMetrics.requestCount < 10) {\n      console.warn('Not enough traffic to canary, skipping checks');\n      return true;\n    }\n\n    console.log('Metrics healthy:', greenMetrics);\n    return true;\n  }\n\n  async rollback() {\n    // Route all traffic to blue\n    await this.updateTrafficSplit(0);\n    \n    // Delete green deployment\n    await this.kubectl.delete('deployment', 'app-green');\n    \n    console.log('Rollback complete');\n  }\n\n  async promote() {\n    // Scale down blue\n    await this.kubectl.scale('deployment', 'app-blue', 0);\n    \n    // Rename green to blue\n    await this.kubectl.label('deployment', 'app-green', 'version=blue');\n    \n    // Route all traffic to green (now blue)\n    await this.updateTrafficSplit(100);\n    \n    console.log('Promotion complete');\n  }\n\n  async waitForReady(deployment) {\n    // Wait for all pods to be ready\n    await this.kubectl.wait(\n      'deployment',\n      deployment,\n      '--for=condition=Available',\n      '--timeout=300s'\n    );\n  }\n\n  sleep(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n  }\n}\n\n// Usage\nconst controller = new CanaryController(metricsService, kubectlClient);\nawait controller.deploy();"
    },
    {
      "id": 75,
      "question": "Design a Distributed Task Scheduler with Priority Queues",
      "answer": "A distributed task scheduler executes tasks across multiple workers with priorities.\n\nComponents:\n• Task Queue: Stores pending tasks (Redis, RabbitMQ, Kafka)\n• Workers: Execute tasks (multiple processes/servers)\n• Scheduler: Assigns tasks to workers\n• Dead Letter Queue: Failed tasks after retries\n\nPriority Queues:\n• High priority: Execute first (critical tasks)\n• Normal priority: Default\n• Low priority: Execute when no high/normal tasks\n\nTask Properties:\n• id: Unique identifier\n• type: Task handler name\n• payload: Task data\n• priority: high/normal/low\n• maxRetries: Retry attempts (e.g., 3)\n• retryDelay: Backoff time (exponential)\n• scheduledAt: Execute at specific time\n• timeout: Max execution time\n\nExecution Flow:\n1. Producer: Add task to queue\n2. Worker: Poll queue (priority order)\n3. Worker: Execute task handler\n4. Success: Remove from queue\n5. Failure: Retry with backoff or move to DLQ\n\nOptimizations:\n• Concurrency: Multiple workers per process\n• Rate limiting: Throttle task execution\n• Job batching: Process multiple tasks together\n• Deduplication: Prevent duplicate tasks\n• Graceful shutdown: Finish tasks before exit",
      "explanation": "Distributed scheduler uses priority queues (high/normal/low), workers poll for tasks, implements retry with exponential backoff, supports scheduled tasks, moves failed tasks to dead letter queue.",
      "difficulty": "Hard",
      "code": "// Distributed Task Scheduler with BullMQ\nconst { Queue, Worker } = require('bullmq');\nconst Redis = require('ioredis');\n\nconst connection = new Redis();\n\n// Create queues for different priorities\nconst highPriorityQueue = new Queue('tasks-high', { connection });\nconst normalPriorityQueue = new Queue('tasks-normal', { connection });\nconst lowPriorityQueue = new Queue('tasks-low', { connection });\n\nclass TaskScheduler {\n  constructor() {\n    this.queues = {\n      high: highPriorityQueue,\n      normal: normalPriorityQueue,\n      low: lowPriorityQueue\n    };\n    this.handlers = new Map();\n  }\n\n  // Register task handler\n  registerHandler(taskType, handler) {\n    this.handlers.set(taskType, handler);\n  }\n\n  // Add task to queue\n  async addTask(task, options = {}) {\n    const {\n      priority = 'normal',\n      delay = 0,\n      maxRetries = 3,\n      scheduledAt = null\n    } = options;\n\n    const queue = this.queues[priority];\n    \n    const jobOptions = {\n      delay: scheduledAt ? scheduledAt - Date.now() : delay,\n      attempts: maxRetries + 1,\n      backoff: {\n        type: 'exponential',\n        delay: 5000  // Start with 5 seconds\n      },\n      removeOnComplete: true,\n      removeOnFail: false\n    };\n\n    const job = await queue.add(task.type, task.payload, jobOptions);\n    console.log(`Task ${task.type} added with priority ${priority}, job ID: ${job.id}`);\n    \n    return job.id;\n  }\n\n  // Start workers\n  startWorkers(concurrency = 5) {\n    for (const [priority, queue] of Object.entries(this.queues)) {\n      const worker = new Worker(\n        queue.name,\n        async (job) => {\n          console.log(`[${priority}] Processing job ${job.id}: ${job.name}`);\n          \n          const handler = this.handlers.get(job.name);\n          if (!handler) {\n            throw new Error(`No handler for task type: ${job.name}`);\n          }\n\n          // Execute task with timeout\n          const timeout = 60000;  // 60 seconds\n          const result = await Promise.race([\n            handler(job.data),\n            this.createTimeout(timeout)\n          ]);\n\n          console.log(`[${priority}] Job ${job.id} completed`);\n          return result;\n        },\n        {\n          connection,\n          concurrency,\n          limiter: {\n            max: 10,  // Max 10 jobs per second\n            duration: 1000\n          }\n        }\n      );\n\n      // Event handlers\n      worker.on('completed', (job) => {\n        console.log(`✓ Job ${job.id} completed`);\n      });\n\n      worker.on('failed', (job, err) => {\n        console.error(`✗ Job ${job.id} failed:`, err.message);\n        \n        if (job.attemptsMade >= job.opts.attempts) {\n          // Move to dead letter queue\n          this.moveToDLQ(job);\n        }\n      });\n\n      worker.on('stalled', (jobId) => {\n        console.warn(`Job ${jobId} stalled`);\n      });\n\n      console.log(`Worker started for ${priority} priority queue`);\n    }\n  }\n\n  createTimeout(ms) {\n    return new Promise((_, reject) => {\n      setTimeout(() => reject(new Error('Task timeout')), ms);\n    });\n  }\n\n  async moveToDLQ(job) {\n    const dlq = new Queue('dead-letter', { connection });\n    await dlq.add('failed-task', {\n      originalJob: job.toJSON(),\n      failedAt: Date.now(),\n      error: job.failedReason\n    });\n    console.log(`Job ${job.id} moved to dead letter queue`);\n  }\n\n  // Get queue stats\n  async getStats() {\n    const stats = {};\n    for (const [priority, queue] of Object.entries(this.queues)) {\n      const counts = await queue.getJobCounts();\n      stats[priority] = counts;\n    }\n    return stats;\n  }\n}\n\n// Usage\nconst scheduler = new TaskScheduler();\n\n// Register task handlers\nscheduler.registerHandler('sendEmail', async (data) => {\n  console.log(`Sending email to ${data.to}`);\n  // Email logic here\n  return { sent: true };\n});\n\nscheduler.registerHandler('processImage', async (data) => {\n  console.log(`Processing image ${data.imageUrl}`);\n  // Image processing logic\n  return { processed: true };\n});\n\nscheduler.registerHandler('generateReport', async (data) => {\n  console.log(`Generating report for user ${data.userId}`);\n  // Report generation logic\n  return { reportId: 'abc123' };\n});\n\n// Start workers\nscheduler.startWorkers(5);\n\n// Add tasks\nawait scheduler.addTask(\n  { type: 'sendEmail', payload: { to: 'user@example.com', subject: 'Welcome' } },\n  { priority: 'high' }\n);\n\nawait scheduler.addTask(\n  { type: 'processImage', payload: { imageUrl: 'https://...' } },\n  { priority: 'normal' }\n);\n\nawait scheduler.addTask(\n  { type: 'generateReport', payload: { userId: 123 } },\n  { priority: 'low', scheduledAt: Date.now() + 3600000 }  // 1 hour later\n);\n\n// Get stats\nconst stats = await scheduler.getStats();\nconsole.log('Queue stats:', stats);"
    },
    {
      "id": 76,
      "question": "Design a Ride Matching System like Uber/Lyft",
      "answer": "A ride matching system connects riders with nearby drivers in real-time.\n\nCore Components:\n• Location Service: Track driver/rider GPS coordinates\n• Matching Engine: Find optimal driver for ride request\n• Routing Service: Calculate routes and ETAs\n• Pricing Service: Calculate fare (surge pricing)\n• Trip Management: Track active trips\n\nMatching Algorithm:\n• Distance: Closest driver (Euclidean or Manhattan distance)\n• ETA: Fastest to reach (considering traffic)\n• Driver rating: Prefer higher-rated drivers\n• Driver acceptance rate: Prioritize reliable drivers\n• Vehicle type: Match SUV/sedan/pool requests\n• Driver direction: Heading towards pickup location\n\nReal-time Requirements:\n• Location updates: Every 5-10 seconds\n• Match response: < 2 seconds\n• ETA updates: Every 30 seconds\n• WebSocket: Bi-directional communication\n\nScalability:\n• Geohashing: Partition map into grids\n• Redis GEO: Store driver locations with GEORADIUS\n• Sharding: Partition by city/region\n• Cache: Frequently accessed data\n\nSurge Pricing:\n• Demand > Supply: Increase price multiplier\n• Grid-based: Different multipliers per area\n• Dynamic: Update every 1-5 minutes",
      "explanation": "Ride matching uses Redis GEORADIUS to find nearby drivers, scores by distance/ETA/rating, implements geohashing for partitioning, handles surge pricing based on supply-demand ratio, updates locations via WebSocket.",
      "difficulty": "Hard",
      "code": "// Ride Matching System\nconst Redis = require('ioredis');\nconst redis = new Redis();\n\nclass RideMatchingService {\n  constructor() {\n    this.SEARCH_RADIUS = 5;  // km\n    this.MAX_CANDIDATES = 10;\n  }\n\n  // Add driver location\n  async updateDriverLocation(driverId, lat, lng, metadata = {}) {\n    // Store in Redis GEO\n    await redis.geoadd(\n      'drivers:available',\n      lng, lat, driverId\n    );\n\n    // Store metadata\n    await redis.hset(`driver:${driverId}`, {\n      lat,\n      lng,\n      rating: metadata.rating || 5.0,\n      vehicleType: metadata.vehicleType || 'sedan',\n      acceptanceRate: metadata.acceptanceRate || 1.0,\n      heading: metadata.heading || 0,  // Direction in degrees\n      updatedAt: Date.now()\n    });\n\n    // Set expiry (remove if no update for 60 seconds)\n    await redis.expire(`driver:${driverId}`, 60);\n  }\n\n  // Find matching drivers\n  async findMatch(rideRequest) {\n    const { pickupLat, pickupLng, vehicleType, destination } = rideRequest;\n\n    // Get nearby drivers\n    const nearbyDrivers = await redis.georadius(\n      'drivers:available',\n      pickupLng,\n      pickupLat,\n      this.SEARCH_RADIUS,\n      'km',\n      'WITHDIST',\n      'ASC',\n      'COUNT', this.MAX_CANDIDATES\n    );\n\n    if (nearbyDrivers.length === 0) {\n      throw new Error('No drivers available');\n    }\n\n    // Get driver details and score\n    const candidates = [];\n    for (let i = 0; i < nearbyDrivers.length; i += 2) {\n      const driverId = nearbyDrivers[i];\n      const distance = parseFloat(nearbyDrivers[i + 1]);\n\n      const driver = await redis.hgetall(`driver:${driverId}`);\n      if (!driver.vehicleType) continue;  // Expired\n\n      // Filter by vehicle type\n      if (vehicleType && driver.vehicleType !== vehicleType) {\n        continue;\n      }\n\n      // Calculate score\n      const score = await this.calculateMatchScore(driver, {\n        distance,\n        pickupLat,\n        pickupLng,\n        destination\n      });\n\n      candidates.push({\n        driverId,\n        driver,\n        distance,\n        score\n      });\n    }\n\n    // Sort by score (highest first)\n    candidates.sort((a, b) => b.score - a.score);\n\n    // Return top candidate\n    return candidates[0];\n  }\n\n  // Score matching (higher is better)\n  async calculateMatchScore(driver, context) {\n    const { distance, pickupLat, pickupLng, destination } = context;\n\n    // Distance score (closer is better)\n    // Max score at 0km, 0 score at 5km\n    const distanceScore = Math.max(0, (1 - distance / 5)) * 40;\n\n    // Rating score (0-5 stars → 0-30 points)\n    const ratingScore = parseFloat(driver.rating) * 6;\n\n    // Acceptance rate score (0-1 → 0-20 points)\n    const acceptanceScore = parseFloat(driver.acceptanceRate) * 20;\n\n    // Direction score: Is driver heading towards pickup?\n    const directionScore = this.calculateDirectionScore(\n      driver,\n      pickupLat,\n      pickupLng\n    );\n\n    return distanceScore + ratingScore + acceptanceScore + directionScore;\n  }\n\n  calculateDirectionScore(driver, pickupLat, pickupLng) {\n    // Calculate bearing from driver to pickup\n    const bearing = this.calculateBearing(\n      parseFloat(driver.lat),\n      parseFloat(driver.lng),\n      pickupLat,\n      pickupLng\n    );\n\n    // Compare with driver's heading\n    const headingDiff = Math.abs(bearing - parseFloat(driver.heading));\n    const normalizedDiff = Math.min(headingDiff, 360 - headingDiff);\n\n    // Score: 10 points if heading towards (< 45°), 0 if opposite (> 135°)\n    if (normalizedDiff < 45) return 10;\n    if (normalizedDiff > 135) return 0;\n    return 10 * (1 - (normalizedDiff - 45) / 90);\n  }\n\n  calculateBearing(lat1, lng1, lat2, lng2) {\n    const dLng = (lng2 - lng1) * Math.PI / 180;\n    const lat1Rad = lat1 * Math.PI / 180;\n    const lat2Rad = lat2 * Math.PI / 180;\n\n    const y = Math.sin(dLng) * Math.cos(lat2Rad);\n    const x = Math.cos(lat1Rad) * Math.sin(lat2Rad) -\n              Math.sin(lat1Rad) * Math.cos(lat2Rad) * Math.cos(dLng);\n    \n    const bearing = Math.atan2(y, x) * 180 / Math.PI;\n    return (bearing + 360) % 360;\n  }\n\n  // Create ride\n  async createRide(riderId, driverId, details) {\n    const rideId = `ride:${Date.now()}:${riderId}`;\n\n    await redis.hset(rideId, {\n      riderId,\n      driverId,\n      status: 'matched',\n      pickupLat: details.pickupLat,\n      pickupLng: details.pickupLng,\n      destLat: details.destLat,\n      destLng: details.destLng,\n      fare: details.fare,\n      createdAt: Date.now()\n    });\n\n    // Remove driver from available pool\n    await redis.zrem('drivers:available', driverId);\n\n    return rideId;\n  }\n}\n\n// Surge Pricing Service\nclass SurgePricingService {\n  constructor() {\n    this.baseFare = 5.0;\n    this.perKm = 1.5;\n    this.perMinute = 0.3;\n  }\n\n  async calculateFare(pickup, destination, geohash) {\n    // Get supply and demand\n    const supply = await this.getSupply(geohash);\n    const demand = await this.getDemand(geohash);\n\n    // Calculate surge multiplier\n    const surgeMultiplier = this.calculateSurge(supply, demand);\n\n    // Calculate base fare\n    const distance = this.calculateDistance(pickup, destination);\n    const estimatedTime = distance / 40 * 60;  // Assuming 40 km/h\n\n    const baseFare = this.baseFare + \n                     (this.perKm * distance) + \n                     (this.perMinute * estimatedTime);\n\n    return {\n      fare: baseFare * surgeMultiplier,\n      surgeMultiplier,\n      distance,\n      estimatedTime\n    };\n  }\n\n  calculateSurge(supply, demand) {\n    if (supply === 0) return 3.0;  // Max surge\n\n    const ratio = demand / supply;\n    \n    if (ratio < 0.5) return 1.0;  // No surge\n    if (ratio < 1.0) return 1.2;\n    if (ratio < 1.5) return 1.5;\n    if (ratio < 2.0) return 2.0;\n    return 3.0;  // Max surge\n  }\n\n  async getSupply(geohash) {\n    return redis.zcard(`drivers:available:${geohash}`);\n  }\n\n  async getDemand(geohash) {\n    return redis.get(`demand:${geohash}`) || 0;\n  }\n\n  calculateDistance(p1, p2) {\n    // Haversine formula\n    const R = 6371;  // Earth radius in km\n    const dLat = (p2.lat - p1.lat) * Math.PI / 180;\n    const dLng = (p2.lng - p1.lng) * Math.PI / 180;\n    \n    const a = Math.sin(dLat / 2) * Math.sin(dLat / 2) +\n              Math.cos(p1.lat * Math.PI / 180) * Math.cos(p2.lat * Math.PI / 180) *\n              Math.sin(dLng / 2) * Math.sin(dLng / 2);\n    \n    const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a));\n    return R * c;\n  }\n}\n\n// Usage\nconst matchingService = new RideMatchingService();\nconst pricingService = new SurgePricingService();\n\n// Update driver location\nawait matchingService.updateDriverLocation('driver123', 37.7749, -122.4194, {\n  rating: 4.8,\n  vehicleType: 'sedan',\n  acceptanceRate: 0.95,\n  heading: 90  // Heading east\n});\n\n// Request ride\nconst match = await matchingService.findMatch({\n  pickupLat: 37.7750,\n  pickupLng: -122.4195,\n  vehicleType: 'sedan'\n});\n\nconsole.log('Matched driver:', match.driverId, 'Score:', match.score);\n\n// Calculate fare\nconst fareDetails = await pricingService.calculateFare(\n  { lat: 37.7750, lng: -122.4195 },\n  { lat: 37.8050, lng: -122.4100 },\n  '9q8yy'\n);\n\nconsole.log('Fare:', fareDetails);"
    },
    {
      "id": 77,
      "question": "How do you design a Video Streaming Platform like YouTube?",
      "answer": "A video streaming platform stores, transcodes, and delivers videos at scale.\n\nCore Components:\n• Upload Service: Receive video files\n• Transcoding Service: Convert to multiple formats/resolutions\n• Storage: Object storage (S3, GCS)\n• CDN: Deliver videos globally\n• Metadata DB: Video info, views, likes\n• Recommendation Engine: Suggest videos\n\nVideo Transcoding:\n• Input: Raw video (e.g., 1080p MP4)\n• Output: Multiple formats (HLS, DASH)\n• Resolutions: 240p, 360p, 480p, 720p, 1080p, 4K\n• Adaptive bitrate: Switch quality based on bandwidth\n• Tools: FFmpeg, AWS MediaConvert\n\nStreaming Protocols:\n• HLS (HTTP Live Streaming): Apple standard, m3u8 playlist\n• DASH (Dynamic Adaptive Streaming): MPEG standard\n• WebRTC: Real-time (live streaming)\n\nArchitecture:\n1. User uploads video → Upload Service\n2. Video stored in S3 → Transcoding queue\n3. Workers transcode to multiple formats\n4. Segments stored in S3\n5. CDN caches segments\n6. Player requests manifest → CDN serves segments\n\nOptimizations:\n• Chunking: Break video into 2-10 second segments\n• Parallel upload: Upload chunks concurrently\n• Thumbnail generation: Extract keyframes\n• Content ID: Detect copyrighted content\n• View count: Count unique views with deduplication",
      "explanation": "Video platform uploads to S3, transcodes with FFmpeg to multiple resolutions, uses HLS/DASH for adaptive streaming, delivers via CDN, generates thumbnails, tracks views with deduplication.",
      "difficulty": "Hard",
      "code": "// Video Streaming Platform\nconst AWS = require('aws-sdk');\nconst ffmpeg = require('fluent-ffmpeg');\nconst { Queue, Worker } = require('bullmq');\nconst Redis = require('ioredis');\n\nconst s3 = new AWS.S3();\nconst redis = new Redis();\n\n// Upload Service\nclass VideoUploadService {\n  async uploadVideo(file, metadata) {\n    const videoId = this.generateVideoId();\n    const uploadKey = `videos/raw/${videoId}/${file.originalname}`;\n\n    // Upload to S3\n    await s3.upload({\n      Bucket: process.env.S3_BUCKET,\n      Key: uploadKey,\n      Body: file.buffer,\n      ContentType: file.mimetype\n    }).promise();\n\n    // Save metadata\n    await this.saveMetadata(videoId, {\n      ...metadata,\n      uploadKey,\n      status: 'uploaded',\n      uploadedAt: new Date()\n    });\n\n    // Add to transcoding queue\n    await transcodingQueue.add('transcode', {\n      videoId,\n      uploadKey\n    });\n\n    return videoId;\n  }\n\n  generateVideoId() {\n    return `vid_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  async saveMetadata(videoId, metadata) {\n    await db.videos.create({\n      id: videoId,\n      ...metadata\n    });\n  }\n}\n\n// Transcoding Service\nclass VideoTranscodingService {\n  constructor() {\n    this.resolutions = [\n      { name: '240p', width: 426, height: 240, bitrate: '400k' },\n      { name: '360p', width: 640, height: 360, bitrate: '800k' },\n      { name: '480p', width: 854, height: 480, bitrate: '1400k' },\n      { name: '720p', width: 1280, height: 720, bitrate: '2800k' },\n      { name: '1080p', width: 1920, height: 1080, bitrate: '5000k' }\n    ];\n  }\n\n  async transcode(videoId, uploadKey) {\n    console.log(`Transcoding ${videoId}...`);\n\n    // Download from S3\n    const inputPath = `/tmp/${videoId}_input.mp4`;\n    await this.downloadFromS3(uploadKey, inputPath);\n\n    // Generate HLS playlist for each resolution\n    for (const resolution of this.resolutions) {\n      await this.transcodeResolution(videoId, inputPath, resolution);\n    }\n\n    // Generate master playlist\n    await this.generateMasterPlaylist(videoId);\n\n    // Generate thumbnail\n    await this.generateThumbnail(videoId, inputPath);\n\n    // Update status\n    await db.videos.update({ id: videoId }, {\n      status: 'ready',\n      transcodedAt: new Date()\n    });\n\n    console.log(`Transcoding complete: ${videoId}`);\n  }\n\n  async transcodeResolution(videoId, inputPath, resolution) {\n    const outputDir = `/tmp/${videoId}/${resolution.name}`;\n    const playlistName = `${resolution.name}.m3u8`;\n\n    return new Promise((resolve, reject) => {\n      ffmpeg(inputPath)\n        .outputOptions([\n          // HLS settings\n          '-hls_time 6',  // 6 second segments\n          '-hls_playlist_type vod',\n          '-hls_segment_filename', `${outputDir}/segment_%03d.ts`,\n          \n          // Video codec\n          '-c:v libx264',\n          '-preset medium',\n          '-crf 23',\n          `-b:v ${resolution.bitrate}`,\n          `-s ${resolution.width}x${resolution.height}`,\n          \n          // Audio codec\n          '-c:a aac',\n          '-b:a 128k',\n          '-ac 2'\n        ])\n        .output(`${outputDir}/${playlistName}`)\n        .on('end', async () => {\n          // Upload segments to S3\n          await this.uploadDirectoryToS3(\n            outputDir,\n            `videos/hls/${videoId}/${resolution.name}`\n          );\n          resolve();\n        })\n        .on('error', reject)\n        .run();\n    });\n  }\n\n  async generateMasterPlaylist(videoId) {\n    let playlist = '#EXTM3U\\n#EXT-X-VERSION:3\\n';\n\n    for (const resolution of this.resolutions) {\n      playlist += `#EXT-X-STREAM-INF:BANDWIDTH=${parseInt(resolution.bitrate) * 1000},RESOLUTION=${resolution.width}x${resolution.height}\\n`;\n      playlist += `${resolution.name}/${resolution.name}.m3u8\\n`;\n    }\n\n    // Upload to S3\n    await s3.upload({\n      Bucket: process.env.S3_BUCKET,\n      Key: `videos/hls/${videoId}/master.m3u8`,\n      Body: playlist,\n      ContentType: 'application/vnd.apple.mpegurl'\n    }).promise();\n  }\n\n  async generateThumbnail(videoId, inputPath) {\n    const thumbnailPath = `/tmp/${videoId}_thumb.jpg`;\n\n    return new Promise((resolve, reject) => {\n      ffmpeg(inputPath)\n        .screenshots({\n          timestamps: ['10%'],  // 10% into video\n          filename: thumbnailPath,\n          size: '1280x720'\n        })\n        .on('end', async () => {\n          await s3.upload({\n            Bucket: process.env.S3_BUCKET,\n            Key: `videos/thumbnails/${videoId}.jpg`,\n            Body: require('fs').readFileSync(thumbnailPath),\n            ContentType: 'image/jpeg'\n          }).promise();\n          resolve();\n        })\n        .on('error', reject);\n    });\n  }\n\n  async uploadDirectoryToS3(localDir, s3Prefix) {\n    const fs = require('fs');\n    const files = fs.readdirSync(localDir);\n\n    for (const file of files) {\n      const filePath = `${localDir}/${file}`;\n      const fileContent = fs.readFileSync(filePath);\n\n      await s3.upload({\n        Bucket: process.env.S3_BUCKET,\n        Key: `${s3Prefix}/${file}`,\n        Body: fileContent\n      }).promise();\n    }\n  }\n\n  async downloadFromS3(key, localPath) {\n    const file = await s3.getObject({\n      Bucket: process.env.S3_BUCKET,\n      Key: key\n    }).promise();\n\n    require('fs').writeFileSync(localPath, file.Body);\n  }\n}\n\n// View Tracking Service\nclass ViewTrackingService {\n  async recordView(videoId, userId, sessionId) {\n    // Deduplicate: Count as view only if > 30 seconds watched\n    const viewKey = `view:${videoId}:${userId || sessionId}`;\n    const alreadyCounted = await redis.get(viewKey);\n\n    if (alreadyCounted) {\n      return false;  // Already counted\n    }\n\n    // Increment view count\n    await redis.hincrby(`video:${videoId}:stats`, 'views', 1);\n\n    // Mark as counted (expire after 24 hours)\n    await redis.setex(viewKey, 86400, '1');\n\n    return true;\n  }\n\n  async getViewCount(videoId) {\n    const views = await redis.hget(`video:${videoId}:stats`, 'views');\n    return parseInt(views) || 0;\n  }\n}\n\n// Transcoding Worker\nconst transcodingQueue = new Queue('transcoding', { connection: redis });\nconst transcodingService = new VideoTranscodingService();\n\nconst worker = new Worker(\n  'transcoding',\n  async (job) => {\n    const { videoId, uploadKey } = job.data;\n    await transcodingService.transcode(videoId, uploadKey);\n  },\n  { connection: redis, concurrency: 2 }\n);\n\n// Express API\nconst express = require('express');\nconst multer = require('multer');\nconst upload = multer();\nconst app = express();\n\nconst uploadService = new VideoUploadService();\nconst viewService = new ViewTrackingService();\n\napp.post('/api/videos/upload', upload.single('video'), async (req, res) => {\n  const videoId = await uploadService.uploadVideo(req.file, {\n    title: req.body.title,\n    description: req.body.description,\n    userId: req.user.id\n  });\n\n  res.json({ videoId, status: 'processing' });\n});\n\napp.get('/api/videos/:videoId/manifest', async (req, res) => {\n  const manifestUrl = `https://cdn.example.com/videos/hls/${req.params.videoId}/master.m3u8`;\n  res.json({ manifestUrl });\n});\n\napp.post('/api/videos/:videoId/view', async (req, res) => {\n  const counted = await viewService.recordView(\n    req.params.videoId,\n    req.user?.id,\n    req.sessionId\n  );\n  res.json({ counted });\n});"
    },
    {
      "id": 78,
      "question": "Design a Real-Time Analytics Dashboard",
      "answer": "A real-time analytics dashboard processes and visualizes streaming data with low latency.\n\nComponents:\n• Data Ingestion: Collect events (Kafka, Kinesis)\n• Stream Processing: Aggregate in real-time (Flink, Spark Streaming)\n• Storage: Time-series DB (InfluxDB, TimescaleDB)\n• Query Engine: Fast queries (Druid, ClickHouse)\n• Visualization: Charts, graphs (Grafana, custom UI)\n• WebSocket: Push updates to clients\n\nData Pipeline:\n1. Events generated → Kafka topic\n2. Stream processor consumes → Aggregate (count, sum, avg)\n3. Write to time-series DB\n4. Query service reads → Returns metrics\n5. WebSocket pushes updates → Dashboard\n\nAggregations:\n• Time windows: Last 5 min, 1 hour, 1 day\n• Metrics: Count, sum, average, percentiles (p50, p95, p99)\n• Dimensions: Group by user, country, device\n\nOptimizations:\n• Pre-aggregation: Compute rollups (minute → hour → day)\n• Materialized views: Cache expensive queries\n• Downsampling: Reduce granularity for old data\n• Partitioning: Partition by time for fast queries\n• Incremental updates: Only send changed data\n\nChallenges:\n• Late-arriving data: Handle out-of-order events\n• Exactly-once processing: Avoid duplicate counts\n• Backfilling: Reprocess historical data",
      "explanation": "Real-time dashboard ingests events via Kafka, processes with Flink (windowed aggregations), stores in time-series DB, pushes updates via WebSocket, uses pre-aggregation and materialized views for performance.",
      "difficulty": "Hard",
      "code": "// Real-Time Analytics Dashboard\nconst { Kafka } = require('kafkajs');\nconst { InfluxDB, Point } = require('@influxdata/influxdb-client');\nconst WebSocket = require('ws');\nconst express = require('express');\n\n// Kafka Consumer\nconst kafka = new Kafka({ brokers: ['localhost:9092'] });\nconst consumer = kafka.consumer({ groupId: 'analytics-group' });\n\n// InfluxDB Client\nconst influx = new InfluxDB({ url: 'http://localhost:8086', token: process.env.INFLUX_TOKEN });\nconst writeApi = influx.getWriteApi('myorg', 'analytics');\nconst queryApi = influx.getQueryApi('myorg');\n\n// Stream Processor\nclass StreamProcessor {\n  constructor() {\n    this.windows = new Map();  // windowKey -> metrics\n    this.windowSize = 60000;  // 1 minute\n  }\n\n  async start() {\n    await consumer.connect();\n    await consumer.subscribe({ topic: 'events', fromBeginning: false });\n\n    await consumer.run({\n      eachMessage: async ({ message }) => {\n        const event = JSON.parse(message.value.toString());\n        await this.processEvent(event);\n      }\n    });\n  }\n\n  async processEvent(event) {\n    const { type, userId, value, timestamp = Date.now() } = event;\n\n    // Assign to time window\n    const windowStart = Math.floor(timestamp / this.windowSize) * this.windowSize;\n    const windowKey = `${type}:${windowStart}`;\n\n    // Update window metrics\n    if (!this.windows.has(windowKey)) {\n      this.windows.set(windowKey, {\n        type,\n        windowStart,\n        count: 0,\n        sum: 0,\n        values: [],\n        users: new Set()\n      });\n\n      // Flush window after delay\n      setTimeout(() => this.flushWindow(windowKey), this.windowSize + 5000);\n    }\n\n    const window = this.windows.get(windowKey);\n    window.count++;\n    window.sum += value || 0;\n    window.values.push(value || 0);\n    if (userId) window.users.add(userId);\n  }\n\n  async flushWindow(windowKey) {\n    const window = this.windows.get(windowKey);\n    if (!window) return;\n\n    // Calculate metrics\n    const avg = window.sum / window.count;\n    const sortedValues = window.values.sort((a, b) => a - b);\n    const p50 = this.percentile(sortedValues, 0.5);\n    const p95 = this.percentile(sortedValues, 0.95);\n    const p99 = this.percentile(sortedValues, 0.99);\n\n    // Write to InfluxDB\n    const point = new Point(window.type)\n      .timestamp(new Date(window.windowStart))\n      .intField('count', window.count)\n      .floatField('sum', window.sum)\n      .floatField('avg', avg)\n      .floatField('p50', p50)\n      .floatField('p95', p95)\n      .floatField('p99', p99)\n      .intField('unique_users', window.users.size);\n\n    writeApi.writePoint(point);\n    await writeApi.flush();\n\n    // Broadcast to WebSocket clients\n    broadcastMetrics({\n      type: window.type,\n      timestamp: window.windowStart,\n      metrics: { count: window.count, avg, p50, p95, p99 }\n    });\n\n    // Clean up\n    this.windows.delete(windowKey);\n  }\n\n  percentile(sortedArr, p) {\n    if (sortedArr.length === 0) return 0;\n    const index = Math.ceil(sortedArr.length * p) - 1;\n    return sortedArr[index];\n  }\n}\n\n// Query Service\nclass AnalyticsQueryService {\n  async getMetrics(eventType, timeRange) {\n    const { start, end } = timeRange;\n\n    const query = `\n      from(bucket: \"analytics\")\n        |> range(start: ${start}, stop: ${end})\n        |> filter(fn: (r) => r._measurement == \"${eventType}\")\n        |> aggregateWindow(every: 1m, fn: mean)\n    `;\n\n    const results = [];\n    return new Promise((resolve, reject) => {\n      queryApi.queryRows(query, {\n        next(row, tableMeta) {\n          const o = tableMeta.toObject(row);\n          results.push({\n            timestamp: o._time,\n            field: o._field,\n            value: o._value\n          });\n        },\n        error: reject,\n        complete() {\n          resolve(results);\n        }\n      });\n    });\n  }\n\n  async getRealtimeMetrics(eventType) {\n    const fiveMinAgo = new Date(Date.now() - 5 * 60 * 1000).toISOString();\n    \n    return this.getMetrics(eventType, {\n      start: fiveMinAgo,\n      end: new Date().toISOString()\n    });\n  }\n\n  async getHistoricalMetrics(eventType, days = 7) {\n    const start = new Date(Date.now() - days * 24 * 60 * 60 * 1000).toISOString();\n    \n    return this.getMetrics(eventType, {\n      start,\n      end: new Date().toISOString()\n    });\n  }\n}\n\n// WebSocket Server\nconst wss = new WebSocket.Server({ port: 8080 });\nconst clients = new Set();\n\nwss.on('connection', (ws) => {\n  clients.add(ws);\n  console.log('Client connected');\n\n  ws.on('close', () => {\n    clients.delete(ws);\n    console.log('Client disconnected');\n  });\n});\n\nfunction broadcastMetrics(data) {\n  const message = JSON.stringify(data);\n  for (const client of clients) {\n    if (client.readyState === WebSocket.OPEN) {\n      client.send(message);\n    }\n  }\n}\n\n// Express API\nconst app = express();\nconst queryService = new AnalyticsQueryService();\n\napp.get('/api/metrics/:eventType/realtime', async (req, res) => {\n  const metrics = await queryService.getRealtimeMetrics(req.params.eventType);\n  res.json(metrics);\n});\n\napp.get('/api/metrics/:eventType/historical', async (req, res) => {\n  const days = parseInt(req.query.days) || 7;\n  const metrics = await queryService.getHistoricalMetrics(req.params.eventType, days);\n  res.json(metrics);\n});\n\napp.listen(3000, () => {\n  console.log('API server listening on :3000');\n});\n\n// Start stream processor\nconst processor = new StreamProcessor();\nprocessor.start();"
    },
    {
      "id": 79,
      "question": "How do you implement Database Connection Pooling?",
      "answer": "Connection pooling reuses database connections to avoid overhead of creating new connections.\n\nWithout Pooling:\n• Each request creates new connection\n• Overhead: TCP handshake, authentication, resource allocation\n• Slow: 10-50ms to establish connection\n• Resource exhaustion: Too many open connections\n\nWith Pooling:\n• Pre-create pool of connections\n• Request borrows connection from pool\n• After use, return to pool (not close)\n• Reuse connections across requests\n\nPool Configuration:\n• minSize: Minimum connections (e.g., 5)\n• maxSize: Maximum connections (e.g., 20)\n• acquireTimeout: Max wait time for connection (e.g., 30s)\n• idleTimeout: Close idle connections after time\n• maxLifetime: Rotate connections periodically\n• connectionTimeout: Connect timeout for new connections\n\nBest Practices:\n• Size calculation: maxSize = (CPU cores × 2) + disk spindles\n• Monitor pool metrics: Active, idle, waiting\n• Graceful degradation: Handle pool exhaustion\n• Health checks: Test connections before use\n• Statement caching: Cache prepared statements\n\nCommon Libraries:\n• PostgreSQL: pg-pool, pg (built-in)\n• MySQL: mysql2 (built-in pool)\n• Generic: HikariCP (Java - gold standard)",
      "explanation": "Connection pooling reuses DB connections to avoid creation overhead, configures min/max pool size, implements acquire/release lifecycle, monitors active/idle connections, rotates connections periodically for health.",
      "difficulty": "Medium",
      "code": "// Database Connection Pool Implementation\nconst { Pool } = require('pg');\n\n// PostgreSQL with pg\nconst pool = new Pool({\n  host: 'localhost',\n  port: 5432,\n  database: 'mydb',\n  user: 'postgres',\n  password: 'password',\n  \n  // Pool configuration\n  min: 5,                    // Minimum connections\n  max: 20,                   // Maximum connections\n  idleTimeoutMillis: 30000,  // Close idle connections after 30s\n  connectionTimeoutMillis: 2000,  // Timeout to acquire connection\n  maxUses: 7500,             // Rotate connection after N uses\n  \n  // Advanced settings\n  allowExitOnIdle: false,\n  statement_timeout: 30000   // Query timeout\n});\n\n// Monitor pool\npool.on('connect', (client) => {\n  console.log('New client connected to pool');\n});\n\npool.on('acquire', (client) => {\n  console.log('Client acquired from pool');\n});\n\npool.on('remove', (client) => {\n  console.log('Client removed from pool');\n});\n\npool.on('error', (err, client) => {\n  console.error('Unexpected error on idle client', err);\n});\n\n// Usage: Query method (auto-acquire and release)\nasync function queryExample() {\n  try {\n    const result = await pool.query(\n      'SELECT * FROM users WHERE id = $1',\n      [123]\n    );\n    return result.rows;\n  } catch (error) {\n    console.error('Query error:', error);\n    throw error;\n  }\n}\n\n// Usage: Manual acquire (for transactions)\nasync function transactionExample() {\n  const client = await pool.connect();\n  \n  try {\n    await client.query('BEGIN');\n    \n    await client.query('INSERT INTO orders (user_id, amount) VALUES ($1, $2)', [123, 99.99]);\n    await client.query('UPDATE users SET balance = balance - $1 WHERE id = $2', [99.99, 123]);\n    \n    await client.query('COMMIT');\n  } catch (error) {\n    await client.query('ROLLBACK');\n    throw error;\n  } finally {\n    client.release();  // Return to pool\n  }\n}\n\n// Custom Connection Pool\nclass ConnectionPool {\n  constructor(config) {\n    this.config = config;\n    this.available = [];\n    this.inUse = new Set();\n    this.waiting = [];\n    this.creating = 0;\n    \n    // Initialize minimum connections\n    this.initialize();\n  }\n\n  async initialize() {\n    for (let i = 0; i < this.config.min; i++) {\n      const conn = await this.createConnection();\n      this.available.push(conn);\n    }\n    console.log(`Pool initialized with ${this.config.min} connections`);\n  }\n\n  async createConnection() {\n    // Create actual database connection\n    const conn = {\n      id: Math.random().toString(36).substr(2, 9),\n      connection: await this.config.factory(),\n      createdAt: Date.now(),\n      usageCount: 0\n    };\n    return conn;\n  }\n\n  async acquire() {\n    // Check available connections\n    if (this.available.length > 0) {\n      const conn = this.available.pop();\n      this.inUse.add(conn);\n      conn.lastAcquired = Date.now();\n      return conn;\n    }\n\n    // Create new connection if under max\n    const totalConnections = this.available.length + this.inUse.size + this.creating;\n    if (totalConnections < this.config.max) {\n      this.creating++;\n      try {\n        const conn = await this.createConnection();\n        this.inUse.add(conn);\n        return conn;\n      } finally {\n        this.creating--;\n      }\n    }\n\n    // Pool exhausted, wait\n    return new Promise((resolve, reject) => {\n      const timeout = setTimeout(() => {\n        const index = this.waiting.indexOf(waiter);\n        if (index > -1) this.waiting.splice(index, 1);\n        reject(new Error('Connection acquire timeout'));\n      }, this.config.acquireTimeout);\n\n      const waiter = { resolve, reject, timeout };\n      this.waiting.push(waiter);\n    });\n  }\n\n  release(conn) {\n    this.inUse.delete(conn);\n    conn.usageCount++;\n\n    // Check if connection should be rotated\n    if (conn.usageCount >= this.config.maxUses) {\n      this.destroyConnection(conn);\n      return;\n    }\n\n    // Give to waiting request\n    if (this.waiting.length > 0) {\n      const waiter = this.waiting.shift();\n      clearTimeout(waiter.timeout);\n      this.inUse.add(conn);\n      conn.lastAcquired = Date.now();\n      waiter.resolve(conn);\n      return;\n    }\n\n    // Return to available pool\n    this.available.push(conn);\n  }\n\n  async destroyConnection(conn) {\n    await conn.connection.end();\n    console.log(`Connection ${conn.id} destroyed`);\n  }\n\n  getStats() {\n    return {\n      available: this.available.length,\n      inUse: this.inUse.size,\n      waiting: this.waiting.length,\n      creating: this.creating,\n      total: this.available.length + this.inUse.size + this.creating\n    };\n  }\n\n  async shutdown() {\n    // Close all connections\n    for (const conn of this.available) {\n      await this.destroyConnection(conn);\n    }\n    for (const conn of this.inUse) {\n      await this.destroyConnection(conn);\n    }\n    console.log('Pool shutdown complete');\n  }\n}\n\n// Usage of custom pool\nconst customPool = new ConnectionPool({\n  min: 5,\n  max: 20,\n  acquireTimeout: 30000,\n  maxUses: 7500,\n  factory: async () => {\n    // Your connection creation logic\n    return require('pg').Client();\n  }\n});\n\nconst conn = await customPool.acquire();\ntry {\n  // Use connection\n  await conn.connection.query('SELECT 1');\n} finally {\n  customPool.release(conn);\n}\n\n// Monitor pool stats\nsetInterval(() => {\n  console.log('Pool stats:', customPool.getStats());\n}, 10000);"
    },
    {
      "id": 80,
      "question": "Design an E-commerce Inventory Management System with Reservations",
      "answer": "An inventory system tracks stock levels and prevents overselling through reservations.\n\nChallenges:\n• Race conditions: Concurrent purchases of same item\n• Overselling: Sell more than available stock\n• Cart abandonment: Reserved items not purchased\n• Distributed system: Multiple servers\n\nReservation Flow:\n1. Add to cart → Soft reservation (temporary)\n2. Start checkout → Hard reservation (committed)\n3. Payment success → Deduct from stock\n4. Payment fail → Release reservation\n5. Cart timeout (30 min) → Release reservation\n\nInventory Models:\n• Available: Total stock - reserved - sold\n• Reserved: Items in carts/checkout\n• Sold: Completed orders\n\nConcurrency Control:\n• Pessimistic locking: SELECT FOR UPDATE (locks row)\n• Optimistic locking: Version number, retry on conflict\n• Distributed locks: Redis lock for multi-server\n• Queue-based: Serialize requests through queue\n\nStock Replenishment:\n• Restock threshold: Alert when stock < threshold\n• Auto-reorder: Trigger purchase order\n• Backorders: Accept orders for out-of-stock items\n\nMulti-warehouse:\n• Allocation: Optimize by distance/cost/availability\n• Split orders: Ship from multiple warehouses",
      "explanation": "Inventory uses soft reservations for carts, hard reservations for checkout, pessimistic locking (SELECT FOR UPDATE) to prevent race conditions, timed expiration to release abandoned carts, tracks available/reserved/sold separately.",
      "difficulty": "Hard",
      "code": "// E-commerce Inventory Management\nconst { Pool } = require('pg');\nconst Redis = require('ioredis');\n\nconst pool = new Pool();\nconst redis = new Redis();\n\nclass InventoryService {\n  async checkAvailability(productId, quantity) {\n    const row = await pool.query(\n      'SELECT available FROM inventory WHERE product_id = $1',\n      [productId]\n    );\n    \n    return row.rows[0]?.available >= quantity;\n  }\n\n  // Soft reservation (add to cart)\n  async softReserve(cartId, productId, quantity) {\n    const client = await pool.connect();\n    \n    try {\n      await client.query('BEGIN');\n\n      // Lock row\n      const row = await client.query(\n        'SELECT available FROM inventory WHERE product_id = $1 FOR UPDATE',\n        [productId]\n      );\n\n      const available = row.rows[0].available;\n      if (available < quantity) {\n        throw new Error('Insufficient stock');\n      }\n\n      // Create soft reservation\n      await client.query(\n        `INSERT INTO reservations (cart_id, product_id, quantity, type, expires_at)\n         VALUES ($1, $2, $3, 'soft', NOW() + INTERVAL '30 minutes')\n         ON CONFLICT (cart_id, product_id) \n         DO UPDATE SET quantity = reservations.quantity + $3`,\n        [cartId, productId, quantity]\n      );\n\n      // Update available inventory\n      await client.query(\n        'UPDATE inventory SET available = available - $1 WHERE product_id = $2',\n        [quantity, productId]\n      );\n\n      await client.query('COMMIT');\n\n      // Set Redis expiration (auto-release after 30 min)\n      await redis.zadd('reservation_expiry', Date.now() + 30 * 60 * 1000, `${cartId}:${productId}`);\n\n      return { success: true };\n\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n\n  // Hard reservation (start checkout)\n  async hardReserve(orderId, cartId) {\n    const client = await pool.connect();\n\n    try {\n      await client.query('BEGIN');\n\n      // Convert soft to hard reservations\n      const result = await client.query(\n        `UPDATE reservations \n         SET type = 'hard', order_id = $1, expires_at = NOW() + INTERVAL '15 minutes'\n         WHERE cart_id = $2 AND type = 'soft'\n         RETURNING product_id, quantity`,\n        [orderId, cartId]\n      );\n\n      if (result.rows.length === 0) {\n        throw new Error('No soft reservations found');\n      }\n\n      await client.query('COMMIT');\n\n      // Update expiry in Redis (15 min for checkout)\n      for (const row of result.rows) {\n        await redis.zadd('reservation_expiry', Date.now() + 15 * 60 * 1000, `${orderId}:${row.product_id}`);\n      }\n\n      return result.rows;\n\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n\n  // Commit reservation (payment successful)\n  async commit(orderId) {\n    const client = await pool.connect();\n\n    try {\n      await client.query('BEGIN');\n\n      // Get reservations\n      const reservations = await client.query(\n        'SELECT product_id, quantity FROM reservations WHERE order_id = $1',\n        [orderId]\n      );\n\n      // Update inventory (deduct from total)\n      for (const res of reservations.rows) {\n        await client.query(\n          'UPDATE inventory SET total = total - $1 WHERE product_id = $2',\n          [res.quantity, res.product_id]\n        );\n      }\n\n      // Delete reservations\n      await client.query(\n        'DELETE FROM reservations WHERE order_id = $1',\n        [orderId]\n      );\n\n      await client.query('COMMIT');\n\n      return { success: true };\n\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n\n  // Release reservation\n  async release(cartIdOrOrderId, productId) {\n    const client = await pool.connect();\n\n    try {\n      await client.query('BEGIN');\n\n      // Get reservation\n      const res = await client.query(\n        `SELECT quantity FROM reservations \n         WHERE (cart_id = $1 OR order_id = $1) AND product_id = $2`,\n        [cartIdOrOrderId, productId]\n      );\n\n      if (res.rows.length === 0) {\n        return { success: false, message: 'Reservation not found' };\n      }\n\n      const quantity = res.rows[0].quantity;\n\n      // Return to available inventory\n      await client.query(\n        'UPDATE inventory SET available = available + $1 WHERE product_id = $2',\n        [quantity, productId]\n      );\n\n      // Delete reservation\n      await client.query(\n        `DELETE FROM reservations \n         WHERE (cart_id = $1 OR order_id = $1) AND product_id = $2`,\n        [cartIdOrOrderId, productId]\n      );\n\n      await client.query('COMMIT');\n\n      return { success: true, quantity };\n\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n\n  // Background job: Expire old reservations\n  async expireReservations() {\n    const now = Date.now();\n\n    // Get expired reservations from Redis\n    const expired = await redis.zrangebyscore('reservation_expiry', 0, now);\n\n    for (const key of expired) {\n      const [id, productId] = key.split(':');\n      await this.release(id, productId);\n      await redis.zrem('reservation_expiry', key);\n      console.log(`Expired reservation: ${key}`);\n    }\n  }\n\n  async getInventoryInfo(productId) {\n    const row = await pool.query(\n      `SELECT \n         i.total,\n         i.available,\n         COALESCE(SUM(r.quantity), 0) as reserved\n       FROM inventory i\n       LEFT JOIN reservations r ON r.product_id = i.product_id\n       WHERE i.product_id = $1\n       GROUP BY i.total, i.available`,\n      [productId]\n    );\n\n    return row.rows[0];\n  }\n}\n\n// Database Schema\nconst schema = `\nCREATE TABLE inventory (\n  product_id VARCHAR(50) PRIMARY KEY,\n  total INTEGER NOT NULL DEFAULT 0,\n  available INTEGER NOT NULL DEFAULT 0,\n  updated_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_inventory_available ON inventory(available);\n\nCREATE TABLE reservations (\n  id SERIAL PRIMARY KEY,\n  cart_id VARCHAR(50),\n  order_id VARCHAR(50),\n  product_id VARCHAR(50) NOT NULL,\n  quantity INTEGER NOT NULL,\n  type VARCHAR(10) NOT NULL CHECK (type IN ('soft', 'hard')),\n  expires_at TIMESTAMP NOT NULL,\n  created_at TIMESTAMP DEFAULT NOW(),\n  FOREIGN KEY (product_id) REFERENCES inventory(product_id),\n  UNIQUE(cart_id, product_id)\n);\n\nCREATE INDEX idx_reservations_expires ON reservations(expires_at);\n`;\n\n// Usage\nconst inventoryService = new InventoryService();\n\n// Add to cart\nawait inventoryService.softReserve('cart123', 'product456', 2);\n\n// Start checkout\nawait inventoryService.hardReserve('order789', 'cart123');\n\n// Payment successful\nawait inventoryService.commit('order789');\n\n// Or payment failed\nawait inventoryService.release('order789', 'product456');\n\n// Background job (run every minute)\nsetInterval(async () => {\n  await inventoryService.expireReservations();\n}, 60000);"
    },
    {
      "id": 81,
      "question": "How would you design a real-time collaboration system like Google Docs?",
      "answer": "A real-time collaboration system requires conflict resolution, operational transforms, and WebSocket connections for instant updates.\n\nCore Components:\n• WebSocket servers for persistent connections\n• Operational Transformation (OT) or CRDT for conflict resolution\n• Document versioning and snapshot management\n• Presence awareness (who's online, cursor positions)\n• Collaborative editing with character-level tracking\n• Real-time synchronization service\n\nArchitecture:\n• Client maintains local document state\n• Server maintains authoritative state\n• OT algorithm transforms concurrent operations\n• Periodic snapshots for fast recovery\n• Event sourcing for full history\n• Conflict-free merging of edits\n\nScalability:\n• Shard documents across servers\n• Use sticky sessions for active editors\n• Redis for presence and cursor tracking\n• Kafka for event streaming\n• CDN for static assets",
      "explanation": "Real-time collaboration requires Operational Transformation or CRDTs to merge concurrent edits without conflicts, WebSocket connections for instant updates, and careful state management between client and server.",
      "difficulty": "Hard",
      "code": "// Operational Transformation Example\nclass DocumentServer {\n  constructor() {\n    this.document = '';\n    this.version = 0;\n    this.clients = new Map();\n  }\n\n  // Apply operation and transform for concurrent edits\n  applyOperation(clientId, operation) {\n    const { type, position, content, baseVersion } = operation;\n    \n    // Transform operation if based on old version\n    let transformedOp = operation;\n    if (baseVersion < this.version) {\n      transformedOp = this.transformOperation(operation, baseVersion);\n    }\n    \n    // Apply to document\n    if (transformedOp.type === 'insert') {\n      this.document = this.document.slice(0, transformedOp.position) +\n                      transformedOp.content +\n                      this.document.slice(transformedOp.position);\n    } else if (transformedOp.type === 'delete') {\n      this.document = this.document.slice(0, transformedOp.position) +\n                      this.document.slice(transformedOp.position + transformedOp.length);\n    }\n    \n    this.version++;\n    \n    // Broadcast to other clients\n    this.broadcastToOthers(clientId, transformedOp);\n    \n    return { success: true, version: this.version };\n  }\n  \n  transformOperation(op, baseVersion) {\n    // Get operations since baseVersion\n    const concurrentOps = this.getOperationsSince(baseVersion);\n    \n    let transformedOp = { ...op };\n    \n    // Transform against each concurrent operation\n    for (const serverOp of concurrentOps) {\n      if (serverOp.type === 'insert' && transformedOp.type === 'insert') {\n        // Both inserts: adjust position if serverOp came before\n        if (serverOp.position <= transformedOp.position) {\n          transformedOp.position += serverOp.content.length;\n        }\n      } else if (serverOp.type === 'delete' && transformedOp.type === 'insert') {\n        // Server deleted, client inserted\n        if (serverOp.position < transformedOp.position) {\n          transformedOp.position -= serverOp.length;\n        }\n      }\n      // More transformation rules...\n    }\n    \n    return transformedOp;\n  }\n}\n\n// Client-side collaborative editor\nclass CollaborativeEditor {\n  constructor(docId, userId) {\n    this.docId = docId;\n    this.userId = userId;\n    this.localDocument = '';\n    this.version = 0;\n    this.pendingOps = [];\n    this.ws = new WebSocket(`wss://collab.example.com/doc/${docId}`);\n    \n    this.ws.onmessage = (event) => this.handleServerMessage(event);\n  }\n  \n  // User types character\n  onUserInsert(position, content) {\n    // Apply locally immediately\n    this.localDocument = this.localDocument.slice(0, position) +\n                        content +\n                        this.localDocument.slice(position);\n    \n    // Send to server\n    const operation = {\n      type: 'insert',\n      position,\n      content,\n      baseVersion: this.version,\n      userId: this.userId,\n      timestamp: Date.now()\n    };\n    \n    this.pendingOps.push(operation);\n    this.ws.send(JSON.stringify(operation));\n  }\n  \n  handleServerMessage(event) {\n    const message = JSON.parse(event.data);\n    \n    if (message.type === 'ack') {\n      // Server acknowledged our operation\n      this.version = message.version;\n      this.pendingOps.shift();\n    } else if (message.type === 'operation') {\n      // Another user's operation\n      this.applyRemoteOperation(message.operation);\n    } else if (message.type === 'presence') {\n      // Update cursor positions\n      this.updateCursors(message.users);\n    }\n  }\n  \n  applyRemoteOperation(op) {\n    // Transform against pending operations\n    let transformedOp = op;\n    for (const pendingOp of this.pendingOps) {\n      transformedOp = this.transform(transformedOp, pendingOp);\n    }\n    \n    // Apply to local document\n    if (transformedOp.type === 'insert') {\n      this.localDocument = this.localDocument.slice(0, transformedOp.position) +\n                          transformedOp.content +\n                          this.localDocument.slice(transformedOp.position);\n    }\n    \n    this.version++;\n    this.render();\n  }\n}\n\n// Redis for presence tracking\nconst Redis = require('ioredis');\nconst redis = new Redis();\n\nclass PresenceManager {\n  async updatePresence(docId, userId, cursor) {\n    const key = `presence:${docId}`;\n    await redis.hset(key, userId, JSON.stringify({\n      userId,\n      cursor,\n      timestamp: Date.now()\n    }));\n    await redis.expire(key, 300); // 5 minutes\n  }\n  \n  async getActiveUsers(docId) {\n    const key = `presence:${docId}`;\n    const users = await redis.hgetall(key);\n    \n    // Filter out stale users\n    const activeUsers = {};\n    const now = Date.now();\n    for (const [userId, data] of Object.entries(users)) {\n      const parsed = JSON.parse(data);\n      if (now - parsed.timestamp < 30000) { // 30 seconds\n        activeUsers[userId] = parsed;\n      }\n    }\n    \n    return activeUsers;\n  }\n}"
    },
    {
      "id": 82,
      "question": "Design a distributed job scheduler like Apache Airflow or AWS Step Functions",
      "answer": "A distributed job scheduler orchestrates workflows with dependencies, retry logic, and distributed execution.\n\nCore Features:\n• DAG (Directed Acyclic Graph) workflow definition\n• Task dependencies and execution order\n• Distributed task execution across workers\n• Retry logic with exponential backoff\n• Task state management (pending, running, success, failed)\n• Scheduling with cron expressions\n• Monitoring and alerting\n\nArchitecture:\n• Scheduler service for triggering workflows\n• Executor service for distributing tasks\n• Worker pool for task execution\n• Metadata database for state\n• Message queue for task distribution\n• Result backend for output storage\n\nScalability:\n• Multiple scheduler replicas with leader election\n• Horizontal scaling of workers\n• Task partitioning by priority/type\n• Database sharding for large workflows\n• Asynchronous execution with callbacks",
      "explanation": "A distributed job scheduler uses DAG workflows to define task dependencies, distributes work across worker nodes via message queues, maintains state in a database, and provides retry logic and monitoring for reliable workflow execution.",
      "difficulty": "Hard",
      "code": "// Workflow DAG Definition\nclass WorkflowDAG {\n  constructor(name) {\n    this.name = name;\n    this.tasks = new Map();\n    this.edges = new Map();\n  }\n  \n  addTask(task) {\n    this.tasks.set(task.id, task);\n    this.edges.set(task.id, []);\n  }\n  \n  addDependency(fromTaskId, toTaskId) {\n    if (!this.edges.has(fromTaskId)) {\n      this.edges.set(fromTaskId, []);\n    }\n    this.edges.get(fromTaskId).push(toTaskId);\n  }\n  \n  // Topological sort to get execution order\n  getExecutionOrder() {\n    const visited = new Set();\n    const result = [];\n    \n    const dfs = (taskId) => {\n      if (visited.has(taskId)) return;\n      visited.add(taskId);\n      \n      const dependencies = this.getDependencies(taskId);\n      for (const depId of dependencies) {\n        dfs(depId);\n      }\n      \n      result.push(taskId);\n    };\n    \n    for (const taskId of this.tasks.keys()) {\n      dfs(taskId);\n    }\n    \n    return result;\n  }\n  \n  getDependencies(taskId) {\n    const deps = [];\n    for (const [from, tos] of this.edges.entries()) {\n      if (tos.includes(taskId)) {\n        deps.push(from);\n      }\n    }\n    return deps;\n  }\n}\n\n// Scheduler Service\nclass JobScheduler {\n  constructor(db, queue) {\n    this.db = db;\n    this.queue = queue;\n    this.running = false;\n  }\n  \n  async start() {\n    this.running = true;\n    \n    while (this.running) {\n      // Check for scheduled workflows\n      const dueWorkflows = await this.db.query(\n        'SELECT * FROM workflows WHERE next_run <= NOW() AND status = \\'active\\''\n      );\n      \n      for (const workflow of dueWorkflows) {\n        await this.triggerWorkflow(workflow.id);\n      }\n      \n      // Check for ready tasks (dependencies met)\n      await this.scheduleReadyTasks();\n      \n      await this.sleep(1000); // Check every second\n    }\n  }\n  \n  async triggerWorkflow(workflowId) {\n    const workflow = await this.db.getWorkflow(workflowId);\n    const dag = this.parseDAG(workflow.definition);\n    \n    // Create workflow run\n    const runId = await this.db.createWorkflowRun({\n      workflowId,\n      status: 'running',\n      startTime: new Date()\n    });\n    \n    // Schedule root tasks (no dependencies)\n    const rootTasks = this.getRootTasks(dag);\n    for (const task of rootTasks) {\n      await this.scheduleTask(runId, task);\n    }\n    \n    // Update next run time\n    await this.updateNextRun(workflowId, workflow.schedule);\n  }\n  \n  async scheduleTask(runId, task) {\n    await this.db.createTaskRun({\n      runId,\n      taskId: task.id,\n      status: 'pending',\n      retries: 0,\n      maxRetries: task.maxRetries || 3\n    });\n    \n    // Send to queue for execution\n    await this.queue.publish('tasks', {\n      runId,\n      taskId: task.id,\n      taskConfig: task.config,\n      timestamp: Date.now()\n    });\n  }\n  \n  async scheduleReadyTasks() {\n    // Find tasks whose dependencies are all complete\n    const readyTasks = await this.db.query(`\n      SELECT tr.* FROM task_runs tr\n      WHERE tr.status = 'waiting'\n      AND NOT EXISTS (\n        SELECT 1 FROM task_dependencies td\n        JOIN task_runs tr2 ON td.dependency_task_id = tr2.task_id\n        WHERE td.task_id = tr.task_id\n        AND tr2.run_id = tr.run_id\n        AND tr2.status != 'success'\n      )\n    `);\n    \n    for (const task of readyTasks) {\n      await this.scheduleTask(task.run_id, task);\n    }\n  }\n}\n\n// Worker Service\nclass JobWorker {\n  constructor(queue, db) {\n    this.queue = queue;\n    this.db = db;\n    this.executors = new Map();\n  }\n  \n  registerExecutor(type, executor) {\n    this.executors.set(type, executor);\n  }\n  \n  async start() {\n    await this.queue.subscribe('tasks', async (message) => {\n      await this.executeTask(message);\n    });\n  }\n  \n  async executeTask(taskMessage) {\n    const { runId, taskId, taskConfig } = taskMessage;\n    \n    try {\n      // Update status to running\n      await this.db.updateTaskRun(runId, taskId, {\n        status: 'running',\n        startTime: new Date()\n      });\n      \n      // Get executor\n      const executor = this.executors.get(taskConfig.type);\n      if (!executor) {\n        throw new Error(`No executor for type: ${taskConfig.type}`);\n      }\n      \n      // Execute\n      const result = await executor.execute(taskConfig);\n      \n      // Mark success\n      await this.db.updateTaskRun(runId, taskId, {\n        status: 'success',\n        endTime: new Date(),\n        result: JSON.stringify(result)\n      });\n      \n      // Trigger dependent tasks\n      await this.triggerDependentTasks(runId, taskId);\n      \n    } catch (error) {\n      await this.handleTaskFailure(runId, taskId, error);\n    }\n  }\n  \n  async handleTaskFailure(runId, taskId, error) {\n    const taskRun = await this.db.getTaskRun(runId, taskId);\n    \n    if (taskRun.retries < taskRun.maxRetries) {\n      // Retry with exponential backoff\n      const delay = Math.pow(2, taskRun.retries) * 1000;\n      \n      await this.db.updateTaskRun(runId, taskId, {\n        retries: taskRun.retries + 1,\n        status: 'retrying'\n      });\n      \n      setTimeout(() => {\n        this.queue.publish('tasks', { runId, taskId, taskConfig: taskRun.config });\n      }, delay);\n      \n    } else {\n      // Mark as failed\n      await this.db.updateTaskRun(runId, taskId, {\n        status: 'failed',\n        endTime: new Date(),\n        error: error.message\n      });\n      \n      // Mark workflow as failed\n      await this.db.updateWorkflowRun(runId, { status: 'failed' });\n    }\n  }\n}\n\n// Example workflow definition\nconst etlWorkflow = new WorkflowDAG('etl-pipeline');\n\netlWorkflow.addTask({\n  id: 'extract',\n  type: 'sql',\n  config: { query: 'SELECT * FROM source_table' },\n  maxRetries: 3\n});\n\netlWorkflow.addTask({\n  id: 'transform',\n  type: 'python',\n  config: { script: 'transform.py' },\n  maxRetries: 2\n});\n\netlWorkflow.addTask({\n  id: 'load',\n  type: 'sql',\n  config: { query: 'INSERT INTO target_table' },\n  maxRetries: 5\n});\n\netlWorkflow.addDependency('extract', 'transform');\netlWorkflow.addDependency('transform', 'load');"
    },
    {
      "id": 83,
      "question": "How would you design a distributed tracing system like Jaeger or Zipkin?",
      "answer": "A distributed tracing system tracks requests across microservices to identify performance bottlenecks and debug issues.\n\nCore Concepts:\n• Trace: End-to-end request journey\n• Span: Individual operation within a trace\n• Trace ID: Unique identifier propagated across services\n• Span ID: Unique identifier for each span\n• Parent-child relationships between spans\n• Trace context propagation via headers\n\nComponents:\n• Instrumentation libraries for collecting spans\n• Collector service for ingesting trace data\n• Storage backend (Elasticsearch, Cassandra)\n• Query service for retrieving traces\n• UI for visualization and analysis\n\nData Collection:\n• Automatic instrumentation via agents/SDKs\n• Manual instrumentation for custom spans\n• Sampling to reduce overhead (head/tail sampling)\n• Asynchronous reporting to avoid latency\n• Baggage for cross-cutting concerns\n\nScalability:\n• Kafka for buffering high-volume traces\n• Time-series database for metrics\n• Retention policies for old traces\n• Compression and batch writes",
      "explanation": "Distributed tracing uses trace IDs propagated across services to link spans (operations) into complete request journeys, enabling performance analysis and debugging in microservice architectures through instrumentation, collection, storage, and visualization.",
      "difficulty": "Hard",
      "code": "// Trace Context Propagation\nclass TraceContext {\n  constructor(traceId, spanId, parentSpanId = null, sampled = true) {\n    this.traceId = traceId || this.generateId();\n    this.spanId = spanId || this.generateId();\n    this.parentSpanId = parentSpanId;\n    this.sampled = sampled;\n    this.baggage = {};\n  }\n  \n  generateId() {\n    return Math.random().toString(36).substring(2) + Date.now().toString(36);\n  }\n  \n  // Create child span\n  createChild() {\n    return new TraceContext(\n      this.traceId,\n      this.generateId(),\n      this.spanId,\n      this.sampled\n    );\n  }\n  \n  // Serialize for HTTP headers\n  toHeaders() {\n    return {\n      'X-Trace-Id': this.traceId,\n      'X-Span-Id': this.spanId,\n      'X-Parent-Span-Id': this.parentSpanId || '',\n      'X-Sampled': this.sampled ? '1' : '0'\n    };\n  }\n  \n  // Deserialize from HTTP headers\n  static fromHeaders(headers) {\n    return new TraceContext(\n      headers['x-trace-id'],\n      headers['x-span-id'],\n      headers['x-parent-span-id'],\n      headers['x-sampled'] === '1'\n    );\n  }\n}\n\n// Span representation\nclass Span {\n  constructor(context, operationName, serviceName) {\n    this.traceId = context.traceId;\n    this.spanId = context.spanId;\n    this.parentSpanId = context.parentSpanId;\n    this.operationName = operationName;\n    this.serviceName = serviceName;\n    this.startTime = Date.now();\n    this.endTime = null;\n    this.tags = {};\n    this.logs = [];\n    this.status = 'ok';\n  }\n  \n  setTag(key, value) {\n    this.tags[key] = value;\n  }\n  \n  log(message, fields = {}) {\n    this.logs.push({\n      timestamp: Date.now(),\n      message,\n      fields\n    });\n  }\n  \n  finish() {\n    this.endTime = Date.now();\n    return this.toJSON();\n  }\n  \n  toJSON() {\n    return {\n      traceId: this.traceId,\n      spanId: this.spanId,\n      parentSpanId: this.parentSpanId,\n      operationName: this.operationName,\n      serviceName: this.serviceName,\n      startTime: this.startTime,\n      endTime: this.endTime,\n      duration: this.endTime - this.startTime,\n      tags: this.tags,\n      logs: this.logs,\n      status: this.status\n    };\n  }\n}\n\n// Express middleware for automatic tracing\nfunction tracingMiddleware(serviceName) {\n  return (req, res, next) => {\n    // Extract or create trace context\n    let context;\n    if (req.headers['x-trace-id']) {\n      context = TraceContext.fromHeaders(req.headers);\n    } else {\n      // Start new trace with sampling decision\n      const sampled = Math.random() < 0.1; // 10% sampling\n      context = new TraceContext(null, null, null, sampled);\n    }\n    \n    // Skip if not sampled\n    if (!context.sampled) {\n      req.traceContext = context;\n      return next();\n    }\n    \n    // Create span for this request\n    const span = new Span(context, `${req.method} ${req.path}`, serviceName);\n    span.setTag('http.method', req.method);\n    span.setTag('http.url', req.url);\n    span.setTag('http.host', req.hostname);\n    \n    req.traceContext = context;\n    req.span = span;\n    \n    // Capture response\n    const originalEnd = res.end;\n    res.end = function(...args) {\n      span.setTag('http.status_code', res.statusCode);\n      if (res.statusCode >= 400) {\n        span.status = 'error';\n      }\n      \n      const spanData = span.finish();\n      \n      // Send to collector asynchronously\n      setImmediate(() => {\n        traceCollector.collect(spanData);\n      });\n      \n      originalEnd.apply(res, args);\n    };\n    \n    next();\n  };\n}\n\n// HTTP Client with tracing\nconst axios = require('axios');\n\nfunction tracedAxios(traceContext) {\n  const instance = axios.create();\n  \n  instance.interceptors.request.use((config) => {\n    if (traceContext && traceContext.sampled) {\n      // Propagate trace context\n      Object.assign(config.headers, traceContext.toHeaders());\n    }\n    return config;\n  });\n  \n  return instance;\n}\n\n// Trace Collector Service\nclass TraceCollector {\n  constructor(kafkaProducer, storageBackend) {\n    this.kafka = kafkaProducer;\n    this.storage = storageBackend;\n    this.buffer = [];\n    this.maxBufferSize = 1000;\n    \n    // Flush buffer periodically\n    setInterval(() => this.flush(), 5000);\n  }\n  \n  collect(span) {\n    this.buffer.push(span);\n    \n    if (this.buffer.length >= this.maxBufferSize) {\n      this.flush();\n    }\n  }\n  \n  async flush() {\n    if (this.buffer.length === 0) return;\n    \n    const spans = this.buffer.splice(0);\n    \n    try {\n      // Send to Kafka for processing\n      await this.kafka.send({\n        topic: 'traces',\n        messages: spans.map(span => ({\n          key: span.traceId,\n          value: JSON.stringify(span)\n        }))\n      });\n    } catch (error) {\n      console.error('Failed to send traces:', error);\n      // Fallback: write directly to storage\n      await this.storage.batchInsert(spans);\n    }\n  }\n}\n\n// Trace Query Service\nclass TraceQueryService {\n  constructor(elasticsearch) {\n    this.es = elasticsearch;\n  }\n  \n  async getTrace(traceId) {\n    const result = await this.es.search({\n      index: 'traces',\n      body: {\n        query: {\n          term: { traceId }\n        },\n        sort: [{ startTime: 'asc' }],\n        size: 1000\n      }\n    });\n    \n    return this.buildTraceTree(result.hits.hits.map(h => h._source));\n  }\n  \n  buildTraceTree(spans) {\n    const spanMap = new Map();\n    const rootSpans = [];\n    \n    // Index all spans\n    spans.forEach(span => spanMap.set(span.spanId, { ...span, children: [] }));\n    \n    // Build tree\n    spans.forEach(span => {\n      if (span.parentSpanId) {\n        const parent = spanMap.get(span.parentSpanId);\n        if (parent) {\n          parent.children.push(spanMap.get(span.spanId));\n        }\n      } else {\n        rootSpans.push(spanMap.get(span.spanId));\n      }\n    });\n    \n    return rootSpans;\n  }\n  \n  async searchTraces(query) {\n    const { service, operation, minDuration, maxDuration, tags, limit = 20 } = query;\n    \n    const mustClauses = [];\n    \n    if (service) mustClauses.push({ term: { serviceName: service } });\n    if (operation) mustClauses.push({ term: { operationName: operation } });\n    if (minDuration) mustClauses.push({ range: { duration: { gte: minDuration } } });\n    if (maxDuration) mustClauses.push({ range: { duration: { lte: maxDuration } } });\n    \n    if (tags) {\n      Object.entries(tags).forEach(([key, value]) => {\n        mustClauses.push({ term: { [`tags.${key}`]: value } });\n      });\n    }\n    \n    const result = await this.es.search({\n      index: 'traces',\n      body: {\n        query: { bool: { must: mustClauses } },\n        aggs: {\n          traces: {\n            terms: { field: 'traceId', size: limit }\n          }\n        }\n      }\n    });\n    \n    return result.aggregations.traces.buckets.map(b => b.key);\n  }\n}\n\n// Usage example\nconst express = require('express');\nconst app = express();\n\napp.use(tracingMiddleware('user-service'));\n\napp.get('/user/:id', async (req, res) => {\n  const { traceContext, span } = req;\n  \n  // Create child span for database query\n  const dbContext = traceContext.createChild();\n  const dbSpan = new Span(dbContext, 'db.query', 'user-service');\n  dbSpan.setTag('db.type', 'postgres');\n  dbSpan.setTag('db.statement', 'SELECT * FROM users WHERE id = $1');\n  \n  const user = await db.query('SELECT * FROM users WHERE id = $1', [req.params.id]);\n  dbSpan.finish();\n  traceCollector.collect(dbSpan.toJSON());\n  \n  // Call downstream service\n  const orderContext = traceContext.createChild();\n  const http = tracedAxios(orderContext);\n  const orders = await http.get(`http://order-service/orders?userId=${req.params.id}`);\n  \n  res.json({ user, orders: orders.data });\n});"
    },
    {
      "id": 84,
      "question": "Design a recommendation engine like Netflix or Amazon",
      "answer": "A recommendation engine suggests relevant items based on user behavior, preferences, and item characteristics using collaborative filtering, content-based filtering, or hybrid approaches.\n\nRecommendation Approaches:\n• Collaborative Filtering: User-user or item-item similarity\n• Content-Based: Match item features to user preferences\n• Hybrid: Combine multiple approaches\n• Matrix Factorization: SVD, ALS for implicit feedback\n• Deep Learning: Neural collaborative filtering, embeddings\n\nArchitecture:\n• Real-time recommendations via API\n• Batch processing for model training\n• Feature store for user/item attributes\n• Vector database for similarity search\n• A/B testing framework for experiments\n• Cold start handling for new users/items\n\nData Pipeline:\n• User interaction tracking (views, clicks, purchases)\n• Feature engineering (user demographics, item metadata)\n• Model training on Spark/Flink\n• Model serving with low latency\n• Online learning for real-time updates\n\nScalability:\n• Precompute recommendations for active users\n• Use approximate nearest neighbors for similarity\n• Cache popular recommendations\n• Shard by user ID\n• Stream processing for real-time signals",
      "explanation": "Recommendation engines use collaborative filtering (user/item similarity), content-based filtering (feature matching), or hybrid approaches to suggest relevant items, combining batch model training with real-time serving and continuous learning from user interactions.",
      "difficulty": "Hard",
      "code": "// Item-Item Collaborative Filtering\nclass CollaborativeFilter {\n  constructor() {\n    this.userItemMatrix = new Map(); // userId -> {itemId -> rating}\n    this.itemSimilarity = new Map(); // itemId -> {itemId -> similarity}\n  }\n  \n  // Add user interaction\n  addRating(userId, itemId, rating) {\n    if (!this.userItemMatrix.has(userId)) {\n      this.userItemMatrix.set(userId, new Map());\n    }\n    this.userItemMatrix.get(userId).set(itemId, rating);\n  }\n  \n  // Compute item-item similarity (cosine similarity)\n  computeItemSimilarity() {\n    const items = this.getAllItems();\n    \n    for (let i = 0; i < items.length; i++) {\n      const itemA = items[i];\n      \n      if (!this.itemSimilarity.has(itemA)) {\n        this.itemSimilarity.set(itemA, new Map());\n      }\n      \n      for (let j = i + 1; j < items.length; j++) {\n        const itemB = items[j];\n        \n        const similarity = this.cosineSimilarity(itemA, itemB);\n        \n        this.itemSimilarity.get(itemA).set(itemB, similarity);\n        \n        if (!this.itemSimilarity.has(itemB)) {\n          this.itemSimilarity.set(itemB, new Map());\n        }\n        this.itemSimilarity.get(itemB).set(itemA, similarity);\n      }\n    }\n  }\n  \n  cosineSimilarity(itemA, itemB) {\n    const usersA = this.getUsersForItem(itemA);\n    const usersB = this.getUsersForItem(itemB);\n    \n    // Find common users\n    const commonUsers = usersA.filter(u => usersB.includes(u));\n    if (commonUsers.length === 0) return 0;\n    \n    let dotProduct = 0;\n    let normA = 0;\n    let normB = 0;\n    \n    for (const user of commonUsers) {\n      const ratingA = this.userItemMatrix.get(user).get(itemA);\n      const ratingB = this.userItemMatrix.get(user).get(itemB);\n      \n      dotProduct += ratingA * ratingB;\n      normA += ratingA * ratingA;\n      normB += ratingB * ratingB;\n    }\n    \n    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n  }\n  \n  // Get recommendations for user\n  recommend(userId, topN = 10) {\n    const userRatings = this.userItemMatrix.get(userId);\n    if (!userRatings) return [];\n    \n    const scores = new Map();\n    \n    // For each item user has rated\n    for (const [itemId, rating] of userRatings.entries()) {\n      const similarItems = this.itemSimilarity.get(itemId);\n      if (!similarItems) continue;\n      \n      // Find similar items\n      for (const [similarItemId, similarity] of similarItems.entries()) {\n        // Skip items user has already rated\n        if (userRatings.has(similarItemId)) continue;\n        \n        // Weighted sum: similarity * rating\n        const currentScore = scores.get(similarItemId) || 0;\n        scores.set(similarItemId, currentScore + similarity * rating);\n      }\n    }\n    \n    // Sort by score and return top N\n    return Array.from(scores.entries())\n      .sort((a, b) => b[1] - a[1])\n      .slice(0, topN)\n      .map(([itemId, score]) => ({ itemId, score }));\n  }\n  \n  getAllItems() {\n    const items = new Set();\n    for (const ratings of this.userItemMatrix.values()) {\n      for (const itemId of ratings.keys()) {\n        items.add(itemId);\n      }\n    }\n    return Array.from(items);\n  }\n  \n  getUsersForItem(itemId) {\n    const users = [];\n    for (const [userId, ratings] of this.userItemMatrix.entries()) {\n      if (ratings.has(itemId)) {\n        users.push(userId);\n      }\n    }\n    return users;\n  }\n}\n\n// Matrix Factorization (Simplified ALS)\nclass MatrixFactorization {\n  constructor(numFactors = 10, learningRate = 0.01, regularization = 0.01) {\n    this.k = numFactors;\n    this.lr = learningRate;\n    this.reg = regularization;\n    this.userFactors = new Map();\n    this.itemFactors = new Map();\n  }\n  \n  // Initialize factors randomly\n  initialize(users, items) {\n    for (const user of users) {\n      this.userFactors.set(user, this.randomVector(this.k));\n    }\n    for (const item of items) {\n      this.itemFactors.set(item, this.randomVector(this.k));\n    }\n  }\n  \n  randomVector(size) {\n    return Array(size).fill(0).map(() => Math.random() * 0.1);\n  }\n  \n  // Train using SGD\n  train(ratings, epochs = 20) {\n    for (let epoch = 0; epoch < epochs; epoch++) {\n      let totalError = 0;\n      \n      for (const { userId, itemId, rating } of ratings) {\n        const userVec = this.userFactors.get(userId);\n        const itemVec = this.itemFactors.get(itemId);\n        \n        // Predict\n        const prediction = this.dotProduct(userVec, itemVec);\n        const error = rating - prediction;\n        \n        totalError += error * error;\n        \n        // Update factors\n        for (let i = 0; i < this.k; i++) {\n          const userGrad = error * itemVec[i] - this.reg * userVec[i];\n          const itemGrad = error * userVec[i] - this.reg * itemVec[i];\n          \n          userVec[i] += this.lr * userGrad;\n          itemVec[i] += this.lr * itemGrad;\n        }\n      }\n      \n      console.log(`Epoch ${epoch + 1}, RMSE: ${Math.sqrt(totalError / ratings.length)}`);\n    }\n  }\n  \n  predict(userId, itemId) {\n    const userVec = this.userFactors.get(userId);\n    const itemVec = this.itemFactors.get(itemId);\n    \n    if (!userVec || !itemVec) return 0;\n    \n    return this.dotProduct(userVec, itemVec);\n  }\n  \n  dotProduct(vec1, vec2) {\n    return vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);\n  }\n  \n  recommendForUser(userId, allItems, topN = 10) {\n    const scores = allItems.map(itemId => ({\n      itemId,\n      score: this.predict(userId, itemId)\n    }));\n    \n    return scores.sort((a, b) => b.score - a.score).slice(0, topN);\n  }\n}\n\n// Real-time Recommendation API\nconst express = require('express');\nconst Redis = require('ioredis');\n\nclass RecommendationService {\n  constructor(redis, modelStore) {\n    this.redis = redis;\n    this.modelStore = modelStore;\n    this.app = express();\n    \n    this.setupRoutes();\n  }\n  \n  setupRoutes() {\n    this.app.get('/recommendations/:userId', async (req, res) => {\n      const { userId } = req.params;\n      const { count = 10, context } = req.query;\n      \n      try {\n        // Check cache first\n        const cached = await this.redis.get(`recs:${userId}`);\n        if (cached) {\n          return res.json(JSON.parse(cached));\n        }\n        \n        // Generate recommendations\n        const recommendations = await this.generateRecommendations(\n          userId,\n          parseInt(count),\n          context\n        );\n        \n        // Cache for 1 hour\n        await this.redis.setex(\n          `recs:${userId}`,\n          3600,\n          JSON.stringify(recommendations)\n        );\n        \n        res.json(recommendations);\n      } catch (error) {\n        res.status(500).json({ error: error.message });\n      }\n    });\n    \n    // Track user interaction\n    this.app.post('/interactions', async (req, res) => {\n      const { userId, itemId, interactionType, timestamp } = req.body;\n      \n      // Store interaction\n      await this.redis.zadd(\n        `interactions:${userId}`,\n        timestamp || Date.now(),\n        JSON.stringify({ itemId, interactionType })\n      );\n      \n      // Invalidate cache\n      await this.redis.del(`recs:${userId}`);\n      \n      // Stream to Kafka for model retraining\n      await this.kafka.send({\n        topic: 'user-interactions',\n        messages: [{ key: userId, value: JSON.stringify(req.body) }]\n      });\n      \n      res.json({ success: true });\n    });\n  }\n  \n  async generateRecommendations(userId, count, context) {\n    // Get user history\n    const history = await this.getUserHistory(userId);\n    \n    // Get candidate items (not in history)\n    const candidates = await this.getCandidateItems(userId, history);\n    \n    // Score candidates\n    const scored = await Promise.all(\n      candidates.map(async (itemId) => {\n        const score = await this.scoreItem(userId, itemId, context);\n        return { itemId, score };\n      })\n    );\n    \n    // Sort and return top N\n    return scored\n      .sort((a, b) => b.score - a.score)\n      .slice(0, count)\n      .map(async ({ itemId, score }) => ({\n        itemId,\n        score,\n        metadata: await this.getItemMetadata(itemId)\n      }));\n  }\n  \n  async scoreItem(userId, itemId, context) {\n    // Combine multiple scoring strategies\n    const cfScore = await this.collaborativeFilteringScore(userId, itemId);\n    const contentScore = await this.contentBasedScore(userId, itemId);\n    const popularityScore = await this.popularityScore(itemId);\n    const contextScore = await this.contextualScore(userId, itemId, context);\n    \n    // Weighted combination\n    return 0.4 * cfScore + 0.3 * contentScore + 0.2 * popularityScore + 0.1 * contextScore;\n  }\n}\n\n// Usage\nconst cf = new CollaborativeFilter();\ncf.addRating('user1', 'item1', 5);\ncf.addRating('user1', 'item2', 3);\ncf.addRating('user2', 'item1', 4);\ncf.addRating('user2', 'item3', 5);\n\ncf.computeItemSimilarity();\nconst recommendations = cf.recommend('user1', 5);\nconsole.log('Recommendations for user1:', recommendations);"
    },
    {
      "id": 85,
      "question": "Design a video streaming platform like YouTube or Twitch",
      "answer": "A video streaming platform handles video upload, transcoding, storage, delivery, and playback with adaptive bitrate streaming for various devices and network conditions.\n\nCore Components:\n• Upload service for video ingestion\n• Transcoding pipeline for multiple formats/resolutions\n• Storage for raw and processed videos\n• CDN for global content delivery\n• Adaptive bitrate streaming (HLS, DASH)\n• Playback service with quality selection\n• Live streaming with low latency\n\nVideo Processing:\n• Chunk videos for parallel transcoding\n• Generate multiple bitrates (360p, 720p, 1080p, 4K)\n• Create thumbnails and preview clips\n• Extract metadata and captions\n• Content analysis for recommendations\n• DRM for protected content\n\nStreaming Protocol:\n• HLS (HTTP Live Streaming) for broad compatibility\n• Segment videos into small chunks (2-10 seconds)\n• Manifest file (.m3u8) with quality variants\n• Client-side ABR logic for quality switching\n\nScalability:\n• Distributed transcoding with worker pools\n• Object storage (S3) for videos\n• Multi-region CDN for low latency\n• Edge caching for popular content\n• Separate hot/cold storage tiers",
      "explanation": "Video streaming platforms use chunked adaptive bitrate streaming (HLS/DASH) to deliver video at optimal quality for each viewer's bandwidth, with distributed transcoding pipelines, CDN distribution, and tiered storage for scalability.",
      "difficulty": "Hard",
      "code": "// Video Upload Service\nconst express = require('express');\nconst multer = require('multer');\nconst AWS = require('aws-sdk');\nconst { v4: uuidv4 } = require('uuid');\n\nclass VideoUploadService {\n  constructor() {\n    this.s3 = new AWS.S3();\n    this.sqs = new AWS.SQS();\n    this.app = express();\n    \n    // Multipart upload configuration\n    this.upload = multer({\n      storage: multer.memoryStorage(),\n      limits: { fileSize: 5 * 1024 * 1024 * 1024 } // 5GB\n    });\n    \n    this.setupRoutes();\n  }\n  \n  setupRoutes() {\n    // Initiate upload\n    this.app.post('/upload/init', async (req, res) => {\n      const { filename, filesize, contentType } = req.body;\n      \n      const videoId = uuidv4();\n      const key = `raw-videos/${videoId}/${filename}`;\n      \n      // Create multipart upload\n      const multipart = await this.s3.createMultipartUpload({\n        Bucket: 'video-platform-raw',\n        Key: key,\n        ContentType: contentType\n      }).promise();\n      \n      // Store metadata\n      await this.storeVideoMetadata({\n        videoId,\n        uploadId: multipart.UploadId,\n        filename,\n        filesize,\n        status: 'uploading',\n        createdAt: new Date()\n      });\n      \n      res.json({\n        videoId,\n        uploadId: multipart.UploadId,\n        key\n      });\n    });\n    \n    // Upload chunk\n    this.app.post('/upload/chunk', this.upload.single('chunk'), async (req, res) => {\n      const { videoId, uploadId, partNumber } = req.body;\n      const key = `raw-videos/${videoId}/${req.body.filename}`;\n      \n      const part = await this.s3.uploadPart({\n        Bucket: 'video-platform-raw',\n        Key: key,\n        PartNumber: parseInt(partNumber),\n        UploadId: uploadId,\n        Body: req.file.buffer\n      }).promise();\n      \n      res.json({ ETag: part.ETag, PartNumber: partNumber });\n    });\n    \n    // Complete upload\n    this.app.post('/upload/complete', async (req, res) => {\n      const { videoId, uploadId, parts } = req.body;\n      const key = `raw-videos/${videoId}/${req.body.filename}`;\n      \n      await this.s3.completeMultipartUpload({\n        Bucket: 'video-platform-raw',\n        Key: key,\n        UploadId: uploadId,\n        MultipartUpload: { Parts: parts }\n      }).promise();\n      \n      // Trigger transcoding pipeline\n      await this.triggerTranscoding(videoId);\n      \n      res.json({ success: true, videoId });\n    });\n  }\n  \n  async triggerTranscoding(videoId) {\n    await this.sqs.sendMessage({\n      QueueUrl: process.env.TRANSCODING_QUEUE_URL,\n      MessageBody: JSON.stringify({\n        videoId,\n        timestamp: Date.now()\n      })\n    }).promise();\n  }\n}\n\n// Video Transcoding Service\nconst ffmpeg = require('fluent-ffmpeg');\nconst fs = require('fs');\nconst path = require('path');\n\nclass TranscodingService {\n  constructor() {\n    this.profiles = [\n      { name: '360p', width: 640, height: 360, bitrate: '800k', audioBitrate: '96k' },\n      { name: '720p', width: 1280, height: 720, bitrate: '2500k', audioBitrate: '128k' },\n      { name: '1080p', width: 1920, height: 1080, bitrate: '5000k', audioBitrate: '192k' }\n    ];\n  }\n  \n  async transcodeVideo(videoId, inputPath) {\n    const outputDir = `/tmp/transcoded/${videoId}`;\n    fs.mkdirSync(outputDir, { recursive: true });\n    \n    // Transcode each profile\n    const promises = this.profiles.map(profile =>\n      this.transcodeProfile(inputPath, outputDir, profile)\n    );\n    \n    await Promise.all(promises);\n    \n    // Generate HLS master playlist\n    await this.generateMasterPlaylist(videoId, outputDir);\n    \n    // Upload to S3\n    await this.uploadToS3(videoId, outputDir);\n    \n    // Update video status\n    await this.updateVideoStatus(videoId, 'ready');\n  }\n  \n  transcodeProfile(inputPath, outputDir, profile) {\n    return new Promise((resolve, reject) => {\n      const outputPath = path.join(outputDir, profile.name);\n      fs.mkdirSync(outputPath, { recursive: true });\n      \n      ffmpeg(inputPath)\n        .outputOptions([\n          `-vf scale=${profile.width}:${profile.height}`,\n          `-c:v libx264`,\n          `-b:v ${profile.bitrate}`,\n          `-c:a aac`,\n          `-b:a ${profile.audioBitrate}`,\n          `-hls_time 6`,\n          `-hls_playlist_type vod`,\n          `-hls_segment_filename ${outputPath}/segment_%03d.ts`\n        ])\n        .output(`${outputPath}/playlist.m3u8`)\n        .on('end', resolve)\n        .on('error', reject)\n        .run();\n    });\n  }\n  \n  async generateMasterPlaylist(videoId, outputDir) {\n    const masterPlaylist = [\n      '#EXTM3U',\n      '#EXT-X-VERSION:3',\n      ''\n    ];\n    \n    for (const profile of this.profiles) {\n      const bandwidth = parseInt(profile.bitrate) * 1000;\n      masterPlaylist.push(\n        `#EXT-X-STREAM-INF:BANDWIDTH=${bandwidth},RESOLUTION=${profile.width}x${profile.height}`,\n        `${profile.name}/playlist.m3u8`,\n        ''\n      );\n    }\n    \n    fs.writeFileSync(\n      path.join(outputDir, 'master.m3u8'),\n      masterPlaylist.join('\\n')\n    );\n  }\n  \n  async uploadToS3(videoId, outputDir) {\n    const s3 = new AWS.S3();\n    \n    // Upload all files recursively\n    const uploadDir = async (dir, prefix = '') => {\n      const files = fs.readdirSync(dir);\n      \n      for (const file of files) {\n        const filePath = path.join(dir, file);\n        const stat = fs.statSync(filePath);\n        \n        if (stat.isDirectory()) {\n          await uploadDir(filePath, `${prefix}${file}/`);\n        } else {\n          await s3.upload({\n            Bucket: 'video-platform-transcoded',\n            Key: `videos/${videoId}/${prefix}${file}`,\n            Body: fs.createReadStream(filePath),\n            ContentType: file.endsWith('.m3u8') ? 'application/vnd.apple.mpegurl' :\n                        file.endsWith('.ts') ? 'video/mp2t' : 'application/octet-stream'\n          }).promise();\n        }\n      }\n    };\n    \n    await uploadDir(outputDir);\n  }\n}\n\n// Video Player Client (React)\nconst VideoPlayer = ({ videoId }) => {\n  const videoRef = React.useRef(null);\n  const [currentQuality, setCurrentQuality] = React.useState('auto');\n  const [availableQualities, setAvailableQualities] = React.useState([]);\n  \n  React.useEffect(() => {\n    if (!videoRef.current) return;\n    \n    // Load HLS.js for browsers that don't support HLS natively\n    if (Hls.isSupported()) {\n      const hls = new Hls({\n        // Adaptive bitrate configuration\n        abrEwmaDefaultEstimate: 500000,\n        abrBandWidthFactor: 0.95,\n        abrBandWidthUpFactor: 0.7\n      });\n      \n      hls.loadSource(`https://cdn.example.com/videos/${videoId}/master.m3u8`);\n      hls.attachMedia(videoRef.current);\n      \n      // Listen for quality levels\n      hls.on(Hls.Events.MANIFEST_PARSED, () => {\n        const levels = hls.levels.map((level, index) => ({\n          index,\n          height: level.height,\n          bitrate: level.bitrate\n        }));\n        setAvailableQualities(levels);\n      });\n      \n      // Monitor quality changes\n      hls.on(Hls.Events.LEVEL_SWITCHED, (event, data) => {\n        console.log('Quality switched to:', data.level);\n      });\n      \n      return () => hls.destroy();\n    }\n  }, [videoId]);\n  \n  const handleQualityChange = (qualityIndex) => {\n    if (qualityIndex === 'auto') {\n      hls.currentLevel = -1; // Auto\n    } else {\n      hls.currentLevel = qualityIndex;\n    }\n    setCurrentQuality(qualityIndex);\n  };\n  \n  return (\n    <div>\n      <video\n        ref={videoRef}\n        controls\n        style={{ width: '100%', maxWidth: '1280px' }}\n      />\n      \n      <div className=\"quality-selector\">\n        <select value={currentQuality} onChange={(e) => handleQualityChange(e.target.value)}>\n          <option value=\"auto\">Auto</option>\n          {availableQualities.map(q => (\n            <option key={q.index} value={q.index}>\n              {q.height}p ({Math.round(q.bitrate / 1000)}kbps)\n            </option>\n          ))}\n        </select>\n      </div>\n    </div>\n  );\n};\n\n// Live Streaming with WebRTC\nclass LiveStreamService {\n  constructor() {\n    this.activeStreams = new Map();\n  }\n  \n  async startStream(streamerId, streamKey) {\n    // Validate stream key\n    const isValid = await this.validateStreamKey(streamerId, streamKey);\n    if (!isValid) throw new Error('Invalid stream key');\n    \n    const streamId = uuidv4();\n    \n    // Start WebRTC connection for RTMP ingest\n    const rtmpUrl = `rtmp://ingest.example.com/live/${streamId}`;\n    \n    // Start transcoding for live\n    await this.startLiveTranscoding(streamId);\n    \n    this.activeStreams.set(streamId, {\n      streamerId,\n      startTime: Date.now(),\n      viewers: new Set()\n    });\n    \n    return { streamId, rtmpUrl };\n  }\n  \n  async startLiveTranscoding(streamId) {\n    // FFmpeg for live transcoding\n    const transcoder = ffmpeg(`rtmp://ingest.example.com/live/${streamId}`)\n      .outputOptions([\n        '-c:v libx264',\n        '-preset veryfast',\n        '-tune zerolatency',\n        '-b:v 2500k',\n        '-maxrate 2500k',\n        '-bufsize 5000k',\n        '-g 60',\n        '-c:a aac',\n        '-b:a 128k',\n        '-f hls',\n        '-hls_time 2',\n        '-hls_list_size 5',\n        '-hls_flags delete_segments'\n      ])\n      .output(`/tmp/live/${streamId}/playlist.m3u8`);\n    \n    transcoder.run();\n  }\n}"
    },
    {
      "id": 86,
      "question": "Design a global CDN (Content Delivery Network) like Cloudflare or Akamai",
      "answer": "A CDN distributes content across geographically distributed edge servers to reduce latency and improve availability for users worldwide.\n\nCore Components:\n• Edge servers (PoPs) in multiple regions\n• Origin servers storing master content\n• DNS-based routing to nearest edge\n• Cache management and invalidation\n• Load balancing across edges\n• DDoS protection and WAF\n• SSL/TLS termination at edge\n\nCaching Strategy:\n• Cache-Control headers from origin\n• TTL-based expiration\n• Cache warming for popular content\n• Stale-while-revalidate patterns\n• Purge API for immediate invalidation\n• Tiered caching (edge -> regional -> origin)\n\nRouting:\n• Anycast IP for automatic routing\n• GeoDNS for location-based routing\n• Latency-based routing\n• Health checks and failover\n• Traffic steering for optimization\n\nScalability:\n• Millions of requests per second per PoP\n• Petabytes of cached content\n• Dynamic content acceleration\n• Origin shielding to reduce load\n• HTTP/2 and HTTP/3 support",
      "explanation": "A CDN uses geographically distributed edge servers with Anycast routing and intelligent caching to serve content from the nearest location to users, reducing latency and origin load while providing DDoS protection.",
      "difficulty": "Hard",
      "code": "// CDN Edge Server Implementation\nclass CDNEdgeServer {\n  constructor(region, originUrl) {\n    this.region = region;\n    this.originUrl = originUrl;\n    this.cache = new LRUCache({\n      max: 10000, // Max items\n      ttl: 1000 * 60 * 60, // 1 hour default\n      updateAgeOnGet: true\n    });\n    this.stats = {\n      hits: 0,\n      misses: 0,\n      bytes: 0\n    };\n  }\n  \n  async handleRequest(req, res) {\n    const cacheKey = this.generateCacheKey(req);\n    \n    // Check cache first\n    let cached = this.cache.get(cacheKey);\n    \n    if (cached) {\n      this.stats.hits++;\n      return this.serveCached(cached, req, res);\n    }\n    \n    this.stats.misses++;\n    \n    // Fetch from origin\n    const response = await this.fetchFromOrigin(req);\n    \n    // Cache if cacheable\n    if (this.isCacheable(response)) {\n      const ttl = this.getTTL(response.headers);\n      this.cache.set(cacheKey, {\n        status: response.status,\n        headers: response.headers,\n        body: response.body,\n        cachedAt: Date.now()\n      }, { ttl });\n    }\n    \n    return this.serveResponse(response, req, res);\n  }\n  \n  generateCacheKey(req) {\n    // Include URL, Accept-Encoding, and other vary headers\n    const vary = ['accept-encoding', 'accept', 'user-agent'];\n    const varyParts = vary.map(h => req.headers[h]).join('|');\n    return `${req.method}:${req.url}:${varyParts}`;\n  }\n  \n  isCacheable(response) {\n    // Check Cache-Control headers\n    const cacheControl = response.headers['cache-control'];\n    \n    if (!cacheControl) return false;\n    \n    if (cacheControl.includes('no-store') || cacheControl.includes('private')) {\n      return false;\n    }\n    \n    // Only cache successful responses\n    return response.status >= 200 && response.status < 400;\n  }\n  \n  getTTL(headers) {\n    const cacheControl = headers['cache-control'];\n    \n    if (cacheControl) {\n      const maxAge = cacheControl.match(/max-age=(\\d+)/);\n      if (maxAge) {\n        return parseInt(maxAge[1]) * 1000;\n      }\n    }\n    \n    // Fallback to Expires header\n    if (headers['expires']) {\n      const expires = new Date(headers['expires']);\n      return expires.getTime() - Date.now();\n    }\n    \n    // Default TTL\n    return 3600000; // 1 hour\n  }\n  \n  async fetchFromOrigin(req) {\n    const url = new URL(req.url, this.originUrl);\n    \n    const response = await fetch(url, {\n      method: req.method,\n      headers: this.sanitizeHeaders(req.headers),\n      body: req.method !== 'GET' && req.method !== 'HEAD' ? req.body : undefined\n    });\n    \n    return {\n      status: response.status,\n      headers: Object.fromEntries(response.headers.entries()),\n      body: await response.buffer()\n    };\n  }\n  \n  serveCached(cached, req, res) {\n    // Add age header\n    const age = Math.floor((Date.now() - cached.cachedAt) / 1000);\n    \n    res.writeHead(cached.status, {\n      ...cached.headers,\n      'Age': age.toString(),\n      'X-Cache': 'HIT',\n      'X-Cache-Region': this.region\n    });\n    \n    res.end(cached.body);\n  }\n}\n\n// Anycast DNS Routing\nclass AnycastDNS {\n  constructor() {\n    this.edgeServers = new Map();\n    this.healthChecks = new Map();\n  }\n  \n  registerEdgeServer(ip, region, healthCheckUrl) {\n    this.edgeServers.set(ip, { region, healthCheckUrl, healthy: true });\n    this.startHealthCheck(ip, healthCheckUrl);\n  }\n  \n  startHealthCheck(ip, url) {\n    const check = setInterval(async () => {\n      try {\n        const response = await fetch(url, { timeout: 5000 });\n        const healthy = response.status === 200;\n        \n        const server = this.edgeServers.get(ip);\n        if (server.healthy !== healthy) {\n          server.healthy = healthy;\n          console.log(`Server ${ip} health changed: ${healthy}`);\n          this.updateRouting();\n        }\n      } catch (error) {\n        const server = this.edgeServers.get(ip);\n        if (server.healthy) {\n          server.healthy = false;\n          console.log(`Server ${ip} failed health check`);\n          this.updateRouting();\n        }\n      }\n    }, 10000); // Check every 10 seconds\n    \n    this.healthChecks.set(ip, check);\n  }\n  \n  getHealthyServers() {\n    return Array.from(this.edgeServers.entries())\n      .filter(([ip, server]) => server.healthy)\n      .map(([ip, server]) => ({ ip, region: server.region }));\n  }\n  \n  // Respond to DNS query with nearest healthy server\n  resolveDNS(clientIP) {\n    const healthyServers = this.getHealthyServers();\n    \n    if (healthyServers.length === 0) {\n      throw new Error('No healthy edge servers available');\n    }\n    \n    // Find geographically closest server\n    const clientLocation = this.getLocationFromIP(clientIP);\n    \n    let nearest = healthyServers[0];\n    let minDistance = this.calculateDistance(\n      clientLocation,\n      this.getRegionLocation(nearest.region)\n    );\n    \n    for (const server of healthyServers.slice(1)) {\n      const distance = this.calculateDistance(\n        clientLocation,\n        this.getRegionLocation(server.region)\n      );\n      \n      if (distance < minDistance) {\n        minDistance = distance;\n        nearest = server;\n      }\n    }\n    \n    return nearest.ip;\n  }\n  \n  calculateDistance(loc1, loc2) {\n    // Haversine formula\n    const R = 6371; // Earth radius in km\n    const dLat = (loc2.lat - loc1.lat) * Math.PI / 180;\n    const dLon = (loc2.lon - loc1.lon) * Math.PI / 180;\n    \n    const a = Math.sin(dLat/2) * Math.sin(dLat/2) +\n              Math.cos(loc1.lat * Math.PI / 180) * Math.cos(loc2.lat * Math.PI / 180) *\n              Math.sin(dLon/2) * Math.sin(dLon/2);\n    \n    const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n    return R * c;\n  }\n}\n\n// Cache Invalidation/Purge API\nclass CDNPurgeService {\n  constructor(edgeServers) {\n    this.edgeServers = edgeServers;\n  }\n  \n  async purgeUrl(url) {\n    // Purge from all edge servers\n    const promises = this.edgeServers.map(server =>\n      this.purgeFromEdge(server, url)\n    );\n    \n    await Promise.all(promises);\n  }\n  \n  async purgePattern(pattern) {\n    // Purge URLs matching pattern (e.g., /api/users/*)\n    const regex = this.patternToRegex(pattern);\n    \n    const promises = this.edgeServers.map(server =>\n      this.purgePatternFromEdge(server, regex)\n    );\n    \n    await Promise.all(promises);\n  }\n  \n  async purgeTags(tags) {\n    // Purge all URLs with specific cache tags\n    const promises = this.edgeServers.map(server =>\n      this.purgeTagsFromEdge(server, tags)\n    );\n    \n    await Promise.all(promises);\n  }\n  \n  async purgeFromEdge(edgeServer, url) {\n    return fetch(`https://${edgeServer}/internal/purge`, {\n      method: 'POST',\n      headers: { 'X-Purge-Token': process.env.PURGE_TOKEN },\n      body: JSON.stringify({ url })\n    });\n  }\n}\n\n// Origin Shield (Regional Cache)\nclass OriginShield {\n  constructor(originUrl, maxSize = 1000000) {\n    this.originUrl = originUrl;\n    this.cache = new LRUCache({ max: maxSize });\n    this.collapsingRequests = new Map();\n  }\n  \n  async fetch(url) {\n    // Check cache\n    const cached = this.cache.get(url);\n    if (cached && !this.isStale(cached)) {\n      return cached.response;\n    }\n    \n    // Collapse concurrent requests for same URL\n    if (this.collapsingRequests.has(url)) {\n      return this.collapsingRequests.get(url);\n    }\n    \n    // Fetch from origin\n    const promise = this.fetchFromOrigin(url);\n    this.collapsingRequests.set(url, promise);\n    \n    try {\n      const response = await promise;\n      \n      // Cache the response\n      this.cache.set(url, {\n        response,\n        cachedAt: Date.now(),\n        ttl: this.getTTL(response.headers) || 3600000\n      });\n      \n      return response;\n    } finally {\n      this.collapsingRequests.delete(url);\n    }\n  }\n  \n  isStale(cached) {\n    return Date.now() - cached.cachedAt > cached.ttl;\n  }\n}\n\n// DDoS Protection\nclass DDoSProtection {\n  constructor() {\n    this.rateLimits = new Map();\n    this.blockedIPs = new Set();\n    this.suspiciousPatterns = [\n      /eval\\(/,\n      /<script>/i,\n      /\\.\\.\\//,\n      /union.*select/i\n    ];\n  }\n  \n  async checkRequest(req) {\n    const clientIP = this.getClientIP(req);\n    \n    // Check if IP is blocked\n    if (this.blockedIPs.has(clientIP)) {\n      return { allowed: false, reason: 'IP blocked' };\n    }\n    \n    // Rate limiting\n    const rateLimit = this.checkRateLimit(clientIP);\n    if (!rateLimit.allowed) {\n      return rateLimit;\n    }\n    \n    // Check for suspicious patterns\n    const suspicious = this.checkSuspiciousPatterns(req);\n    if (suspicious) {\n      this.blockIP(clientIP, 3600000); // Block for 1 hour\n      return { allowed: false, reason: 'Suspicious pattern detected' };\n    }\n    \n    return { allowed: true };\n  }\n  \n  checkRateLimit(clientIP) {\n    const now = Date.now();\n    const window = 60000; // 1 minute\n    const maxRequests = 100;\n    \n    if (!this.rateLimits.has(clientIP)) {\n      this.rateLimits.set(clientIP, []);\n    }\n    \n    const requests = this.rateLimits.get(clientIP);\n    \n    // Remove old requests\n    const recent = requests.filter(t => now - t < window);\n    \n    if (recent.length >= maxRequests) {\n      return { allowed: false, reason: 'Rate limit exceeded' };\n    }\n    \n    recent.push(now);\n    this.rateLimits.set(clientIP, recent);\n    \n    return { allowed: true };\n  }\n  \n  checkSuspiciousPatterns(req) {\n    const url = req.url;\n    const body = req.body ? req.body.toString() : '';\n    \n    for (const pattern of this.suspiciousPatterns) {\n      if (pattern.test(url) || pattern.test(body)) {\n        return true;\n      }\n    }\n    \n    return false;\n  }\n}\n\n// Usage Example\nconst cdn = new CDNEdgeServer('us-east-1', 'https://origin.example.com');\n\napp.use(async (req, res) => {\n  // DDoS check\n  const ddosCheck = await ddosProtection.checkRequest(req);\n  if (!ddosCheck.allowed) {\n    return res.status(429).send(ddosCheck.reason);\n  }\n  \n  // Serve from CDN\n  await cdn.handleRequest(req, res);\n});"
    },
    {
      "id": 87,
      "question": "Design an IoT platform for managing millions of connected devices",
      "answer": "An IoT platform handles device connectivity, data ingestion, command/control, and analytics for millions of devices sending telemetry data.\n\nCore Components:\n• Device registry and identity management\n• MQTT/CoAP message broker for pub/sub\n• Time-series database for telemetry\n• Device shadow/twin for state sync\n• Rules engine for real-time processing\n• Firmware OTA update management\n• Device management and monitoring\n\nProtocols:\n• MQTT for lightweight messaging\n• CoAP for constrained devices\n• HTTP/HTTPS for REST APIs\n• WebSocket for real-time updates\n• TLS for security\n\nData Flow:\n• Devices publish telemetry to topics\n• Message broker routes to subscribers\n• Stream processing for rules/alerts\n• Time-series storage for history\n• Analytics and ML on aggregated data\n\nScalability:\n• Horizontal scaling of MQTT brokers\n• Kafka for message buffering\n• Time-series partitioning by device/time\n• Batch processing for historical analysis\n• Edge computing for local processing",
      "explanation": "IoT platforms use lightweight protocols like MQTT for bidirectional communication with millions of devices, employing message brokers for pub/sub, time-series databases for telemetry, and device twins for state management at scale.",
      "difficulty": "Hard",
      "code": "// IoT Device Registry\nclass DeviceRegistry {\n  constructor(database) {\n    this.db = database;\n  }\n  \n  async registerDevice(device) {\n    const deviceId = this.generateDeviceId();\n    const credentials = this.generateCredentials();\n    \n    await this.db.insertDevice({\n      deviceId,\n      name: device.name,\n      type: device.type,\n      metadata: device.metadata,\n      credentials,\n      status: 'registered',\n      createdAt: new Date(),\n      lastSeen: null\n    });\n    \n    // Create device shadow\n    await this.createDeviceShadow(deviceId);\n    \n    return { deviceId, credentials };\n  }\n  \n  async createDeviceShadow(deviceId) {\n    await this.db.insertShadow({\n      deviceId,\n      desired: {},\n      reported: {},\n      metadata: {\n        desired: {},\n        reported: {}\n      },\n      version: 1,\n      timestamp: Date.now()\n    });\n  }\n  \n  async updateDeviceShadow(deviceId, state, type = 'reported') {\n    const shadow = await this.db.getShadow(deviceId);\n    \n    // Merge new state\n    Object.assign(shadow[type], state);\n    \n    // Update metadata timestamps\n    for (const key of Object.keys(state)) {\n      shadow.metadata[type][key] = {\n        timestamp: Date.now()\n      };\n    }\n    \n    shadow.version++;\n    shadow.timestamp = Date.now();\n    \n    await this.db.updateShadow(deviceId, shadow);\n    \n    // Notify subscribers of shadow update\n    await this.publishShadowUpdate(deviceId, shadow);\n    \n    return shadow;\n  }\n  \n  generateDeviceId() {\n    return `device-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n  \n  generateCredentials() {\n    return {\n      clientId: this.generateDeviceId(),\n      password: crypto.randomBytes(32).toString('hex'),\n      certificate: this.generateCertificate()\n    };\n  }\n}\n\n// MQTT Broker for Device Communication\nconst Aedes = require('aedes');\nconst net = require('net');\n\nclass IoTMQTTBroker {\n  constructor(port = 1883) {\n    this.aedes = Aedes({\n      authenticate: this.authenticate.bind(this),\n      authorizePublish: this.authorizePublish.bind(this),\n      authorizeSubscribe: this.authorizeSubscribe.bind(this)\n    });\n    \n    this.server = net.createServer(this.aedes.handle);\n    this.port = port;\n    \n    this.setupEventHandlers();\n  }\n  \n  start() {\n    this.server.listen(this.port, () => {\n      console.log(`MQTT broker listening on port ${this.port}`);\n    });\n  }\n  \n  authenticate(client, username, password, callback) {\n    // Verify device credentials\n    deviceRegistry.verifyCredentials(username, password.toString())\n      .then(valid => {\n        if (valid) {\n          client.deviceId = username;\n          callback(null, true);\n        } else {\n          callback(new Error('Authentication failed'), false);\n        }\n      })\n      .catch(err => callback(err, false));\n  }\n  \n  authorizePublish(client, packet, callback) {\n    // Check if device can publish to this topic\n    const deviceId = client.deviceId;\n    const topic = packet.topic;\n    \n    // Devices can only publish to their own topics\n    if (topic.startsWith(`devices/${deviceId}/`)) {\n      callback(null);\n    } else {\n      callback(new Error('Unauthorized topic'));\n    }\n  }\n  \n  authorizeSubscribe(client, subscription, callback) {\n    const deviceId = client.deviceId;\n    const topic = subscription.topic;\n    \n    // Devices can subscribe to their own topics and broadcast topics\n    if (topic.startsWith(`devices/${deviceId}/`) || topic.startsWith('broadcast/')) {\n      callback(null, subscription);\n    } else {\n      callback(new Error('Unauthorized subscription'));\n    }\n  }\n  \n  setupEventHandlers() {\n    this.aedes.on('client', (client) => {\n      console.log(`Device connected: ${client.id}`);\n      this.updateDeviceStatus(client.deviceId, 'online');\n    });\n    \n    this.aedes.on('clientDisconnect', (client) => {\n      console.log(`Device disconnected: ${client.id}`);\n      this.updateDeviceStatus(client.deviceId, 'offline');\n    });\n    \n    this.aedes.on('publish', async (packet, client) => {\n      if (!client) return; // Server publish\n      \n      // Process telemetry data\n      await this.processTelemetry(client.deviceId, packet.topic, packet.payload);\n    });\n  }\n  \n  async processTelemetry(deviceId, topic, payload) {\n    try {\n      const data = JSON.parse(payload.toString());\n      \n      // Store in time-series database\n      await telemetryStore.insert({\n        deviceId,\n        topic,\n        data,\n        timestamp: Date.now()\n      });\n      \n      // Check rules engine\n      await rulesEngine.evaluate(deviceId, data);\n      \n      // Update device shadow if state update\n      if (topic.endsWith('/state')) {\n        await deviceRegistry.updateDeviceShadow(deviceId, data, 'reported');\n      }\n    } catch (error) {\n      console.error('Error processing telemetry:', error);\n    }\n  }\n}\n\n// Time-Series Telemetry Storage\nclass TelemetryStore {\n  constructor(timescaleDB) {\n    this.db = timescaleDB;\n  }\n  \n  async insert(telemetry) {\n    await this.db.query(\n      `INSERT INTO telemetry (device_id, topic, data, timestamp)\n       VALUES ($1, $2, $3, to_timestamp($4 / 1000.0))`,\n      [telemetry.deviceId, telemetry.topic, telemetry.data, telemetry.timestamp]\n    );\n  }\n  \n  async queryRange(deviceId, startTime, endTime) {\n    const result = await this.db.query(\n      `SELECT * FROM telemetry\n       WHERE device_id = $1\n       AND timestamp BETWEEN to_timestamp($2 / 1000.0) AND to_timestamp($3 / 1000.0)\n       ORDER BY timestamp ASC`,\n      [deviceId, startTime, endTime]\n    );\n    \n    return result.rows;\n  }\n  \n  async getLatest(deviceId, limit = 100) {\n    const result = await this.db.query(\n      `SELECT * FROM telemetry\n       WHERE device_id = $1\n       ORDER BY timestamp DESC\n       LIMIT $2`,\n      [deviceId, limit]\n    );\n    \n    return result.rows;\n  }\n  \n  async aggregate(deviceId, metric, interval = '1hour', startTime, endTime) {\n    const result = await this.db.query(\n      `SELECT\n         time_bucket($1::interval, timestamp) AS bucket,\n         AVG((data->>$2)::float) AS avg_value,\n         MIN((data->>$2)::float) AS min_value,\n         MAX((data->>$2)::float) AS max_value\n       FROM telemetry\n       WHERE device_id = $3\n       AND timestamp BETWEEN to_timestamp($4 / 1000.0) AND to_timestamp($5 / 1000.0)\n       GROUP BY bucket\n       ORDER BY bucket`,\n      [interval, metric, deviceId, startTime, endTime]\n    );\n    \n    return result.rows;\n  }\n}\n\n// Rules Engine for Real-time Processing\nclass RulesEngine {\n  constructor() {\n    this.rules = new Map();\n  }\n  \n  addRule(rule) {\n    this.rules.set(rule.id, rule);\n  }\n  \n  async evaluate(deviceId, data) {\n    for (const [ruleId, rule] of this.rules.entries()) {\n      // Check if rule applies to this device\n      if (rule.deviceFilter && !this.matchesFilter(deviceId, rule.deviceFilter)) {\n        continue;\n      }\n      \n      // Evaluate condition\n      if (this.evaluateCondition(data, rule.condition)) {\n        await this.executeAction(deviceId, data, rule.action);\n      }\n    }\n  }\n  \n  evaluateCondition(data, condition) {\n    const { field, operator, value } = condition;\n    const fieldValue = this.getNestedValue(data, field);\n    \n    switch (operator) {\n      case 'gt': return fieldValue > value;\n      case 'lt': return fieldValue < value;\n      case 'eq': return fieldValue === value;\n      case 'gte': return fieldValue >= value;\n      case 'lte': return fieldValue <= value;\n      case 'in': return value.includes(fieldValue);\n      default: return false;\n    }\n  }\n  \n  async executeAction(deviceId, data, action) {\n    switch (action.type) {\n      case 'alert':\n        await this.sendAlert(deviceId, action.message, data);\n        break;\n      case 'command':\n        await this.sendCommand(deviceId, action.command);\n        break;\n      case 'webhook':\n        await this.callWebhook(action.url, { deviceId, data });\n        break;\n    }\n  }\n  \n  async sendCommand(deviceId, command) {\n    // Publish command to device topic\n    mqtt.publish(`devices/${deviceId}/commands`, JSON.stringify(command));\n  }\n}\n\n// OTA Firmware Update Manager\nclass FirmwareUpdateManager {\n  constructor() {\n    this.updates = new Map();\n  }\n  \n  async createUpdate(firmwareVersion, deviceType, fileUrl) {\n    const updateId = uuidv4();\n    \n    this.updates.set(updateId, {\n      firmwareVersion,\n      deviceType,\n      fileUrl,\n      status: 'pending',\n      createdAt: new Date(),\n      devices: new Map()\n    });\n    \n    return updateId;\n  }\n  \n  async deployToDevice(updateId, deviceId) {\n    const update = this.updates.get(updateId);\n    \n    // Send update notification to device\n    mqtt.publish(`devices/${deviceId}/ota`, JSON.stringify({\n      updateId,\n      firmwareVersion: update.firmwareVersion,\n      fileUrl: update.fileUrl,\n      checksum: update.checksum\n    }));\n    \n    update.devices.set(deviceId, {\n      status: 'notified',\n      notifiedAt: Date.now()\n    });\n  }\n  \n  async reportProgress(updateId, deviceId, progress) {\n    const update = this.updates.get(updateId);\n    const deviceUpdate = update.devices.get(deviceId);\n    \n    deviceUpdate.progress = progress;\n    deviceUpdate.lastUpdate = Date.now();\n    \n    if (progress >= 100) {\n      deviceUpdate.status = 'completed';\n      deviceUpdate.completedAt = Date.now();\n    }\n  }\n}\n\n// Device Client Example\nconst mqtt = require('mqtt');\n\nclass IoTDevice {\n  constructor(deviceId, credentials) {\n    this.deviceId = deviceId;\n    this.client = mqtt.connect('mqtt://broker.example.com', {\n      clientId: credentials.clientId,\n      username: deviceId,\n      password: credentials.password\n    });\n    \n    this.setupHandlers();\n  }\n  \n  setupHandlers() {\n    this.client.on('connect', () => {\n      console.log('Connected to IoT platform');\n      \n      // Subscribe to commands\n      this.client.subscribe(`devices/${this.deviceId}/commands`);\n      this.client.subscribe(`devices/${this.deviceId}/ota`);\n      this.client.subscribe(`devices/${this.deviceId}/shadow/desired`);\n    });\n    \n    this.client.on('message', (topic, message) => {\n      const data = JSON.parse(message.toString());\n      \n      if (topic.endsWith('/commands')) {\n        this.handleCommand(data);\n      } else if (topic.endsWith('/ota')) {\n        this.handleOTAUpdate(data);\n      } else if (topic.endsWith('/shadow/desired')) {\n        this.handleDesiredState(data);\n      }\n    });\n  }\n  \n  publishTelemetry(data) {\n    this.client.publish(\n      `devices/${this.deviceId}/telemetry`,\n      JSON.stringify({ ...data, timestamp: Date.now() })\n    );\n  }\n  \n  reportState(state) {\n    this.client.publish(\n      `devices/${this.deviceId}/state`,\n      JSON.stringify(state)\n    );\n  }\n  \n  handleCommand(command) {\n    console.log('Received command:', command);\n    // Execute command logic\n  }\n}\n\n// Usage\nconst device = new IoTDevice('device-123', credentials);\n\n// Send telemetry every 10 seconds\nsetInterval(() => {\n  device.publishTelemetry({\n    temperature: 25 + Math.random() * 5,\n    humidity: 60 + Math.random() * 10,\n    pressure: 1013 + Math.random() * 2\n  });\n}, 10000);"
    },
    {
      "id": 88,
      "question": "Design a distributed configuration management system like etcd or Consul",
      "answer": "A distributed configuration system provides strongly consistent key-value storage with watch capabilities for dynamic configuration across distributed services.\n\nCore Features:\n• Strongly consistent key-value store (Raft consensus)\n• Watch mechanism for configuration changes\n• Hierarchical key namespaces\n• TTL for automatic expiration\n• Transactions and compare-and-swap\n• Service discovery and health checks\n• Multi-datacenter replication\n\nConsensus:\n• Raft algorithm for leader election\n• Quorum-based writes for consistency\n• Log replication to followers\n• Leader handles all writes\n• Followers can serve reads\n• Automatic failover on leader failure\n\nWatch Mechanism:\n• Long-polling or streaming watches\n• Prefix-based watches for hierarchies\n• Notification on key changes\n• Historical watch from revision\n\nScalability:\n• Read replicas for read-heavy workloads\n• Lease-based sessions\n• Batch operations\n• Connection pooling\n• Caching with invalidation",
      "explanation": "Distributed configuration systems use Raft consensus for strong consistency, providing a reliable key-value store with watch capabilities that notify clients of changes, enabling dynamic configuration updates across distributed services.",
      "difficulty": "Hard",
      "code": "// Simplified Raft Implementation\nclass RaftNode {\n  constructor(nodeId, peers) {\n    this.nodeId = nodeId;\n    this.peers = peers;\n    this.state = 'follower'; // follower, candidate, leader\n    this.currentTerm = 0;\n    this.votedFor = null;\n    this.log = [];\n    this.commitIndex = 0;\n    this.lastApplied = 0;\n    \n    // Leader state\n    this.nextIndex = new Map();\n    this.matchIndex = new Map();\n    \n    this.resetElectionTimeout();\n  }\n  \n  resetElectionTimeout() {\n    clearTimeout(this.electionTimeout);\n    \n    // Random timeout between 150-300ms\n    const timeout = 150 + Math.random() * 150;\n    \n    this.electionTimeout = setTimeout(() => {\n      this.startElection();\n    }, timeout);\n  }\n  \n  startElection() {\n    this.state = 'candidate';\n    this.currentTerm++;\n    this.votedFor = this.nodeId;\n    \n    let votesReceived = 1; // Vote for self\n    const votesNeeded = Math.floor(this.peers.length / 2) + 1;\n    \n    // Request votes from peers\n    for (const peer of this.peers) {\n      this.requestVote(peer).then(granted => {\n        if (granted && this.state === 'candidate') {\n          votesReceived++;\n          \n          if (votesReceived >= votesNeeded) {\n            this.becomeLeader();\n          }\n        }\n      });\n    }\n    \n    this.resetElectionTimeout();\n  }\n  \n  async requestVote(peer) {\n    const request = {\n      term: this.currentTerm,\n      candidateId: this.nodeId,\n      lastLogIndex: this.log.length - 1,\n      lastLogTerm: this.log.length > 0 ? this.log[this.log.length - 1].term : 0\n    };\n    \n    const response = await peer.handleVoteRequest(request);\n    \n    if (response.term > this.currentTerm) {\n      this.currentTerm = response.term;\n      this.state = 'follower';\n      this.votedFor = null;\n    }\n    \n    return response.voteGranted;\n  }\n  \n  handleVoteRequest(request) {\n    // Update term if higher\n    if (request.term > this.currentTerm) {\n      this.currentTerm = request.term;\n      this.state = 'follower';\n      this.votedFor = null;\n    }\n    \n    // Grant vote if haven't voted and candidate's log is up-to-date\n    const voteGranted =\n      request.term === this.currentTerm &&\n      (this.votedFor === null || this.votedFor === request.candidateId) &&\n      this.isLogUpToDate(request.lastLogIndex, request.lastLogTerm);\n    \n    if (voteGranted) {\n      this.votedFor = request.candidateId;\n      this.resetElectionTimeout();\n    }\n    \n    return {\n      term: this.currentTerm,\n      voteGranted\n    };\n  }\n  \n  becomeLeader() {\n    this.state = 'leader';\n    \n    // Initialize leader state\n    for (const peer of this.peers) {\n      this.nextIndex.set(peer.nodeId, this.log.length);\n      this.matchIndex.set(peer.nodeId, -1);\n    }\n    \n    // Start sending heartbeats\n    this.sendHeartbeats();\n  }\n  \n  sendHeartbeats() {\n    if (this.state !== 'leader') return;\n    \n    for (const peer of this.peers) {\n      this.replicateLog(peer);\n    }\n    \n    // Send heartbeats every 50ms\n    setTimeout(() => this.sendHeartbeats(), 50);\n  }\n  \n  async replicateLog(peer) {\n    const nextIndex = this.nextIndex.get(peer.nodeId);\n    const prevLogIndex = nextIndex - 1;\n    const prevLogTerm = prevLogIndex >= 0 ? this.log[prevLogIndex].term : 0;\n    \n    const entries = this.log.slice(nextIndex);\n    \n    const request = {\n      term: this.currentTerm,\n      leaderId: this.nodeId,\n      prevLogIndex,\n      prevLogTerm,\n      entries,\n      leaderCommit: this.commitIndex\n    };\n    \n    const response = await peer.handleAppendEntries(request);\n    \n    if (response.success) {\n      this.nextIndex.set(peer.nodeId, nextIndex + entries.length);\n      this.matchIndex.set(peer.nodeId, nextIndex + entries.length - 1);\n      \n      // Update commit index\n      this.updateCommitIndex();\n    } else {\n      // Decrement nextIndex and retry\n      this.nextIndex.set(peer.nodeId, Math.max(0, nextIndex - 1));\n    }\n  }\n  \n  updateCommitIndex() {\n    // Find highest index replicated on majority\n    const indices = Array.from(this.matchIndex.values()).sort((a, b) => b - a);\n    const majorityIndex = indices[Math.floor(this.peers.length / 2)];\n    \n    if (majorityIndex > this.commitIndex && this.log[majorityIndex].term === this.currentTerm) {\n      this.commitIndex = majorityIndex;\n      this.applyCommitted();\n    }\n  }\n  \n  applyCommitted() {\n    while (this.lastApplied < this.commitIndex) {\n      this.lastApplied++;\n      const entry = this.log[this.lastApplied];\n      this.applyToStateMachine(entry.command);\n    }\n  }\n}\n\n// Configuration Store with Watch\nclass ConfigStore {\n  constructor(raftCluster) {\n    this.raft = raftCluster;\n    this.data = new Map();\n    this.watchers = new Map();\n    this.revision = 0;\n  }\n  \n  async put(key, value, options = {}) {\n    // Only leader can handle writes\n    if (this.raft.state !== 'leader') {\n      throw new Error('Not leader');\n    }\n    \n    const command = {\n      type: 'put',\n      key,\n      value,\n      ttl: options.ttl,\n      lease: options.lease,\n      prevKv: options.prevKv\n    };\n    \n    // Append to Raft log\n    const result = await this.raft.propose(command);\n    \n    return result;\n  }\n  \n  async get(key, options = {}) {\n    const value = this.data.get(key);\n    \n    if (!value) {\n      return null;\n    }\n    \n    // Check if expired\n    if (value.expireAt && value.expireAt < Date.now()) {\n      this.data.delete(key);\n      return null;\n    }\n    \n    return {\n      key,\n      value: value.value,\n      createRevision: value.createRevision,\n      modRevision: value.modRevision,\n      version: value.version\n    };\n  }\n  \n  async getPrefix(prefix) {\n    const results = [];\n    \n    for (const [key, value] of this.data.entries()) {\n      if (key.startsWith(prefix)) {\n        if (!value.expireAt || value.expireAt >= Date.now()) {\n          results.push({\n            key,\n            value: value.value,\n            createRevision: value.createRevision,\n            modRevision: value.modRevision\n          });\n        }\n      }\n    }\n    \n    return results;\n  }\n  \n  async delete(key) {\n    if (this.raft.state !== 'leader') {\n      throw new Error('Not leader');\n    }\n    \n    const command = { type: 'delete', key };\n    return this.raft.propose(command);\n  }\n  \n  // Watch for changes\n  watch(key, callback, options = {}) {\n    const watcherId = Math.random().toString(36);\n    \n    if (!this.watchers.has(key)) {\n      this.watchers.set(key, new Map());\n    }\n    \n    this.watchers.get(key).set(watcherId, {\n      callback,\n      prefix: options.prefix || false,\n      startRevision: options.startRevision || this.revision\n    });\n    \n    // Return cancel function\n    return () => {\n      this.watchers.get(key)?.delete(watcherId);\n    };\n  }\n  \n  // Apply committed command\n  apply(command) {\n    this.revision++;\n    \n    switch (command.type) {\n      case 'put':\n        this.applyPut(command);\n        break;\n      case 'delete':\n        this.applyDelete(command);\n        break;\n    }\n  }\n  \n  applyPut(command) {\n    const { key, value, ttl, lease } = command;\n    \n    const existing = this.data.get(key);\n    \n    const entry = {\n      value,\n      createRevision: existing ? existing.createRevision : this.revision,\n      modRevision: this.revision,\n      version: existing ? existing.version + 1 : 1,\n      expireAt: ttl ? Date.now() + ttl : null,\n      lease\n    };\n    \n    this.data.set(key, entry);\n    \n    // Notify watchers\n    this.notifyWatchers(key, 'PUT', entry);\n  }\n  \n  applyDelete(command) {\n    const { key } = command;\n    const existing = this.data.get(key);\n    \n    if (existing) {\n      this.data.delete(key);\n      this.notifyWatchers(key, 'DELETE', existing);\n    }\n  }\n  \n  notifyWatchers(key, eventType, value) {\n    // Exact key watchers\n    if (this.watchers.has(key)) {\n      for (const watcher of this.watchers.get(key).values()) {\n        if (this.revision >= watcher.startRevision) {\n          watcher.callback({\n            type: eventType,\n            key,\n            value: value.value,\n            revision: this.revision\n          });\n        }\n      }\n    }\n    \n    // Prefix watchers\n    for (const [watchKey, watchers] of this.watchers.entries()) {\n      if (key.startsWith(watchKey)) {\n        for (const watcher of watchers.values()) {\n          if (watcher.prefix && this.revision >= watcher.startRevision) {\n            watcher.callback({\n              type: eventType,\n              key,\n              value: value.value,\n              revision: this.revision\n            });\n          }\n        }\n      }\n    }\n  }\n}\n\n// Client Usage\nclass ConfigClient {\n  constructor(endpoints) {\n    this.endpoints = endpoints;\n    this.leaderEndpoint = null;\n  }\n  \n  async put(key, value, options = {}) {\n    const endpoint = await this.getLeader();\n    \n    const response = await fetch(`${endpoint}/v3/kv/put`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ key, value, ...options })\n    });\n    \n    if (!response.ok) {\n      throw new Error(`Put failed: ${response.statusText}`);\n    }\n    \n    return response.json();\n  }\n  \n  async get(key) {\n    const endpoint = this.endpoints[0]; // Any endpoint can serve reads\n    \n    const response = await fetch(`${endpoint}/v3/kv/range`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ key })\n    });\n    \n    const data = await response.json();\n    return data.kvs?.[0] || null;\n  }\n  \n  watch(key, callback, options = {}) {\n    const endpoint = this.endpoints[0];\n    const ws = new WebSocket(`${endpoint}/v3/watch`);\n    \n    ws.onopen = () => {\n      ws.send(JSON.stringify({\n        create_request: {\n          key,\n          start_revision: options.startRevision || 0,\n          prefix: options.prefix || false\n        }\n      }));\n    };\n    \n    ws.onmessage = (event) => {\n      const data = JSON.parse(event.data);\n      if (data.result?.events) {\n        data.result.events.forEach(callback);\n      }\n    };\n    \n    return () => ws.close();\n  }\n}\n\n// Example Usage\nconst client = new ConfigClient(['http://localhost:2379']);\n\n// Put configuration\nawait client.put('/config/database/host', 'localhost:5432');\nawait client.put('/config/database/pool_size', '10');\n\n// Get configuration\nconst host = await client.get('/config/database/host');\nconsole.log('Database host:', host.value);\n\n// Watch for changes\nconst cancel = client.watch('/config/database/', (event) => {\n  console.log('Config changed:', event.type, event.key, event.value);\n  \n  // Reload configuration\n  reloadConfig();\n}, { prefix: true });\n\n// Service discovery example\nawait client.put('/services/api/node1', JSON.stringify({\n  address: '10.0.1.5:8080',\n  healthy: true\n}), { ttl: 30000 }); // 30 second TTL\n\n// Heartbeat to keep service registered\nsetInterval(async () => {\n  await client.put('/services/api/node1', JSON.stringify({\n    address: '10.0.1.5:8080',\n    healthy: true\n  }), { ttl: 30000 });\n}, 15000);"
    },
    {
      "id": 89,
      "question": "Design a multi-tenant SaaS platform with data isolation",
      "answer": "A multi-tenant SaaS platform serves multiple customers (tenants) from shared infrastructure while ensuring complete data isolation, security, and customization.\n\nTenancy Models:\n• Shared database, shared schema (row-level isolation with tenant_id)\n• Shared database, separate schemas (schema per tenant)\n• Separate database per tenant (maximum isolation)\n• Hybrid approach based on tenant tier\n\nData Isolation:\n• Row-level security with tenant_id column\n• Database/schema-level isolation\n• Encryption at rest with tenant-specific keys\n• Query filters enforcing tenant boundaries\n• API gateway tenant routing\n\nCustomization:\n• Tenant-specific configurations\n• Feature flags per tenant\n• Custom domains and branding\n• Tenant-level extensions/plugins\n• Separate resource quotas\n\nScalability:\n• Shard tenants across databases\n• Dedicated resources for enterprise tenants\n• Connection pooling per tenant\n• Cache isolation with tenant prefixes\n• Rate limiting per tenant",
      "explanation": "Multi-tenant SaaS uses shared infrastructure with strict data isolation through tenant IDs, separate schemas, or databases, combined with tenant-aware routing, encryption, quotas, and customization to serve many customers securely from common resources.",
      "difficulty": "Hard",
      "code": "// Tenant Context Middleware\nclass TenantContext {\n  constructor() {\n    this.tenantStore = new AsyncLocalStorage();\n  }\n  \n  middleware() {\n    return (req, res, next) => {\n      const tenantId = this.extractTenantId(req);\n      \n      if (!tenantId) {\n        return res.status(400).json({ error: 'Tenant identification required' });\n      }\n      \n      // Store tenant context for the request lifecycle\n      this.tenantStore.run({ tenantId }, () => {\n        req.tenantId = tenantId;\n        next();\n      });\n    };\n  }\n  \n  extractTenantId(req) {\n    // Option 1: From subdomain (tenant1.saas.com)\n    const subdomain = req.hostname.split('.')[0];\n    if (subdomain !== 'www' && subdomain !== 'saas') {\n      return this.getTenantIdFromSubdomain(subdomain);\n    }\n    \n    // Option 2: From custom header\n    if (req.headers['x-tenant-id']) {\n      return req.headers['x-tenant-id'];\n    }\n    \n    // Option 3: From JWT token\n    if (req.user) {\n      return req.user.tenantId;\n    }\n    \n    return null;\n  }\n  \n  getCurrentTenant() {\n    const context = this.tenantStore.getStore();\n    return context?.tenantId;\n  }\n}\n\nconst tenantContext = new TenantContext();\n\n// Shared Schema Model with Row-Level Security\nclass TenantAwareModel {\n  constructor(tableName, db) {\n    this.tableName = tableName;\n    this.db = db;\n  }\n  \n  // All queries automatically include tenant filter\n  async find(conditions = {}) {\n    const tenantId = tenantContext.getCurrentTenant();\n    \n    if (!tenantId) {\n      throw new Error('No tenant context');\n    }\n    \n    return this.db.query(\n      `SELECT * FROM ${this.tableName}\n       WHERE tenant_id = $1 AND ${this.buildConditions(conditions)}`,\n      [tenantId, ...Object.values(conditions)]\n    );\n  }\n  \n  async create(data) {\n    const tenantId = tenantContext.getCurrentTenant();\n    \n    // Automatically inject tenant_id\n    const record = { ...data, tenant_id: tenantId };\n    \n    return this.db.query(\n      `INSERT INTO ${this.tableName} (${Object.keys(record).join(', ')})\n       VALUES (${Object.keys(record).map((_, i) => `$${i + 1}`).join(', ')})\n       RETURNING *`,\n      Object.values(record)\n    );\n  }\n  \n  async update(id, data) {\n    const tenantId = tenantContext.getCurrentTenant();\n    \n    // Ensure can only update own tenant data\n    return this.db.query(\n      `UPDATE ${this.tableName}\n       SET ${Object.keys(data).map((k, i) => `${k} = $${i + 2}`).join(', ')}\n       WHERE id = $1 AND tenant_id = $${Object.keys(data).length + 2}\n       RETURNING *`,\n      [id, ...Object.values(data), tenantId]\n    );\n  }\n  \n  async delete(id) {\n    const tenantId = tenantContext.getCurrentTenant();\n    \n    return this.db.query(\n      `DELETE FROM ${this.tableName}\n       WHERE id = $1 AND tenant_id = $2`,\n      [id, tenantId]\n    );\n  }\n}\n\n// Separate Schema Per Tenant\nclass SchemaPerTenantDB {\n  constructor(dbPool) {\n    this.pool = dbPool;\n    this.schemaCache = new Map();\n  }\n  \n  async getConnection(tenantId) {\n    const schemaName = `tenant_${tenantId}`;\n    \n    // Create schema if doesn't exist\n    if (!this.schemaCache.has(tenantId)) {\n      await this.createTenantSchema(schemaName);\n      this.schemaCache.set(tenantId, true);\n    }\n    \n    const client = await this.pool.connect();\n    \n    // Set search_path to tenant schema\n    await client.query(`SET search_path TO ${schemaName}, public`);\n    \n    return client;\n  }\n  \n  async createTenantSchema(schemaName) {\n    const client = await this.pool.connect();\n    \n    try {\n      await client.query('BEGIN');\n      \n      // Create schema\n      await client.query(`CREATE SCHEMA IF NOT EXISTS ${schemaName}`);\n      \n      // Run migrations for tenant schema\n      await this.runMigrations(client, schemaName);\n      \n      await client.query('COMMIT');\n    } catch (error) {\n      await client.query('ROLLBACK');\n      throw error;\n    } finally {\n      client.release();\n    }\n  }\n  \n  async runMigrations(client, schemaName) {\n    await client.query(`SET search_path TO ${schemaName}`);\n    \n    // Create tables\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS users (\n        id SERIAL PRIMARY KEY,\n        email VARCHAR(255) UNIQUE NOT NULL,\n        name VARCHAR(255),\n        created_at TIMESTAMP DEFAULT NOW()\n      )\n    `);\n    \n    await client.query(`\n      CREATE TABLE IF NOT EXISTS documents (\n        id SERIAL PRIMARY KEY,\n        user_id INTEGER REFERENCES users(id),\n        title VARCHAR(255),\n        content TEXT,\n        created_at TIMESTAMP DEFAULT NOW()\n      )\n    `);\n  }\n}\n\n// Separate Database Per Tenant\nclass DatabasePerTenant {\n  constructor() {\n    this.tenantDatabases = new Map();\n  }\n  \n  async getTenantDatabase(tenantId) {\n    if (this.tenantDatabases.has(tenantId)) {\n      return this.tenantDatabases.get(tenantId);\n    }\n    \n    const config = await this.getTenantDBConfig(tenantId);\n    \n    const pool = new Pool(config);\n    this.tenantDatabases.set(tenantId, pool);\n    \n    return pool;\n  }\n  \n  async getTenantDBConfig(tenantId) {\n    // Retrieve from tenant registry\n    const tenant = await tenantRegistry.get(tenantId);\n    \n    return {\n      host: tenant.dbHost,\n      port: tenant.dbPort,\n      database: tenant.dbName,\n      user: tenant.dbUser,\n      password: tenant.dbPassword,\n      ssl: true\n    };\n  }\n  \n  async provisionTenantDatabase(tenantId) {\n    const dbName = `tenant_${tenantId}`;\n    \n    // Create database\n    await adminDB.query(`CREATE DATABASE ${dbName}`);\n    \n    // Create user\n    const password = this.generatePassword();\n    await adminDB.query(\n      `CREATE USER tenant_user_${tenantId} WITH PASSWORD '${password}'`\n    );\n    \n    // Grant permissions\n    await adminDB.query(\n      `GRANT ALL PRIVILEGES ON DATABASE ${dbName} TO tenant_user_${tenantId}`\n    );\n    \n    // Run migrations\n    const pool = new Pool({\n      database: dbName,\n      user: `tenant_user_${tenantId}`,\n      password\n    });\n    \n    await this.runMigrations(pool);\n    \n    // Store config in tenant registry\n    await tenantRegistry.set(tenantId, {\n      dbHost: 'localhost',\n      dbPort: 5432,\n      dbName,\n      dbUser: `tenant_user_${tenantId}`,\n      dbPassword: password\n    });\n  }\n}\n\n// Tenant Configuration Management\nclass TenantConfigManager {\n  constructor(redis) {\n    this.redis = redis;\n    this.cache = new LRUCache({ max: 1000, ttl: 300000 }); // 5 min\n  }\n  \n  async getConfig(tenantId) {\n    // Check cache first\n    let config = this.cache.get(tenantId);\n    if (config) return config;\n    \n    // Load from Redis\n    const data = await this.redis.get(`tenant:${tenantId}:config`);\n    \n    if (data) {\n      config = JSON.parse(data);\n      this.cache.set(tenantId, config);\n      return config;\n    }\n    \n    // Load from database\n    config = await this.loadFromDB(tenantId);\n    \n    // Cache in Redis and LRU\n    await this.redis.setex(\n      `tenant:${tenantId}:config`,\n      3600,\n      JSON.stringify(config)\n    );\n    this.cache.set(tenantId, config);\n    \n    return config;\n  }\n  \n  async updateConfig(tenantId, updates) {\n    const config = await this.getConfig(tenantId);\n    const newConfig = { ...config, ...updates };\n    \n    // Save to database\n    await this.saveToDB(tenantId, newConfig);\n    \n    // Update Redis\n    await this.redis.setex(\n      `tenant:${tenantId}:config`,\n      3600,\n      JSON.stringify(newConfig)\n    );\n    \n    // Update cache\n    this.cache.set(tenantId, newConfig);\n    \n    // Broadcast to other instances\n    await this.redis.publish('tenant:config:update', JSON.stringify({\n      tenantId,\n      config: newConfig\n    }));\n  }\n}\n\n// Tenant-specific Rate Limiting\nclass TenantRateLimiter {\n  constructor(redis) {\n    this.redis = redis;\n    this.limits = new Map();\n  }\n  \n  async checkLimit(tenantId, endpoint) {\n    const config = await tenantConfig.getConfig(tenantId);\n    const limit = config.rateLimits?.[endpoint] || { requests: 1000, window: 60000 };\n    \n    const key = `ratelimit:${tenantId}:${endpoint}`;\n    const now = Date.now();\n    \n    // Use Redis sorted set with timestamps\n    await this.redis.zremrangebyscore(key, 0, now - limit.window);\n    const count = await this.redis.zcard(key);\n    \n    if (count >= limit.requests) {\n      return {\n        allowed: false,\n        retryAfter: limit.window / 1000\n      };\n    }\n    \n    await this.redis.zadd(key, now, `${now}-${Math.random()}`);\n    await this.redis.expire(key, Math.ceil(limit.window / 1000));\n    \n    return {\n      allowed: true,\n      remaining: limit.requests - count - 1\n    };\n  }\n}\n\n// Application Setup\nconst app = express();\n\n// Tenant context middleware (must be first)\napp.use(tenantContext.middleware());\n\n// Rate limiting per tenant\napp.use(async (req, res, next) => {\n  const result = await rateLimiter.checkLimit(\n    req.tenantId,\n    req.path\n  );\n  \n  if (!result.allowed) {\n    return res.status(429).json({\n      error: 'Rate limit exceeded',\n      retryAfter: result.retryAfter\n    });\n  }\n  \n  res.set('X-RateLimit-Remaining', result.remaining);\n  next();\n});\n\n// API routes with automatic tenant isolation\napp.get('/api/users', async (req, res) => {\n  const userModel = new TenantAwareModel('users', db);\n  const users = await userModel.find();\n  res.json(users);\n});\n\napp.post('/api/users', async (req, res) => {\n  const userModel = new TenantAwareModel('users', db);\n  const user = await userModel.create(req.body);\n  res.json(user);\n});"
    },
    {
      "id": 90,
      "question": "Design a blockchain-based system or smart contract platform",
      "answer": "A blockchain system maintains a distributed ledger of transactions with cryptographic security, consensus mechanisms, and immutability.\n\nCore Components:\n• Distributed ledger of blocks\n• Consensus mechanism (PoW, PoS, PBFT)\n• Peer-to-peer network\n• Cryptographic hashing and signing\n• Transaction pool (mempool)\n• Smart contract execution (if applicable)\n• Wallet and key management\n\nBlock Structure:\n• Block header (hash, previous hash, timestamp, nonce)\n• Merkle root of transactions\n• List of transactions\n• Difficulty target\n\nConsensus:\n• Proof of Work: Miners solve computational puzzles\n• Proof of Stake: Validators stake tokens\n• PBFT: Byzantine fault tolerance for permissioned chains\n• Leader election and block proposal\n• Fork resolution rules\n\nSmart Contracts:\n• Turing-complete scripting (EVM for Ethereum)\n• Deterministic execution\n• Gas metering for resource limits\n• Contract state storage\n• Events and logs\n\nScalability:\n• Layer 2 solutions (Lightning, Plasma)\n• Sharding for parallel processing\n• Off-chain computation with on-chain verification\n• State channels for micropayments",
      "explanation": "Blockchain systems use distributed consensus (PoW/PoS/PBFT) to maintain an immutable ledger of cryptographically linked blocks, enabling decentralized trust, smart contracts, and verifiable transactions without central authority.",
      "difficulty": "Hard",
      "code": "// Basic Blockchain Implementation\nconst crypto = require('crypto');\n\nclass Block {\n  constructor(index, timestamp, transactions, previousHash = '') {\n    this.index = index;\n    this.timestamp = timestamp;\n    this.transactions = transactions;\n    this.previousHash = previousHash;\n    this.nonce = 0;\n    this.hash = this.calculateHash();\n  }\n  \n  calculateHash() {\n    return crypto\n      .createHash('sha256')\n      .update(\n        this.index +\n        this.previousHash +\n        this.timestamp +\n        JSON.stringify(this.transactions) +\n        this.nonce\n      )\n      .digest('hex');\n  }\n  \n  // Proof of Work\n  mineBlock(difficulty) {\n    const target = Array(difficulty + 1).join('0');\n    \n    while (this.hash.substring(0, difficulty) !== target) {\n      this.nonce++;\n      this.hash = this.calculateHash();\n    }\n    \n    console.log(`Block mined: ${this.hash}`);\n  }\n}\n\nclass Transaction {\n  constructor(fromAddress, toAddress, amount) {\n    this.fromAddress = fromAddress;\n    this.toAddress = toAddress;\n    this.amount = amount;\n    this.timestamp = Date.now();\n  }\n  \n  calculateHash() {\n    return crypto\n      .createHash('sha256')\n      .update(\n        this.fromAddress +\n        this.toAddress +\n        this.amount +\n        this.timestamp\n      )\n      .digest('hex');\n  }\n  \n  signTransaction(signingKey) {\n    // Can only send from your own address\n    if (signingKey.getPublic('hex') !== this.fromAddress) {\n      throw new Error('Cannot sign transactions for other wallets');\n    }\n    \n    const hashTx = this.calculateHash();\n    const sig = signingKey.sign(hashTx, 'base64');\n    \n    this.signature = sig.toDER('hex');\n  }\n  \n  isValid() {\n    // Mining reward transactions don't have from address\n    if (this.fromAddress === null) return true;\n    \n    if (!this.signature || this.signature.length === 0) {\n      throw new Error('No signature in this transaction');\n    }\n    \n    const publicKey = ec.keyFromPublic(this.fromAddress, 'hex');\n    return publicKey.verify(this.calculateHash(), this.signature);\n  }\n}\n\nclass Blockchain {\n  constructor() {\n    this.chain = [this.createGenesisBlock()];\n    this.difficulty = 4;\n    this.pendingTransactions = [];\n    this.miningReward = 100;\n  }\n  \n  createGenesisBlock() {\n    return new Block(0, Date.now(), [], '0');\n  }\n  \n  getLatestBlock() {\n    return this.chain[this.chain.length - 1];\n  }\n  \n  minePendingTransactions(miningRewardAddress) {\n    // Create reward transaction\n    const rewardTx = new Transaction(\n      null,\n      miningRewardAddress,\n      this.miningReward\n    );\n    this.pendingTransactions.push(rewardTx);\n    \n    // Create new block\n    const block = new Block(\n      this.chain.length,\n      Date.now(),\n      this.pendingTransactions,\n      this.getLatestBlock().hash\n    );\n    \n    block.mineBlock(this.difficulty);\n    \n    console.log('Block successfully mined!');\n    this.chain.push(block);\n    \n    // Reset pending transactions\n    this.pendingTransactions = [];\n  }\n  \n  addTransaction(transaction) {\n    if (!transaction.fromAddress || !transaction.toAddress) {\n      throw new Error('Transaction must include from and to address');\n    }\n    \n    if (!transaction.isValid()) {\n      throw new Error('Cannot add invalid transaction to chain');\n    }\n    \n    // Check if sender has sufficient balance\n    const balance = this.getBalanceOfAddress(transaction.fromAddress);\n    if (balance < transaction.amount) {\n      throw new Error('Insufficient balance');\n    }\n    \n    this.pendingTransactions.push(transaction);\n  }\n  \n  getBalanceOfAddress(address) {\n    let balance = 0;\n    \n    for (const block of this.chain) {\n      for (const trans of block.transactions) {\n        if (trans.fromAddress === address) {\n          balance -= trans.amount;\n        }\n        \n        if (trans.toAddress === address) {\n          balance += trans.amount;\n        }\n      }\n    }\n    \n    return balance;\n  }\n  \n  isChainValid() {\n    for (let i = 1; i < this.chain.length; i++) {\n      const currentBlock = this.chain[i];\n      const previousBlock = this.chain[i - 1];\n      \n      // Verify all transactions in block\n      for (const tx of currentBlock.transactions) {\n        if (!tx.isValid()) {\n          return false;\n        }\n      }\n      \n      // Verify block hash\n      if (currentBlock.hash !== currentBlock.calculateHash()) {\n        return false;\n      }\n      \n      // Verify chain linkage\n      if (currentBlock.previousHash !== previousBlock.hash) {\n        return false;\n      }\n    }\n    \n    return true;\n  }\n}\n\n// Smart Contract Platform (Simplified EVM)\nclass SmartContract {\n  constructor(code, creator) {\n    this.address = this.generateAddress();\n    this.code = code;\n    this.creator = creator;\n    this.storage = new Map();\n    this.balance = 0;\n  }\n  \n  generateAddress() {\n    return crypto.randomBytes(20).toString('hex');\n  }\n  \n  execute(functionName, args, caller, value = 0) {\n    const context = {\n      storage: this.storage,\n      balance: this.balance + value,\n      caller,\n      timestamp: Date.now(),\n      blockNumber: blockchain.chain.length\n    };\n    \n    // Gas metering\n    let gasUsed = 0;\n    const gasLimit = 100000;\n    \n    try {\n      // Execute contract function with gas limiting\n      const result = this.code[functionName].call(context, ...args, (opcode) => {\n        gasUsed += this.getGasCost(opcode);\n        if (gasUsed > gasLimit) {\n          throw new Error('Out of gas');\n        }\n      });\n      \n      // Update balance\n      this.balance = context.balance;\n      \n      return { result, gasUsed };\n    } catch (error) {\n      // Revert state changes on error\n      throw error;\n    }\n  }\n  \n  getGasCost(opcode) {\n    const costs = {\n      'ADD': 3,\n      'MUL': 5,\n      'SUB': 3,\n      'DIV': 5,\n      'SLOAD': 200, // Storage read\n      'SSTORE': 20000, // Storage write\n      'CALL': 700\n    };\n    return costs[opcode] || 1;\n  }\n}\n\n// Example: Token Contract\nconst tokenContract = new SmartContract({\n  balances: new Map(),\n  totalSupply: 0,\n  \n  constructor(initialSupply) {\n    this.totalSupply = initialSupply;\n    this.storage.set('totalSupply', initialSupply);\n    this.balances.set(this.creator, initialSupply);\n  },\n  \n  transfer: function(to, amount, gasCallback) {\n    gasCallback('SLOAD'); // Read from balance\n    const senderBalance = this.balances.get(this.caller) || 0;\n    \n    if (senderBalance < amount) {\n      throw new Error('Insufficient balance');\n    }\n    \n    gasCallback('SSTORE'); // Write to sender balance\n    this.balances.set(this.caller, senderBalance - amount);\n    \n    gasCallback('SLOAD'); // Read recipient balance\n    const recipientBalance = this.balances.get(to) || 0;\n    \n    gasCallback('SSTORE'); // Write to recipient balance\n    this.balances.set(to, recipientBalance + amount);\n    \n    return true;\n  },\n  \n  balanceOf: function(address, gasCallback) {\n    gasCallback('SLOAD');\n    return this.balances.get(address) || 0;\n  }\n}, 'creator-address');\n\n// Consensus - Proof of Stake\nclass ProofOfStake {\n  constructor(blockchain) {\n    this.blockchain = blockchain;\n    this.validators = new Map(); // address -> stake amount\n    this.minimumStake = 1000;\n  }\n  \n  stake(address, amount) {\n    if (amount < this.minimumStake) {\n      throw new Error('Stake amount too low');\n    }\n    \n    const currentStake = this.validators.get(address) || 0;\n    this.validators.set(address, currentStake + amount);\n  }\n  \n  selectValidator() {\n    // Weighted random selection based on stake\n    const totalStake = Array.from(this.validators.values())\n      .reduce((sum, stake) => sum + stake, 0);\n    \n    let random = Math.random() * totalStake;\n    \n    for (const [address, stake] of this.validators.entries()) {\n      random -= stake;\n      if (random <= 0) {\n        return address;\n      }\n    }\n  }\n  \n  proposeBlock(validatorAddress) {\n    if (!this.validators.has(validatorAddress)) {\n      throw new Error('Not a validator');\n    }\n    \n    // Validator creates new block\n    const block = new Block(\n      this.blockchain.chain.length,\n      Date.now(),\n      this.blockchain.pendingTransactions,\n      this.blockchain.getLatestBlock().hash\n    );\n    \n    // No mining needed in PoS\n    block.validator = validatorAddress;\n    \n    return block;\n  }\n}\n\n// P2P Network Node\nclass BlockchainNode {\n  constructor(port) {\n    this.blockchain = new Blockchain();\n    this.peers = [];\n    this.port = port;\n  }\n  \n  connectToPeer(peerAddress) {\n    const ws = new WebSocket(peerAddress);\n    \n    ws.on('open', () => {\n      this.peers.push(ws);\n      this.syncChain(ws);\n    });\n    \n    ws.on('message', (data) => {\n      this.handleMessage(JSON.parse(data), ws);\n    });\n  }\n  \n  handleMessage(message, ws) {\n    switch (message.type) {\n      case 'NEW_TRANSACTION':\n        this.blockchain.addTransaction(message.transaction);\n        this.broadcastTransaction(message.transaction, ws);\n        break;\n        \n      case 'NEW_BLOCK':\n        this.receiveBlock(message.block);\n        break;\n        \n      case 'REQUEST_CHAIN':\n        ws.send(JSON.stringify({\n          type: 'CHAIN',\n          chain: this.blockchain.chain\n        }));\n        break;\n        \n      case 'CHAIN':\n        this.replaceChain(message.chain);\n        break;\n    }\n  }\n  \n  receiveBlock(block) {\n    const latestBlock = this.blockchain.getLatestBlock();\n    \n    if (block.previousHash === latestBlock.hash &&\n        block.index === latestBlock.index + 1) {\n      this.blockchain.chain.push(block);\n      console.log('Block added to chain');\n    }\n  }\n  \n  replaceChain(newChain) {\n    if (newChain.length > this.blockchain.chain.length) {\n      // Verify new chain\n      const tempBlockchain = new Blockchain();\n      tempBlockchain.chain = newChain;\n      \n      if (tempBlockchain.isChainValid()) {\n        this.blockchain.chain = newChain;\n        console.log('Chain replaced with longer valid chain');\n      }\n    }\n  }\n  \n  broadcastTransaction(transaction, excludeWs) {\n    this.peers.forEach(peer => {\n      if (peer !== excludeWs) {\n        peer.send(JSON.stringify({\n          type: 'NEW_TRANSACTION',\n          transaction\n        }));\n      }\n    });\n  }\n}\n\n// Usage\nconst EC = require('elliptic').ec;\nconst ec = new EC('secp256k1');\n\n// Create wallets\nconst myKey = ec.genKeyPair();\nconst myWalletAddress = myKey.getPublic('hex');\n\n// Create blockchain\nconst blockchain = new Blockchain();\n\n// Create and sign transaction\nconst tx1 = new Transaction(myWalletAddress, 'recipient-address', 100);\ntx1.signTransaction(myKey);\nblockchain.addTransaction(tx1);\n\n// Mine block\nblockchain.minePendingTransactions(myWalletAddress);\n\nconsole.log('Balance:', blockchain.getBalanceOfAddress(myWalletAddress));\nconsole.log('Is chain valid?', blockchain.isChainValid());"
    },
    {
      "id": 91,
      "question": "Design a real-time multiplayer gaming backend like Fortnite or PUBG",
      "answer": "A multiplayer gaming backend handles real-time player interactions, game state synchronization, matchmaking, and anti-cheat systems with minimal latency.\n\nCore Components:\n• Game servers for match hosting\n• Matchmaking service for player pairing\n• Real-time messaging (WebSocket/UDP)\n• Game state synchronization\n• Anti-cheat detection\n• Player inventory and progression\n• Leaderboards and statistics\n• Voice chat infrastructure\n\nNetworking:\n• Client-server or peer-to-peer architecture\n• Client-side prediction for responsiveness\n• Server reconciliation for authoritative state\n• Lag compensation techniques\n• UDP for low-latency game data\n• WebSocket for reliable messaging\n\nMatchmaking:\n• Skill-based matching (MMR/ELO)\n• Region-based server selection\n• Queue management\n• Team balancing algorithms\n• Quick match vs ranked modes\n\nScalability:\n• Dedicated game server instances\n• Dynamic server scaling based on player count\n• Regional server clusters\n• Session-based game state\n• Stateless API servers",
      "explanation": "Real-time multiplayer backends use UDP for low-latency game data, WebSocket for reliable messaging, client-side prediction with server reconciliation for responsive gameplay, skill-based matchmaking, and dedicated game servers that scale dynamically with player demand.",
      "difficulty": "Hard",
      "code": "// Game Server Architecture\nconst dgram = require('dgram');\nconst WebSocket = require('ws');\n\nclass GameServer {\n  constructor(port) {\n    this.port = port;\n    this.players = new Map();\n    this.gameState = {\n      entities: new Map(),\n      projectiles: [],\n      timestamp: 0\n    };\n    \n    // UDP for game state (low latency)\n    this.udpServer = dgram.createSocket('udp4');\n    \n    // WebSocket for reliable messaging\n    this.wsServer = new WebSocket.Server({ port: port + 1 });\n    \n    this.tickRate = 64; // 64 ticks per second\n    this.tickInterval = 1000 / this.tickRate;\n    \n    this.setupUDP();\n    this.setupWebSocket();\n    this.startGameLoop();\n  }\n  \n  setupUDP() {\n    this.udpServer.on('message', (msg, rinfo) => {\n      this.handleUDPMessage(msg, rinfo);\n    });\n    \n    this.udpServer.bind(this.port);\n  }\n  \n  setupWebSocket() {\n    this.wsServer.on('connection', (ws, req) => {\n      const playerId = this.generatePlayerId();\n      \n      this.players.set(playerId, {\n        id: playerId,\n        ws,\n        udpAddress: null,\n        position: { x: 0, y: 0, z: 0 },\n        rotation: { x: 0, y: 0, z: 0 },\n        health: 100,\n        inventory: [],\n        lastUpdateTime: Date.now(),\n        sequence: 0\n      });\n      \n      ws.on('message', (data) => {\n        this.handleWebSocketMessage(playerId, JSON.parse(data));\n      });\n      \n      ws.on('close', () => {\n        this.removePlayer(playerId);\n      });\n      \n      // Send initial game state\n      this.sendInitialState(playerId);\n    });\n  }\n  \n  handleUDPMessage(msg, rinfo) {\n    try {\n      const data = JSON.parse(msg.toString());\n      const player = this.players.get(data.playerId);\n      \n      if (!player) return;\n      \n      // Store UDP address for responses\n      player.udpAddress = rinfo;\n      \n      // Handle player input\n      switch (data.type) {\n        case 'PLAYER_INPUT':\n          this.processPlayerInput(data.playerId, data.input, data.sequence);\n          break;\n      }\n    } catch (error) {\n      console.error('UDP message error:', error);\n    }\n  }\n  \n  processPlayerInput(playerId, input, sequence) {\n    const player = this.players.get(playerId);\n    if (!player) return;\n    \n    // Server-authoritative movement with client prediction\n    const deltaTime = this.tickInterval / 1000;\n    \n    // Apply input to player state\n    if (input.forward) player.position.z += 5 * deltaTime;\n    if (input.backward) player.position.z -= 5 * deltaTime;\n    if (input.left) player.position.x -= 5 * deltaTime;\n    if (input.right) player.position.x += 5 * deltaTime;\n    \n    player.rotation = input.rotation;\n    player.sequence = sequence;\n    player.lastUpdateTime = Date.now();\n    \n    // Handle shooting\n    if (input.shoot) {\n      this.createProjectile(playerId, player.position, player.rotation);\n    }\n    \n    // Acknowledge input (for client reconciliation)\n    this.sendUDP(player, {\n      type: 'INPUT_ACK',\n      sequence,\n      position: player.position,\n      timestamp: Date.now()\n    });\n  }\n  \n  startGameLoop() {\n    const tick = () => {\n      this.gameState.timestamp = Date.now();\n      \n      // Update game state\n      this.updateProjectiles();\n      this.checkCollisions();\n      \n      // Broadcast state to all players\n      this.broadcastGameState();\n    };\n    \n    setInterval(tick, this.tickInterval);\n  }\n  \n  broadcastGameState() {\n    const state = this.serializeGameState();\n    \n    // Send via UDP for low latency\n    this.players.forEach(player => {\n      if (player.udpAddress) {\n        this.sendUDP(player, {\n          type: 'GAME_STATE',\n          state,\n          timestamp: this.gameState.timestamp\n        });\n      }\n    });\n  }\n  \n  serializeGameState() {\n    const players = [];\n    \n    this.players.forEach((player, id) => {\n      players.push({\n        id,\n        position: player.position,\n        rotation: player.rotation,\n        health: player.health\n      });\n    });\n    \n    return {\n      players,\n      projectiles: this.gameState.projectiles\n    };\n  }\n  \n  sendUDP(player, data) {\n    if (!player.udpAddress) return;\n    \n    const msg = Buffer.from(JSON.stringify(data));\n    this.udpServer.send(\n      msg,\n      player.udpAddress.port,\n      player.udpAddress.address\n    );\n  }\n}\n\n// Client-Side Prediction and Reconciliation\nclass GameClient {\n  constructor(serverUrl, udpPort) {\n    this.ws = new WebSocket(serverUrl);\n    this.udpSocket = dgram.createSocket('udp4');\n    this.udpPort = udpPort;\n    \n    this.playerId = null;\n    this.localPlayer = {\n      position: { x: 0, y: 0, z: 0 },\n      rotation: { x: 0, y: 0, z: 0 },\n      velocity: { x: 0, y: 0, z: 0 }\n    };\n    \n    this.otherPlayers = new Map();\n    this.pendingInputs = [];\n    this.sequence = 0;\n    \n    this.setupNetworking();\n    this.startRenderLoop();\n  }\n  \n  setupNetworking() {\n    this.ws.on('message', (data) => {\n      const message = JSON.parse(data);\n      \n      if (message.type === 'INITIAL_STATE') {\n        this.playerId = message.playerId;\n        this.localPlayer.position = message.position;\n      }\n    });\n    \n    this.udpSocket.on('message', (msg) => {\n      const data = JSON.parse(msg.toString());\n      \n      if (data.type === 'INPUT_ACK') {\n        this.reconcileServerState(data);\n      } else if (data.type === 'GAME_STATE') {\n        this.interpolateGameState(data.state);\n      }\n    });\n  }\n  \n  processInput() {\n    const input = this.getPlayerInput();\n    \n    // Client-side prediction: apply input immediately\n    this.applyInput(input);\n    \n    // Store for server reconciliation\n    this.sequence++;\n    this.pendingInputs.push({\n      sequence: this.sequence,\n      input,\n      timestamp: Date.now()\n    });\n    \n    // Send to server via UDP\n    this.sendUDP({\n      type: 'PLAYER_INPUT',\n      playerId: this.playerId,\n      input,\n      sequence: this.sequence\n    });\n  }\n  \n  applyInput(input) {\n    const deltaTime = 1 / 60; // Assuming 60fps\n    \n    if (input.forward) this.localPlayer.position.z += 5 * deltaTime;\n    if (input.backward) this.localPlayer.position.z -= 5 * deltaTime;\n    if (input.left) this.localPlayer.position.x -= 5 * deltaTime;\n    if (input.right) this.localPlayer.position.x += 5 * deltaTime;\n    \n    this.localPlayer.rotation = input.rotation;\n  }\n  \n  reconcileServerState(serverAck) {\n    // Remove acknowledged inputs\n    this.pendingInputs = this.pendingInputs.filter(\n      input => input.sequence > serverAck.sequence\n    );\n    \n    // Set position to server's authoritative position\n    this.localPlayer.position = serverAck.position;\n    \n    // Replay pending inputs\n    for (const pending of this.pendingInputs) {\n      this.applyInput(pending.input);\n    }\n  }\n  \n  interpolateGameState(state) {\n    // Update other players with interpolation\n    state.players.forEach(playerData => {\n      if (playerData.id === this.playerId) return;\n      \n      let player = this.otherPlayers.get(playerData.id);\n      \n      if (!player) {\n        player = {\n          current: playerData.position,\n          previous: playerData.position,\n          target: playerData.position\n        };\n        this.otherPlayers.set(playerData.id, player);\n      }\n      \n      player.previous = player.current;\n      player.target = playerData.position;\n    });\n  }\n  \n  startRenderLoop() {\n    const render = () => {\n      this.processInput();\n      this.render();\n      requestAnimationFrame(render);\n    };\n    \n    requestAnimationFrame(render);\n  }\n  \n  render() {\n    // Interpolate other players for smooth movement\n    const now = Date.now();\n    \n    this.otherPlayers.forEach(player => {\n      const alpha = 0.25; // Interpolation factor\n      \n      player.current = {\n        x: player.current.x + (player.target.x - player.current.x) * alpha,\n        y: player.current.y + (player.target.y - player.current.y) * alpha,\n        z: player.current.z + (player.target.z - player.current.z) * alpha\n      };\n    });\n    \n    // Render game (Three.js, Unity, etc.)\n    this.renderScene();\n  }\n}\n\n// Matchmaking System\nclass MatchmakingService {\n  constructor() {\n    this.queue = [];\n    this.gameServers = [];\n  }\n  \n  async joinQueue(player) {\n    this.queue.push({\n      player,\n      mmr: player.mmr || 1000,\n      region: player.region,\n      joinTime: Date.now()\n    });\n    \n    // Try to form matches\n    await this.processQueue();\n  }\n  \n  async processQueue() {\n    // Group by region\n    const regionQueues = this.groupByRegion();\n    \n    for (const [region, players] of regionQueues.entries()) {\n      // Sort by MMR\n      players.sort((a, b) => a.mmr - b.mmr);\n      \n      // Form matches of 10 players\n      while (players.length >= 10) {\n        const match = [];\n        \n        // Take players with similar MMR\n        for (let i = 0; i < 10 && players.length > 0; i++) {\n          // Find player within MMR range\n          const targetMMR = players[0].mmr;\n          const maxRange = this.getMMRRange(players[0].joinTime);\n          \n          const playerIndex = players.findIndex(\n            p => Math.abs(p.mmr - targetMMR) <= maxRange\n          );\n          \n          if (playerIndex !== -1) {\n            match.push(players.splice(playerIndex, 1)[0]);\n          } else {\n            break;\n          }\n        }\n        \n        if (match.length === 10) {\n          await this.createGame(match, region);\n        } else {\n          // Put players back in queue\n          players.push(...match);\n          break;\n        }\n      }\n    }\n  }\n  \n  getMMRRange(joinTime) {\n    // Expand MMR range over time to reduce wait\n    const waitTime = Date.now() - joinTime;\n    const baseRange = 100;\n    const expansionRate = 50; // +50 MMR per 30 seconds\n    return baseRange + (waitTime / 30000) * expansionRate;\n  }\n  \n  async createGame(players, region) {\n    // Allocate game server\n    const server = await this.allocateGameServer(region);\n    \n    // Balance teams\n    const teams = this.balanceTeams(players);\n    \n    // Notify players\n    players.forEach(entry => {\n      entry.player.ws.send(JSON.stringify({\n        type: 'MATCH_FOUND',\n        serverId: server.id,\n        serverAddress: server.address,\n        team: teams.get(entry.player.id)\n      }));\n    });\n  }\n  \n  balanceTeams(players) {\n    // Sort by MMR\n    players.sort((a, b) => b.mmr - a.mmr);\n    \n    const team1 = [];\n    const team2 = [];\n    let team1MMR = 0;\n    let team2MMR = 0;\n    \n    // Greedy team balancing\n    for (const player of players) {\n      if (team1MMR <= team2MMR) {\n        team1.push(player);\n        team1MMR += player.mmr;\n      } else {\n        team2.push(player);\n        team2MMR += player.mmr;\n      }\n    }\n    \n    const teams = new Map();\n    team1.forEach(p => teams.set(p.player.id, 1));\n    team2.forEach(p => teams.set(p.player.id, 2));\n    \n    return teams;\n  }\n}\n\n// Anti-Cheat System\nclass AntiCheatSystem {\n  constructor() {\n    this.suspiciousPlayers = new Map();\n    this.flagThreshold = 10;\n  }\n  \n  checkPlayerAction(playerId, action, gameState) {\n    const checks = [\n      this.checkSpeedHack(playerId, action, gameState),\n      this.checkAimbot(playerId, action, gameState),\n      this.checkWallhack(playerId, action, gameState),\n      this.checkImpossibleAction(playerId, action, gameState)\n    ];\n    \n    const flags = checks.filter(c => c.suspicious).length;\n    \n    if (flags > 0) {\n      this.incrementSuspicion(playerId, flags);\n    }\n  }\n  \n  checkSpeedHack(playerId, action, gameState) {\n    const player = gameState.players.get(playerId);\n    const lastPos = player.lastPosition;\n    const currentPos = action.position;\n    \n    const distance = this.calculateDistance(lastPos, currentPos);\n    const timeElapsed = action.timestamp - player.lastUpdateTime;\n    const maxSpeed = 10; // units per second\n    \n    const speed = distance / (timeElapsed / 1000);\n    \n    return {\n      suspicious: speed > maxSpeed * 1.2,\n      confidence: Math.min(speed / maxSpeed, 2)\n    };\n  }\n  \n  checkAimbot(playerId, action, gameState) {\n    if (action.type !== 'SHOOT') return { suspicious: false };\n    \n    const player = gameState.players.get(playerId);\n    const target = action.target;\n    \n    // Check if aim snapped directly to target\n    const aimAngleDelta = this.calculateAngleDelta(\n      player.lastAimAngle,\n      action.aimAngle\n    );\n    \n    const distanceToTarget = this.calculateDistance(\n      player.position,\n      target.position\n    );\n    \n    // Humanly impossible aim adjustment\n    const suspicious = aimAngleDelta > 90 && action.hit && distanceToTarget > 50;\n    \n    return { suspicious, confidence: suspicious ? 0.8 : 0 };\n  }\n  \n  incrementSuspicion(playerId, flags) {\n    const current = this.suspiciousPlayers.get(playerId) || 0;\n    const newScore = current + flags;\n    \n    this.suspiciousPlayers.set(playerId, newScore);\n    \n    if (newScore >= this.flagThreshold) {\n      this.banPlayer(playerId, 'Anti-cheat detection');\n    }\n  }\n}\n\n// Usage\nconst gameServer = new GameServer(7777);\nconst matchmaking = new MatchmakingService();\nconst antiCheat = new AntiCheatSystem();\n\n// Player connects\nconst player = new GameClient('ws://localhost:7778', 7777);"
    },
    {
      "id": 92,
      "question": "Design a geospatial indexing system for location-based services like Uber or Yelp",
      "answer": "A geospatial indexing system efficiently queries and stores location data for finding nearby points of interest, drivers, or businesses.\n\nIndexing Strategies:\n• Geohash: Hierarchical grid system\n• Quadtree: Recursive spatial partitioning\n• R-tree: Bounding box-based indexing\n• S2 Geometry: Spherical geometry (Google)\n• H3: Hexagonal hierarchical spatial index (Uber)\n\nCommon Operations:\n• Find nearby points within radius\n• K-nearest neighbors search\n• Bounding box queries\n• Polygon containment checks\n• Route distance calculations\n\nGeohash Benefits:\n• String-based, easy to store\n• Prefix matching for proximity\n• Variable precision (1-12 characters)\n• Works with standard databases\n• Simple proximity search\n\nScalability:\n• Shard by geohash prefix\n• Cache frequently queried areas\n• Update positions in real-time\n• Batch updates for stationary objects\n• In-memory index for active entities",
      "explanation": "Geospatial systems use hierarchical indexing like Geohash to encode locations into strings where nearby places share prefixes, enabling efficient proximity queries and spatial partitioning for scalable location-based services.",
      "difficulty": "Hard",
      "code": "// Geohash Implementation\nclass Geohash {\n  static BASE32 = '0123456789bcdefghjkmnpqrstuvwxyz';\n  \n  // Encode lat/lon to geohash\n  static encode(latitude, longitude, precision = 9) {\n    const latRange = [-90, 90];\n    const lonRange = [-180, 180];\n    let geohash = '';\n    let evenBit = true;\n    let bit = 0;\n    let ch = 0;\n    \n    while (geohash.length < precision) {\n      if (evenBit) {\n        // Longitude\n        const mid = (lonRange[0] + lonRange[1]) / 2;\n        if (longitude > mid) {\n          ch |= (1 << (4 - bit));\n          lonRange[0] = mid;\n        } else {\n          lonRange[1] = mid;\n        }\n      } else {\n        // Latitude\n        const mid = (latRange[0] + latRange[1]) / 2;\n        if (latitude > mid) {\n          ch |= (1 << (4 - bit));\n          latRange[0] = mid;\n        } else {\n          latRange[1] = mid;\n        }\n      }\n      \n      evenBit = !evenBit;\n      \n      if (bit < 4) {\n        bit++;\n      } else {\n        geohash += this.BASE32[ch];\n        bit = 0;\n        ch = 0;\n      }\n    }\n    \n    return geohash;\n  }\n  \n  // Decode geohash to lat/lon bounds\n  static decode(geohash) {\n    const latRange = [-90, 90];\n    const lonRange = [-180, 180];\n    let evenBit = true;\n    \n    for (const char of geohash) {\n      const idx = this.BASE32.indexOf(char);\n      \n      for (let i = 4; i >= 0; i--) {\n        const bit = (idx >> i) & 1;\n        \n        if (evenBit) {\n          // Longitude\n          const mid = (lonRange[0] + lonRange[1]) / 2;\n          if (bit === 1) {\n            lonRange[0] = mid;\n          } else {\n            lonRange[1] = mid;\n          }\n        } else {\n          // Latitude\n          const mid = (latRange[0] + latRange[1]) / 2;\n          if (bit === 1) {\n            latRange[0] = mid;\n          } else {\n            latRange[1] = mid;\n          }\n        }\n        \n        evenBit = !evenBit;\n      }\n    }\n    \n    return {\n      latitude: (latRange[0] + latRange[1]) / 2,\n      longitude: (lonRange[0] + lonRange[1]) / 2,\n      latError: (latRange[1] - latRange[0]) / 2,\n      lonError: (lonRange[1] - lonRange[0]) / 2\n    };\n  }\n  \n  // Get neighboring geohashes\n  static neighbors(geohash) {\n    const directions = ['n', 'ne', 'e', 'se', 's', 'sw', 'w', 'nw'];\n    const neighbors = {};\n    \n    directions.forEach(dir => {\n      neighbors[dir] = this.getNeighbor(geohash, dir);\n    });\n    \n    return neighbors;\n  }\n  \n  static getNeighbor(geohash, direction) {\n    // Simplified neighbor calculation\n    const { latitude, longitude } = this.decode(geohash);\n    const precision = geohash.length;\n    \n    // Approximate offset based on precision\n    const latOffset = 0.001 * Math.pow(2, 9 - precision);\n    const lonOffset = 0.001 * Math.pow(2, 9 - precision);\n    \n    const offsets = {\n      'n': [latOffset, 0],\n      'ne': [latOffset, lonOffset],\n      'e': [0, lonOffset],\n      'se': [-latOffset, lonOffset],\n      's': [-latOffset, 0],\n      'sw': [-latOffset, -lonOffset],\n      'w': [0, -lonOffset],\n      'nw': [latOffset, -lonOffset]\n    };\n    \n    const [latDelta, lonDelta] = offsets[direction];\n    return this.encode(latitude + latDelta, longitude + lonDelta, precision);\n  }\n}\n\n// Location Service with Geohash\nclass LocationService {\n  constructor(redis, db) {\n    this.redis = redis;\n    this.db = db;\n    this.precision = 6; // ~1.2km accuracy\n  }\n  \n  // Update entity location\n  async updateLocation(entityId, latitude, longitude) {\n    const geohash = Geohash.encode(latitude, longitude, this.precision);\n    \n    // Store in Redis sorted set by geohash\n    await this.redis.zadd(\n      `geo:${geohash}`,\n      Date.now(),\n      JSON.stringify({\n        entityId,\n        latitude,\n        longitude,\n        timestamp: Date.now()\n      })\n    );\n    \n    // Set expiry for cleanup\n    await this.redis.expire(`geo:${geohash}`, 3600);\n    \n    // Store entity -> geohash mapping\n    await this.redis.set(`entity:${entityId}:geohash`, geohash,  'EX', 3600);\n  }\n  \n  // Find nearby entities\n  async findNearby(latitude, longitude, radiusKm) {\n    const centerGeohash = Geohash.encode(latitude, longitude, this.precision);\n    \n    // Get center and neighbor geohashes\n    const searchHashes = [centerGeohash];\n    const neighbors = Geohash.neighbors(centerGeohash);\n    searchHashes.push(...Object.values(neighbors));\n    \n    // Query all relevant geohash buckets\n    const results = [];\n    \n    for (const hash of searchHashes) {\n      const entities = await this.redis.zrange(`geo:${hash}`, 0, -1);\n      \n      for (const entityData of entities) {\n        const entity = JSON.parse(entityData);\n        \n        // Calculate actual distance\n        const distance = this.haversineDistance(\n          latitude, longitude,\n          entity.latitude, entity.longitude\n        );\n        \n        if (distance <= radiusKm) {\n          results.push({ ...entity, distance });\n        }\n      }\n    }\n    \n    // Sort by distance\n    results.sort((a, b) => a.distance - b.distance);\n    \n    return results;\n  }\n  \n  // Haversine distance formula\n  haversineDistance(lat1, lon1, lat2, lon2) {\n    const R = 6371; // Earth radius in km\n    const dLat = (lat2 - lat1) * Math.PI / 180;\n    const dLon = (lon2 - lon1) * Math.PI / 180;\n    \n    const a = Math.sin(dLat/2) * Math.sin(dLat/2) +\n              Math.cos(lat1 * Math.PI / 180) * Math.cos(lat2 * Math.PI / 180) *\n              Math.sin(dLon/2) * Math.sin(dLon/2);\n    \n    const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n    return R * c;\n  }\n  \n  // K-nearest neighbors\n  async findKNearest(latitude, longitude, k = 10) {\n    let precision = this.precision;\n    let results = [];\n    \n    // Expand search area until k entities found\n    while (results.length < k && precision > 1) {\n      const geohash = Geohash.encode(latitude, longitude, precision);\n      const neighbors = Geohash.neighbors(geohash);\n      const searchHashes = [geohash, ...Object.values(neighbors)];\n      \n      for (const hash of searchHashes) {\n        const entities = await this.redis.zrange(`geo:${hash}`, 0, -1);\n        \n        for (const entityData of entities) {\n          const entity = JSON.parse(entityData);\n          const distance = this.haversineDistance(\n            latitude, longitude,\n            entity.latitude, entity.longitude\n          );\n          \n          results.push({ ...entity, distance });\n        }\n      }\n      \n      if (results.length < k) {\n        precision--; // Expand search area\n      }\n    }\n    \n    // Sort and return k nearest\n    results.sort((a, b) => a.distance - b.distance);\n    return results.slice(0, k);\n  }\n}\n\n// PostgreSQL with PostGIS for Complex Queries\nclass PostGISLocationService {\n  constructor(db) {\n    this.db = db;\n  }\n  \n  async createTables() {\n    await this.db.query(`\n      CREATE EXTENSION IF NOT EXISTS postgis;\n      \n      CREATE TABLE IF NOT EXISTS locations (\n        id SERIAL PRIMARY KEY,\n        entity_id VARCHAR(255) UNIQUE,\n        location GEOGRAPHY(POINT, 4326),\n        metadata JSONB,\n        updated_at TIMESTAMP DEFAULT NOW()\n      );\n      \n      CREATE INDEX idx_locations_geo ON locations USING GIST(location);\n      CREATE INDEX idx_locations_entity ON locations(entity_id);\n    `);\n  }\n  \n  async updateLocation(entityId, latitude, longitude, metadata = {}) {\n    await this.db.query(\n      `INSERT INTO locations (entity_id, location, metadata, updated_at)\n       VALUES ($1, ST_SetSRID(ST_MakePoint($2, $3), 4326), $4, NOW())\n       ON CONFLICT (entity_id)\n       DO UPDATE SET\n         location = ST_SetSRID(ST_MakePoint($2, $3), 4326),\n         metadata = $4,\n         updated_at = NOW()`,\n      [entityId, longitude, latitude, JSON.stringify(metadata)]\n    );\n  }\n  \n  async findNearby(latitude, longitude, radiusMeters, limit = 100) {\n    const result = await this.db.query(\n      `SELECT\n         entity_id,\n         ST_Y(location::geometry) AS latitude,\n         ST_X(location::geometry) AS longitude,\n         ST_Distance(location, ST_SetSRID(ST_MakePoint($1, $2), 4326)) AS distance,\n         metadata\n       FROM locations\n       WHERE ST_DWithin(\n         location,\n         ST_SetSRID(ST_MakePoint($1, $2), 4326),\n         $3\n       )\n       ORDER BY distance\n       LIMIT $4`,\n      [longitude, latitude, radiusMeters, limit]\n    );\n    \n    return result.rows;\n  }\n  \n  async findWithinPolygon(polygon) {\n    // polygon: array of [lat, lon] pairs\n    const wkt = `POLYGON((${polygon.map(p => `${p[1]} ${p[0]}`).join(', ')}))`;;\n    \n    const result = await this.db.query(\n      `SELECT\n         entity_id,\n         ST_Y(location::geometry) AS latitude,\n         ST_X(location::geometry) AS longitude,\n         metadata\n       FROM locations\n       WHERE ST_Contains(\n         ST_GeomFromText($1, 4326),\n         location::geometry\n       )`,\n      [wkt]\n    );\n    \n    return result.rows;\n  }\n  \n  async findAlongRoute(waypoints, corridorMeters = 100) {\n    // waypoints: array of [lat, lon]\n    const linestring = `LINESTRING(${waypoints.map(p => `${p[1]} ${p[0]}`).join(', ')})`;\n    \n    const result = await this.db.query(\n      `SELECT\n         entity_id,\n         ST_Y(location::geometry) AS latitude,\n         ST_X(location::geometry) AS longitude,\n         ST_Distance(location, ST_GeomFromText($1, 4326)) AS distance,\n         metadata\n       FROM locations\n       WHERE ST_DWithin(\n         location,\n         ST_GeomFromText($1, 4326),\n         $2\n       )\n       ORDER BY distance`,\n      [linestring, corridorMeters]\n    );\n    \n    return result.rows;\n  }\n}\n\n// Uber-like Driver Matching\nclass DriverMatchingService {\n  constructor(locationService) {\n    this.locationService = locationService;\n  }\n  \n  async findNearestDriver(pickupLat, pickupLon, maxDistanceKm = 5) {\n    // Find nearby drivers\n    const drivers = await this.locationService.findNearby(\n      pickupLat,\n      pickupLon,\n      maxDistanceKm\n    );\n    \n    // Filter available drivers\n    const availableDrivers = drivers.filter(d =>\n      JSON.parse(d.metadata || '{}').available === true\n    );\n    \n    if (availableDrivers.length === 0) {\n      return null;\n    }\n    \n    // Return closest available driver\n    return availableDrivers[0];\n  }\n  \n  async matchRide(rideRequest) {\n    const { pickupLat, pickupLon, dropoffLat, dropoffLon } = rideRequest;\n    \n    // Find drivers heading in similar direction\n    const drivers = await this.locationService.findKNearest(\n      pickupLat,\n      pickupLon,\n      20\n    );\n    \n    // Score drivers based on multiple factors\n    const scored = drivers.map(driver => {\n      const metadata = JSON.parse(driver.metadata || '{}');\n      \n      if (!metadata.available) {\n        return null;\n      }\n      \n      // Distance to pickup\n      const pickupDistance = driver.distance;\n      \n      // Direction compatibility\n      const driverHeading = metadata.heading || 0;\n      const rideHeading = this.calculateBearing(\n        pickupLat, pickupLon,\n        dropoffLat, dropoffLon\n      );\n      const headingDelta = Math.abs(driverHeading - rideHeading);\n      \n      // Combined score (lower is better)\n      const score = pickupDistance * 0.7 + (headingDelta / 180) * 0.3;\n      \n      return { driver, score };\n    }).filter(s => s !== null);\n    \n    // Sort by score\n    scored.sort((a, b) => a.score - b.score);\n    \n    return scored[0]?.driver || null;\n  }\n  \n  calculateBearing(lat1, lon1, lat2, lon2) {\n    const dLon = (lon2 - lon1) * Math.PI / 180;\n    const y = Math.sin(dLon) * Math.cos(lat2 * Math.PI / 180);\n    const x = Math.cos(lat1 * Math.PI / 180) * Math.sin(lat2 * Math.PI / 180) -\n              Math.sin(lat1 * Math.PI / 180) * Math.cos(lat2 * Math.PI / 180) * Math.cos(dLon);\n    \n    return (Math.atan2(y, x) * 180 / Math.PI + 360) % 360;\n  }\n}\n\n// Usage Example\nconst locationService = new LocationService(redis, db);\n\n// Update driver location\nawait locationService.updateLocation(\n  'driver-123',\n  37.7749, // lat\n  -122.4194 // lon\n);\n\n// Find nearby drivers\nconst nearbyDrivers = await locationService.findNearby(\n  37.7750,\n  -122.4195,\n  2 // 2km radius\n);\n\nconsole.log('Nearby drivers:', nearbyDrivers);\n\n// Find K nearest\nconst nearest10 = await locationService.findKNearest(\n  37.7750,\n  -122.4195,\n  10\n);\n\nconsole.log('10 nearest drivers:', nearest10);"
    },
    {
      "id": 93,
      "question": "Design a feature flag/toggle system for gradual rollouts and A/B testing",
      "answer": "A feature flag system controls feature visibility and rollouts dynamically without code deployments, enabling A/B testing, canary releases, and rapid rollback.\n\nCore Components:\n• Flag management service (centralized config)\n• Client SDKs for feature evaluation\n• Targeting rules (user attributes, percentage rollout)\n• Real-time flag updates (WebSocket/SSE)\n• Analytics and metrics tracking\n• Audit logs for flag changes\n• Flag lifecycle management\n\nFlag Types:\n• Boolean: Simple on/off toggle\n• Multivariate: Multiple variants (A/B/C testing)\n• Percentage rollout: Gradual user adoption\n• User-targeted: Specific users or groups\n• Kill switch: Emergency feature disable\n\nEvaluation:\n• Server-side evaluation for security\n• Client-side for performance\n• Sticky bucketing for consistent experience\n• Default values for fallback\n• Caching with TTL\n\nIntegration:\n• CI/CD pipeline integration\n• Automated cleanup of stale flags\n• Feature flag best practices enforcement\n• Documentation and governance",
      "explanation": "Feature flag systems use centralized configuration with targeting rules to control feature visibility per user/group, enabling gradual rollouts, A/B testing, and instant rollback without deployments through real-time flag evaluation and consistent user bucketing.",
      "difficulty": "Hard",
      "code": "// Feature Flag Service\nclass FeatureFlagService {\n  constructor(redis, db) {\n    this.redis = redis;\n    this.db = db;\n    this.cache = new LRUCache({ max: 1000, ttl: 60000 }); // 1 min\n  }\n  \n  async createFlag(flag) {\n    const flagData = {\n      key: flag.key,\n      name: flag.name,\n      description: flag.description,\n      environment: flag.environment || 'production',\n      enabled: flag.enabled !== false,\n      type: flag.type || 'boolean', // boolean, multivariate, percentage\n      variants: flag.variants || [],\n      rules: flag.rules || [],\n      defaultValue: flag.defaultValue,\n      createdAt: new Date(),\n      createdBy: flag.createdBy\n    };\n    \n    // Store in database\n    await this.db.insertFlag(flagData);\n    \n    // Update cache\n    await this.redis.set(\n      `flag:${flag.environment}:${flag.key}`,\n      JSON.stringify(flagData),\n      'EX',\n      3600\n    );\n    \n    // Publish update to subscribers\n    await this.redis.publish('flag:updates', JSON.stringify({\n      type: 'created',\n      flag: flagData\n    }));\n    \n    return flagData;\n  }\n  \n  async evaluateFlag(flagKey, context, environment = 'production') {\n    // Get flag configuration\n    const flag = await this.getFlag(flagKey, environment);\n    \n    if (!flag) {\n      return flag.defaultValue;\n    }\n    \n    if (!flag.enabled) {\n      return flag.defaultValue;\n    }\n    \n    // Evaluate targeting rules\n    for (const rule of flag.rules) {\n      if (this.evaluateRule(rule, context)) {\n        return this.getVariant(flag, context, rule.variant);\n      }\n    }\n    \n    // Default variant\n    return this.getVariant(flag, context);\n  }\n  \n  evaluateRule(rule, context) {\n    // User targeting\n    if (rule.userIds && rule.userIds.includes(context.userId)) {\n      return true;\n    }\n    \n    // Group targeting\n    if (rule.groups && rule.groups.some(g => context.groups?.includes(g))) {\n      return true;\n    }\n    \n    // Attribute matching\n    if (rule.attributes) {\n      for (const [key, condition] of Object.entries(rule.attributes)) {\n        if (!this.matchesCondition(context[key], condition)) {\n          return false;\n        }\n      }\n      return true;\n    }\n    \n    // Percentage rollout\n    if (rule.percentage !== undefined) {\n      const bucket = this.getBucket(context.userId, rule.salt || '');\n      return bucket < rule.percentage;\n    }\n    \n    return false;\n  }\n  \n  matchesCondition(value, condition) {\n    const { operator, operand } = condition;\n    \n    switch (operator) {\n      case 'equals':\n        return value === operand;\n      case 'not_equals':\n        return value !== operand;\n      case 'contains':\n        return String(value).includes(operand);\n      case 'starts_with':\n        return String(value).startsWith(operand);\n      case 'ends_with':\n        return String(value).endsWith(operand);\n      case 'greater_than':\n        return value > operand;\n      case 'less_than':\n        return value < operand;\n      case 'in':\n        return operand.includes(value);\n      case 'regex':\n        return new RegExp(operand).test(String(value));\n      default:\n        return false;\n    }\n  }\n  \n  getBucket(userId, salt) {\n    // Consistent hashing for sticky bucketing\n    const hash = crypto\n      .createHash('md5')\n      .update(`${userId}${salt}`)\n      .digest('hex');\n    \n    // Convert first 8 chars to int and normalize to 0-100\n    const num = parseInt(hash.substring(0, 8), 16);\n    return (num % 100);\n  }\n  \n  getVariant(flag, context, variantKey) {\n    if (flag.type === 'boolean') {\n      return variantKey !== 'off';\n    }\n    \n    if (flag.type === 'multivariate') {\n      if (variantKey) {\n        const variant = flag.variants.find(v => v.key === variantKey);\n        return variant?.value || flag.defaultValue;\n      }\n      \n      // Distribute traffic across variants\n      const bucket = this.getBucket(context.userId, flag.key);\n      let cumulative = 0;\n      \n      for (const variant of flag.variants) {\n        cumulative += variant.weight || 0;\n        if (bucket < cumulative) {\n          return variant.value;\n        }\n      }\n    }\n    \n    return flag.defaultValue;\n  }\n  \n  async getFlag(key, environment) {\n    // Check local cache\n    const cacheKey = `${environment}:${key}`;\n    let flag = this.cache.get(cacheKey);\n    if (flag) return flag;\n    \n    // Check Redis\n    const cached = await this.redis.get(`flag:${cacheKey}`);\n    if (cached) {\n      flag = JSON.parse(cached);\n      this.cache.set(cacheKey, flag);\n      return flag;\n    }\n    \n    // Load from database\n    flag = await this.db.getFlag(key, environment);\n    \n    if (flag) {\n      await this.redis.set(\n        `flag:${cacheKey}`,\n        JSON.stringify(flag),\n        'EX',\n        3600\n      );\n      this.cache.set(cacheKey, flag);\n    }\n    \n    return flag;\n  }\n}\n\n// Client SDK\nclass FeatureFlagClient {\n  constructor(apiUrl, apiKey) {\n    this.apiUrl = apiUrl;\n    this.apiKey = apiKey;\n    this.flags = new Map();\n    this.context = {};\n    \n    this.setupRealtimeUpdates();\n  }\n  \n  setContext(context) {\n    this.context = { ...this.context, ...context };\n  }\n  \n  async initialize() {\n    // Fetch all flags\n    const response = await fetch(`${this.apiUrl}/flags`, {\n      headers: { 'Authorization': `Bearer ${this.apiKey}` }\n    });\n    \n    const flags = await response.json();\n    \n    flags.forEach(flag => {\n      this.flags.set(flag.key, flag);\n    });\n  }\n  \n  setupRealtimeUpdates() {\n    const ws = new WebSocket(`${this.apiUrl}/stream`);\n    \n    ws.onmessage = (event) => {\n      const update = JSON.parse(event.data);\n      \n      if (update.type === 'flag:updated') {\n        this.flags.set(update.flag.key, update.flag);\n      } else if (update.type === 'flag:deleted') {\n        this.flags.delete(update.flagKey);\n      }\n    };\n  }\n  \n  isEnabled(flagKey, defaultValue = false) {\n    const flag = this.flags.get(flagKey);\n    \n    if (!flag) return defaultValue;\n    \n    return this.evaluate(flag);\n  }\n  \n  getVariant(flagKey, defaultValue = null) {\n    const flag = this.flags.get(flagKey);\n    \n    if (!flag) return defaultValue;\n    \n    return this.evaluate(flag);\n  }\n  \n  evaluate(flag) {\n    if (!flag.enabled) return flag.defaultValue;\n    \n    // Evaluate rules client-side (if safe)\n    for (const rule of flag.rules) {\n      if (this.evaluateRule(rule)) {\n        return this.getVariantValue(flag, rule.variant);\n      }\n    }\n    \n    return this.getVariantValue(flag);\n  }\n  \n  evaluateRule(rule) {\n    // Simplified client-side evaluation\n    if (rule.userIds && this.context.userId) {\n      return rule.userIds.includes(this.context.userId);\n    }\n    \n    if (rule.percentage !== undefined && this.context.userId) {\n      const bucket = this.getBucket(this.context.userId, rule.salt || '');\n      return bucket < rule.percentage;\n    }\n    \n    return false;\n  }\n  \n  getBucket(userId, salt) {\n    // Simple hash function for demo\n    let hash = 0;\n    const str = `${userId}${salt}`;\n    \n    for (let i = 0; i < str.length; i++) {\n      hash = ((hash << 5) - hash) + str.charCodeAt(i);\n      hash = hash & hash;\n    }\n    \n    return Math.abs(hash % 100);\n  }\n}\n\n// A/B Testing Integration\nclass ABTestingService {\n  constructor(flagService, analyticsService) {\n    this.flagService = flagService;\n    this.analytics = analyticsService;\n  }\n  \n  async createExperiment(experiment) {\n    const flag = await this.flagService.createFlag({\n      key: experiment.key,\n      name: experiment.name,\n      type: 'multivariate',\n      variants: experiment.variants.map((v, i) => ({\n        key: v.key,\n        value: v.value,\n        weight: v.traffic || (100 / experiment.variants.length)\n      })),\n      rules: [{\n        percentage: experiment.traffic || 100,\n        salt: experiment.key\n      }]\n    });\n    \n    return {\n      id: experiment.key,\n      flag,\n      startDate: new Date(),\n      status: 'running'\n    };\n  }\n  \n  async trackExperimentExposure(experimentKey, userId, variant) {\n    await this.analytics.track({\n      event: 'experiment_exposure',\n      userId,\n      properties: {\n        experimentKey,\n        variant\n      }\n    });\n  }\n  \n  async trackConversion(experimentKey, userId, metricName, value = 1) {\n    await this.analytics.track({\n      event: 'experiment_conversion',\n      userId,\n      properties: {\n        experimentKey,\n        metricName,\n        value\n      }\n    });\n  }\n  \n  async getExperimentResults(experimentKey) {\n    const exposures = await this.analytics.query({\n      event: 'experiment_exposure',\n      groupBy: 'variant',\n      filters: { experimentKey }\n    });\n    \n    const conversions = await this.analytics.query({\n      event: 'experiment_conversion',\n      groupBy: 'variant',\n      aggregation: 'sum',\n      property: 'value',\n      filters: { experimentKey }\n    });\n    \n    const results = {};\n    \n    for (const [variant, exposure] of Object.entries(exposures)) {\n      const conversion = conversions[variant] || 0;\n      \n      results[variant] = {\n        exposures: exposure,\n        conversions: conversion,\n        conversionRate: conversion / exposure,\n        uplift: this.calculateUplift(variant, results)\n      };\n    }\n    \n    return results;\n  }\n}\n\n// Express Middleware\nfunction featureFlagMiddleware(flagClient) {\n  return async (req, res, next) => {\n    // Set context from request\n    flagClient.setContext({\n      userId: req.user?.id,\n      email: req.user?.email,\n      groups: req.user?.groups || [],\n      country: req.headers['cf-ipcountry'],\n      userAgent: req.headers['user-agent']\n    });\n    \n    // Add helper methods to request\n    req.isEnabled = (flagKey, defaultValue) =>\n      flagClient.isEnabled(flagKey, defaultValue);\n    \n    req.getVariant = (flagKey, defaultValue) =>\n      flagClient.getVariant(flagKey, defaultValue);\n    \n    next();\n  };\n}\n\n// Usage Example\nconst flagService = new FeatureFlagService(redis, db);\n\n// Create a feature flag\nawait flagService.createFlag({\n  key: 'new-checkout',\n  name: 'New Checkout Flow',\n  description: 'Redesigned checkout experience',\n  type: 'boolean',\n  enabled: true,\n  rules: [\n    {\n      // Target beta users\n      groups: ['beta'],\n      variant: 'on'\n    },\n    {\n      // 10% rollout to everyone else\n      percentage: 10,\n      variant: 'on'\n    }\n  ],\n  defaultValue: false\n});\n\n// In application code\napp.get('/checkout', async (req, res) => {\n  const useNewCheckout = await flagService.evaluateFlag(\n    'new-checkout',\n    { userId: req.user.id, groups: req.user.groups }\n  );\n  \n  if (useNewCheckout) {\n    res.render('checkout-v2');\n  } else {\n    res.render('checkout-v1');\n  }\n});\n\n// Create A/B test\nconst abTesting = new ABTestingService(flagService, analytics);\n\nconst experiment = await abTesting.createExperiment({\n  key: 'pricing-test',\n  name: 'Pricing Page Test',\n  variants: [\n    { key: 'control', value: 'original' },\n    { key: 'treatment', value: 'new-design' }\n  ],\n  traffic: 50 // 50% of users\n});\n\n// Track exposure\nconst variant = await flagService.evaluateFlag(\n  'pricing-test',\n  { userId: req.user.id }\n);\n\nawait abTesting.trackExperimentExposure('pricing-test', req.user.id, variant);\n\n// Track conversion\nawait abTesting.trackConversion('pricing-test', req.user.id, 'purchase', 99.99);\n\n// Get results\nconst results = await abTesting.getExperimentResults('pricing-test');\nconsole.log('Experiment results:', results);"
    },
    {
      "id": 94,
      "question": "Design a disaster recovery and backup system for critical data",
      "answer": "A disaster recovery system ensures business continuity through backup strategies, replication, failover procedures, and tested recovery plans.\n\nBackup Strategy:\n• Full backups: Complete data copy (weekly)\n• Incremental backups: Changed data since last backup (daily)\n• Differential backups: Changed data since last full backup\n• Continuous backup: Real-time replication\n• Snapshot-based backups: Point-in-time copies\n• 3-2-1 rule: 3 copies, 2 media types, 1 offsite\n\nRecovery Metrics:\n• RTO (Recovery Time Objective): Max acceptable downtime\n• RPO (Recovery Point Objective): Max acceptable data loss\n• RLO (Recovery Level Objective): Granularity of recovery\n\nReplication:\n• Synchronous: Zero data loss, higher latency\n• Asynchronous: Minimal latency, potential data loss\n• Multi-region replication for geographic redundancy\n• Active-active or active-passive configurations\n\nTesting:\n• Regular disaster recovery drills\n• Automated restore testing\n• Runbook documentation\n• Chaos engineering\n• Recovery verification and validation",
      "explanation": "Disaster recovery uses multi-layered backup strategies (full/incremental), geographic replication, defined RTO/RPO targets, automated failover procedures, and regular testing to ensure critical data can be recovered quickly after failures or disasters.",
      "difficulty": "Hard",
      "code": "// Backup Management System\nconst AWS = require('aws-sdk');\nconst cron = require('node-cron');\n\nclass BackupManager {\n  constructor(config) {\n    this.s3 = new AWS.S3();\n    this.rds = new AWS.RDS();\n    this.dynamodb = new AWS.DynamoDB();\n    \n    this.backupBucket = config.backupBucket;\n    this.retentionDays = config.retentionDays || 30;\n    this.archiveDays = config.archiveDays || 90;\n  }\n  \n  // Schedule automated backups\n  scheduleBackups() {\n    // Full backup weekly (Sunday 2 AM)\n    cron.schedule('0 2 * * 0', async () => {\n      await this.performFullBackup();\n    });\n    \n    // Incremental backup daily (2 AM)\n    cron.schedule('0 2 * * 1-6', async () => {\n      await this.performIncrementalBackup();\n    });\n    \n    // Continuous backup every 5 minutes\n    cron.schedule('*/5 * * * *', async () => {\n      await this.performContinuousBackup();\n    });\n    \n    // Cleanup old backups daily (3 AM)\n    cron.schedule('0 3 * * *', async () => {\n      await this.cleanupOldBackups();\n    });\n  }\n  \n  async performFullBackup() {\n    console.log('Starting full backup...');\n    const timestamp = new Date().toISOString();\n    const backupId = `full-${timestamp}`;\n    \n    try {\n      // Backup databases\n      await this.backupRDS(backupId);\n      await this.backupDynamoDB(backupId);\n      \n      // Backup files\n      await this.backupFiles(backupId);\n      \n      // Store metadata\n      await this.storeBackupMetadata({\n        backupId,\n        type: 'full',\n        timestamp,\n        status: 'completed',\n        size: await this.calculateBackupSize(backupId)\n      });\n      \n      console.log(`Full backup completed: ${backupId}`);\n    } catch (error) {\n      console.error('Full backup failed:', error);\n      await this.notifyBackupFailure(backupId, error);\n    }\n  }\n  \n  async backupRDS(backupId) {\n    const dbInstances = await this.rds.describeDBInstances().promise();\n    \n    for (const db of dbInstances.DBInstances) {\n      // Create RDS snapshot\n      await this.rds.createDBSnapshot({\n        DBSnapshotIdentifier: `${backupId}-${db.DBInstanceIdentifier}`,\n        DBInstanceIdentifier: db.DBInstanceIdentifier,\n        Tags: [\n          { Key: 'BackupId', Value: backupId },\n          { Key: 'Type', Value: 'automated' },\n          { Key: 'Timestamp', Value: new Date().toISOString() }\n        ]\n      }).promise();\n      \n      console.log(`RDS snapshot created: ${db.DBInstanceIdentifier}`);\n      \n      // Export to S3 for long-term storage\n      await this.exportSnapshotToS3(\n        `${backupId}-${db.DBInstanceIdentifier}`,\n        `${this.backupBucket}/rds/${backupId}`\n      );\n    }\n  }\n  \n  async backupDynamoDB(backupId) {\n    const tables = await this.dynamodb.listTables().promise();\n    \n    for (const table of tables.TableNames) {\n      // Create DynamoDB backup\n      await this.dynamodb.createBackup({\n        TableName: table,\n        BackupName: `${backupId}-${table}`\n      }).promise();\n      \n      console.log(`DynamoDB backup created: ${table}`);\n      \n      // Export to S3\n      await this.exportDynamoDBToS3(table, `${this.backupBucket}/dynamodb/${backupId}`);\n    }\n  }\n  \n  async backupFiles(backupId) {\n    // Backup application files, configs, logs\n    const filesToBackup = [\n      '/app/config',\n      '/app/uploads',\n      '/app/logs'\n    ];\n    \n    for (const path of filesToBackup) {\n      await this.uploadToS3(\n        path,\n        `${this.backupBucket}/files/${backupId}`,\n        { recursive: true }\n      );\n    }\n  }\n  \n  async performIncrementalBackup() {\n    console.log('Starting incremental backup...');\n    const timestamp = new Date().toISOString();\n    const backupId = `incremental-${timestamp}`;\n    \n    // Get last full backup\n    const lastFullBackup = await this.getLastFullBackup();\n    \n    if (!lastFullBackup) {\n      console.log('No full backup found, performing full backup');\n      return this.performFullBackup();\n    }\n    \n    try {\n      // Backup only changed data since last backup\n      await this.backupChangedData(backupId, lastFullBackup.timestamp);\n      \n      await this.storeBackupMetadata({\n        backupId,\n        type: 'incremental',\n        timestamp,\n        baseBackup: lastFullBackup.backupId,\n        status: 'completed'\n      });\n      \n      console.log(`Incremental backup completed: ${backupId}`);\n    } catch (error) {\n      console.error('Incremental backup failed:', error);\n    }\n  }\n  \n  async cleanupOldBackups() {\n    console.log('Cleaning up old backups...');\n    \n    const cutoffDate = new Date();\n    cutoffDate.setDate(cutoffDate.getDate() - this.retentionDays);\n    \n    const archiveDate = new Date();\n    archiveDate.setDate(archiveDate.getDate() - this.archiveDays);\n    \n    const backups = await this.listAllBackups();\n    \n    for (const backup of backups) {\n      const backupDate = new Date(backup.timestamp);\n      \n      if (backupDate < archiveDate) {\n        // Delete very old backups\n        await this.deleteBackup(backup.backupId);\n        console.log(`Deleted backup: ${backup.backupId}`);\n      } else if (backupDate < cutoffDate) {\n        // Move to glacier for archival\n        await this.archiveBackup(backup.backupId);\n        console.log(`Archived backup: ${backup.backupId}`);\n      }\n    }\n  }\n  \n  async archiveBackup(backupId) {\n    // Move to Glacier for long-term storage\n    await this.s3.copyObject({\n      Bucket: this.backupBucket,\n      CopySource: `${this.backupBucket}/${backupId}`,\n      Key: `archive/${backupId}`,\n      StorageClass: 'GLACIER'\n    }).promise();\n  }\n}\n\n// Disaster Recovery Orchestrator\nclass DisasterRecoveryOrchestrator {\n  constructor(config) {\n    this.primaryRegion = config.primaryRegion;\n    this.drRegion = config.drRegion;\n    this.rto = config.rto || 3600; // 1 hour\n    this.rpo = config.rpo || 300; // 5 minutes\n    \n    this.route53 = new AWS.Route53();\n    this.ec2 = new AWS.EC2();\n    this.rds = new AWS.RDS();\n  }\n  \n  // Initiate failover to DR site\n  async initiateFailover(reason) {\n    console.log(`Initiating failover: ${reason}`);\n    const failoverId = `failover-${Date.now()}`;\n    \n    try {\n      // 1. Verify DR site readiness\n      await this.verifyDRSite();\n      \n      // 2. Stop writes to primary\n      await this.stopPrimaryWrites();\n      \n      // 3. Promote DR database replicas\n      await this.promoteDRReplicas();\n      \n      // 4. Start application servers in DR region\n      await this.startDRApplications();\n      \n      // 5. Update DNS to point to DR site\n      await this.updateDNS();\n      \n      // 6. Verify DR site is operational\n      await this.verifyDROperational();\n      \n      // 7. Notify stakeholders\n      await this.notifyFailoverComplete(failoverId);\n      \n      console.log('Failover completed successfully');\n      return { success: true, failoverId };\n    } catch (error) {\n      console.error('Failover failed:', error);\n      await this.notifyFailoverFailure(failoverId, error);\n      throw error;\n    }\n  }\n  \n  async verifyDRSite() {\n    console.log('Verifying DR site...');\n    \n    // Check RDS replicas\n    const replicas = await this.rds.describeDBInstances({\n      Filters: [\n        { Name: 'db-instance-id', Values: ['dr-*'] }\n      ]\n    }).promise();\n    \n    for (const replica of replicas.DBInstances) {\n      if (replica.DBInstanceStatus !== 'available') {\n        throw new Error(`Replica ${replica.DBInstanceIdentifier} not available`);\n      }\n      \n      // Check replication lag\n      const lag = await this.getReplicationLag(replica.DBInstanceIdentifier);\n      if (lag > this.rpo) {\n        throw new Error(`Replication lag (${lag}s) exceeds RPO (${this.rpo}s)`);\n      }\n    }\n    \n    // Check application servers\n    const instances = await this.ec2.describeInstances({\n      Filters: [\n        { Name: 'tag:Environment', Values: ['dr'] },\n        { Name: 'instance-state-name', Values: ['stopped', 'running'] }\n      ]\n    }).promise();\n    \n    if (instances.Reservations.length === 0) {\n      throw new Error('No DR instances found');\n    }\n  }\n  \n  async promoteDRReplicas() {\n    console.log('Promoting DR replicas...');\n    \n    const replicas = await this.rds.describeDBInstances({\n      Filters: [{ Name: 'db-instance-id', Values: ['dr-*'] }]\n    }).promise();\n    \n    for (const replica of replicas.DBInstances) {\n      // Promote read replica to standalone instance\n      await this.rds.promoteReadReplica({\n        DBInstanceIdentifier: replica.DBInstanceIdentifier,\n        BackupRetentionPeriod: 7\n      }).promise();\n      \n      console.log(`Promoted replica: ${replica.DBInstanceIdentifier}`);\n      \n      // Wait for promotion to complete\n      await this.waitForDBInstance(replica.DBInstanceIdentifier);\n    }\n  }\n  \n  async startDRApplications() {\n    console.log('Starting DR applications...');\n    \n    const instances = await this.ec2.describeInstances({\n      Filters: [\n        { Name: 'tag:Environment', Values: ['dr'] },\n        { Name: 'instance-state-name', Values: ['stopped'] }\n      ]\n    }).promise();\n    \n    const instanceIds = instances.Reservations\n      .flatMap(r => r.Instances)\n      .map(i => i.InstanceId);\n    \n    if (instanceIds.length > 0) {\n      await this.ec2.startInstances({\n        InstanceIds: instanceIds\n      }).promise();\n      \n      // Wait for instances to be running\n      await this.ec2.waitFor('instanceRunning', {\n        InstanceIds: instanceIds\n      }).promise();\n      \n      console.log(`Started ${instanceIds.length} DR instances`);\n    }\n  }\n  \n  async updateDNS() {\n    console.log('Updating DNS...');\n    \n    // Update Route53 to point to DR load balancer\n    const drLoadBalancer = await this.getDRLoadBalancer();\n    \n    await this.route53.changeResourceRecordSets({\n      HostedZoneId: process.env.HOSTED_ZONE_ID,\n      ChangeBatch: {\n        Changes: [{\n          Action: 'UPSERT',\n          ResourceRecordSet: {\n            Name: 'api.example.com',\n            Type: 'A',\n            AliasTarget: {\n              HostedZoneId: drLoadBalancer.CanonicalHostedZoneId,\n              DNSName: drLoadBalancer.DNSName,\n              EvaluateTargetHealth: true\n            }\n          }\n        }]\n      }\n    }).promise();\n    \n    console.log('DNS updated to DR site');\n  }\n  \n  async verifyDROperational() {\n    console.log('Verifying DR site operational...');\n    \n    // Health check DR site\n    const maxRetries = 10;\n    let retries = 0;\n    \n    while (retries < maxRetries) {\n      try {\n        const response = await fetch('https://api.example.com/health');\n        if (response.ok) {\n          console.log('DR site is operational');\n          return;\n        }\n      } catch (error) {\n        console.log(`Health check failed, retry ${retries + 1}/${maxRetries}`);\n      }\n      \n      retries++;\n      await new Promise(resolve => setTimeout(resolve, 5000));\n    }\n    \n    throw new Error('DR site health checks failed');\n  }\n}\n\n// Recovery Testing\nclass RecoveryTester {\n  constructor(backupManager, drOrchestrator) {\n    this.backupManager = backupManager;\n    this.drOrchestrator = drOrchestrator;\n  }\n  \n  // Schedule regular DR drills\n  scheduleDRDrills() {\n    // Monthly DR drill (first Sunday of month, 2 AM)\n    cron.schedule('0 2 1-7 * 0', async () => {\n      await this.performDRDrill();\n    });\n  }\n  \n  async performDRDrill() {\n    console.log('Starting DR drill...');\n    const drillId = `drill-${Date.now()}`;\n    \n    try {\n      // 1. Restore backup to test environment\n      const backup = await this.backupManager.getLatestBackup();\n      await this.restoreToTestEnvironment(backup);\n      \n      // 2. Simulate failover\n      await this.drOrchestrator.initiateFailover('DR drill');\n      \n      // 3. Run automated tests\n      const testResults = await this.runValidationTests();\n      \n      // 4. Measure RTO/RPO\n      const metrics = await this.measureRecoveryMetrics(drillId);\n      \n      // 5. Generate report\n      await this.generateDRDrillReport(drillId, testResults, metrics);\n      \n      console.log('DR drill completed successfully');\n    } catch (error) {\n      console.error('DR drill failed:', error);\n      await this.notifyDRDrillFailure(drillId, error);\n    } finally {\n      // Cleanup test environment\n      await this.cleanupTestEnvironment();\n    }\n  }\n}\n\n// Usage\nconst backupManager = new BackupManager({\n  backupBucket: 'my-app-backups',\n  retentionDays: 30,\n  archiveDays: 90\n});\n\nbackupManager.scheduleBackups();\n\nconst drOrchestrator = new DisasterRecoveryOrchestrator({\n  primaryRegion: 'us-east-1',\n  drRegion: 'us-west-2',\n  rto: 3600, // 1 hour\n  rpo: 300 // 5 minutes\n});\n\n// Initiate failover when needed\nawait drOrchestrator.initiateFailover('Primary region outage');\n\nconst recoveryTester = new RecoveryTester(backupManager, drOrchestrator);\nrecoveryTester.scheduleDRDrills();"
    },
    {
      "id": 95,
      "question": "Design a service mesh architecture like Istio or Linkerd",
      "answer": "A service mesh provides service-to-service communication infrastructure with traffic management, security, and observability for microservices.\n\nCore Components:\n• Data plane: Sidecar proxies (Envoy) for each service\n• Control plane: Configuration and policy management\n• Service discovery and load balancing\n• Traffic routing and splitting\n• Circuit breaking and retries\n• mTLS for service-to-service encryption\n• Distributed tracing and metrics\n\nTraffic Management:\n• Intelligent routing (header-based, weight-based)\n• Traffic splitting for canary deployments\n• Fault injection for chaos testing\n• Timeout and retry policies\n• Circuit breakers for resilience\n• Rate limiting per service\n\nSecurity:\n• Automatic mTLS between services\n• Certificate rotation\n• Authorization policies\n• Request authentication\n• Security policy enforcement\n\nObservability:\n• Distributed tracing (Jaeger/Zipkin)\n• Metrics collection (Prometheus)\n• Access logs\n• Service topology visualization\n• Golden signals monitoring",
      "explanation": "Service mesh uses sidecar proxies deployed alongside each service to intercept all network traffic, providing transparent traffic management, mTLS security, observability, and resilience patterns without changing application code.",
      "difficulty": "Hard",
      "code": "// Service Mesh Sidecar Proxy (Simplified Envoy-like)\nclass ServiceMeshProxy {\n  constructor(serviceName, config) {\n    this.serviceName = serviceName;\n    this.config = config;\n    this.upstreamServices = new Map();\n    this.circuitBreakers = new Map();\n    this.metrics = {\n      requests: 0,\n      errors: 0,\n      latency: []\n    };\n  }\n  \n  // Intercept outbound requests\n  async handleOutboundRequest(request) {\n    const targetService = request.headers['x-target-service'];\n    const startTime = Date.now();\n    \n    try {\n      // 1. Service discovery\n      const endpoint = await this.discoverService(targetService);\n      \n      // 2. Load balancing\n      const instance = this.selectInstance(endpoint.instances);\n      \n      // 3. Circuit breaker check\n      if (this.isCircuitOpen(targetService)) {\n        throw new Error(`Circuit breaker open for ${targetService}`);\n      }\n      \n      // 4. Add tracing headers\n      this.injectTracingHeaders(request);\n      \n      // 5. Add mTLS\n      const secureRequest = await this.addMTLS(request, targetService);\n      \n      // 6. Apply retry policy\n      const response = await this.retryableRequest(instance, secureRequest);\n      \n      // 7. Record metrics\n      this.recordSuccess(targetService, Date.now() - startTime);\n      \n      return response;\n    } catch (error) {\n      this.recordFailure(targetService, error);\n      this.updateCircuitBreaker(targetService, false);\n      throw error;\n    }\n  }\n  \n  async discoverService(serviceName) {\n    // Query service registry\n    const response = await fetch(\n      `${this.config.controlPlane}/v1/discovery/services/${serviceName}`\n    );\n    \n    return response.json();\n  }\n  \n  selectInstance(instances) {\n    // Load balancing strategy: Round robin\n    const healthy = instances.filter(i => i.healthy);\n    \n    if (healthy.length === 0) {\n      throw new Error('No healthy instances available');\n    }\n    \n    const key = `lb:${this.serviceName}`;\n    const current = this.upstreamServices.get(key) || 0;\n    const selected = healthy[current % healthy.length];\n    \n    this.upstreamServices.set(key, current + 1);\n    \n    return selected;\n  }\n  \n  isCircuitOpen(serviceName) {\n    const breaker = this.circuitBreakers.get(serviceName);\n    \n    if (!breaker) return false;\n    \n    if (breaker.state === 'open') {\n      // Check if should try half-open\n      if (Date.now() - breaker.openedAt > breaker.timeout) {\n        breaker.state = 'half-open';\n        return false;\n      }\n      return true;\n    }\n    \n    return false;\n  }\n  \n  updateCircuitBreaker(serviceName, success) {\n    let breaker = this.circuitBreakers.get(serviceName);\n    \n    if (!breaker) {\n      breaker = {\n        failures: 0,\n        threshold: 5,\n        timeout: 30000, // 30 seconds\n        state: 'closed'\n      };\n      this.circuitBreakers.set(serviceName, breaker);\n    }\n    \n    if (success) {\n      breaker.failures = 0;\n      if (breaker.state === 'half-open') {\n        breaker.state = 'closed';\n      }\n    } else {\n      breaker.failures++;\n      \n      if (breaker.failures >= breaker.threshold) {\n        breaker.state = 'open';\n        breaker.openedAt = Date.now();\n        console.log(`Circuit breaker opened for ${serviceName}`);\n      }\n    }\n  }\n  \n  injectTracingHeaders(request) {\n    // Generate or propagate trace context\n    if (!request.headers['x-trace-id']) {\n      request.headers['x-trace-id'] = this.generateTraceId();\n      request.headers['x-span-id'] = this.generateSpanId();\n    } else {\n      // Create child span\n      request.headers['x-parent-span-id'] = request.headers['x-span-id'];\n      request.headers['x-span-id'] = this.generateSpanId();\n    }\n    \n    request.headers['x-service-name'] = this.serviceName;\n  }\n  \n  async addMTLS(request, targetService) {\n    // Get certificates from control plane\n    const cert = await this.getCertificate(this.serviceName);\n    const targetCert = await this.getCertificate(targetService);\n    \n    // Create TLS context\n    request.tls = {\n      cert: cert.certificate,\n      key: cert.privateKey,\n      ca: targetCert.rootCA\n    };\n    \n    return request;\n  }\n  \n  async retryableRequest(instance, request) {\n    const maxRetries = this.config.maxRetries || 3;\n    const backoffBase = this.config.retryBackoff || 100;\n    \n    let lastError;\n    \n    for (let attempt = 0; attempt <= maxRetries; attempt++) {\n      try {\n        const response = await this.sendRequest(instance, request);\n        \n        // Don't retry on success\n        if (response.status < 500) {\n          return response;\n        }\n        \n        lastError = new Error(`Server error: ${response.status}`);\n      } catch (error) {\n        lastError = error;\n      }\n      \n      if (attempt < maxRetries) {\n        // Exponential backoff\n        const delay = backoffBase * Math.pow(2, attempt);\n        await new Promise(resolve => setTimeout(resolve, delay));\n      }\n    }\n    \n    throw lastError;\n  }\n  \n  recordSuccess(service, latency) {\n    this.metrics.requests++;\n    this.metrics.latency.push(latency);\n    this.updateCircuitBreaker(service, true);\n    \n    // Send to metrics collector\n    this.sendMetrics({\n      service: this.serviceName,\n      target: service,\n      status: 'success',\n      latency\n    });\n  }\n  \n  recordFailure(service, error) {\n    this.metrics.requests++;\n    this.metrics.errors++;\n    \n    this.sendMetrics({\n      service: this.serviceName,\n      target: service,\n      status: 'error',\n      error: error.message\n    });\n  }\n}\n\n// Service Mesh Control Plane\nclass ServiceMeshControlPlane {\n  constructor() {\n    this.services = new Map();\n    this.policies = new Map();\n    this.certificates = new Map();\n  }\n  \n  // Service registration\n  async registerService(service) {\n    this.services.set(service.name, {\n      name: service.name,\n      instances: [],\n      config: service.config\n    });\n  }\n  \n  async registerInstance(serviceName, instance) {\n    const service = this.services.get(serviceName);\n    \n    if (!service) {\n      throw new Error(`Service ${serviceName} not found`);\n    }\n    \n    service.instances.push({\n      id: instance.id,\n      address: instance.address,\n      port: instance.port,\n      healthy: true,\n      metadata: instance.metadata,\n      registeredAt: Date.now()\n    });\n    \n    // Push config to sidecars\n    await this.pushConfigToProxies(serviceName);\n  }\n  \n  // Traffic management policies\n  async applyTrafficPolicy(serviceName, policy) {\n    this.policies.set(serviceName, policy);\n    \n    // Example policy:\n    // {\n    //   routes: [\n    //     {\n    //       match: { headers: { 'version': 'v2' } },\n    //       destination: { subset: 'v2', weight: 100 }\n    //     },\n    //     {\n    //       destination: { subset: 'v1', weight: 100 }\n    //     }\n    //   ],\n    //   retries: { attempts: 3, perTryTimeout: '2s' },\n    //   timeout: '10s'\n    // }\n    \n    await this.pushConfigToProxies(serviceName);\n  }\n  \n  // mTLS certificate management\n  async issueCertificate(serviceName) {\n    // Generate certificate for service\n    const cert = await this.generateCertificate(serviceName);\n    \n    this.certificates.set(serviceName, {\n      certificate: cert.cert,\n      privateKey: cert.key,\n      rootCA: cert.ca,\n      expiresAt: Date.now() + (365 * 24 * 60 * 60 * 1000), // 1 year\n      createdAt: Date.now()\n    });\n    \n    // Schedule rotation\n    this.scheduleRotation(serviceName);\n    \n    return this.certificates.get(serviceName);\n  }\n  \n  scheduleRotation(serviceName) {\n    const cert = this.certificates.get(serviceName);\n    const rotationTime = cert.expiresAt - (30 * 24 * 60 * 60 * 1000); // 30 days before expiry\n    \n    setTimeout(async () => {\n      console.log(`Rotating certificate for ${serviceName}`);\n      await this.issueCertificate(serviceName);\n    }, rotationTime - Date.now());\n  }\n  \n  async pushConfigToProxies(serviceName) {\n    // xDS (Discovery Service) protocol\n    const config = {\n      service: this.services.get(serviceName),\n      policy: this.policies.get(serviceName),\n      certificate: this.certificates.get(serviceName)\n    };\n    \n    // Push to all proxies via gRPC stream\n    await this.xdsServer.pushConfig(serviceName, config);\n  }\n}\n\n// Kubernetes Integration\nclass KubernetesServiceMeshInjector {\n  async injectSidecar(pod) {\n    // Add sidecar container to pod spec\n    const sidecarContainer = {\n      name: 'istio-proxy',\n      image: 'istio/proxyv2:latest',\n      args: [\n        'proxy',\n        'sidecar',\n        '--domain',\n        `${pod.metadata.namespace}.svc.cluster.local`,\n        '--configPath',\n        '/etc/istio/proxy',\n        '--serviceCluster',\n        pod.metadata.labels.app\n      ],\n      env: [\n        {\n          name: 'POD_NAME',\n          valueFrom: { fieldRef: { fieldPath: 'metadata.name' } }\n        },\n        {\n          name: 'POD_NAMESPACE',\n          valueFrom: { fieldRef: { fieldPath: 'metadata.namespace' } }\n        }\n      ],\n      volumeMounts: [\n        { name: 'istio-envoy', mountPath: '/etc/istio/proxy' },\n        { name: 'istio-certs', mountPath: '/etc/certs/' }\n      ]\n    };\n    \n    pod.spec.containers.push(sidecarContainer);\n    \n    // Add init container for iptables rules\n    const initContainer = {\n      name: 'istio-init',\n      image: 'istio/proxyv2:latest',\n      command: ['istio-iptables'],\n      args: [\n        '-p', '15001',\n        '-u', '1337',\n        '-m', 'REDIRECT',\n        '-i', '*',\n        '-x', '',\n        '-b', '*',\n        '-d', '15090,15021,15020'\n      ],\n      securityContext: {\n        capabilities: { add: ['NET_ADMIN', 'NET_RAW'] },\n        privileged: true\n      }\n    };\n    \n    pod.spec.initContainers = pod.spec.initContainers || [];\n    pod.spec.initContainers.push(initContainer);\n    \n    return pod;\n  }\n}\n\n// Usage example\nconst controlPlane = new ServiceMeshControlPlane();\n\n// Register service\nawait controlPlane.registerService({\n  name: 'user-service',\n  config: {\n    protocol: 'http',\n    port: 8080\n  }\n});\n\n// Apply traffic policy for canary deployment\nawait controlPlane.applyTrafficPolicy('user-service', {\n  routes: [\n    {\n      match: { headers: { 'x-canary': 'true' } },\n      destination: { subset: 'v2', weight: 100 }\n    },\n    {\n      destination: [\n        { subset: 'v2', weight: 10 },\n        { subset: 'v1', weight: 90 }\n      ]\n    }\n  ],\n  retries: { attempts: 3 },\n  timeout: '10s',\n  circuitBreaker: {\n    consecutiveErrors: 5,\n    interval: '30s'\n  }\n});\n\n// Sidecar proxy intercepts requests\nconst proxy = new ServiceMeshProxy('order-service', {\n  controlPlane: 'http://istio-pilot:15010',\n  maxRetries: 3\n});\n\nconst response = await proxy.handleOutboundRequest({\n  method: 'GET',\n  url: '/api/users/123',\n  headers: {\n    'x-target-service': 'user-service'\n  }\n});"
    },
    {
      "id": 96,
      "question": "Design a machine learning model serving infrastructure like TensorFlow Serving or Seldon",
      "answer": "An ML serving infrastructure deploys, scales, and monitors machine learning models for real-time and batch predictions with versioning and experimentation.\n\nCore Components:\n• Model registry for versioned artifacts\n• Model serving endpoints (REST/gRPC)\n• Feature store for consistent feature engineering\n• Batch prediction pipeline\n• A/B testing for model comparison\n• Model monitoring and drift detection\n• Auto-scaling based on load\n• GPU acceleration support\n\nModel Management:\n• Version control for models\n• Canary deployments for new versions\n• Rollback capabilities\n• Model lineage tracking\n• Metadata and experiment tracking\n\nPrediction Serving:\n• Low-latency inference (<100ms)\n• Batching for throughput optimization\n• Caching for repeated predictions\n• Multi-model serving on same instance\n• Dynamic model loading/unloading\n\nMonitoring:\n• Prediction latency and throughput\n• Model accuracy metrics\n• Data drift detection\n• Feature distribution shifts\n• Resource utilization tracking",
      "explanation": "ML serving infrastructure uses model registries for versioning, feature stores for consistent inputs, optimized prediction servers with batching and caching, A/B testing for experimentation, and continuous monitoring for drift detection to serve production ML models reliably.",
      "difficulty": "Hard",
      "code": "// ML Model Serving Platform\nconst tf = require('@tensorflow/tfjs-node');\nconst express = require('express');\n\nclass ModelRegistry {\n  constructor(storage) {\n    this.storage = storage; // S3, GCS, etc.\n    this.models = new Map();\n  }\n  \n  async registerModel(modelInfo) {\n    const modelData = {\n      name: modelInfo.name,\n      version: modelInfo.version,\n      path: modelInfo.path,\n      framework: modelInfo.framework, // tensorflow, pytorch, sklearn\n      metadata: {\n        metrics: modelInfo.metrics,\n        features: modelInfo.features,\n        target: modelInfo.target,\n        trainedAt: Date.now(),\n        trainedBy: modelInfo.trainedBy\n      },\n      status: 'registered'\n    };\n    \n    const key = `${modelInfo.name}:${modelInfo.version}`;\n    this.models.set(key, modelData);\n    \n    // Store model artifacts\n    await this.storage.upload(\n      modelInfo.localPath,\n      `models/${modelInfo.name}/${modelInfo.version}`\n    );\n    \n    return modelData;\n  }\n  \n  async getModel(name, version = 'latest') {\n    if (version === 'latest') {\n      // Find latest version\n      const versions = Array.from(this.models.keys())\n        .filter(k => k.startsWith(`${name}:`))\n        .map(k => k.split(':')[1])\n        .sort((a, b) => b.localeCompare(a));\n      \n      version = versions[0];\n    }\n    \n    return this.models.get(`${name}:${version}`);\n  }\n  \n  async promoteModel(name, version, stage) {\n    // Stages: staging, production, archived\n    const key = `${name}:${version}`;\n    const model = this.models.get(key);\n    \n    if (!model) {\n      throw new Error('Model not found');\n    }\n    \n    model.stage = stage;\n    model.promotedAt = Date.now();\n    \n    return model;\n  }\n}\n\n// Feature Store\nclass FeatureStore {\n  constructor(redis, db) {\n    this.redis = redis;\n    this.db = db;\n  }\n  \n  // Register feature definitions\n  async registerFeature(feature) {\n    const featureData = {\n      name: feature.name,\n      type: feature.type, // numerical, categorical, embedding\n      source: feature.source, // SQL query or transformation\n      transformation: feature.transformation,\n      version: feature.version\n    };\n    \n    await this.db.insertFeature(featureData);\n  }\n  \n  // Get features for online serving\n  async getOnlineFeatures(featureNames, entityId) {\n    const features = {};\n    \n    for (const name of featureNames) {\n      // Try cache first\n      const cached = await this.redis.hget(\n        `features:${entityId}`,\n        name\n      );\n      \n      if (cached) {\n        features[name] = JSON.parse(cached);\n      } else {\n        // Compute feature\n        const value = await this.computeFeature(name, entityId);\n        features[name] = value;\n        \n        // Cache for future requests\n        await this.redis.hset(\n          `features:${entityId}`,\n          name,\n          JSON.stringify(value),\n          'EX',\n          3600\n        );\n      }\n    }\n    \n    return features;\n  }\n  \n  async computeFeature(featureName, entityId) {\n    const featureDef = await this.db.getFeature(featureName);\n    \n    // Execute feature computation\n    const rawData = await this.db.query(\n      featureDef.source.replace('$entityId', entityId)\n    );\n    \n    // Apply transformation\n    if (featureDef.transformation) {\n      return this.applyTransformation(\n        rawData,\n        featureDef.transformation\n      );\n    }\n    \n    return rawData;\n  }\n}\n\n// Model Server\nclass ModelServer {\n  constructor(modelRegistry, featureStore) {\n    this.modelRegistry = modelRegistry;\n    this.featureStore = featureStore;\n    this.loadedModels = new Map();\n    this.predictionCache = new LRUCache({ max: 10000, ttl: 300000 });\n    this.batchQueue = [];\n    this.batchSize = 32;\n    this.batchTimeout = 100; // ms\n    \n    this.startBatchProcessor();\n  }\n  \n  async loadModel(name, version = 'latest') {\n    const modelInfo = await this.modelRegistry.getModel(name, version);\n    \n    if (!modelInfo) {\n      throw new Error(`Model ${name}:${version} not found`);\n    }\n    \n    // Download and load model\n    const modelPath = await this.downloadModel(modelInfo.path);\n    const model = await tf.loadLayersModel(`file://${modelPath}`);\n    \n    const key = `${name}:${version}`;\n    this.loadedModels.set(key, {\n      model,\n      info: modelInfo,\n      loadedAt: Date.now(),\n      requestCount: 0\n    });\n    \n    console.log(`Loaded model: ${key}`);\n    return model;\n  }\n  \n  async predict(modelName, input, options = {}) {\n    const version = options.version || 'latest';\n    const key = `${modelName}:${version}`;\n    \n    // Check cache\n    const cacheKey = `${key}:${JSON.stringify(input)}`;\n    const cached = this.predictionCache.get(cacheKey);\n    \n    if (cached && !options.skipCache) {\n      return cached;\n    }\n    \n    // Get or load model\n    let modelData = this.loadedModels.get(key);\n    \n    if (!modelData) {\n      await this.loadModel(modelName, version);\n      modelData = this.loadedModels.get(key);\n    }\n    \n    // Get features if needed\n    let features = input;\n    if (options.entityId && modelData.info.metadata.features) {\n      features = await this.featureStore.getOnlineFeatures(\n        modelData.info.metadata.features,\n        options.entityId\n      );\n    }\n    \n    // Preprocess input\n    const tensor = this.preprocessInput(features, modelData.info);\n    \n    // Run prediction\n    const startTime = Date.now();\n    const prediction = await modelData.model.predict(tensor);\n    const result = await prediction.array();\n    const latency = Date.now() - startTime;\n    \n    // Cleanup tensors\n    tensor.dispose();\n    prediction.dispose();\n    \n    // Postprocess result\n    const output = this.postprocessOutput(result, modelData.info);\n    \n    // Cache result\n    this.predictionCache.set(cacheKey, output);\n    \n    // Record metrics\n    modelData.requestCount++;\n    this.recordPrediction(modelName, version, latency);\n    \n    return output;\n  }\n  \n  async batchPredict(modelName, inputs, options = {}) {\n    // Add to batch queue\n    return new Promise((resolve) => {\n      this.batchQueue.push({ modelName, inputs, options, resolve });\n    });\n  }\n  \n  startBatchProcessor() {\n    setInterval(async () => {\n      if (this.batchQueue.length === 0) return;\n      \n      // Group by model\n      const batches = new Map();\n      \n      while (this.batchQueue.length > 0) {\n        const item = this.batchQueue.shift();\n        \n        if (!batches.has(item.modelName)) {\n          batches.set(item.modelName, []);\n        }\n        \n        batches.get(item.modelName).push(item);\n        \n        // Process batch if full\n        const batch = batches.get(item.modelName);\n        if (batch.length >= this.batchSize) {\n          await this.processBatch(item.modelName, batch);\n          batches.set(item.modelName, []);\n        }\n      }\n      \n      // Process remaining batches\n      for (const [modelName, batch] of batches.entries()) {\n        if (batch.length > 0) {\n          await this.processBatch(modelName, batch);\n        }\n      }\n    }, this.batchTimeout);\n  }\n  \n  async processBatch(modelName, batch) {\n    const modelData = this.loadedModels.get(modelName);\n    \n    if (!modelData) {\n      batch.forEach(item => item.resolve({ error: 'Model not loaded' }));\n      return;\n    }\n    \n    try {\n      // Stack inputs into batch tensor\n      const batchInputs = batch.map(item => \n        this.preprocessInput(item.inputs, modelData.info)\n      );\n      \n      const batchTensor = tf.stack(batchInputs);\n      \n      // Run batch prediction\n      const predictions = await modelData.model.predict(batchTensor);\n      const results = await predictions.array();\n      \n      // Cleanup\n      batchTensor.dispose();\n      predictions.dispose();\n      batchInputs.forEach(t => t.dispose());\n      \n      // Resolve individual promises\n      results.forEach((result, i) => {\n        const output = this.postprocessOutput([result], modelData.info);\n        batch[i].resolve(output);\n      });\n    } catch (error) {\n      batch.forEach(item => item.resolve({ error: error.message }));\n    }\n  }\n  \n  preprocessInput(input, modelInfo) {\n    // Convert input to tensor based on model requirements\n    const features = modelInfo.metadata.features;\n    const values = features.map(f => input[f] || 0);\n    \n    return tf.tensor2d([values]);\n  }\n  \n  postprocessOutput(output, modelInfo) {\n    // Convert model output to readable format\n    if (modelInfo.metadata.target === 'classification') {\n      const probabilities = output[0];\n      const classIndex = probabilities.indexOf(Math.max(...probabilities));\n      \n      return {\n        class: modelInfo.metadata.classes[classIndex],\n        probability: probabilities[classIndex],\n        allProbabilities: probabilities\n      };\n    }\n    \n    return { prediction: output[0][0] };\n  }\n}\n\n// Model Monitoring\nclass ModelMonitor {\n  constructor(prometheus) {\n    this.prometheus = prometheus;\n    \n    // Metrics\n    this.predictionLatency = new prometheus.Histogram({\n      name: 'model_prediction_latency',\n      help: 'Model prediction latency in ms',\n      labelNames: ['model', 'version'],\n      buckets: [10, 50, 100, 200, 500, 1000]\n    });\n    \n    this.predictionCount = new prometheus.Counter({\n      name: 'model_predictions_total',\n      help: 'Total number of predictions',\n      labelNames: ['model', 'version']\n    });\n  }\n  \n  recordPrediction(model, version, latency) {\n    this.predictionLatency.labels(model, version).observe(latency);\n    this.predictionCount.labels(model, version).inc();\n  }\n  \n  async detectDataDrift(modelName, recentPredictions) {\n    // Compare feature distributions\n    const model = await this.modelRegistry.getModel(modelName);\n    const trainingStats = model.metadata.featureStats;\n    \n    const drift = {};\n    \n    for (const feature of model.metadata.features) {\n      const recentValues = recentPredictions.map(p => p.features[feature]);\n      \n      // Calculate statistics\n      const recentMean = this.mean(recentValues);\n      const recentStd = this.std(recentValues);\n      \n      // Compare with training distribution\n      const trainingMean = trainingStats[feature].mean;\n      const trainingStd = trainingStats[feature].std;\n      \n      // Z-score for drift detection\n      const zScore = Math.abs(recentMean - trainingMean) / trainingStd;\n      \n      if (zScore > 3) {\n        drift[feature] = {\n          status: 'significant_drift',\n          zScore,\n          trainingMean,\n          recentMean\n        };\n      }\n    }\n    \n    return drift;\n  }\n}\n\n// A/B Testing for Models\nclass ModelExperiment {\n  constructor(modelServer) {\n    this.modelServer = modelServer;\n    this.experiments = new Map();\n  }\n  \n  async createExperiment(experiment) {\n    this.experiments.set(experiment.id, {\n      id: experiment.id,\n      models: experiment.models, // [{ name, version, traffic }]\n      startDate: Date.now(),\n      metrics: {}\n    });\n  }\n  \n  async predict(experimentId, input, userId) {\n    const experiment = this.experiments.get(experimentId);\n    \n    // Assign user to model variant\n    const variant = this.assignVariant(userId, experiment.models);\n    \n    // Get prediction\n    const result = await this.modelServer.predict(\n      variant.name,\n      input,\n      { version: variant.version }\n    );\n    \n    // Track assignment\n    await this.trackAssignment(experimentId, userId, variant.name);\n    \n    return { ...result, variant: variant.name };\n  }\n  \n  assignVariant(userId, models) {\n    // Consistent hashing for sticky assignment\n    const hash = this.hashUserId(userId);\n    \n    let cumulative = 0;\n    for (const model of models) {\n      cumulative += model.traffic;\n      if (hash < cumulative) {\n        return model;\n      }\n    }\n    \n    return models[models.length - 1];\n  }\n}\n\n// REST API\nconst app = express();\napp.use(express.json());\n\nconst modelRegistry = new ModelRegistry(s3Storage);\nconst featureStore = new FeatureStore(redis, db);\nconst modelServer = new ModelServer(modelRegistry, featureStore);\nconst modelMonitor = new ModelMonitor(prometheus);\n\n// Prediction endpoint\napp.post('/predict/:model', async (req, res) => {\n  try {\n    const { model } = req.params;\n    const { input, version, entityId } = req.body;\n    \n    const result = await modelServer.predict(model, input, {\n      version,\n      entityId\n    });\n    \n    res.json(result);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Batch prediction\napp.post('/predict/:model/batch', async (req, res) => {\n  try {\n    const { model } = req.params;\n    const { inputs, version } = req.body;\n    \n    const results = await Promise.all(\n      inputs.map(input =>\n        modelServer.batchPredict(model, input, { version })\n      )\n    );\n    \n    res.json({ predictions: results });\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Model management\napp.post('/models', async (req, res) => {\n  const model = await modelRegistry.registerModel(req.body);\n  res.json(model);\n});\n\napp.post('/models/:name/:version/promote', async (req, res) => {\n  const { name, version } = req.params;\n  const { stage } = req.body;\n  \n  const model = await modelRegistry.promoteModel(name, version, stage);\n  res.json(model);\n});\n\napp.listen(8080, () => {\n  console.log('Model serving API listening on port 8080');\n});"
    },
    {
      "id": 97,
      "question": "Design a secrets management system like HashiCorp Vault or AWS Secrets Manager",
      "answer": "A secrets management system securely stores, accesses, and manages sensitive data like API keys, passwords, and certificates with encryption, access control, and audit logging.\n\nCore Features:\n• Centralized secret storage with encryption at rest\n• Dynamic secrets generation with TTL\n• Secret rotation and versioning\n• Access control policies (RBAC, ACL)\n• Detailed audit logging\n• Encryption as a service\n• Secret leasing and renewal\n• High availability and replication\n\nSecret Types:\n• Static secrets: API keys, passwords\n• Dynamic secrets: Generated on demand\n• Certificate management\n• Database credentials\n• Cloud provider credentials\n• Encryption keys\n\nAccess Patterns:\n• Authentication methods (tokens, LDAP, AWS IAM, Kubernetes SA)\n• Short-lived access tokens\n• Policy-based authorization\n• Secret paths and namespaces\n• Lease management with renewal\n\nSecurity:\n• Encryption at rest and in transit\n• HSM integration for key management\n• Seal/unseal mechanism\n• Zero-knowledge deployment\n• Secret rotation automation",
      "explanation": "Secrets management systems use encryption, dynamic secret generation with TTL-based expiration, policy-based access control, automatic rotation, and comprehensive audit logging to securely manage sensitive credentials centrally while preventing exposure and unauthorized access.",
      "difficulty": "Hard",
      "code": "// Secrets Management System\nconst crypto = require('crypto');\nconst AWS = require('aws-sdk');\n\nclass SecretsVault {\n  constructor(config) {\n    this.kms = new AWS.KMS();\n    this.db = config.database;\n    this.masterKey = null;\n    this.sealed = true;\n    this.policies = new Map();\n    this.activeLeases = new Map();\n  }\n  \n  // Unsealing vault with master key shares (Shamir's Secret Sharing)\n  async unseal(keyShares) {\n    const threshold = 3; // Require 3 out of 5 key shares\n    \n    if (keyShares.length < threshold) {\n      throw new Error(`Need ${threshold} key shares to unseal`);\n    }\n    \n    // Reconstruct master key from shares\n    this.masterKey = this.reconstructSecret(keyShares);\n    \n    // Decrypt data encryption keys\n    await this.loadEncryptionKeys();\n    \n    this.sealed = false;\n    console.log('Vault unsealed');\n  }\n  \n  async seal() {\n    // Clear master key from memory\n    this.masterKey = null;\n    this.sealed = true;\n    \n    // Revoke all active leases\n    this.activeLeases.clear();\n    \n    console.log('Vault sealed');\n  }\n  \n  // Store static secret\n  async createSecret(path, data, options = {}) {\n    this.ensureUnsealed();\n    \n    // Encrypt secret data\n    const encrypted = await this.encrypt(JSON.stringify(data));\n    \n    const secret = {\n      path,\n      encryptedData: encrypted,\n      version: options.version || 1,\n      metadata: {\n        createdAt: Date.now(),\n        createdBy: options.user,\n        description: options.description\n      },\n      options: {\n        ttl: options.ttl,\n        maxVersions: options.maxVersions || 10\n      }\n    };\n    \n    // Store in database\n    await this.db.insertSecret(secret);\n    \n    // Audit log\n    await this.auditLog('secret.create', {\n      path,\n      user: options.user,\n      version: secret.version\n    });\n    \n    return { path, version: secret.version };\n  }\n  \n  // Read secret with lease\n  async readSecret(path, token) {\n    this.ensureUnsealed();\n    \n    // Validate token and check policy\n    const auth = await this.validateToken(token);\n    \n    if (!this.hasPermission(auth, path, 'read')) {\n      throw new Error('Access denied');\n    }\n    \n    // Get secret\n    const secret = await this.db.getSecret(path);\n    \n    if (!secret) {\n      throw new Error('Secret not found');\n    }\n    \n    // Decrypt\n    const data = JSON.parse(await this.decrypt(secret.encryptedData));\n    \n    // Create lease\n    const lease = await this.createLease(path, auth.entityId, secret.options.ttl);\n    \n    // Audit log\n    await this.auditLog('secret.read', {\n      path,\n      user: auth.entityId,\n      leaseId: lease.id\n    });\n    \n    return {\n      data,\n      leaseId: lease.id,\n      leaseDuration: lease.duration,\n      renewable: lease.renewable\n    };\n  }\n  \n  // Generate dynamic secret (e.g., database credentials)\n  async generateDynamicSecret(engine, role, token) {\n    this.ensureUnsealed();\n    \n    const auth = await this.validateToken(token);\n    \n    if (!this.hasPermission(auth, `${engine}/${role}`, 'generate')) {\n      throw new Error('Access denied');\n    }\n    \n    let credential;\n    \n    switch (engine) {\n      case 'database':\n        credential = await this.generateDatabaseCreds(role);\n        break;\n      case 'aws':\n        credential = await this.generateAWSCreds(role);\n        break;\n      case 'pki':\n        credential = await this.generateCertificate(role);\n        break;\n      default:\n        throw new Error(`Unknown engine: ${engine}`);\n    }\n    \n    // Create lease\n    const lease = await this.createLease(\n      `${engine}/${role}`,\n      auth.entityId,\n      credential.ttl\n    );\n    \n    // Store credential for revocation\n    await this.storeLeasedCredential(lease.id, credential);\n    \n    // Audit log\n    await this.auditLog('dynamic_secret.generate', {\n      engine,\n      role,\n      user: auth.entityId,\n      leaseId: lease.id\n    });\n    \n    return {\n      ...credential,\n      leaseId: lease.id,\n      leaseDuration: lease.duration\n    };\n  }\n  \n  async generateDatabaseCreds(role) {\n    // Get role configuration\n    const roleConfig = await this.db.getDatabaseRole(role);\n    \n    const username = `v-${role}-${Date.now()}`;\n    const password = crypto.randomBytes(32).toString('base64');\n    \n    // Create database user\n    await this.db.query(\n      `CREATE USER ${username} WITH PASSWORD '${password}' VALID UNTIL NOW() + INTERVAL '${roleConfig.ttl} seconds'`\n    );\n    \n    // Grant permissions\n    for (const grant of roleConfig.grants) {\n      await this.db.query(grant.replace('{{name}}', username));\n    }\n    \n    return {\n      username,\n      password,\n      ttl: roleConfig.ttl,\n      connectionString: roleConfig.connectionString\n    };\n  }\n  \n  async generateAWSCreds(role) {\n    const sts = new AWS.STS();\n    \n    const roleConfig = await this.db.getAWSRole(role);\n    \n    // Assume IAM role\n    const assumed = await sts.assumeRole({\n      RoleArn: roleConfig.roleArn,\n      RoleSessionName: `vault-${Date.now()}`,\n      DurationSeconds: roleConfig.ttl\n    }).promise();\n    \n    return {\n      accessKeyId: assumed.Credentials.AccessKeyId,\n      secretAccessKey: assumed.Credentials.SecretAccessKey,\n      sessionToken: assumed.Credentials.SessionToken,\n      expiration: assumed.Credentials.Expiration,\n      ttl: roleConfig.ttl\n    };\n  }\n  \n  // Lease management\n  async createLease(path, entityId, ttl = 3600) {\n    const lease = {\n      id: `lease-${crypto.randomBytes(16).toString('hex')}`,\n      path,\n      entityId,\n      createdAt: Date.now(),\n      duration: ttl,\n      expiresAt: Date.now() + (ttl * 1000),\n      renewable: true\n    };\n    \n    this.activeLeases.set(lease.id, lease);\n    \n    // Schedule revocation\n    setTimeout(() => {\n      this.revokeLease(lease.id);\n    }, ttl * 1000);\n    \n    return lease;\n  }\n  \n  async renewLease(leaseId, increment) {\n    const lease = this.activeLeases.get(leaseId);\n    \n    if (!lease) {\n      throw new Error('Lease not found');\n    }\n    \n    if (!lease.renewable) {\n      throw new Error('Lease is not renewable');\n    }\n    \n    // Extend expiration\n    lease.expiresAt = Date.now() + (increment * 1000);\n    lease.duration = increment;\n    \n    await this.auditLog('lease.renew', {\n      leaseId,\n      increment\n    });\n    \n    return lease;\n  }\n  \n  async revokeLease(leaseId) {\n    const lease = this.activeLeases.get(leaseId);\n    \n    if (!lease) return;\n    \n    // Get credential\n    const credential = await this.getLeasedCredential(leaseId);\n    \n    if (credential) {\n      // Revoke credential\n      await this.revokeCredential(credential);\n    }\n    \n    this.activeLeases.delete(leaseId);\n    \n    await this.auditLog('lease.revoke', { leaseId });\n  }\n  \n  // Secret rotation\n  async rotateSecret(path, newData) {\n    const secret = await this.db.getSecret(path);\n    \n    if (!secret) {\n      throw new Error('Secret not found');\n    }\n    \n    // Archive old version\n    await this.archiveSecretVersion(secret);\n    \n    // Create new version\n    const newVersion = secret.version + 1;\n    \n    await this.createSecret(path, newData, {\n      version: newVersion\n    });\n    \n    await this.auditLog('secret.rotate', {\n      path,\n      oldVersion: secret.version,\n      newVersion\n    });\n  }\n  \n  // Automatic rotation for database credentials\n  async setupAutoRotation(path, schedule) {\n    // Schedule using cron\n    const cron = require('node-cron');\n    \n    cron.schedule(schedule, async () => {\n      console.log(`Auto-rotating secret: ${path}`);\n      \n      const secret = await this.db.getSecret(path);\n      const data = JSON.parse(await this.decrypt(secret.encryptedData));\n      \n      if (data.type === 'database') {\n        // Generate new password\n        const newPassword = crypto.randomBytes(32).toString('base64');\n        \n        // Update database password\n        await this.db.query(\n          `ALTER USER ${data.username} WITH PASSWORD '${newPassword}'`\n        );\n        \n        // Store new secret version\n        await this.rotateSecret(path, {\n          ...data,\n          password: newPassword\n        });\n      }\n    });\n  }\n  \n  // Access control\n  async createPolicy(name, rules) {\n    const policy = {\n      name,\n      rules: rules.map(r => ({\n        path: r.path,\n        capabilities: r.capabilities, // ['read', 'write', 'delete']\n        conditions: r.conditions\n      })),\n      createdAt: Date.now()\n    };\n    \n    this.policies.set(name, policy);\n    await this.db.insertPolicy(policy);\n    \n    return policy;\n  }\n  \n  hasPermission(auth, path, capability) {\n    // Check if entity has required capability for path\n    for (const policyName of auth.policies) {\n      const policy = this.policies.get(policyName);\n      \n      if (!policy) continue;\n      \n      for (const rule of policy.rules) {\n        if (this.pathMatches(path, rule.path)) {\n          if (rule.capabilities.includes(capability)) {\n            return true;\n          }\n        }\n      }\n    }\n    \n    return false;\n  }\n  \n  pathMatches(path, pattern) {\n    // Support wildcards: secret/data/* matches secret/data/foo\n    const regex = new RegExp('^' + pattern.replace('*', '.*') + '$');\n    return regex.test(path);\n  }\n  \n  // Encryption\n  async encrypt(plaintext) {\n    // Use AES-256-GCM\n    const iv = crypto.randomBytes(16);\n    const cipher = crypto.createCipheriv('aes-256-gcm', this.masterKey, iv);\n    \n    let encrypted = cipher.update(plaintext, 'utf8', 'base64');\n    encrypted += cipher.final('base64');\n    \n    const authTag = cipher.getAuthTag();\n    \n    return JSON.stringify({\n      iv: iv.toString('base64'),\n      data: encrypted,\n      authTag: authTag.toString('base64')\n    });\n  }\n  \n  async decrypt(ciphertext) {\n    const { iv, data, authTag } = JSON.parse(ciphertext);\n    \n    const decipher = crypto.createDecipheriv(\n      'aes-256-gcm',\n      this.masterKey,\n      Buffer.from(iv, 'base64')\n    );\n    \n    decipher.setAuthTag(Buffer.from(authTag, 'base64'));\n    \n    let decrypted = decipher.update(data, 'base64', 'utf8');\n    decrypted += decipher.final('utf8');\n    \n    return decrypted;\n  }\n  \n  // Audit logging\n  async auditLog(operation, details) {\n    await this.db.insertAuditLog({\n      operation,\n      details,\n      timestamp: Date.now()\n    });\n  }\n  \n  ensureUnsealed() {\n    if (this.sealed) {\n      throw new Error('Vault is sealed');\n    }\n  }\n}\n\n// Client SDK\nclass VaultClient {\n  constructor(vaultUrl, token) {\n    this.vaultUrl = vaultUrl;\n    this.token = token;\n  }\n  \n  async read(path) {\n    const response = await fetch(`${this.vaultUrl}/v1/secret/${path}`, {\n      headers: { 'X-Vault-Token': this.token }\n    });\n    \n    return response.json();\n  }\n  \n  async write(path, data) {\n    const response = await fetch(`${this.vaultUrl}/v1/secret/${path}`, {\n      method: 'POST',\n      headers: {\n        'X-Vault-Token': this.token,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({ data })\n    });\n    \n    return response.json();\n  }\n  \n  async generateDatabaseCreds(role) {\n    const response = await fetch(\n      `${this.vaultUrl}/v1/database/creds/${role}`,\n      { headers: { 'X-Vault-Token': this.token } }\n    );\n    \n    return response.json();\n  }\n}\n\n// Usage\nconst vault = new SecretsVault({ database: db });\n\n// Unseal vault\nawait vault.unseal([keyShare1, keyShare2, keyShare3]);\n\n// Create policy\nawait vault.createPolicy('app-policy', [\n  {\n    path: 'secret/app/*',\n    capabilities: ['read']\n  },\n  {\n    path: 'database/creds/app-role',\n    capabilities: ['generate']\n  }\n]);\n\n// Store secret\nawait vault.createSecret('secret/app/api-key', {\n  key: 'sk_live_abc123',\n  provider: 'stripe'\n}, { user: 'admin' });\n\n// Read secret with lease\nconst { data, leaseId } = await vault.readSecret(\n  'secret/app/api-key',\n  userToken\n);\n\n// Generate dynamic database credentials\nconst dbCreds = await vault.generateDynamicSecret(\n  'database',\n  'app-role',\n  userToken\n);\n\nconsole.log('DB credentials:', dbCreds);\n\n// Auto-rotate every month\nawait vault.setupAutoRotation(\n  'secret/app/api-key',\n  '0 0 1 * *' // First day of month\n);"
    },
    {
      "id": 98,
      "question": "Design a chaos engineering platform like Chaos Monkey or Gremlin",
      "answer": "A chaos engineering platform intentionally injects failures into production systems to test resilience, identify weaknesses, and build confidence in system behavior under adverse conditions.\n\nCore Experiments:\n• Latency injection: Add network delays\n• Service shutdown: Kill instances/containers\n• Resource exhaustion: CPU/memory/disk pressure\n• Network partitioning: Isolate services\n• DNS failures: Break service discovery\n• Clock skew: Manipulate time\n• Disk failures: Fill disk or corrupt data\n• Configuration errors: Inject bad config\n\nPlatform Features:\n• Experiment definition and scheduling\n• Blast radius control (limit scope)\n• Safety guardrails and circuit breakers\n• Rollback mechanisms\n• Real-time monitoring\n• Hypothesis validation\n• Results analysis and reporting\n• Automated steady-state verification\n\nSafety:\n• Gradual rollout (canary experiments)\n• Business hour restrictions\n• Service health checks before/during/after\n• Automatic experiment abortion on alerts\n• Manual kill switch\n• Opt-in per service\n\nObservability:\n• Baseline metrics collection\n• Experiment impact tracking\n• Anomaly detection\n• Alert correlation\n• Incident linkage",
      "explanation": "Chaos engineering platforms systematically inject controlled failures (latency, resource exhaustion, service crashes) into systems with safety guardrails, monitor impact on steady-state metrics, and validate hypotheses to proactively discover weaknesses before they cause real incidents.",
      "difficulty": "Hard",
      "code": "// Chaos Engineering Platform\nconst Kubernetes = require('@kubernetes/client-node');\nconst AWS = require('aws-sdk');\n\nclass ChaosEngineeringPlatform {\n  constructor(config) {\n    this.k8s = new Kubernetes.KubeConfig();\n    this.k8s.loadFromDefault();\n    this.coreApi = this.k8s.makeApiClient(Kubernetes.CoreV1Api);\n    this.appsApi = this.k8s.makeApiClient(Kubernetes.AppsV1Api);\n    \n    this.experiments = new Map();\n    this.activeExperiments = new Map();\n    this.safetyChecks = [];\n  }\n  \n  // Define chaos experiment\n  async createExperiment(experiment) {\n    const exp = {\n      id: `exp-${Date.now()}`,\n      name: experiment.name,\n      description: experiment.description,\n      hypothesis: experiment.hypothesis,\n      steadyState: experiment.steadyState, // Metrics that should remain stable\n      method: experiment.method, // How to inject chaos\n      scope: experiment.scope, // Which services/instances\n      duration: experiment.duration || 60, // seconds\n      rollbackStrategy: experiment.rollbackStrategy,\n      safetyChecks: experiment.safetyChecks || [],\n      schedule: experiment.schedule, // Optional cron schedule\n      createdAt: Date.now(),\n      createdBy: experiment.createdBy\n    };\n    \n    this.experiments.set(exp.id, exp);\n    \n    // Schedule if needed\n    if (exp.schedule) {\n      this.scheduleExperiment(exp);\n    }\n    \n    return exp;\n  }\n  \n  // Execute experiment\n  async runExperiment(experimentId) {\n    const experiment = this.experiments.get(experimentId);\n    \n    if (!experiment) {\n      throw new Error('Experiment not found');\n    }\n    \n    console.log(`Starting experiment: ${experiment.name}`);\n    \n    const execution = {\n      id: `exec-${Date.now()}`,\n      experimentId,\n      startTime: Date.now(),\n      status: 'running',\n      metrics: {\n        baseline: {},\n        during: {},\n        after: {}\n      },\n      events: []\n    };\n    \n    this.activeExperiments.set(execution.id, execution);\n    \n    try {\n      // 1. Verify safety checks\n      await this.verifySafetyChecks(experiment);\n      execution.events.push({ type: 'safety_verified', time: Date.now() });\n      \n      // 2. Collect baseline metrics\n      execution.metrics.baseline = await this.collectMetrics(\n        experiment.steadyState\n      );\n      execution.events.push({ type: 'baseline_collected', time: Date.now() });\n      \n      // 3. Inject chaos\n      const chaosHandle = await this.injectChaos(experiment);\n      execution.events.push({ type: 'chaos_injected', time: Date.now() });\n      \n      // 4. Monitor during experiment\n      const monitoringPromise = this.monitorExperiment(experiment, execution);\n      \n      // 5. Wait for duration\n      await new Promise(resolve => setTimeout(resolve, experiment.duration * 1000));\n      \n      // 6. Revert chaos\n      await this.revertChaos(chaosHandle);\n      execution.events.push({ type: 'chaos_reverted', time: Date.now() });\n      \n      // Stop monitoring\n      await monitoringPromise;\n      \n      // 7. Collect post-experiment metrics\n      execution.metrics.after = await this.collectMetrics(\n        experiment.steadyState\n      );\n      \n      // 8. Analyze results\n      const analysis = await this.analyzeResults(experiment, execution);\n      execution.analysis = analysis;\n      execution.status = 'completed';\n      execution.endTime = Date.now();\n      \n      console.log(`Experiment completed: ${experiment.name}`);\n      console.log('Analysis:', analysis);\n      \n      return execution;\n    } catch (error) {\n      console.error('Experiment failed:', error);\n      execution.status = 'failed';\n      execution.error = error.message;\n      execution.endTime = Date.now();\n      \n      // Emergency rollback\n      await this.emergencyRollback(experiment, execution);\n      \n      throw error;\n    } finally {\n      this.activeExperiments.delete(execution.id);\n    }\n  }\n  \n  // Chaos injection methods\n  async injectChaos(experiment) {\n    const { method, scope } = experiment;\n    \n    switch (method.type) {\n      case 'pod_delete':\n        return this.injectPodDelete(scope, method.config);\n      case 'latency':\n        return this.injectLatency(scope, method.config);\n      case 'cpu_stress':\n        return this.injectCPUStress(scope, method.config);\n      case 'memory_stress':\n        return this.injectMemoryStress(scope, method.config);\n      case 'network_partition':\n        return this.injectNetworkPartition(scope, method.config);\n      case 'disk_fill':\n        return this.injectDiskFill(scope, method.config);\n      default:\n        throw new Error(`Unknown chaos method: ${method.type}`);\n    }\n  }\n  \n  async injectPodDelete(scope, config) {\n    // Kill random pods matching label selector\n    const { namespace, labelSelector, count = 1 } = scope;\n    \n    const pods = await this.coreApi.listNamespacedPod(\n      namespace,\n      undefined,\n      undefined,\n      undefined,\n      undefined,\n      labelSelector\n    );\n    \n    const targets = this.selectRandomPods(pods.body.items, count);\n    const deletedPods = [];\n    \n    for (const pod of targets) {\n      await this.coreApi.deleteNamespacedPod(\n        pod.metadata.name,\n        namespace\n      );\n      \n      deletedPods.push(pod.metadata.name);\n      console.log(`Deleted pod: ${pod.metadata.name}`);\n    }\n    \n    return {\n      type: 'pod_delete',\n      namespace,\n      deletedPods,\n      revert: async () => {\n        // Pods will be recreated by deployment controller\n        console.log('Waiting for pods to be recreated...');\n        await new Promise(resolve => setTimeout(resolve, 30000));\n      }\n    };\n  }\n  \n  async injectLatency(scope, config) {\n    // Inject network latency using tc (traffic control)\n    const { namespace, labelSelector, delay = '100ms', jitter = '10ms' } = config;\n    \n    const pods = await this.selectTargetPods(namespace, labelSelector);\n    const injections = [];\n    \n    for (const pod of pods) {\n      // Execute tc command in pod\n      await this.execInPod(\n        namespace,\n        pod.metadata.name,\n        ['tc', 'qdisc', 'add', 'dev', 'eth0', 'root', 'netem',\n         'delay', delay, jitter]\n      );\n      \n      injections.push(pod.metadata.name);\n      console.log(`Injected ${delay} latency in pod: ${pod.metadata.name}`);\n    }\n    \n    return {\n      type: 'latency',\n      namespace,\n      pods: injections,\n      revert: async () => {\n        for (const podName of injections) {\n          await this.execInPod(\n            namespace,\n            podName,\n            ['tc', 'qdisc', 'del', 'dev', 'eth0', 'root']\n          );\n        }\n        console.log('Reverted latency injection');\n      }\n    };\n  }\n  \n  async injectCPUStress(scope, config) {\n    // Inject CPU stress using stress-ng\n    const { namespace, labelSelector, workers = 2, load = 80 } = config;\n    \n    const pods = await this.selectTargetPods(namespace, labelSelector);\n    const processes = [];\n    \n    for (const pod of pods) {\n      // Start stress-ng process\n      const process = await this.execInPodBackground(\n        namespace,\n        pod.metadata.name,\n        ['stress-ng', '--cpu', workers.toString(),\n         '--cpu-load', load.toString(), '--timeout', '0']\n      );\n      \n      processes.push({ pod: pod.metadata.name, process });\n      console.log(`Injected CPU stress in pod: ${pod.metadata.name}`);\n    }\n    \n    return {\n      type: 'cpu_stress',\n      namespace,\n      processes,\n      revert: async () => {\n        for (const { pod, process } of processes) {\n          await this.killProcessInPod(namespace, pod, process);\n        }\n        console.log('Reverted CPU stress');\n      }\n    };\n  }\n  \n  async injectNetworkPartition(scope, config) {\n    // Create network partition using iptables\n    const { namespace, from, to } = config;\n    \n    const sourcePods = await this.selectTargetPods(namespace, from);\n    const targetPods = await this.selectTargetPods(namespace, to);\n    \n    const rules = [];\n    \n    for (const source of sourcePods) {\n      for (const target of targetPods) {\n        const targetIP = target.status.podIP;\n        \n        // Block traffic to target\n        await this.execInPod(\n          namespace,\n          source.metadata.name,\n          ['iptables', '-A', 'OUTPUT', '-d', targetIP, '-j', 'DROP']\n        );\n        \n        rules.push({\n          source: source.metadata.name,\n          targetIP\n        });\n      }\n    }\n    \n    console.log(`Created network partition: ${from} -> ${to}`);\n    \n    return {\n      type: 'network_partition',\n      namespace,\n      rules,\n      revert: async () => {\n        for (const { source, targetIP } of rules) {\n          await this.execInPod(\n            namespace,\n            source,\n            ['iptables', '-D', 'OUTPUT', '-d', targetIP, '-j', 'DROP']\n          );\n        }\n        console.log('Reverted network partition');\n      }\n    };\n  }\n  \n  // Safety checks\n  async verifySafetyChecks(experiment) {\n    for (const check of experiment.safetyChecks) {\n      const result = await this.runSafetyCheck(check);\n      \n      if (!result.passed) {\n        throw new Error(`Safety check failed: ${check.name} - ${result.reason}`);\n      }\n    }\n    \n    // Global safety checks\n    await this.checkBusinessHours();\n    await this.checkOngoingIncidents();\n    await this.checkDeploymentHistory();\n  }\n  \n  async checkBusinessHours() {\n    const hour = new Date().getHours();\n    \n    // Only run during off-peak hours (e.g., 2 AM - 6 AM)\n    if (hour < 2 || hour > 6) {\n      throw new Error('Chaos experiments only allowed during off-peak hours');\n    }\n  }\n  \n  async checkOngoingIncidents() {\n    // Check PagerDuty for active incidents\n    const incidents = await this.pagerduty.getActiveIncidents();\n    \n    if (incidents.length > 0) {\n      throw new Error('Active incidents detected, aborting experiment');\n    }\n  }\n  \n  // Monitoring during experiment\n  async monitorExperiment(experiment, execution) {\n    const checkInterval = 5000; // 5 seconds\n    const startTime = Date.now();\n    \n    while (Date.now() - startTime < experiment.duration * 1000) {\n      // Collect current metrics\n      const current = await this.collectMetrics(experiment.steadyState);\n      execution.metrics.during = current;\n      \n      // Check if steady state violated\n      const violation = this.detectSteadyStateViolation(\n        execution.metrics.baseline,\n        current,\n        experiment.steadyState.thresholds\n      );\n      \n      if (violation) {\n        execution.events.push({\n          type: 'steady_state_violated',\n          metric: violation.metric,\n          time: Date.now()\n        });\n        \n        // Abort experiment\n        throw new Error(`Steady state violated: ${violation.metric}`);\n      }\n      \n      await new Promise(resolve => setTimeout(resolve, checkInterval));\n    }\n  }\n  \n  // Result analysis\n  async analyzeResults(experiment, execution) {\n    const { baseline, during, after } = execution.metrics;\n    \n    const analysis = {\n      hypothesisValidated: true,\n      steadyStatePreserved: true,\n      observations: [],\n      recommendations: []\n    };\n    \n    // Compare metrics\n    for (const metric of experiment.steadyState.metrics) {\n      const baselineValue = baseline[metric.name];\n      const duringValue = during[metric.name];\n      const afterValue = after[metric.name];\n      \n      // Check if metric deviated significantly\n      const deviation = Math.abs(duringValue - baselineValue) / baselineValue;\n      \n      if (deviation > metric.threshold) {\n        analysis.steadyStatePreserved = false;\n        analysis.observations.push({\n          metric: metric.name,\n          baseline: baselineValue,\n          during: duringValue,\n          deviation: deviation * 100,\n          severity: deviation > 0.5 ? 'critical' : 'warning'\n        });\n      }\n      \n      // Check recovery\n      const recovered = Math.abs(afterValue - baselineValue) / baselineValue < 0.1;\n      \n      if (!recovered) {\n        analysis.recommendations.push({\n          type: 'slow_recovery',\n          metric: metric.name,\n          message: `${metric.name} did not recover to baseline after chaos ended`\n        });\n      }\n    }\n    \n    return analysis;\n  }\n}\n\n// Usage\nconst chaosEng = new ChaosEngineeringPlatform();\n\n// Create experiment\nconst experiment = await chaosEng.createExperiment({\n  name: 'Pod Deletion Resilience',\n  description: 'Verify system resilience when random pods are deleted',\n  hypothesis: 'When pods are deleted, the system continues to serve requests with < 1% error rate',\n  steadyState: {\n    metrics: [\n      { name: 'error_rate', threshold: 0.01 },\n      { name: 'latency_p99', threshold: 0.2 },\n      { name: 'throughput', threshold: 0.1 }\n    ]\n  },\n  method: {\n    type: 'pod_delete',\n    config: { count: 1 }\n  },\n  scope: {\n    namespace: 'production',\n    labelSelector: 'app=api-server'\n  },\n  duration: 120, // 2 minutes\n  safetyChecks: [\n    { name: 'min_replicas', check: 'replicas >= 3' },\n    { name: 'health_check', check: 'healthy_instances >= 2' }\n  ],\n  schedule: '0 3 * * *', // Daily at 3 AM\n  createdBy: 'chaos-team'\n});\n\n// Run experiment\nconst result = await chaosEng.runExperiment(experiment.id);\n\nconsole.log('Experiment results:', result.analysis);"
    },
    {
      "id": 99,
      "question": "Design a data governance platform for regulatory compliance (GDPR, CCPA, HIPAA)",
      "answer": "A data governance platform manages data lifecycle, ensures compliance with regulations, tracks data lineage, and provides audit trails for sensitive information.\n\nCore Features:\n• Data catalog and discovery\n• Data classification (PII, PHI, sensitive)\n• Access control and authorization\n• Data lineage tracking\n• Consent management\n• Data retention policies\n• Right to deletion/portability\n• Encryption and anonymization\n• Audit logging and reporting\n\nCompliance Requirements:\n• GDPR: Consent, right to erasure, data portability\n• CCPA: Opt-out, data disclosure, deletion requests\n• HIPAA: PHI protection, access logs, breach notification\n• SOC 2: Security controls, audit trails\n\nData Classification:\n• Automated PII/PHI detection\n• Tagging and labeling\n• Sensitivity levels\n• Data ownership\n• Purpose limitation\n\nPrivacy Features:\n• Consent tracking per purpose\n• Cookie management\n• Privacy notices\n• Data subject requests (DSR)\n• Anonymization and pseudonymization\n• Data minimization\n\nAudit and Reports:\n• Access logs with retention\n• Data processing records\n• Breach detection and notification\n• Compliance dashboards\n• Regulatory reporting",
      "explanation": "Data governance platforms automatically classify sensitive data (PII/PHI), enforce access policies, track data lineage, manage consent, handle data subject requests (deletion, portability), maintain audit logs, and generate compliance reports to meet regulatory requirements like GDPR and HIPAA.",
      "difficulty": "Hard",
      "code": "// Data Governance Platform\nclass DataGovernancePlatform {\n  constructor(config) {\n    this.db = config.database;\n    this.catalog = new DataCatalog();\n    this.classificationEngine = new DataClassifier();\n    this.consentManager = new ConsentManager();\n    this.auditLog = new AuditLogger();\n  }\n  \n  // Data discovery and cataloging\n  async discoverData(dataSource) {\n    console.log(`Discovering data in: ${dataSource.name}`);\n    \n    const tables = await this.scanDataSource(dataSource);\n    \n    for (const table of tables) {\n      // Classify data\n      const classification = await this.classificationEngine.classifyTable(\n        table\n      );\n      \n      // Register in catalog\n      await this.catalog.registerDataset({\n        source: dataSource.name,\n        table: table.name,\n        schema: table.schema,\n        classification: classification,\n        owner: table.owner,\n        discoveredAt: Date.now()\n      });\n      \n      console.log(`Cataloged table: ${table.name}`);\n      console.log(`  Classification: ${JSON.stringify(classification)}`);\n    }\n  }\n  \n  // Handle data subject request (GDPR Article 15-22)\n  async handleDataSubjectRequest(request) {\n    const { type, subjectId, email } = request;\n    \n    // Log request\n    await this.auditLog.log('dsr_received', {\n      type,\n      subjectId,\n      email,\n      timestamp: Date.now()\n    });\n    \n    switch (type) {\n      case 'access': // Right to access (Article 15)\n        return this.handleAccessRequest(subjectId);\n      case 'rectification': // Right to rectification (Article 16)\n        return this.handleRectificationRequest(request);\n      case 'erasure': // Right to erasure (Article 17)\n        return this.handleErasureRequest(subjectId);\n      case 'portability': // Right to data portability (Article 20)\n        return this.handlePortabilityRequest(subjectId);\n      case 'restriction': // Right to restriction (Article 18)\n        return this.handleRestrictionRequest(subjectId);\n      default:\n        throw new Error(`Unknown request type: ${type}`);\n    }\n  }\n  \n  async handleAccessRequest(subjectId) {\n    // Collect all data for the subject\n    const datasets = await this.catalog.findDatasetsBySubject(subjectId);\n    const data = {};\n    \n    for (const dataset of datasets) {\n      // Check consent\n      const hasConsent = await this.consentManager.hasConsent(\n        subjectId,\n        dataset.purpose\n      );\n      \n      if (hasConsent) {\n        data[dataset.name] = await this.retrieveData(\n          dataset.source,\n          dataset.table,\n          subjectId\n        );\n      }\n    }\n    \n    // Generate report\n    const report = {\n      subjectId,\n      requestDate: Date.now(),\n      data,\n      processingActivities: await this.getProcessingActivities(subjectId),\n      dataRecipients: await this.getDataRecipients(subjectId),\n      retentionPeriods: await this.getRetentionPeriods(subjectId)\n    };\n    \n    await this.auditLog.log('dsr_access_completed', {\n      subjectId,\n      datasetsAccessed: datasets.length\n    });\n    \n    return report;\n  }\n  \n  async handleErasureRequest(subjectId) {\n    console.log(`Processing erasure request for: ${subjectId}`);\n    \n    // Find all data for subject\n    const datasets = await this.catalog.findDatasetsBySubject(subjectId);\n    const deletionResults = [];\n    \n    for (const dataset of datasets) {\n      // Check if erasure is allowed\n      if (!this.canErase(dataset)) {\n        deletionResults.push({\n          dataset: dataset.name,\n          status: 'retained',\n          reason: 'Legal obligation or legitimate interest'\n        });\n        continue;\n      }\n      \n      try {\n        // Delete or anonymize data\n        if (dataset.retentionPolicy === 'anonymize') {\n          await this.anonymizeData(dataset, subjectId);\n          deletionResults.push({\n            dataset: dataset.name,\n            status: 'anonymized'\n          });\n        } else {\n          await this.deleteData(dataset, subjectId);\n          deletionResults.push({\n            dataset: dataset.name,\n            status: 'deleted'\n          });\n        }\n      } catch (error) {\n        deletionResults.push({\n          dataset: dataset.name,\n          status: 'failed',\n          error: error.message\n        });\n      }\n    }\n    \n    // Update consent records\n    await this.consentManager.withdrawAllConsent(subjectId);\n    \n    await this.auditLog.log('dsr_erasure_completed', {\n      subjectId,\n      results: deletionResults\n    });\n    \n    return { subjectId, deletionResults, completedAt: Date.now() };\n  }\n  \n  async handlePortabilityRequest(subjectId) {\n    // Export data in machine-readable format\n    const datasets = await this.catalog.findDatasetsBySubject(subjectId);\n    const exportData = {};\n    \n    for (const dataset of datasets) {\n      // Only include data provided by the subject\n      if (dataset.providedBySubject) {\n        exportData[dataset.name] = await this.retrieveData(\n          dataset.source,\n          dataset.table,\n          subjectId\n        );\n      }\n    }\n    \n    // Generate portable format (JSON, CSV, XML)\n    const portableData = {\n      format: 'JSON',\n      version: '1.0',\n      exportedAt: new Date().toISOString(),\n      subjectId,\n      data: exportData\n    };\n    \n    await this.auditLog.log('dsr_portability_completed', { subjectId });\n    \n    return portableData;\n  }\n}\n\n// Data Classification Engine\nclass DataClassifier {\n  async classifyTable(table) {\n    const classification = {\n      hasPII: false,\n      hasPHI: false,\n      sensitivityLevel: 'public',\n      classifications: []\n    };\n    \n    for (const column of table.schema) {\n      const columnClass = await this.classifyColumn(column);\n      \n      if (columnClass.type !== 'none') {\n        classification.classifications.push({\n          column: column.name,\n          ...columnClass\n        });\n        \n        if (columnClass.type === 'PII') {\n          classification.hasPII = true;\n          classification.sensitivityLevel = 'confidential';\n        }\n        \n        if (columnClass.type === 'PHI') {\n          classification.hasPHI = true;\n          classification.sensitivityLevel = 'highly_confidential';\n        }\n      }\n    }\n    \n    return classification;\n  }\n  \n  async classifyColumn(column) {\n    // Pattern matching for common PII/PHI\n    const patterns = {\n      email: /email|e-mail/i,\n      phone: /phone|telephone|mobile/i,\n      ssn: /ssn|social_security/i,\n      creditCard: /credit_card|card_number/i,\n      address: /address|street|city|zip/i,\n      dob: /birth|dob|date_of_birth/i,\n      name: /first_name|last_name|full_name/i,\n      medicalRecord: /diagnosis|prescription|medical_record/i,\n      ipAddress: /ip_address|ip_addr/i\n    };\n    \n    for (const [category, pattern] of Object.entries(patterns)) {\n      if (pattern.test(column.name)) {\n        return {\n          type: this.isPHI(category) ? 'PHI' : 'PII',\n          category,\n          confidence: 0.9\n        };\n      }\n    }\n    \n    // Sample data analysis\n    if (column.sampleData) {\n      const detectedType = this.analyzeDataSamples(column.sampleData);\n      if (detectedType) {\n        return detectedType;\n      }\n    }\n    \n    return { type: 'none' };\n  }\n  \n  isPHI(category) {\n    const phiCategories = ['medicalRecord', 'diagnosis', 'prescription'];\n    return phiCategories.includes(category);\n  }\n  \n  analyzeDataSamples(samples) {\n    // Regex patterns for data analysis\n    const emailPattern = /^[^@]+@[^@]+\\.[^@]+$/;\n    const phonePattern = /^\\+?[0-9]{10,15}$/;\n    const ssnPattern = /^\\d{3}-\\d{2}-\\d{4}$/;\n    \n    let emailCount = 0;\n    let phoneCount = 0;\n    let ssnCount = 0;\n    \n    for (const sample of samples) {\n      if (emailPattern.test(sample)) emailCount++;\n      if (phonePattern.test(sample)) phoneCount++;\n      if (ssnPattern.test(sample)) ssnCount++;\n    }\n    \n    const threshold = samples.length * 0.8;\n    \n    if (emailCount >= threshold) {\n      return { type: 'PII', category: 'email', confidence: 0.95 };\n    }\n    if (phoneCount >= threshold) {\n      return { type: 'PII', category: 'phone', confidence: 0.95 };\n    }\n    if (ssnCount >= threshold) {\n      return { type: 'PII', category: 'ssn', confidence: 0.95 };\n    }\n    \n    return null;\n  }\n}\n\n// Consent Management\nclass ConsentManager {\n  constructor(db) {\n    this.db = db;\n  }\n  \n  async recordConsent(consent) {\n    const record = {\n      subjectId: consent.subjectId,\n      purpose: consent.purpose,\n      granted: consent.granted,\n      timestamp: Date.now(),\n      expiresAt: consent.expiresAt,\n      method: consent.method, // 'explicit', 'implicit', 'opt-in'\n      ipAddress: consent.ipAddress,\n      userAgent: consent.userAgent\n    };\n    \n    await this.db.insertConsent(record);\n    \n    return record;\n  }\n  \n  async hasConsent(subjectId, purpose) {\n    const consent = await this.db.getConsent(subjectId, purpose);\n    \n    if (!consent || !consent.granted) {\n      return false;\n    }\n    \n    // Check expiration\n    if (consent.expiresAt && Date.now() > consent.expiresAt) {\n      return false;\n    }\n    \n    return true;\n  }\n  \n  async withdrawAllConsent(subjectId) {\n    await this.db.updateConsent(\n      { subjectId },\n      { granted: false, withdrawnAt: Date.now() }\n    );\n  }\n}\n\n// Data Anonymization\nclass DataAnonymizer {\n  async anonymize(data, technique = 'hash') {\n    switch (technique) {\n      case 'hash':\n        return this.hashAnonymization(data);\n      case 'generalization':\n        return this.generalization(data);\n      case 'suppression':\n        return this.suppression(data);\n      case 'pseudonymization':\n        return this.pseudonymization(data);\n      default:\n        throw new Error(`Unknown technique: ${technique}`);\n    }\n  }\n  \n  hashAnonymization(data) {\n    // One-way hash\n    return crypto.createHash('sha256').update(data).digest('hex');\n  }\n  \n  generalization(data) {\n    // Reduce precision (e.g., age 25 -> 20-30)\n    if (typeof data === 'number') {\n      const range = 10;\n      const lower = Math.floor(data / range) * range;\n      return `${lower}-${lower + range}`;\n    }\n    return data;\n  }\n  \n  pseudonymization(data) {\n    // Reversible with key\n    const key = process.env.ANONYMIZATION_KEY;\n    const cipher = crypto.createCipher('aes-256-cbc', key);\n    let encrypted = cipher.update(data, 'utf8', 'hex');\n    encrypted += cipher.final('hex');\n    return encrypted;\n  }\n}\n\n// Usage\nconst governance = new DataGovernancePlatform({ database: db });\n\n// Discover and classify data\nawait governance.discoverData({\n  name: 'user_database',\n  type: 'postgres',\n  connection: dbConfig\n});\n\n// Handle GDPR erasure request\nconst erasureResult = await governance.handleDataSubjectRequest({\n  type: 'erasure',\n  subjectId: 'user-12345',\n  email: 'user@example.com'\n});\n\nconsole.log('Erasure completed:', erasureResult);\n\n// Handle data portability request\nconst portableData = await governance.handleDataSubjectRequest({\n  type: 'portability',\n  subjectId: 'user-12345'\n});\n\n// Export as JSON file\nconst fs = require('fs');\nfs.writeFileSync(\n  'user-data-export.json',\n  JSON.stringify(portableData, null, 2)\n);"
    },
    {
      "id": 100,
      "question": "Design a unified observability platform combining logs, metrics, and traces",
      "answer": "A unified observability platform correlates logs, metrics, and distributed traces to provide comprehensive system visibility and faster issue diagnosis.\n\nThree Pillars:\n• Logs: Event records with context (application, error, audit logs)\n• Metrics: Time-series measurements (latency, throughput, error rates)\n• Traces: Request flows across services (distributed tracing)\n\nCore Components:\n• Data ingestion pipeline for all telemetry types\n• Unified storage (time-series DB, object storage)\n• Correlation engine (trace ID linking)\n• Query engine for cross-signal analysis\n• Visualization dashboards\n• Alerting and anomaly detection\n• Service topology mapping\n\nCorrelation:\n• Trace ID propagation in logs\n• Span IDs linking log events\n• Metrics tagged with service/endpoint\n• Exemplars linking metrics to traces\n• Log-to-trace navigation\n• Trace-to-metrics drill-down\n\nQuery Capabilities:\n• Join queries across signals\n• Find traces with specific log patterns\n• Correlate metric spikes with error logs\n• Service dependency analysis\n• Root cause analysis workflows",
      "explanation": "Unified observability platforms correlate logs, metrics, and traces using common identifiers (trace IDs, span IDs) to provide complete system visibility, enabling quick navigation from high-level metrics to specific traces to detailed logs for comprehensive troubleshooting and root cause analysis.",
      "difficulty": "Hard",
      "code": "// Unified Observability Platform\nconst express = require('express');\nconst { InfluxDB } = require('@influxdata/influxdb-client');\nconst Elasticsearch = require('@elastic/elasticsearch');\n\nclass ObservabilityPlatform {\n  constructor(config) {\n    // Storage backends\n    this.elasticsearch = new Elasticsearch.Client({\n      node: config.elasticsearchUrl\n    });\n    \n    this.influxdb = new InfluxDB({\n      url: config.influxdbUrl,\n      token: config.influxdbToken\n    });\n    \n    this.traceStore = config.traceStore; // Jaeger or Tempo\n    \n    this.correlationIndex = new Map();\n  }\n  \n  // Ingest log with correlation\n  async ingestLog(log) {\n    const enrichedLog = {\n      ...log,\n      timestamp: Date.now(),\n      // Extract trace context\n      traceId: log.traceId || this.extractTraceId(log.message),\n      spanId: log.spanId,\n      // Standardize fields\n      service: log.service || 'unknown',\n      level: log.level || 'info',\n      message: log.message,\n      // Add context\n      host: log.host,\n      environment: log.environment || 'production',\n      version: log.version\n    };\n    \n    // Store in Elasticsearch\n    await this.elasticsearch.index({\n      index: `logs-${this.getIndexDate()}`,\n      body: enrichedLog\n    });\n    \n    // Build correlation index\n    if (enrichedLog.traceId) {\n      await this.correlateLogWithTrace(enrichedLog);\n    }\n    \n    // Check for error patterns\n    if (enrichedLog.level === 'error') {\n      await this.analyzeErrorPattern(enrichedLog);\n    }\n  }\n  \n  // Ingest metric\n  async ingestMetric(metric) {\n    const point = {\n      measurement: metric.name,\n      tags: {\n        service: metric.service,\n        endpoint: metric.endpoint,\n        environment: metric.environment,\n        host: metric.host\n      },\n      fields: {\n        value: metric.value\n      },\n      timestamp: Date.now() * 1000000 // nanoseconds\n    };\n    \n    // Add exemplar (link to trace)\n    if (metric.traceId) {\n      point.tags.traceId = metric.traceId;\n      point.tags.spanId = metric.spanId;\n    }\n    \n    // Write to InfluxDB\n    const writeApi = this.influxdb.getWriteApi('observability', 'metrics');\n    writeApi.writePoint(point);\n    await writeApi.close();\n    \n    // Check for anomalies\n    await this.detectMetricAnomaly(metric);\n  }\n  \n  // Ingest distributed trace\n  async ingestTrace(trace) {\n    // Store trace spans\n    await this.traceStore.writeTrace(trace);\n    \n    // Extract metrics from trace\n    await this.extractMetricsFromTrace(trace);\n    \n    // Link logs to trace\n    await this.linkLogsToTrace(trace);\n    \n    // Update service map\n    await this.updateServiceTopology(trace);\n  }\n  \n  // Correlation engine\n  async correlateLogWithTrace(log) {\n    // Store log reference in trace\n    const key = `trace:${log.traceId}`;\n    \n    if (!this.correlationIndex.has(key)) {\n      this.correlationIndex.set(key, {\n        traceId: log.traceId,\n        logs: [],\n        metrics: [],\n        events: []\n      });\n    }\n    \n    const correlation = this.correlationIndex.get(key);\n    correlation.logs.push({\n      timestamp: log.timestamp,\n      service: log.service,\n      level: log.level,\n      message: log.message\n    });\n  }\n  \n  // Query interface\n  async query(request) {\n    const { type, filters, timeRange, correlate } = request;\n    \n    let results;\n    \n    switch (type) {\n      case 'logs':\n        results = await this.queryLogs(filters, timeRange);\n        break;\n      case 'metrics':\n        results = await this.queryMetrics(filters, timeRange);\n        break;\n      case 'traces':\n        results = await this.queryTraces(filters, timeRange);\n        break;\n      case 'correlated':\n        results = await this.queryCorrelated(filters, timeRange);\n        break;\n      default:\n        throw new Error(`Unknown query type: ${type}`);\n    }\n    \n    // Enrich with correlations if requested\n    if (correlate) {\n      results = await this.enrichWithCorrelations(results);\n    }\n    \n    return results;\n  }\n  \n  async queryLogs(filters, timeRange) {\n    const query = {\n      bool: {\n        must: [\n          {\n            range: {\n              timestamp: {\n                gte: timeRange.start,\n                lte: timeRange.end\n              }\n            }\n          }\n        ]\n      }\n    };\n    \n    // Add filters\n    if (filters.service) {\n      query.bool.must.push({ term: { service: filters.service } });\n    }\n    if (filters.level) {\n      query.bool.must.push({ term: { level: filters.level } });\n    }\n    if (filters.traceId) {\n      query.bool.must.push({ term: { traceId: filters.traceId } });\n    }\n    if (filters.message) {\n      query.bool.must.push({ match: { message: filters.message } });\n    }\n    \n    const result = await this.elasticsearch.search({\n      index: 'logs-*',\n      body: { query, size: 1000, sort: [{ timestamp: 'desc' }] }\n    });\n    \n    return result.body.hits.hits.map(hit => hit._source);\n  }\n  \n  async queryMetrics(filters, timeRange) {\n    const queryApi = this.influxdb.getQueryApi('observability');\n    \n    let fluxQuery = `\n      from(bucket: \"metrics\")\n        |> range(start: ${timeRange.start}, stop: ${timeRange.end})\n    `;\n    \n    if (filters.measurement) {\n      fluxQuery += `  |> filter(fn: (r) => r._measurement == \"${filters.measurement}\")`;\n    }\n    if (filters.service) {\n      fluxQuery += `  |> filter(fn: (r) => r.service == \"${filters.service}\")`;\n    }\n    \n    fluxQuery += `  |> aggregateWindow(every: 1m, fn: mean)`;\n    \n    const results = [];\n    \n    await new Promise((resolve, reject) => {\n      queryApi.queryRows(fluxQuery, {\n        next: (row, tableMeta) => {\n          const record = tableMeta.toObject(row);\n          results.push(record);\n        },\n        error: reject,\n        complete: resolve\n      });\n    });\n    \n    return results;\n  }\n  \n  async queryCorrelated(filters, timeRange) {\n    // Start with trace query\n    const traces = await this.queryTraces(filters, timeRange);\n    \n    // For each trace, get correlated logs and metrics\n    const correlated = [];\n    \n    for (const trace of traces) {\n      const traceId = trace.traceID;\n      \n      // Get logs for trace\n      const logs = await this.queryLogs(\n        { traceId },\n        timeRange\n      );\n      \n      // Get metrics with exemplars\n      const metrics = await this.queryMetrics(\n        { traceId },\n        timeRange\n      );\n      \n      correlated.push({\n        trace,\n        logs,\n        metrics,\n        summary: {\n          duration: trace.duration,\n          errorCount: logs.filter(l => l.level === 'error').length,\n          services: [...new Set(trace.spans.map(s => s.process.serviceName))]\n        }\n      });\n    }\n    \n    return correlated;\n  }\n  \n  // Anomaly detection\n  async detectMetricAnomaly(metric) {\n    // Get historical data\n    const historical = await this.queryMetrics(\n      { measurement: metric.name, service: metric.service },\n      { start: Date.now() - 7 * 24 * 60 * 60 * 1000, end: Date.now() }\n    );\n    \n    if (historical.length < 100) {\n      return; // Not enough data\n    }\n    \n    // Calculate statistics\n    const values = historical.map(h => h._value);\n    const mean = values.reduce((a, b) => a + b) / values.length;\n    const variance = values.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / values.length;\n    const stdDev = Math.sqrt(variance);\n    \n    // Check if current value is anomalous (> 3 standard deviations)\n    const zScore = Math.abs(metric.value - mean) / stdDev;\n    \n    if (zScore > 3) {\n      await this.createAlert({\n        type: 'anomaly',\n        metric: metric.name,\n        service: metric.service,\n        currentValue: metric.value,\n        expectedRange: [mean - 2 * stdDev, mean + 2 * stdDev],\n        zScore,\n        traceId: metric.traceId\n      });\n    }\n  }\n  \n  // Service topology\n  async updateServiceTopology(trace) {\n    const edges = [];\n    \n    for (const span of trace.spans) {\n      if (span.references && span.references.length > 0) {\n        const parentSpan = trace.spans.find(\n          s => s.spanID === span.references[0].spanID\n        );\n        \n        if (parentSpan) {\n          edges.push({\n            from: parentSpan.process.serviceName,\n            to: span.process.serviceName,\n            operation: span.operationName,\n            latency: span.duration\n          });\n        }\n      }\n    }\n    \n    // Update service graph\n    await this.elasticsearch.index({\n      index: 'service-topology',\n      body: {\n        timestamp: Date.now(),\n        traceId: trace.traceID,\n        edges\n      }\n    });\n  }\n  \n  // Dashboard API\n  async getDashboard(dashboardId) {\n    const dashboard = await this.loadDashboard(dashboardId);\n    \n    // Execute all queries\n    const panels = await Promise.all(\n      dashboard.panels.map(async panel => {\n        const data = await this.query(panel.query);\n        return { ...panel, data };\n      })\n    );\n    \n    return { ...dashboard, panels };\n  }\n}\n\n// Instrumentation SDK\nclass ObservabilitySDK {\n  constructor(config) {\n    this.serviceName = config.serviceName;\n    this.platform = config.platform;\n    this.traceId = null;\n    this.spanId = null;\n  }\n  \n  // Start trace\n  startTrace(operation) {\n    this.traceId = this.generateId();\n    this.spanId = this.generateId();\n    \n    return {\n      traceId: this.traceId,\n      spanId: this.spanId,\n      operation,\n      startTime: Date.now()\n    };\n  }\n  \n  // Log with trace context\n  log(level, message, metadata = {}) {\n    this.platform.ingestLog({\n      service: this.serviceName,\n      level,\n      message,\n      traceId: this.traceId,\n      spanId: this.spanId,\n      ...metadata\n    });\n  }\n  \n  // Record metric\n  metric(name, value, tags = {}) {\n    this.platform.ingestMetric({\n      name,\n      value,\n      service: this.serviceName,\n      traceId: this.traceId,\n      spanId: this.spanId,\n      ...tags\n    });\n  }\n  \n  // Middleware for Express\n  middleware() {\n    return (req, res, next) => {\n      const trace = this.startTrace(`${req.method} ${req.path}`);\n      \n      req.traceId = trace.traceId;\n      req.spanId = trace.spanId;\n      \n      // Log request\n      this.log('info', 'Request started', {\n        method: req.method,\n        path: req.path\n      });\n      \n      // Measure duration\n      const startTime = Date.now();\n      \n      res.on('finish', () => {\n        const duration = Date.now() - startTime;\n        \n        // Record metrics\n        this.metric('http_request_duration', duration, {\n          method: req.method,\n          path: req.path,\n          status: res.statusCode\n        });\n        \n        // Log response\n        this.log('info', 'Request completed', {\n          duration,\n          status: res.statusCode\n        });\n      });\n      \n      next();\n    };\n  }\n}\n\n// Usage\nconst platform = new ObservabilityPlatform({\n  elasticsearchUrl: 'http://localhost:9200',\n  influxdbUrl: 'http://localhost:8086',\n  influxdbToken: 'my-token',\n  traceStore: jaeger\n});\n\nconst sdk = new ObservabilitySDK({\n  serviceName: 'api-server',\n  platform\n});\n\nconst app = express();\napp.use(sdk.middleware());\n\napp.get('/api/users/:id', async (req, res) => {\n  sdk.log('info', 'Fetching user', { userId: req.params.id });\n  \n  const user = await db.getUser(req.params.id);\n  \n  sdk.metric('user_fetch', 1, { cached: user.cached });\n  \n  res.json(user);\n});\n\n// Query correlated data\nconst results = await platform.query({\n  type: 'correlated',\n  filters: {\n    service: 'api-server',\n    error: true\n  },\n  timeRange: {\n    start: Date.now() - 3600000,\n    end: Date.now()\n  }\n});\n\nconsole.log('Correlated errors:', results);"
    }
  ]
}
